I0516 14:11:20.721963      22 e2e.go:126] Starting e2e run "38fb2494-031f-4052-9538-25d04cdaece2" on Ginkgo node 1
May 16 14:11:20.739: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1684246280 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
May 16 14:11:20.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 14:11:20.850: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 16 14:11:20.869: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 16 14:11:20.881: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 16 14:11:20.881: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
May 16 14:11:20.881: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 16 14:11:20.884: INFO: e2e test version: v1.26.3
May 16 14:11:20.885: INFO: kube-apiserver version: v1.26.3+b404935
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
May 16 14:11:20.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 14:11:20.889: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.041 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    May 16 14:11:20.848: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 14:11:20.850: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    May 16 14:11:20.869: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    May 16 14:11:20.881: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    May 16 14:11:20.881: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
    May 16 14:11:20.881: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    May 16 14:11:20.884: INFO: e2e test version: v1.26.3
    May 16 14:11:20.885: INFO: kube-apiserver version: v1.26.3+b404935
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    May 16 14:11:20.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 14:11:20.889: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:11:20.909
May 16 14:11:20.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 14:11:20.909
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:11:20.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:11:20.932
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-9b5ded16-c994-41d6-ad73-0feb2afe3f9e 05/16/23 14:11:20.935
STEP: Creating a pod to test consume configMaps 05/16/23 14:11:20.941
May 16 14:11:20.969: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cfd3c6c3-a267-43ad-b496-c1133b609c87" in namespace "projected-9570" to be "Succeeded or Failed"
May 16 14:11:20.981: INFO: Pod "pod-projected-configmaps-cfd3c6c3-a267-43ad-b496-c1133b609c87": Phase="Pending", Reason="", readiness=false. Elapsed: 11.865868ms
May 16 14:11:22.986: INFO: Pod "pod-projected-configmaps-cfd3c6c3-a267-43ad-b496-c1133b609c87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017062131s
May 16 14:11:24.985: INFO: Pod "pod-projected-configmaps-cfd3c6c3-a267-43ad-b496-c1133b609c87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015940363s
May 16 14:11:26.984: INFO: Pod "pod-projected-configmaps-cfd3c6c3-a267-43ad-b496-c1133b609c87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015691717s
STEP: Saw pod success 05/16/23 14:11:26.984
May 16 14:11:26.985: INFO: Pod "pod-projected-configmaps-cfd3c6c3-a267-43ad-b496-c1133b609c87" satisfied condition "Succeeded or Failed"
May 16 14:11:26.988: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-configmaps-cfd3c6c3-a267-43ad-b496-c1133b609c87 container agnhost-container: <nil>
STEP: delete the pod 05/16/23 14:11:27
May 16 14:11:27.012: INFO: Waiting for pod pod-projected-configmaps-cfd3c6c3-a267-43ad-b496-c1133b609c87 to disappear
May 16 14:11:27.014: INFO: Pod pod-projected-configmaps-cfd3c6c3-a267-43ad-b496-c1133b609c87 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May 16 14:11:27.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9570" for this suite. 05/16/23 14:11:27.019
------------------------------
â€¢ [SLOW TEST] [6.117 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:11:20.909
    May 16 14:11:20.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 14:11:20.909
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:11:20.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:11:20.932
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-9b5ded16-c994-41d6-ad73-0feb2afe3f9e 05/16/23 14:11:20.935
    STEP: Creating a pod to test consume configMaps 05/16/23 14:11:20.941
    May 16 14:11:20.969: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cfd3c6c3-a267-43ad-b496-c1133b609c87" in namespace "projected-9570" to be "Succeeded or Failed"
    May 16 14:11:20.981: INFO: Pod "pod-projected-configmaps-cfd3c6c3-a267-43ad-b496-c1133b609c87": Phase="Pending", Reason="", readiness=false. Elapsed: 11.865868ms
    May 16 14:11:22.986: INFO: Pod "pod-projected-configmaps-cfd3c6c3-a267-43ad-b496-c1133b609c87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017062131s
    May 16 14:11:24.985: INFO: Pod "pod-projected-configmaps-cfd3c6c3-a267-43ad-b496-c1133b609c87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015940363s
    May 16 14:11:26.984: INFO: Pod "pod-projected-configmaps-cfd3c6c3-a267-43ad-b496-c1133b609c87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015691717s
    STEP: Saw pod success 05/16/23 14:11:26.984
    May 16 14:11:26.985: INFO: Pod "pod-projected-configmaps-cfd3c6c3-a267-43ad-b496-c1133b609c87" satisfied condition "Succeeded or Failed"
    May 16 14:11:26.988: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-configmaps-cfd3c6c3-a267-43ad-b496-c1133b609c87 container agnhost-container: <nil>
    STEP: delete the pod 05/16/23 14:11:27
    May 16 14:11:27.012: INFO: Waiting for pod pod-projected-configmaps-cfd3c6c3-a267-43ad-b496-c1133b609c87 to disappear
    May 16 14:11:27.014: INFO: Pod pod-projected-configmaps-cfd3c6c3-a267-43ad-b496-c1133b609c87 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May 16 14:11:27.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9570" for this suite. 05/16/23 14:11:27.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:11:27.026
May 16 14:11:27.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename container-lifecycle-hook 05/16/23 14:11:27.026
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:11:27.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:11:27.045
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 05/16/23 14:11:27.052
May 16 14:11:27.063: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6494" to be "running and ready"
May 16 14:11:27.067: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.598951ms
May 16 14:11:27.067: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 16 14:11:29.074: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010315197s
May 16 14:11:29.074: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May 16 14:11:29.074: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 05/16/23 14:11:29.076
May 16 14:11:29.083: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6494" to be "running and ready"
May 16 14:11:29.087: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.765879ms
May 16 14:11:29.087: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
May 16 14:11:31.149: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.065516344s
May 16 14:11:31.149: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
May 16 14:11:31.149: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 05/16/23 14:11:31.151
May 16 14:11:31.199: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 16 14:11:31.202: INFO: Pod pod-with-prestop-http-hook still exists
May 16 14:11:33.203: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 16 14:11:33.208: INFO: Pod pod-with-prestop-http-hook still exists
May 16 14:11:35.203: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 16 14:11:35.207: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 05/16/23 14:11:35.207
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
May 16 14:11:35.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6494" for this suite. 05/16/23 14:11:35.218
------------------------------
â€¢ [SLOW TEST] [8.199 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:11:27.026
    May 16 14:11:27.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/16/23 14:11:27.026
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:11:27.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:11:27.045
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 05/16/23 14:11:27.052
    May 16 14:11:27.063: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6494" to be "running and ready"
    May 16 14:11:27.067: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.598951ms
    May 16 14:11:27.067: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:11:29.074: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010315197s
    May 16 14:11:29.074: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May 16 14:11:29.074: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 05/16/23 14:11:29.076
    May 16 14:11:29.083: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6494" to be "running and ready"
    May 16 14:11:29.087: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.765879ms
    May 16 14:11:29.087: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:11:31.149: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.065516344s
    May 16 14:11:31.149: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    May 16 14:11:31.149: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 05/16/23 14:11:31.151
    May 16 14:11:31.199: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May 16 14:11:31.202: INFO: Pod pod-with-prestop-http-hook still exists
    May 16 14:11:33.203: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May 16 14:11:33.208: INFO: Pod pod-with-prestop-http-hook still exists
    May 16 14:11:35.203: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    May 16 14:11:35.207: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 05/16/23 14:11:35.207
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    May 16 14:11:35.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6494" for this suite. 05/16/23 14:11:35.218
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:11:35.225
May 16 14:11:35.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename webhook 05/16/23 14:11:35.225
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:11:35.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:11:35.245
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/16/23 14:11:35.262
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 14:11:35.361
STEP: Deploying the webhook pod 05/16/23 14:11:35.392
STEP: Wait for the deployment to be ready 05/16/23 14:11:35.406
May 16 14:11:35.411: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 16 14:11:37.421: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 11, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 11, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 11, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 11, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/16/23 14:11:39.426
STEP: Verifying the service has paired with the endpoint 05/16/23 14:11:39.438
May 16 14:11:40.438: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
May 16 14:11:40.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8966-crds.webhook.example.com via the AdmissionRegistration API 05/16/23 14:11:40.955
STEP: Creating a custom resource that should be mutated by the webhook 05/16/23 14:11:40.971
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 14:11:43.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7299" for this suite. 05/16/23 14:11:43.608
STEP: Destroying namespace "webhook-7299-markers" for this suite. 05/16/23 14:11:43.619
------------------------------
â€¢ [SLOW TEST] [8.403 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:11:35.225
    May 16 14:11:35.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename webhook 05/16/23 14:11:35.225
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:11:35.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:11:35.245
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/16/23 14:11:35.262
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 14:11:35.361
    STEP: Deploying the webhook pod 05/16/23 14:11:35.392
    STEP: Wait for the deployment to be ready 05/16/23 14:11:35.406
    May 16 14:11:35.411: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    May 16 14:11:37.421: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 11, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 11, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 11, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 11, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/16/23 14:11:39.426
    STEP: Verifying the service has paired with the endpoint 05/16/23 14:11:39.438
    May 16 14:11:40.438: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    May 16 14:11:40.444: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8966-crds.webhook.example.com via the AdmissionRegistration API 05/16/23 14:11:40.955
    STEP: Creating a custom resource that should be mutated by the webhook 05/16/23 14:11:40.971
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 14:11:43.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7299" for this suite. 05/16/23 14:11:43.608
    STEP: Destroying namespace "webhook-7299-markers" for this suite. 05/16/23 14:11:43.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:11:43.629
May 16 14:11:43.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename csistoragecapacity 05/16/23 14:11:43.629
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:11:43.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:11:43.66
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 05/16/23 14:11:43.664
STEP: getting /apis/storage.k8s.io 05/16/23 14:11:43.668
STEP: getting /apis/storage.k8s.io/v1 05/16/23 14:11:43.67
STEP: creating 05/16/23 14:11:43.672
STEP: watching 05/16/23 14:11:43.708
May 16 14:11:43.708: INFO: starting watch
STEP: getting 05/16/23 14:11:43.718
STEP: listing in namespace 05/16/23 14:11:43.742
STEP: listing across namespaces 05/16/23 14:11:43.747
STEP: patching 05/16/23 14:11:43.751
STEP: updating 05/16/23 14:11:43.756
May 16 14:11:43.762: INFO: waiting for watch events with expected annotations in namespace
May 16 14:11:43.762: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 05/16/23 14:11:43.762
STEP: deleting a collection 05/16/23 14:11:43.775
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
May 16 14:11:43.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-5098" for this suite. 05/16/23 14:11:43.794
------------------------------
â€¢ [0.173 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:11:43.629
    May 16 14:11:43.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename csistoragecapacity 05/16/23 14:11:43.629
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:11:43.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:11:43.66
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 05/16/23 14:11:43.664
    STEP: getting /apis/storage.k8s.io 05/16/23 14:11:43.668
    STEP: getting /apis/storage.k8s.io/v1 05/16/23 14:11:43.67
    STEP: creating 05/16/23 14:11:43.672
    STEP: watching 05/16/23 14:11:43.708
    May 16 14:11:43.708: INFO: starting watch
    STEP: getting 05/16/23 14:11:43.718
    STEP: listing in namespace 05/16/23 14:11:43.742
    STEP: listing across namespaces 05/16/23 14:11:43.747
    STEP: patching 05/16/23 14:11:43.751
    STEP: updating 05/16/23 14:11:43.756
    May 16 14:11:43.762: INFO: waiting for watch events with expected annotations in namespace
    May 16 14:11:43.762: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 05/16/23 14:11:43.762
    STEP: deleting a collection 05/16/23 14:11:43.775
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    May 16 14:11:43.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-5098" for this suite. 05/16/23 14:11:43.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:11:43.803
May 16 14:11:43.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename runtimeclass 05/16/23 14:11:43.803
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:11:43.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:11:43.822
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May 16 14:11:43.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-6387" for this suite. 05/16/23 14:11:43.842
------------------------------
â€¢ [0.053 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:11:43.803
    May 16 14:11:43.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename runtimeclass 05/16/23 14:11:43.803
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:11:43.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:11:43.822
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May 16 14:11:43.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-6387" for this suite. 05/16/23 14:11:43.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:11:43.856
May 16 14:11:43.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename secrets 05/16/23 14:11:43.857
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:11:43.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:11:43.884
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-b4eafbe6-24a0-497c-a73d-44aa77dae256 05/16/23 14:11:43.887
STEP: Creating a pod to test consume secrets 05/16/23 14:11:43.892
May 16 14:11:43.922: INFO: Waiting up to 5m0s for pod "pod-secrets-c85441cd-7982-4d69-91ef-51e52cc5caa3" in namespace "secrets-5680" to be "Succeeded or Failed"
May 16 14:11:43.959: INFO: Pod "pod-secrets-c85441cd-7982-4d69-91ef-51e52cc5caa3": Phase="Pending", Reason="", readiness=false. Elapsed: 36.40482ms
May 16 14:11:45.962: INFO: Pod "pod-secrets-c85441cd-7982-4d69-91ef-51e52cc5caa3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039646281s
May 16 14:11:47.964: INFO: Pod "pod-secrets-c85441cd-7982-4d69-91ef-51e52cc5caa3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041476445s
STEP: Saw pod success 05/16/23 14:11:47.964
May 16 14:11:47.964: INFO: Pod "pod-secrets-c85441cd-7982-4d69-91ef-51e52cc5caa3" satisfied condition "Succeeded or Failed"
May 16 14:11:47.967: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-secrets-c85441cd-7982-4d69-91ef-51e52cc5caa3 container secret-volume-test: <nil>
STEP: delete the pod 05/16/23 14:11:47.973
May 16 14:11:47.983: INFO: Waiting for pod pod-secrets-c85441cd-7982-4d69-91ef-51e52cc5caa3 to disappear
May 16 14:11:47.985: INFO: Pod pod-secrets-c85441cd-7982-4d69-91ef-51e52cc5caa3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May 16 14:11:47.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5680" for this suite. 05/16/23 14:11:47.99
------------------------------
â€¢ [4.140 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:11:43.856
    May 16 14:11:43.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename secrets 05/16/23 14:11:43.857
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:11:43.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:11:43.884
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-b4eafbe6-24a0-497c-a73d-44aa77dae256 05/16/23 14:11:43.887
    STEP: Creating a pod to test consume secrets 05/16/23 14:11:43.892
    May 16 14:11:43.922: INFO: Waiting up to 5m0s for pod "pod-secrets-c85441cd-7982-4d69-91ef-51e52cc5caa3" in namespace "secrets-5680" to be "Succeeded or Failed"
    May 16 14:11:43.959: INFO: Pod "pod-secrets-c85441cd-7982-4d69-91ef-51e52cc5caa3": Phase="Pending", Reason="", readiness=false. Elapsed: 36.40482ms
    May 16 14:11:45.962: INFO: Pod "pod-secrets-c85441cd-7982-4d69-91ef-51e52cc5caa3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039646281s
    May 16 14:11:47.964: INFO: Pod "pod-secrets-c85441cd-7982-4d69-91ef-51e52cc5caa3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041476445s
    STEP: Saw pod success 05/16/23 14:11:47.964
    May 16 14:11:47.964: INFO: Pod "pod-secrets-c85441cd-7982-4d69-91ef-51e52cc5caa3" satisfied condition "Succeeded or Failed"
    May 16 14:11:47.967: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-secrets-c85441cd-7982-4d69-91ef-51e52cc5caa3 container secret-volume-test: <nil>
    STEP: delete the pod 05/16/23 14:11:47.973
    May 16 14:11:47.983: INFO: Waiting for pod pod-secrets-c85441cd-7982-4d69-91ef-51e52cc5caa3 to disappear
    May 16 14:11:47.985: INFO: Pod pod-secrets-c85441cd-7982-4d69-91ef-51e52cc5caa3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May 16 14:11:47.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5680" for this suite. 05/16/23 14:11:47.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:11:47.997
May 16 14:11:47.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename emptydir 05/16/23 14:11:47.998
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:11:48.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:11:48.017
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 05/16/23 14:11:48.02
May 16 14:11:48.039: INFO: Waiting up to 5m0s for pod "pod-943e39c2-da76-44ac-854d-08f8f979da91" in namespace "emptydir-9307" to be "Succeeded or Failed"
May 16 14:11:48.044: INFO: Pod "pod-943e39c2-da76-44ac-854d-08f8f979da91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.90888ms
May 16 14:11:50.047: INFO: Pod "pod-943e39c2-da76-44ac-854d-08f8f979da91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008145954s
May 16 14:11:52.049: INFO: Pod "pod-943e39c2-da76-44ac-854d-08f8f979da91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009990376s
STEP: Saw pod success 05/16/23 14:11:52.049
May 16 14:11:52.049: INFO: Pod "pod-943e39c2-da76-44ac-854d-08f8f979da91" satisfied condition "Succeeded or Failed"
May 16 14:11:52.053: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-943e39c2-da76-44ac-854d-08f8f979da91 container test-container: <nil>
STEP: delete the pod 05/16/23 14:11:52.057
May 16 14:11:52.068: INFO: Waiting for pod pod-943e39c2-da76-44ac-854d-08f8f979da91 to disappear
May 16 14:11:52.071: INFO: Pod pod-943e39c2-da76-44ac-854d-08f8f979da91 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 16 14:11:52.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9307" for this suite. 05/16/23 14:11:52.075
------------------------------
â€¢ [4.085 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:11:47.997
    May 16 14:11:47.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename emptydir 05/16/23 14:11:47.998
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:11:48.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:11:48.017
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 05/16/23 14:11:48.02
    May 16 14:11:48.039: INFO: Waiting up to 5m0s for pod "pod-943e39c2-da76-44ac-854d-08f8f979da91" in namespace "emptydir-9307" to be "Succeeded or Failed"
    May 16 14:11:48.044: INFO: Pod "pod-943e39c2-da76-44ac-854d-08f8f979da91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.90888ms
    May 16 14:11:50.047: INFO: Pod "pod-943e39c2-da76-44ac-854d-08f8f979da91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008145954s
    May 16 14:11:52.049: INFO: Pod "pod-943e39c2-da76-44ac-854d-08f8f979da91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009990376s
    STEP: Saw pod success 05/16/23 14:11:52.049
    May 16 14:11:52.049: INFO: Pod "pod-943e39c2-da76-44ac-854d-08f8f979da91" satisfied condition "Succeeded or Failed"
    May 16 14:11:52.053: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-943e39c2-da76-44ac-854d-08f8f979da91 container test-container: <nil>
    STEP: delete the pod 05/16/23 14:11:52.057
    May 16 14:11:52.068: INFO: Waiting for pod pod-943e39c2-da76-44ac-854d-08f8f979da91 to disappear
    May 16 14:11:52.071: INFO: Pod pod-943e39c2-da76-44ac-854d-08f8f979da91 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 16 14:11:52.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9307" for this suite. 05/16/23 14:11:52.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:11:52.083
May 16 14:11:52.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename dns 05/16/23 14:11:52.083
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:11:52.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:11:52.105
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 05/16/23 14:11:52.108
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1298 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1298;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1298 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1298;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1298.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1298.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1298.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1298.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1298.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1298.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1298.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1298.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1298.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1298.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1298.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1298.svc;check="$$(dig +notcp +noall +answer +search 87.30.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.30.87_udp@PTR;check="$$(dig +tcp +noall +answer +search 87.30.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.30.87_tcp@PTR;sleep 1; done
 05/16/23 14:11:52.134
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1298 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1298;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1298 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1298;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1298.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1298.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1298.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1298.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1298.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1298.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1298.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1298.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1298.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1298.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1298.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1298.svc;check="$$(dig +notcp +noall +answer +search 87.30.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.30.87_udp@PTR;check="$$(dig +tcp +noall +answer +search 87.30.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.30.87_tcp@PTR;sleep 1; done
 05/16/23 14:11:52.134
STEP: creating a pod to probe DNS 05/16/23 14:11:52.134
STEP: submitting the pod to kubernetes 05/16/23 14:11:52.134
May 16 14:11:52.154: INFO: Waiting up to 15m0s for pod "dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335" in namespace "dns-1298" to be "running"
May 16 14:11:52.166: INFO: Pod "dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335": Phase="Pending", Reason="", readiness=false. Elapsed: 11.915274ms
May 16 14:11:54.179: INFO: Pod "dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025210404s
May 16 14:11:56.170: INFO: Pod "dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016035477s
May 16 14:11:58.169: INFO: Pod "dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015030862s
May 16 14:12:00.170: INFO: Pod "dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335": Phase="Running", Reason="", readiness=true. Elapsed: 8.015927092s
May 16 14:12:00.170: INFO: Pod "dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335" satisfied condition "running"
STEP: retrieving the pod 05/16/23 14:12:00.17
STEP: looking for the results for each expected name from probers 05/16/23 14:12:00.174
May 16 14:12:00.181: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
May 16 14:12:00.184: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
May 16 14:12:00.187: INFO: Unable to read wheezy_udp@dns-test-service.dns-1298 from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
May 16 14:12:00.191: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1298 from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
May 16 14:12:00.195: INFO: Unable to read wheezy_udp@dns-test-service.dns-1298.svc from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
May 16 14:12:00.199: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1298.svc from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
May 16 14:12:00.225: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
May 16 14:12:00.229: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
May 16 14:12:00.232: INFO: Unable to read jessie_udp@dns-test-service.dns-1298 from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
May 16 14:12:00.236: INFO: Unable to read jessie_tcp@dns-test-service.dns-1298 from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
May 16 14:12:00.240: INFO: Unable to read jessie_udp@dns-test-service.dns-1298.svc from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
May 16 14:12:00.243: INFO: Unable to read jessie_tcp@dns-test-service.dns-1298.svc from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
May 16 14:12:00.264: INFO: Lookups using dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1298 wheezy_tcp@dns-test-service.dns-1298 wheezy_udp@dns-test-service.dns-1298.svc wheezy_tcp@dns-test-service.dns-1298.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1298 jessie_tcp@dns-test-service.dns-1298 jessie_udp@dns-test-service.dns-1298.svc jessie_tcp@dns-test-service.dns-1298.svc]

May 16 14:12:05.369: INFO: DNS probes using dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335 succeeded

STEP: deleting the pod 05/16/23 14:12:05.369
STEP: deleting the test service 05/16/23 14:12:05.381
STEP: deleting the test headless service 05/16/23 14:12:05.408
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May 16 14:12:05.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1298" for this suite. 05/16/23 14:12:05.437
------------------------------
â€¢ [SLOW TEST] [13.365 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:11:52.083
    May 16 14:11:52.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename dns 05/16/23 14:11:52.083
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:11:52.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:11:52.105
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 05/16/23 14:11:52.108
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1298 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1298;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1298 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1298;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1298.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1298.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1298.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1298.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1298.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1298.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1298.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1298.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1298.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1298.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1298.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1298.svc;check="$$(dig +notcp +noall +answer +search 87.30.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.30.87_udp@PTR;check="$$(dig +tcp +noall +answer +search 87.30.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.30.87_tcp@PTR;sleep 1; done
     05/16/23 14:11:52.134
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1298 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1298;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1298 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1298;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1298.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1298.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1298.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1298.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1298.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1298.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1298.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1298.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1298.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1298.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1298.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1298.svc;check="$$(dig +notcp +noall +answer +search 87.30.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.30.87_udp@PTR;check="$$(dig +tcp +noall +answer +search 87.30.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.30.87_tcp@PTR;sleep 1; done
     05/16/23 14:11:52.134
    STEP: creating a pod to probe DNS 05/16/23 14:11:52.134
    STEP: submitting the pod to kubernetes 05/16/23 14:11:52.134
    May 16 14:11:52.154: INFO: Waiting up to 15m0s for pod "dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335" in namespace "dns-1298" to be "running"
    May 16 14:11:52.166: INFO: Pod "dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335": Phase="Pending", Reason="", readiness=false. Elapsed: 11.915274ms
    May 16 14:11:54.179: INFO: Pod "dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025210404s
    May 16 14:11:56.170: INFO: Pod "dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016035477s
    May 16 14:11:58.169: INFO: Pod "dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015030862s
    May 16 14:12:00.170: INFO: Pod "dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335": Phase="Running", Reason="", readiness=true. Elapsed: 8.015927092s
    May 16 14:12:00.170: INFO: Pod "dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335" satisfied condition "running"
    STEP: retrieving the pod 05/16/23 14:12:00.17
    STEP: looking for the results for each expected name from probers 05/16/23 14:12:00.174
    May 16 14:12:00.181: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
    May 16 14:12:00.184: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
    May 16 14:12:00.187: INFO: Unable to read wheezy_udp@dns-test-service.dns-1298 from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
    May 16 14:12:00.191: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1298 from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
    May 16 14:12:00.195: INFO: Unable to read wheezy_udp@dns-test-service.dns-1298.svc from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
    May 16 14:12:00.199: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1298.svc from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
    May 16 14:12:00.225: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
    May 16 14:12:00.229: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
    May 16 14:12:00.232: INFO: Unable to read jessie_udp@dns-test-service.dns-1298 from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
    May 16 14:12:00.236: INFO: Unable to read jessie_tcp@dns-test-service.dns-1298 from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
    May 16 14:12:00.240: INFO: Unable to read jessie_udp@dns-test-service.dns-1298.svc from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
    May 16 14:12:00.243: INFO: Unable to read jessie_tcp@dns-test-service.dns-1298.svc from pod dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335: the server could not find the requested resource (get pods dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335)
    May 16 14:12:00.264: INFO: Lookups using dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1298 wheezy_tcp@dns-test-service.dns-1298 wheezy_udp@dns-test-service.dns-1298.svc wheezy_tcp@dns-test-service.dns-1298.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1298 jessie_tcp@dns-test-service.dns-1298 jessie_udp@dns-test-service.dns-1298.svc jessie_tcp@dns-test-service.dns-1298.svc]

    May 16 14:12:05.369: INFO: DNS probes using dns-1298/dns-test-a1bed3af-f1da-4c8e-bd24-64cb18ea9335 succeeded

    STEP: deleting the pod 05/16/23 14:12:05.369
    STEP: deleting the test service 05/16/23 14:12:05.381
    STEP: deleting the test headless service 05/16/23 14:12:05.408
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May 16 14:12:05.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1298" for this suite. 05/16/23 14:12:05.437
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:12:05.448
May 16 14:12:05.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename custom-resource-definition 05/16/23 14:12:05.449
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:12:05.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:12:05.475
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
May 16 14:12:05.478: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 14:12:06.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2726" for this suite. 05/16/23 14:12:06.506
------------------------------
â€¢ [1.070 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:12:05.448
    May 16 14:12:05.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename custom-resource-definition 05/16/23 14:12:05.449
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:12:05.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:12:05.475
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    May 16 14:12:05.478: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 14:12:06.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2726" for this suite. 05/16/23 14:12:06.506
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:12:06.518
May 16 14:12:06.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 14:12:06.518
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:12:06.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:12:06.542
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 05/16/23 14:12:06.545
May 16 14:12:06.557: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b0f31a26-62a4-4570-92a1-fbd1a27fc035" in namespace "projected-7848" to be "Succeeded or Failed"
May 16 14:12:06.562: INFO: Pod "downwardapi-volume-b0f31a26-62a4-4570-92a1-fbd1a27fc035": Phase="Pending", Reason="", readiness=false. Elapsed: 5.626674ms
May 16 14:12:08.566: INFO: Pod "downwardapi-volume-b0f31a26-62a4-4570-92a1-fbd1a27fc035": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009786865s
May 16 14:12:10.567: INFO: Pod "downwardapi-volume-b0f31a26-62a4-4570-92a1-fbd1a27fc035": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00988838s
STEP: Saw pod success 05/16/23 14:12:10.567
May 16 14:12:10.567: INFO: Pod "downwardapi-volume-b0f31a26-62a4-4570-92a1-fbd1a27fc035" satisfied condition "Succeeded or Failed"
May 16 14:12:10.570: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-b0f31a26-62a4-4570-92a1-fbd1a27fc035 container client-container: <nil>
STEP: delete the pod 05/16/23 14:12:10.575
May 16 14:12:10.588: INFO: Waiting for pod downwardapi-volume-b0f31a26-62a4-4570-92a1-fbd1a27fc035 to disappear
May 16 14:12:10.591: INFO: Pod downwardapi-volume-b0f31a26-62a4-4570-92a1-fbd1a27fc035 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 16 14:12:10.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7848" for this suite. 05/16/23 14:12:10.596
------------------------------
â€¢ [4.084 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:12:06.518
    May 16 14:12:06.518: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 14:12:06.518
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:12:06.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:12:06.542
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 05/16/23 14:12:06.545
    May 16 14:12:06.557: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b0f31a26-62a4-4570-92a1-fbd1a27fc035" in namespace "projected-7848" to be "Succeeded or Failed"
    May 16 14:12:06.562: INFO: Pod "downwardapi-volume-b0f31a26-62a4-4570-92a1-fbd1a27fc035": Phase="Pending", Reason="", readiness=false. Elapsed: 5.626674ms
    May 16 14:12:08.566: INFO: Pod "downwardapi-volume-b0f31a26-62a4-4570-92a1-fbd1a27fc035": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009786865s
    May 16 14:12:10.567: INFO: Pod "downwardapi-volume-b0f31a26-62a4-4570-92a1-fbd1a27fc035": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00988838s
    STEP: Saw pod success 05/16/23 14:12:10.567
    May 16 14:12:10.567: INFO: Pod "downwardapi-volume-b0f31a26-62a4-4570-92a1-fbd1a27fc035" satisfied condition "Succeeded or Failed"
    May 16 14:12:10.570: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-b0f31a26-62a4-4570-92a1-fbd1a27fc035 container client-container: <nil>
    STEP: delete the pod 05/16/23 14:12:10.575
    May 16 14:12:10.588: INFO: Waiting for pod downwardapi-volume-b0f31a26-62a4-4570-92a1-fbd1a27fc035 to disappear
    May 16 14:12:10.591: INFO: Pod downwardapi-volume-b0f31a26-62a4-4570-92a1-fbd1a27fc035 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 16 14:12:10.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7848" for this suite. 05/16/23 14:12:10.596
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:12:10.602
May 16 14:12:10.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename webhook 05/16/23 14:12:10.603
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:12:10.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:12:10.626
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/16/23 14:12:10.658
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 14:12:11.006
STEP: Deploying the webhook pod 05/16/23 14:12:11.017
STEP: Wait for the deployment to be ready 05/16/23 14:12:11.031
May 16 14:12:11.038: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/16/23 14:12:13.048
STEP: Verifying the service has paired with the endpoint 05/16/23 14:12:13.061
May 16 14:12:14.062: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/16/23 14:12:14.065
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/16/23 14:12:14.079
STEP: Creating a dummy validating-webhook-configuration object 05/16/23 14:12:14.092
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 05/16/23 14:12:14.105
STEP: Creating a dummy mutating-webhook-configuration object 05/16/23 14:12:14.111
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 05/16/23 14:12:14.119
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 14:12:14.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4236" for this suite. 05/16/23 14:12:14.195
STEP: Destroying namespace "webhook-4236-markers" for this suite. 05/16/23 14:12:14.207
------------------------------
â€¢ [3.613 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:12:10.602
    May 16 14:12:10.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename webhook 05/16/23 14:12:10.603
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:12:10.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:12:10.626
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/16/23 14:12:10.658
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 14:12:11.006
    STEP: Deploying the webhook pod 05/16/23 14:12:11.017
    STEP: Wait for the deployment to be ready 05/16/23 14:12:11.031
    May 16 14:12:11.038: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/16/23 14:12:13.048
    STEP: Verifying the service has paired with the endpoint 05/16/23 14:12:13.061
    May 16 14:12:14.062: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/16/23 14:12:14.065
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 05/16/23 14:12:14.079
    STEP: Creating a dummy validating-webhook-configuration object 05/16/23 14:12:14.092
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 05/16/23 14:12:14.105
    STEP: Creating a dummy mutating-webhook-configuration object 05/16/23 14:12:14.111
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 05/16/23 14:12:14.119
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 14:12:14.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4236" for this suite. 05/16/23 14:12:14.195
    STEP: Destroying namespace "webhook-4236-markers" for this suite. 05/16/23 14:12:14.207
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:12:14.216
May 16 14:12:14.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename deployment 05/16/23 14:12:14.216
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:12:14.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:12:14.253
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
May 16 14:12:14.267: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 16 14:12:19.274: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/16/23 14:12:19.274
May 16 14:12:19.274: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 05/16/23 14:12:19.282
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 16 14:12:19.292: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7304  c6c00e84-0edb-4e6a-9440-adbf950d43b4 34622 1 2023-05-16 14:12:19 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-05-16 14:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0002ea0e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

May 16 14:12:19.295: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
May 16 14:12:19.295: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 16 14:12:19.295: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7304  056fb058-890a-4757-943f-556dafef3aef 34623 1 2023-05-16 14:12:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment c6c00e84-0edb-4e6a-9440-adbf950d43b4 0xc0002ea457 0xc0002ea458}] [] [{e2e.test Update apps/v1 2023-05-16 14:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 14:12:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-16 14:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"c6c00e84-0edb-4e6a-9440-adbf950d43b4\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0002ea518 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 16 14:12:19.299: INFO: Pod "test-cleanup-controller-k57b9" is available:
&Pod{ObjectMeta:{test-cleanup-controller-k57b9 test-cleanup-controller- deployment-7304  80dc1953-29da-4969-a4e3-d349b25227c5 34612 0 2023-05-16 14:12:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.23/23"],"mac_address":"0a:58:0a:83:00:17","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.23/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.23"
    ],
    "mac": "0a:58:0a:83:00:17",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller 056fb058-890a-4757-943f-556dafef3aef 0xc0002ea9c7 0xc0002ea9c8}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"056fb058-890a-4757-943f-556dafef3aef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-05-16 14:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-05-16 14:12:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7v9j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7v9j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c28,c27,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:12:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:12:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:12:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:12:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:10.131.0.23,StartTime:2023-05-16 14:12:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:12:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://d90b21e2ec148f48274d65d860198d1f3654f21dca007a9fec7ca43411ff9e48,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May 16 14:12:19.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7304" for this suite. 05/16/23 14:12:19.304
------------------------------
â€¢ [SLOW TEST] [5.094 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:12:14.216
    May 16 14:12:14.216: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename deployment 05/16/23 14:12:14.216
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:12:14.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:12:14.253
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    May 16 14:12:14.267: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    May 16 14:12:19.274: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/16/23 14:12:19.274
    May 16 14:12:19.274: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 05/16/23 14:12:19.282
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 16 14:12:19.292: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7304  c6c00e84-0edb-4e6a-9440-adbf950d43b4 34622 1 2023-05-16 14:12:19 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-05-16 14:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0002ea0e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    May 16 14:12:19.295: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    May 16 14:12:19.295: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    May 16 14:12:19.295: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7304  056fb058-890a-4757-943f-556dafef3aef 34623 1 2023-05-16 14:12:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment c6c00e84-0edb-4e6a-9440-adbf950d43b4 0xc0002ea457 0xc0002ea458}] [] [{e2e.test Update apps/v1 2023-05-16 14:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 14:12:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-16 14:12:19 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"c6c00e84-0edb-4e6a-9440-adbf950d43b4\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0002ea518 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May 16 14:12:19.299: INFO: Pod "test-cleanup-controller-k57b9" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-k57b9 test-cleanup-controller- deployment-7304  80dc1953-29da-4969-a4e3-d349b25227c5 34612 0 2023-05-16 14:12:14 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.23/23"],"mac_address":"0a:58:0a:83:00:17","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.23/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.23"
        ],
        "mac": "0a:58:0a:83:00:17",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller 056fb058-890a-4757-943f-556dafef3aef 0xc0002ea9c7 0xc0002ea9c8}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"056fb058-890a-4757-943f-556dafef3aef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-05-16 14:12:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-05-16 14:12:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7v9j4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7v9j4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c28,c27,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:12:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:12:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:12:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:12:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:10.131.0.23,StartTime:2023-05-16 14:12:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:12:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://d90b21e2ec148f48274d65d860198d1f3654f21dca007a9fec7ca43411ff9e48,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May 16 14:12:19.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7304" for this suite. 05/16/23 14:12:19.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:12:19.31
May 16 14:12:19.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename cronjob 05/16/23 14:12:19.31
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:12:19.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:12:19.382
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 05/16/23 14:12:19.385
STEP: Ensuring a job is scheduled 05/16/23 14:12:19.392
STEP: Ensuring exactly one is scheduled 05/16/23 14:13:01.396
STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/16/23 14:13:01.4
STEP: Ensuring the job is replaced with a new one 05/16/23 14:13:01.402
STEP: Removing cronjob 05/16/23 14:14:01.407
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May 16 14:14:01.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2232" for this suite. 05/16/23 14:14:01.417
------------------------------
â€¢ [SLOW TEST] [102.114 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:12:19.31
    May 16 14:12:19.310: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename cronjob 05/16/23 14:12:19.31
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:12:19.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:12:19.382
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 05/16/23 14:12:19.385
    STEP: Ensuring a job is scheduled 05/16/23 14:12:19.392
    STEP: Ensuring exactly one is scheduled 05/16/23 14:13:01.396
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/16/23 14:13:01.4
    STEP: Ensuring the job is replaced with a new one 05/16/23 14:13:01.402
    STEP: Removing cronjob 05/16/23 14:14:01.407
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May 16 14:14:01.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2232" for this suite. 05/16/23 14:14:01.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:14:01.424
May 16 14:14:01.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubectl 05/16/23 14:14:01.425
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:14:01.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:14:01.463
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 05/16/23 14:14:01.466
May 16 14:14:01.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 create -f -'
May 16 14:14:03.109: INFO: stderr: ""
May 16 14:14:03.109: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/16/23 14:14:03.109
May 16 14:14:03.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 16 14:14:03.163: INFO: stderr: ""
May 16 14:14:03.163: INFO: stdout: "update-demo-nautilus-hr2cg update-demo-nautilus-n8zff "
May 16 14:14:03.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 get pods update-demo-nautilus-hr2cg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 16 14:14:03.212: INFO: stderr: ""
May 16 14:14:03.212: INFO: stdout: ""
May 16 14:14:03.212: INFO: update-demo-nautilus-hr2cg is created but not running
May 16 14:14:08.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 16 14:14:08.265: INFO: stderr: ""
May 16 14:14:08.265: INFO: stdout: "update-demo-nautilus-hr2cg update-demo-nautilus-n8zff "
May 16 14:14:08.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 get pods update-demo-nautilus-hr2cg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 16 14:14:08.311: INFO: stderr: ""
May 16 14:14:08.311: INFO: stdout: "true"
May 16 14:14:08.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 get pods update-demo-nautilus-hr2cg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 16 14:14:08.359: INFO: stderr: ""
May 16 14:14:08.359: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May 16 14:14:08.359: INFO: validating pod update-demo-nautilus-hr2cg
May 16 14:14:08.366: INFO: got data: {
  "image": "nautilus.jpg"
}

May 16 14:14:08.366: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 16 14:14:08.366: INFO: update-demo-nautilus-hr2cg is verified up and running
May 16 14:14:08.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 get pods update-demo-nautilus-n8zff -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 16 14:14:08.412: INFO: stderr: ""
May 16 14:14:08.412: INFO: stdout: "true"
May 16 14:14:08.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 get pods update-demo-nautilus-n8zff -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 16 14:14:08.459: INFO: stderr: ""
May 16 14:14:08.459: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May 16 14:14:08.459: INFO: validating pod update-demo-nautilus-n8zff
May 16 14:14:08.464: INFO: got data: {
  "image": "nautilus.jpg"
}

May 16 14:14:08.464: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 16 14:14:08.464: INFO: update-demo-nautilus-n8zff is verified up and running
STEP: using delete to clean up resources 05/16/23 14:14:08.464
May 16 14:14:08.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 delete --grace-period=0 --force -f -'
May 16 14:14:08.516: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 14:14:08.516: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 16 14:14:08.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 get rc,svc -l name=update-demo --no-headers'
May 16 14:14:08.570: INFO: stderr: "No resources found in kubectl-4972 namespace.\n"
May 16 14:14:08.570: INFO: stdout: ""
May 16 14:14:08.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 16 14:14:08.621: INFO: stderr: ""
May 16 14:14:08.621: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 16 14:14:08.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4972" for this suite. 05/16/23 14:14:08.627
------------------------------
â€¢ [SLOW TEST] [7.208 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:14:01.424
    May 16 14:14:01.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubectl 05/16/23 14:14:01.425
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:14:01.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:14:01.463
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 05/16/23 14:14:01.466
    May 16 14:14:01.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 create -f -'
    May 16 14:14:03.109: INFO: stderr: ""
    May 16 14:14:03.109: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/16/23 14:14:03.109
    May 16 14:14:03.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 16 14:14:03.163: INFO: stderr: ""
    May 16 14:14:03.163: INFO: stdout: "update-demo-nautilus-hr2cg update-demo-nautilus-n8zff "
    May 16 14:14:03.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 get pods update-demo-nautilus-hr2cg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 16 14:14:03.212: INFO: stderr: ""
    May 16 14:14:03.212: INFO: stdout: ""
    May 16 14:14:03.212: INFO: update-demo-nautilus-hr2cg is created but not running
    May 16 14:14:08.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 16 14:14:08.265: INFO: stderr: ""
    May 16 14:14:08.265: INFO: stdout: "update-demo-nautilus-hr2cg update-demo-nautilus-n8zff "
    May 16 14:14:08.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 get pods update-demo-nautilus-hr2cg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 16 14:14:08.311: INFO: stderr: ""
    May 16 14:14:08.311: INFO: stdout: "true"
    May 16 14:14:08.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 get pods update-demo-nautilus-hr2cg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 16 14:14:08.359: INFO: stderr: ""
    May 16 14:14:08.359: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May 16 14:14:08.359: INFO: validating pod update-demo-nautilus-hr2cg
    May 16 14:14:08.366: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 16 14:14:08.366: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 16 14:14:08.366: INFO: update-demo-nautilus-hr2cg is verified up and running
    May 16 14:14:08.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 get pods update-demo-nautilus-n8zff -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 16 14:14:08.412: INFO: stderr: ""
    May 16 14:14:08.412: INFO: stdout: "true"
    May 16 14:14:08.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 get pods update-demo-nautilus-n8zff -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 16 14:14:08.459: INFO: stderr: ""
    May 16 14:14:08.459: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May 16 14:14:08.459: INFO: validating pod update-demo-nautilus-n8zff
    May 16 14:14:08.464: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 16 14:14:08.464: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 16 14:14:08.464: INFO: update-demo-nautilus-n8zff is verified up and running
    STEP: using delete to clean up resources 05/16/23 14:14:08.464
    May 16 14:14:08.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 delete --grace-period=0 --force -f -'
    May 16 14:14:08.516: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 16 14:14:08.516: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    May 16 14:14:08.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 get rc,svc -l name=update-demo --no-headers'
    May 16 14:14:08.570: INFO: stderr: "No resources found in kubectl-4972 namespace.\n"
    May 16 14:14:08.570: INFO: stdout: ""
    May 16 14:14:08.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-4972 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May 16 14:14:08.621: INFO: stderr: ""
    May 16 14:14:08.621: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 16 14:14:08.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4972" for this suite. 05/16/23 14:14:08.627
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:14:08.633
May 16 14:14:08.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename init-container 05/16/23 14:14:08.634
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:14:08.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:14:08.652
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 05/16/23 14:14:08.655
May 16 14:14:08.655: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May 16 14:14:12.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1535" for this suite. 05/16/23 14:14:12.493
------------------------------
â€¢ [3.867 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:14:08.633
    May 16 14:14:08.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename init-container 05/16/23 14:14:08.634
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:14:08.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:14:08.652
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 05/16/23 14:14:08.655
    May 16 14:14:08.655: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May 16 14:14:12.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1535" for this suite. 05/16/23 14:14:12.493
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:14:12.5
May 16 14:14:12.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename services 05/16/23 14:14:12.501
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:14:12.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:14:12.525
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-5937 05/16/23 14:14:12.529
STEP: creating service affinity-clusterip-transition in namespace services-5937 05/16/23 14:14:12.529
STEP: creating replication controller affinity-clusterip-transition in namespace services-5937 05/16/23 14:14:12.544
I0516 14:14:12.559200      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5937, replica count: 3
I0516 14:14:15.609900      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0516 14:14:18.611224      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 16 14:14:18.619: INFO: Creating new exec pod
May 16 14:14:18.629: INFO: Waiting up to 5m0s for pod "execpod-affinity4bf9q" in namespace "services-5937" to be "running"
May 16 14:14:18.632: INFO: Pod "execpod-affinity4bf9q": Phase="Pending", Reason="", readiness=false. Elapsed: 3.404553ms
May 16 14:14:20.636: INFO: Pod "execpod-affinity4bf9q": Phase="Running", Reason="", readiness=true. Elapsed: 2.00743252s
May 16 14:14:20.636: INFO: Pod "execpod-affinity4bf9q" satisfied condition "running"
May 16 14:14:21.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5937 exec execpod-affinity4bf9q -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
May 16 14:14:22.792: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May 16 14:14:22.792: INFO: stdout: ""
May 16 14:14:22.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5937 exec execpod-affinity4bf9q -- /bin/sh -x -c nc -v -z -w 2 172.30.236.81 80'
May 16 14:14:22.899: INFO: stderr: "+ nc -v -z -w 2 172.30.236.81 80\nConnection to 172.30.236.81 80 port [tcp/http] succeeded!\n"
May 16 14:14:22.899: INFO: stdout: ""
May 16 14:14:22.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5937 exec execpod-affinity4bf9q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.236.81:80/ ; done'
May 16 14:14:23.059: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n"
May 16 14:14:23.059: INFO: stdout: "\naffinity-clusterip-transition-6gmtj\naffinity-clusterip-transition-6gmtj\naffinity-clusterip-transition-7xh5c\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-7xh5c\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-7xh5c\naffinity-clusterip-transition-7xh5c\naffinity-clusterip-transition-6gmtj\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-6gmtj\naffinity-clusterip-transition-6gmtj\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-6gmtj"
May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-6gmtj
May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-6gmtj
May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-7xh5c
May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-7xh5c
May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-7xh5c
May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-7xh5c
May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-6gmtj
May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-6gmtj
May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-6gmtj
May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-6gmtj
May 16 14:14:23.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5937 exec execpod-affinity4bf9q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.236.81:80/ ; done'
May 16 14:14:23.220: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n"
May 16 14:14:23.220: INFO: stdout: "\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4"
May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
May 16 14:14:23.220: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5937, will wait for the garbage collector to delete the pods 05/16/23 14:14:23.231
May 16 14:14:23.293: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.890643ms
May 16 14:14:23.393: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.139611ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 16 14:14:25.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5937" for this suite. 05/16/23 14:14:25.72
------------------------------
â€¢ [SLOW TEST] [13.233 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:14:12.5
    May 16 14:14:12.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename services 05/16/23 14:14:12.501
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:14:12.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:14:12.525
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-5937 05/16/23 14:14:12.529
    STEP: creating service affinity-clusterip-transition in namespace services-5937 05/16/23 14:14:12.529
    STEP: creating replication controller affinity-clusterip-transition in namespace services-5937 05/16/23 14:14:12.544
    I0516 14:14:12.559200      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5937, replica count: 3
    I0516 14:14:15.609900      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0516 14:14:18.611224      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 16 14:14:18.619: INFO: Creating new exec pod
    May 16 14:14:18.629: INFO: Waiting up to 5m0s for pod "execpod-affinity4bf9q" in namespace "services-5937" to be "running"
    May 16 14:14:18.632: INFO: Pod "execpod-affinity4bf9q": Phase="Pending", Reason="", readiness=false. Elapsed: 3.404553ms
    May 16 14:14:20.636: INFO: Pod "execpod-affinity4bf9q": Phase="Running", Reason="", readiness=true. Elapsed: 2.00743252s
    May 16 14:14:20.636: INFO: Pod "execpod-affinity4bf9q" satisfied condition "running"
    May 16 14:14:21.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5937 exec execpod-affinity4bf9q -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    May 16 14:14:22.792: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    May 16 14:14:22.792: INFO: stdout: ""
    May 16 14:14:22.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5937 exec execpod-affinity4bf9q -- /bin/sh -x -c nc -v -z -w 2 172.30.236.81 80'
    May 16 14:14:22.899: INFO: stderr: "+ nc -v -z -w 2 172.30.236.81 80\nConnection to 172.30.236.81 80 port [tcp/http] succeeded!\n"
    May 16 14:14:22.899: INFO: stdout: ""
    May 16 14:14:22.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5937 exec execpod-affinity4bf9q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.236.81:80/ ; done'
    May 16 14:14:23.059: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n"
    May 16 14:14:23.059: INFO: stdout: "\naffinity-clusterip-transition-6gmtj\naffinity-clusterip-transition-6gmtj\naffinity-clusterip-transition-7xh5c\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-7xh5c\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-7xh5c\naffinity-clusterip-transition-7xh5c\naffinity-clusterip-transition-6gmtj\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-6gmtj\naffinity-clusterip-transition-6gmtj\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-6gmtj"
    May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-6gmtj
    May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-6gmtj
    May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-7xh5c
    May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-7xh5c
    May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-7xh5c
    May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-7xh5c
    May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-6gmtj
    May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-6gmtj
    May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-6gmtj
    May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.059: INFO: Received response from host: affinity-clusterip-transition-6gmtj
    May 16 14:14:23.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5937 exec execpod-affinity4bf9q -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.236.81:80/ ; done'
    May 16 14:14:23.220: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.236.81:80/\n"
    May 16 14:14:23.220: INFO: stdout: "\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4\naffinity-clusterip-transition-vzjc4"
    May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.220: INFO: Received response from host: affinity-clusterip-transition-vzjc4
    May 16 14:14:23.220: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5937, will wait for the garbage collector to delete the pods 05/16/23 14:14:23.231
    May 16 14:14:23.293: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.890643ms
    May 16 14:14:23.393: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.139611ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 16 14:14:25.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5937" for this suite. 05/16/23 14:14:25.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:14:25.733
May 16 14:14:25.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename svc-latency 05/16/23 14:14:25.734
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:14:25.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:14:25.763
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
May 16 14:14:25.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3607 05/16/23 14:14:25.766
W0516 14:14:25.773940      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "svc-latency-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "svc-latency-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "svc-latency-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "svc-latency-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0516 14:14:25.774071      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3607, replica count: 1
I0516 14:14:26.825257      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0516 14:14:27.826001      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 16 14:14:27.936: INFO: Created: latency-svc-m5js9
May 16 14:14:27.947: INFO: Got endpoints: latency-svc-m5js9 [20.927971ms]
May 16 14:14:27.968: INFO: Created: latency-svc-p4j2q
May 16 14:14:27.981: INFO: Got endpoints: latency-svc-p4j2q [34.190162ms]
May 16 14:14:27.982: INFO: Created: latency-svc-6sdvq
May 16 14:14:27.993: INFO: Got endpoints: latency-svc-6sdvq [45.505247ms]
May 16 14:14:27.994: INFO: Created: latency-svc-zmp2k
May 16 14:14:28.001: INFO: Got endpoints: latency-svc-zmp2k [54.228832ms]
May 16 14:14:28.008: INFO: Created: latency-svc-jxqnb
May 16 14:14:28.024: INFO: Created: latency-svc-mf97k
May 16 14:14:28.024: INFO: Got endpoints: latency-svc-jxqnb [76.34595ms]
May 16 14:14:28.031: INFO: Created: latency-svc-zjhcf
May 16 14:14:28.031: INFO: Got endpoints: latency-svc-mf97k [83.999025ms]
May 16 14:14:28.040: INFO: Got endpoints: latency-svc-zjhcf [92.6449ms]
May 16 14:14:28.055: INFO: Created: latency-svc-mxfjw
May 16 14:14:28.060: INFO: Created: latency-svc-5fcnk
May 16 14:14:28.085: INFO: Got endpoints: latency-svc-mxfjw [137.517621ms]
May 16 14:14:28.085: INFO: Got endpoints: latency-svc-5fcnk [137.948584ms]
May 16 14:14:28.090: INFO: Created: latency-svc-xn62z
May 16 14:14:28.098: INFO: Created: latency-svc-nrzcb
May 16 14:14:28.107: INFO: Got endpoints: latency-svc-xn62z [159.023234ms]
May 16 14:14:28.112: INFO: Got endpoints: latency-svc-nrzcb [164.217276ms]
May 16 14:14:28.117: INFO: Created: latency-svc-4gghv
May 16 14:14:28.126: INFO: Created: latency-svc-sv5sp
May 16 14:14:28.128: INFO: Got endpoints: latency-svc-4gghv [180.066018ms]
May 16 14:14:28.144: INFO: Created: latency-svc-tzfnb
May 16 14:14:28.148: INFO: Got endpoints: latency-svc-sv5sp [200.732975ms]
May 16 14:14:28.156: INFO: Created: latency-svc-kgz2q
May 16 14:14:28.165: INFO: Got endpoints: latency-svc-kgz2q [218.140963ms]
May 16 14:14:28.166: INFO: Got endpoints: latency-svc-tzfnb [217.954308ms]
May 16 14:14:28.173: INFO: Created: latency-svc-dz2fd
May 16 14:14:28.187: INFO: Got endpoints: latency-svc-dz2fd [239.39366ms]
May 16 14:14:28.190: INFO: Created: latency-svc-4jgb6
May 16 14:14:28.197: INFO: Got endpoints: latency-svc-4jgb6 [215.948839ms]
May 16 14:14:28.200: INFO: Created: latency-svc-kxvkq
May 16 14:14:28.209: INFO: Created: latency-svc-6qmwc
May 16 14:14:28.213: INFO: Got endpoints: latency-svc-kxvkq [220.000716ms]
May 16 14:14:28.224: INFO: Got endpoints: latency-svc-6qmwc [222.77513ms]
May 16 14:14:28.225: INFO: Created: latency-svc-m6628
May 16 14:14:28.230: INFO: Created: latency-svc-pv6ck
May 16 14:14:28.234: INFO: Got endpoints: latency-svc-m6628 [210.395743ms]
May 16 14:14:28.239: INFO: Got endpoints: latency-svc-pv6ck [208.059896ms]
May 16 14:14:28.242: INFO: Created: latency-svc-gs7gc
May 16 14:14:28.251: INFO: Got endpoints: latency-svc-gs7gc [211.072221ms]
May 16 14:14:28.252: INFO: Created: latency-svc-mtv8n
May 16 14:14:28.259: INFO: Got endpoints: latency-svc-mtv8n [174.099449ms]
May 16 14:14:28.272: INFO: Created: latency-svc-4v5bk
May 16 14:14:28.276: INFO: Created: latency-svc-cfxrj
May 16 14:14:28.280: INFO: Got endpoints: latency-svc-4v5bk [194.481119ms]
May 16 14:14:28.282: INFO: Got endpoints: latency-svc-cfxrj [175.736177ms]
May 16 14:14:28.287: INFO: Created: latency-svc-64jcn
May 16 14:14:28.295: INFO: Got endpoints: latency-svc-64jcn [182.846313ms]
May 16 14:14:28.296: INFO: Created: latency-svc-n46mt
May 16 14:14:28.311: INFO: Got endpoints: latency-svc-n46mt [183.427974ms]
May 16 14:14:28.313: INFO: Created: latency-svc-nf5j6
May 16 14:14:28.323: INFO: Got endpoints: latency-svc-nf5j6 [174.713313ms]
May 16 14:14:28.327: INFO: Created: latency-svc-2h9xw
May 16 14:14:28.335: INFO: Got endpoints: latency-svc-2h9xw [169.835387ms]
May 16 14:14:28.339: INFO: Created: latency-svc-q9ll9
May 16 14:14:28.348: INFO: Created: latency-svc-bq74g
May 16 14:14:28.348: INFO: Got endpoints: latency-svc-q9ll9 [182.219692ms]
May 16 14:14:28.357: INFO: Got endpoints: latency-svc-bq74g [170.26423ms]
May 16 14:14:28.364: INFO: Created: latency-svc-qwk2h
May 16 14:14:28.368: INFO: Created: latency-svc-jpc6p
May 16 14:14:28.376: INFO: Got endpoints: latency-svc-qwk2h [178.13453ms]
May 16 14:14:28.377: INFO: Got endpoints: latency-svc-jpc6p [164.149038ms]
May 16 14:14:28.380: INFO: Created: latency-svc-pqrqw
May 16 14:14:28.388: INFO: Created: latency-svc-c7vdp
May 16 14:14:28.390: INFO: Got endpoints: latency-svc-pqrqw [165.865605ms]
May 16 14:14:28.400: INFO: Got endpoints: latency-svc-c7vdp [166.275805ms]
May 16 14:14:28.406: INFO: Created: latency-svc-n4jxl
May 16 14:14:28.412: INFO: Created: latency-svc-k7rkb
May 16 14:14:28.419: INFO: Got endpoints: latency-svc-n4jxl [179.4972ms]
May 16 14:14:28.421: INFO: Got endpoints: latency-svc-k7rkb [170.040394ms]
May 16 14:14:28.425: INFO: Created: latency-svc-57rs6
May 16 14:14:28.432: INFO: Created: latency-svc-7fc9n
May 16 14:14:28.434: INFO: Got endpoints: latency-svc-57rs6 [175.12411ms]
May 16 14:14:28.440: INFO: Got endpoints: latency-svc-7fc9n [160.787193ms]
May 16 14:14:28.442: INFO: Created: latency-svc-74vxr
May 16 14:14:28.449: INFO: Got endpoints: latency-svc-74vxr [166.861677ms]
May 16 14:14:28.452: INFO: Created: latency-svc-vktzt
May 16 14:14:28.461: INFO: Got endpoints: latency-svc-vktzt [166.014008ms]
May 16 14:14:28.467: INFO: Created: latency-svc-xxgdm
May 16 14:14:28.476: INFO: Got endpoints: latency-svc-xxgdm [165.396534ms]
May 16 14:14:28.477: INFO: Created: latency-svc-8cpxj
May 16 14:14:28.487: INFO: Got endpoints: latency-svc-8cpxj [164.172691ms]
May 16 14:14:28.492: INFO: Created: latency-svc-wpr8n
May 16 14:14:28.503: INFO: Created: latency-svc-vpp5q
May 16 14:14:28.504: INFO: Got endpoints: latency-svc-wpr8n [168.191472ms]
May 16 14:14:28.511: INFO: Got endpoints: latency-svc-vpp5q [163.363047ms]
May 16 14:14:28.513: INFO: Created: latency-svc-g7666
May 16 14:14:28.525: INFO: Got endpoints: latency-svc-g7666 [167.362847ms]
May 16 14:14:28.525: INFO: Created: latency-svc-knkrm
May 16 14:14:28.537: INFO: Got endpoints: latency-svc-knkrm [161.761466ms]
May 16 14:14:28.544: INFO: Created: latency-svc-h6wq7
May 16 14:14:28.548: INFO: Created: latency-svc-npb7b
May 16 14:14:28.552: INFO: Got endpoints: latency-svc-h6wq7 [175.040425ms]
May 16 14:14:28.561: INFO: Got endpoints: latency-svc-npb7b [171.253428ms]
May 16 14:14:28.562: INFO: Created: latency-svc-642f4
May 16 14:14:28.571: INFO: Got endpoints: latency-svc-642f4 [170.317111ms]
May 16 14:14:28.572: INFO: Created: latency-svc-4bfbj
May 16 14:14:28.582: INFO: Got endpoints: latency-svc-4bfbj [163.479955ms]
May 16 14:14:28.585: INFO: Created: latency-svc-sh22r
May 16 14:14:28.596: INFO: Created: latency-svc-llpgq
May 16 14:14:28.600: INFO: Got endpoints: latency-svc-sh22r [178.561304ms]
May 16 14:14:28.603: INFO: Got endpoints: latency-svc-llpgq [169.160143ms]
May 16 14:14:28.609: INFO: Created: latency-svc-ff8k4
May 16 14:14:28.616: INFO: Got endpoints: latency-svc-ff8k4 [176.132237ms]
May 16 14:14:28.617: INFO: Created: latency-svc-fdz5g
May 16 14:14:28.627: INFO: Got endpoints: latency-svc-fdz5g [178.04601ms]
May 16 14:14:28.633: INFO: Created: latency-svc-hzhjl
May 16 14:14:28.639: INFO: Got endpoints: latency-svc-hzhjl [178.749829ms]
May 16 14:14:28.644: INFO: Created: latency-svc-7ctft
May 16 14:14:28.654: INFO: Got endpoints: latency-svc-7ctft [177.142794ms]
May 16 14:14:28.654: INFO: Created: latency-svc-798hs
May 16 14:14:28.661: INFO: Created: latency-svc-hr4qt
May 16 14:14:28.663: INFO: Got endpoints: latency-svc-798hs [175.819988ms]
May 16 14:14:28.670: INFO: Got endpoints: latency-svc-hr4qt [165.946131ms]
May 16 14:14:28.681: INFO: Created: latency-svc-l7f86
May 16 14:14:28.695: INFO: Got endpoints: latency-svc-l7f86 [183.401485ms]
May 16 14:14:28.699: INFO: Created: latency-svc-gxj7p
May 16 14:14:28.707: INFO: Got endpoints: latency-svc-gxj7p [182.631294ms]
May 16 14:14:28.711: INFO: Created: latency-svc-vkkjt
May 16 14:14:28.719: INFO: Got endpoints: latency-svc-vkkjt [181.440122ms]
May 16 14:14:28.719: INFO: Created: latency-svc-qwqbl
May 16 14:14:28.732: INFO: Got endpoints: latency-svc-qwqbl [179.453563ms]
May 16 14:14:28.740: INFO: Created: latency-svc-z6g4z
May 16 14:14:28.741: INFO: Created: latency-svc-ml4jv
May 16 14:14:28.747: INFO: Got endpoints: latency-svc-ml4jv [185.48749ms]
May 16 14:14:28.750: INFO: Got endpoints: latency-svc-z6g4z [179.220802ms]
May 16 14:14:28.758: INFO: Created: latency-svc-gcmbw
May 16 14:14:28.768: INFO: Created: latency-svc-whdn6
May 16 14:14:28.768: INFO: Got endpoints: latency-svc-gcmbw [186.040665ms]
May 16 14:14:28.780: INFO: Got endpoints: latency-svc-whdn6 [179.684027ms]
May 16 14:14:28.786: INFO: Created: latency-svc-hpvlz
May 16 14:14:28.792: INFO: Got endpoints: latency-svc-hpvlz [188.200385ms]
May 16 14:14:28.792: INFO: Created: latency-svc-wg7bq
May 16 14:14:28.800: INFO: Got endpoints: latency-svc-wg7bq [183.145038ms]
May 16 14:14:28.804: INFO: Created: latency-svc-zgxm2
May 16 14:14:28.811: INFO: Got endpoints: latency-svc-zgxm2 [183.450014ms]
May 16 14:14:28.813: INFO: Created: latency-svc-tvfdf
May 16 14:14:28.822: INFO: Got endpoints: latency-svc-tvfdf [182.2098ms]
May 16 14:14:28.827: INFO: Created: latency-svc-sk7sq
May 16 14:14:28.837: INFO: Created: latency-svc-r527b
May 16 14:14:28.841: INFO: Got endpoints: latency-svc-sk7sq [187.297259ms]
May 16 14:14:28.850: INFO: Got endpoints: latency-svc-r527b [187.097432ms]
May 16 14:14:28.858: INFO: Created: latency-svc-6q4w9
May 16 14:14:28.861: INFO: Created: latency-svc-dmzng
May 16 14:14:28.874: INFO: Got endpoints: latency-svc-6q4w9 [204.603211ms]
May 16 14:14:28.882: INFO: Got endpoints: latency-svc-dmzng [187.559453ms]
May 16 14:14:28.888: INFO: Created: latency-svc-wmvwb
May 16 14:14:28.899: INFO: Created: latency-svc-9nrbd
May 16 14:14:28.902: INFO: Got endpoints: latency-svc-wmvwb [194.793621ms]
May 16 14:14:28.908: INFO: Created: latency-svc-s7f5l
May 16 14:14:28.914: INFO: Got endpoints: latency-svc-9nrbd [195.597108ms]
May 16 14:14:28.916: INFO: Created: latency-svc-ttf29
May 16 14:14:28.921: INFO: Got endpoints: latency-svc-s7f5l [188.959426ms]
May 16 14:14:28.932: INFO: Created: latency-svc-nh6wr
May 16 14:14:28.936: INFO: Got endpoints: latency-svc-ttf29 [188.617227ms]
May 16 14:14:28.939: INFO: Created: latency-svc-gncbx
May 16 14:14:28.952: INFO: Created: latency-svc-lgh9d
May 16 14:14:28.952: INFO: Got endpoints: latency-svc-nh6wr [202.196603ms]
May 16 14:14:28.953: INFO: Got endpoints: latency-svc-gncbx [184.470274ms]
May 16 14:14:28.989: INFO: Got endpoints: latency-svc-lgh9d [209.303617ms]
May 16 14:14:28.993: INFO: Created: latency-svc-cjh4s
May 16 14:14:29.013: INFO: Got endpoints: latency-svc-cjh4s [221.38858ms]
May 16 14:14:29.024: INFO: Created: latency-svc-wf8sn
May 16 14:14:29.027: INFO: Created: latency-svc-nqxp9
May 16 14:14:29.031: INFO: Got endpoints: latency-svc-wf8sn [231.42395ms]
May 16 14:14:29.038: INFO: Got endpoints: latency-svc-nqxp9 [227.128304ms]
May 16 14:14:29.042: INFO: Created: latency-svc-wzsgk
May 16 14:14:29.052: INFO: Got endpoints: latency-svc-wzsgk [229.789921ms]
May 16 14:14:29.052: INFO: Created: latency-svc-tkqk8
May 16 14:14:29.061: INFO: Got endpoints: latency-svc-tkqk8 [220.09521ms]
May 16 14:14:29.064: INFO: Created: latency-svc-j62qj
May 16 14:14:29.076: INFO: Got endpoints: latency-svc-j62qj [225.425848ms]
May 16 14:14:29.087: INFO: Created: latency-svc-rnmdm
May 16 14:14:29.097: INFO: Got endpoints: latency-svc-rnmdm [222.51776ms]
May 16 14:14:29.097: INFO: Created: latency-svc-knnfs
May 16 14:14:29.105: INFO: Created: latency-svc-xz96z
May 16 14:14:29.106: INFO: Got endpoints: latency-svc-knnfs [223.535609ms]
May 16 14:14:29.112: INFO: Got endpoints: latency-svc-xz96z [209.47279ms]
May 16 14:14:29.117: INFO: Created: latency-svc-rjppg
May 16 14:14:29.126: INFO: Created: latency-svc-l6nwg
May 16 14:14:29.126: INFO: Got endpoints: latency-svc-rjppg [211.79348ms]
May 16 14:14:29.144: INFO: Got endpoints: latency-svc-l6nwg [223.432456ms]
May 16 14:14:29.151: INFO: Created: latency-svc-7c46p
May 16 14:14:29.160: INFO: Created: latency-svc-b4vzh
May 16 14:14:29.167: INFO: Got endpoints: latency-svc-7c46p [231.717967ms]
May 16 14:14:29.171: INFO: Got endpoints: latency-svc-b4vzh [218.285945ms]
May 16 14:14:29.175: INFO: Created: latency-svc-5w85d
May 16 14:14:29.183: INFO: Got endpoints: latency-svc-5w85d [230.671002ms]
May 16 14:14:29.184: INFO: Created: latency-svc-45t6x
May 16 14:14:29.195: INFO: Got endpoints: latency-svc-45t6x [205.64168ms]
May 16 14:14:29.197: INFO: Created: latency-svc-hhf8b
May 16 14:14:29.206: INFO: Created: latency-svc-gztsv
May 16 14:14:29.210: INFO: Got endpoints: latency-svc-hhf8b [196.693475ms]
May 16 14:14:29.220: INFO: Got endpoints: latency-svc-gztsv [188.869491ms]
May 16 14:14:29.228: INFO: Created: latency-svc-xt5kl
May 16 14:14:29.239: INFO: Got endpoints: latency-svc-xt5kl [200.64554ms]
May 16 14:14:29.239: INFO: Created: latency-svc-f6psp
May 16 14:14:29.250: INFO: Got endpoints: latency-svc-f6psp [198.675097ms]
May 16 14:14:29.254: INFO: Created: latency-svc-tz8ln
May 16 14:14:29.262: INFO: Created: latency-svc-fnm62
May 16 14:14:29.265: INFO: Got endpoints: latency-svc-tz8ln [203.854199ms]
May 16 14:14:29.275: INFO: Got endpoints: latency-svc-fnm62 [199.683133ms]
May 16 14:14:29.278: INFO: Created: latency-svc-xx9ps
May 16 14:14:29.292: INFO: Created: latency-svc-bx2mg
May 16 14:14:29.294: INFO: Got endpoints: latency-svc-xx9ps [197.284905ms]
May 16 14:14:29.301: INFO: Got endpoints: latency-svc-bx2mg [195.66951ms]
May 16 14:14:29.304: INFO: Created: latency-svc-j9dqg
May 16 14:14:29.313: INFO: Got endpoints: latency-svc-j9dqg [201.466007ms]
May 16 14:14:29.316: INFO: Created: latency-svc-pkwzj
May 16 14:14:29.325: INFO: Got endpoints: latency-svc-pkwzj [198.670791ms]
May 16 14:14:29.336: INFO: Created: latency-svc-9gpx4
May 16 14:14:29.346: INFO: Created: latency-svc-mhk26
May 16 14:14:29.350: INFO: Created: latency-svc-kztr7
May 16 14:14:29.350: INFO: Got endpoints: latency-svc-9gpx4 [206.409201ms]
May 16 14:14:29.358: INFO: Got endpoints: latency-svc-mhk26 [190.445393ms]
May 16 14:14:29.363: INFO: Created: latency-svc-jchwq
May 16 14:14:29.365: INFO: Got endpoints: latency-svc-kztr7 [194.105347ms]
May 16 14:14:29.375: INFO: Got endpoints: latency-svc-jchwq [191.517846ms]
May 16 14:14:29.380: INFO: Created: latency-svc-x5tsz
May 16 14:14:29.393: INFO: Got endpoints: latency-svc-x5tsz [198.372921ms]
May 16 14:14:29.394: INFO: Created: latency-svc-6zkgv
May 16 14:14:29.402: INFO: Got endpoints: latency-svc-6zkgv [192.182337ms]
May 16 14:14:29.403: INFO: Created: latency-svc-2r2nh
May 16 14:14:29.412: INFO: Got endpoints: latency-svc-2r2nh [192.017035ms]
May 16 14:14:29.416: INFO: Created: latency-svc-sz8kh
May 16 14:14:29.430: INFO: Got endpoints: latency-svc-sz8kh [191.871869ms]
May 16 14:14:29.438: INFO: Created: latency-svc-nzbw6
May 16 14:14:29.447: INFO: Created: latency-svc-9lnx2
May 16 14:14:29.453: INFO: Got endpoints: latency-svc-nzbw6 [202.702646ms]
May 16 14:14:29.459: INFO: Got endpoints: latency-svc-9lnx2 [194.54443ms]
May 16 14:14:29.462: INFO: Created: latency-svc-znvr7
May 16 14:14:29.469: INFO: Got endpoints: latency-svc-znvr7 [193.943754ms]
May 16 14:14:29.470: INFO: Created: latency-svc-xfkw4
May 16 14:14:29.481: INFO: Got endpoints: latency-svc-xfkw4 [186.583329ms]
May 16 14:14:29.497: INFO: Created: latency-svc-w4qjq
May 16 14:14:29.508: INFO: Created: latency-svc-w5d65
May 16 14:14:29.514: INFO: Got endpoints: latency-svc-w4qjq [212.420295ms]
May 16 14:14:29.517: INFO: Got endpoints: latency-svc-w5d65 [204.344728ms]
May 16 14:14:29.530: INFO: Created: latency-svc-znlxl
May 16 14:14:29.537: INFO: Created: latency-svc-grss2
May 16 14:14:29.537: INFO: Got endpoints: latency-svc-znlxl [212.45657ms]
May 16 14:14:29.542: INFO: Got endpoints: latency-svc-grss2 [191.101306ms]
May 16 14:14:29.553: INFO: Created: latency-svc-96q9x
May 16 14:14:29.565: INFO: Got endpoints: latency-svc-96q9x [206.672097ms]
May 16 14:14:29.566: INFO: Created: latency-svc-xmwxr
May 16 14:14:29.577: INFO: Got endpoints: latency-svc-xmwxr [211.822757ms]
May 16 14:14:29.581: INFO: Created: latency-svc-7ftj7
May 16 14:14:29.590: INFO: Created: latency-svc-k7rmm
May 16 14:14:29.590: INFO: Got endpoints: latency-svc-7ftj7 [215.05686ms]
May 16 14:14:29.597: INFO: Got endpoints: latency-svc-k7rmm [204.467306ms]
May 16 14:14:29.608: INFO: Created: latency-svc-p9n7k
May 16 14:14:29.617: INFO: Created: latency-svc-q8w9c
May 16 14:14:29.618: INFO: Got endpoints: latency-svc-p9n7k [215.595642ms]
May 16 14:14:29.631: INFO: Created: latency-svc-hnrl8
May 16 14:14:29.632: INFO: Got endpoints: latency-svc-q8w9c [219.934935ms]
May 16 14:14:29.645: INFO: Created: latency-svc-gp4qs
May 16 14:14:29.645: INFO: Got endpoints: latency-svc-hnrl8 [214.347891ms]
May 16 14:14:29.651: INFO: Got endpoints: latency-svc-gp4qs [197.561625ms]
May 16 14:14:29.658: INFO: Created: latency-svc-44p9w
May 16 14:14:29.661: INFO: Created: latency-svc-6b8fd
May 16 14:14:29.666: INFO: Got endpoints: latency-svc-44p9w [206.981699ms]
May 16 14:14:29.675: INFO: Created: latency-svc-mhbnh
May 16 14:14:29.676: INFO: Got endpoints: latency-svc-6b8fd [206.416131ms]
May 16 14:14:29.685: INFO: Got endpoints: latency-svc-mhbnh [204.319536ms]
May 16 14:14:29.686: INFO: Created: latency-svc-knw9q
May 16 14:14:29.693: INFO: Got endpoints: latency-svc-knw9q [178.961175ms]
May 16 14:14:29.700: INFO: Created: latency-svc-92wcs
May 16 14:14:29.711: INFO: Created: latency-svc-5klhw
May 16 14:14:29.712: INFO: Got endpoints: latency-svc-92wcs [194.096819ms]
May 16 14:14:29.723: INFO: Got endpoints: latency-svc-5klhw [185.433575ms]
May 16 14:14:29.727: INFO: Created: latency-svc-49dvr
May 16 14:14:29.736: INFO: Got endpoints: latency-svc-49dvr [194.954379ms]
May 16 14:14:29.737: INFO: Created: latency-svc-bsjd7
May 16 14:14:29.749: INFO: Got endpoints: latency-svc-bsjd7 [184.34442ms]
May 16 14:14:29.757: INFO: Created: latency-svc-r9x47
May 16 14:14:29.769: INFO: Got endpoints: latency-svc-r9x47 [192.186724ms]
May 16 14:14:29.769: INFO: Created: latency-svc-5nvgd
May 16 14:14:29.777: INFO: Got endpoints: latency-svc-5nvgd [187.175313ms]
May 16 14:14:29.783: INFO: Created: latency-svc-9htfc
May 16 14:14:29.786: INFO: Created: latency-svc-w2wfz
May 16 14:14:29.795: INFO: Got endpoints: latency-svc-9htfc [197.486858ms]
May 16 14:14:29.802: INFO: Got endpoints: latency-svc-w2wfz [184.626522ms]
May 16 14:14:29.803: INFO: Created: latency-svc-spmqq
May 16 14:14:29.823: INFO: Created: latency-svc-ks5vx
May 16 14:14:29.823: INFO: Got endpoints: latency-svc-spmqq [190.900965ms]
May 16 14:14:29.834: INFO: Created: latency-svc-vql2q
May 16 14:14:29.834: INFO: Got endpoints: latency-svc-ks5vx [189.063339ms]
May 16 14:14:29.846: INFO: Got endpoints: latency-svc-vql2q [195.607626ms]
May 16 14:14:29.949: INFO: Created: latency-svc-h7s7b
May 16 14:14:29.950: INFO: Created: latency-svc-m29f2
May 16 14:14:29.951: INFO: Created: latency-svc-fk4xm
May 16 14:14:29.951: INFO: Created: latency-svc-l5xsp
May 16 14:14:29.951: INFO: Created: latency-svc-n4cgp
May 16 14:14:29.951: INFO: Created: latency-svc-hckmp
May 16 14:14:29.951: INFO: Created: latency-svc-8l544
May 16 14:14:29.951: INFO: Created: latency-svc-ghr7c
May 16 14:14:29.951: INFO: Created: latency-svc-8gmrx
May 16 14:14:29.951: INFO: Created: latency-svc-mf6ph
May 16 14:14:29.951: INFO: Created: latency-svc-2f2pj
May 16 14:14:29.953: INFO: Created: latency-svc-mmkt4
May 16 14:14:29.958: INFO: Created: latency-svc-ftclz
May 16 14:14:29.958: INFO: Created: latency-svc-j9srt
May 16 14:14:29.958: INFO: Created: latency-svc-tbwg4
May 16 14:14:29.961: INFO: Got endpoints: latency-svc-ghr7c [276.095102ms]
May 16 14:14:29.968: INFO: Got endpoints: latency-svc-m29f2 [301.092284ms]
May 16 14:14:29.971: INFO: Got endpoints: latency-svc-l5xsp [175.9504ms]
May 16 14:14:29.971: INFO: Got endpoints: latency-svc-h7s7b [124.794885ms]
May 16 14:14:29.972: INFO: Got endpoints: latency-svc-8l544 [296.002276ms]
May 16 14:14:29.976: INFO: Got endpoints: latency-svc-mf6ph [199.090616ms]
May 16 14:14:29.988: INFO: Got endpoints: latency-svc-hckmp [238.603346ms]
May 16 14:14:29.992: INFO: Got endpoints: latency-svc-8gmrx [269.55504ms]
May 16 14:14:29.999: INFO: Got endpoints: latency-svc-mmkt4 [175.92047ms]
May 16 14:14:29.999: INFO: Got endpoints: latency-svc-tbwg4 [230.366052ms]
May 16 14:14:29.999: INFO: Got endpoints: latency-svc-fk4xm [287.430505ms]
May 16 14:14:29.999: INFO: Got endpoints: latency-svc-n4cgp [262.833207ms]
May 16 14:14:30.004: INFO: Got endpoints: latency-svc-2f2pj [201.744349ms]
May 16 14:14:30.014: INFO: Created: latency-svc-fbl7f
May 16 14:14:30.015: INFO: Got endpoints: latency-svc-ftclz [322.395682ms]
May 16 14:14:30.018: INFO: Created: latency-svc-k4c5k
May 16 14:14:30.021: INFO: Got endpoints: latency-svc-j9srt [187.566731ms]
May 16 14:14:30.024: INFO: Got endpoints: latency-svc-fbl7f [62.923462ms]
May 16 14:14:30.026: INFO: Got endpoints: latency-svc-k4c5k [58.860096ms]
May 16 14:14:30.054: INFO: Created: latency-svc-x5cpg
May 16 14:14:30.055: INFO: Created: latency-svc-slxzv
May 16 14:14:30.061: INFO: Got endpoints: latency-svc-x5cpg [89.861351ms]
May 16 14:14:30.070: INFO: Created: latency-svc-plj96
May 16 14:14:30.076: INFO: Got endpoints: latency-svc-slxzv [105.104812ms]
May 16 14:14:30.082: INFO: Created: latency-svc-wjmms
May 16 14:14:30.092: INFO: Got endpoints: latency-svc-plj96 [119.965951ms]
May 16 14:14:30.093: INFO: Got endpoints: latency-svc-wjmms [116.232728ms]
May 16 14:14:30.114: INFO: Created: latency-svc-h8kgs
May 16 14:14:30.123: INFO: Created: latency-svc-rkfdr
May 16 14:14:30.124: INFO: Got endpoints: latency-svc-h8kgs [131.311221ms]
May 16 14:14:30.138: INFO: Got endpoints: latency-svc-rkfdr [150.870082ms]
May 16 14:14:30.142: INFO: Created: latency-svc-vh2h7
May 16 14:14:30.150: INFO: Got endpoints: latency-svc-vh2h7 [151.013446ms]
May 16 14:14:30.155: INFO: Created: latency-svc-wbqbj
May 16 14:14:30.168: INFO: Got endpoints: latency-svc-wbqbj [168.179516ms]
May 16 14:14:30.170: INFO: Created: latency-svc-8v67q
May 16 14:14:30.182: INFO: Got endpoints: latency-svc-8v67q [182.380128ms]
May 16 14:14:30.182: INFO: Created: latency-svc-xkw42
May 16 14:14:30.187: INFO: Got endpoints: latency-svc-xkw42 [188.412145ms]
May 16 14:14:30.197: INFO: Created: latency-svc-2kz72
May 16 14:14:30.207: INFO: Got endpoints: latency-svc-2kz72 [202.698632ms]
May 16 14:14:30.209: INFO: Created: latency-svc-k9dt7
May 16 14:14:30.214: INFO: Got endpoints: latency-svc-k9dt7 [199.198983ms]
May 16 14:14:30.227: INFO: Created: latency-svc-t42rl
May 16 14:14:30.236: INFO: Created: latency-svc-rf4d4
May 16 14:14:30.236: INFO: Got endpoints: latency-svc-t42rl [214.428707ms]
May 16 14:14:30.252: INFO: Created: latency-svc-xcn7q
May 16 14:14:30.267: INFO: Got endpoints: latency-svc-rf4d4 [243.053555ms]
May 16 14:14:30.273: INFO: Created: latency-svc-kfjwz
May 16 14:14:30.274: INFO: Got endpoints: latency-svc-xcn7q [247.233485ms]
May 16 14:14:30.280: INFO: Got endpoints: latency-svc-kfjwz [219.510757ms]
May 16 14:14:30.292: INFO: Created: latency-svc-bhsxq
May 16 14:14:30.299: INFO: Created: latency-svc-qdb97
May 16 14:14:30.302: INFO: Got endpoints: latency-svc-bhsxq [225.960241ms]
May 16 14:14:30.307: INFO: Got endpoints: latency-svc-qdb97 [215.752239ms]
May 16 14:14:30.312: INFO: Created: latency-svc-fv25w
May 16 14:14:30.321: INFO: Got endpoints: latency-svc-fv25w [228.862873ms]
May 16 14:14:30.327: INFO: Created: latency-svc-n4dmh
May 16 14:14:30.341: INFO: Got endpoints: latency-svc-n4dmh [217.205102ms]
May 16 14:14:30.342: INFO: Created: latency-svc-nhnqs
May 16 14:14:30.345: INFO: Created: latency-svc-k9g5n
May 16 14:14:30.351: INFO: Got endpoints: latency-svc-nhnqs [212.380665ms]
May 16 14:14:30.363: INFO: Got endpoints: latency-svc-k9g5n [212.878676ms]
May 16 14:14:30.368: INFO: Created: latency-svc-znl7p
May 16 14:14:30.372: INFO: Got endpoints: latency-svc-znl7p [204.257466ms]
May 16 14:14:30.376: INFO: Created: latency-svc-mhpz7
May 16 14:14:30.388: INFO: Got endpoints: latency-svc-mhpz7 [206.213184ms]
May 16 14:14:30.395: INFO: Created: latency-svc-vxzbv
May 16 14:14:30.400: INFO: Created: latency-svc-dmrlt
May 16 14:14:30.401: INFO: Got endpoints: latency-svc-vxzbv [213.284177ms]
May 16 14:14:30.417: INFO: Got endpoints: latency-svc-dmrlt [210.658676ms]
May 16 14:14:30.421: INFO: Created: latency-svc-qp47c
May 16 14:14:30.430: INFO: Got endpoints: latency-svc-qp47c [216.02633ms]
May 16 14:14:30.433: INFO: Created: latency-svc-dr7tc
May 16 14:14:30.439: INFO: Got endpoints: latency-svc-dr7tc [202.944585ms]
May 16 14:14:30.442: INFO: Created: latency-svc-s7fdq
May 16 14:14:30.451: INFO: Created: latency-svc-zdwz8
May 16 14:14:30.452: INFO: Got endpoints: latency-svc-s7fdq [185.381328ms]
May 16 14:14:30.461: INFO: Got endpoints: latency-svc-zdwz8 [187.794818ms]
May 16 14:14:30.466: INFO: Created: latency-svc-jcvts
May 16 14:14:30.476: INFO: Got endpoints: latency-svc-jcvts [195.545604ms]
May 16 14:14:30.478: INFO: Created: latency-svc-p66zq
May 16 14:14:30.487: INFO: Got endpoints: latency-svc-p66zq [184.78148ms]
May 16 14:14:30.492: INFO: Created: latency-svc-wfxp8
May 16 14:14:30.495: INFO: Created: latency-svc-qgk5l
May 16 14:14:30.502: INFO: Got endpoints: latency-svc-wfxp8 [194.296332ms]
May 16 14:14:30.511: INFO: Got endpoints: latency-svc-qgk5l [189.479416ms]
May 16 14:14:30.512: INFO: Created: latency-svc-gphvj
May 16 14:14:30.522: INFO: Got endpoints: latency-svc-gphvj [180.867417ms]
May 16 14:14:30.526: INFO: Created: latency-svc-64ww6
May 16 14:14:30.533: INFO: Got endpoints: latency-svc-64ww6 [182.644367ms]
May 16 14:14:30.540: INFO: Created: latency-svc-6n942
May 16 14:14:30.541: INFO: Created: latency-svc-zb2fq
May 16 14:14:30.547: INFO: Got endpoints: latency-svc-6n942 [183.255026ms]
May 16 14:14:30.549: INFO: Got endpoints: latency-svc-zb2fq [177.431254ms]
May 16 14:14:30.549: INFO: Latencies: [34.190162ms 45.505247ms 54.228832ms 58.860096ms 62.923462ms 76.34595ms 83.999025ms 89.861351ms 92.6449ms 105.104812ms 116.232728ms 119.965951ms 124.794885ms 131.311221ms 137.517621ms 137.948584ms 150.870082ms 151.013446ms 159.023234ms 160.787193ms 161.761466ms 163.363047ms 163.479955ms 164.149038ms 164.172691ms 164.217276ms 165.396534ms 165.865605ms 165.946131ms 166.014008ms 166.275805ms 166.861677ms 167.362847ms 168.179516ms 168.191472ms 169.160143ms 169.835387ms 170.040394ms 170.26423ms 170.317111ms 171.253428ms 174.099449ms 174.713313ms 175.040425ms 175.12411ms 175.736177ms 175.819988ms 175.92047ms 175.9504ms 176.132237ms 177.142794ms 177.431254ms 178.04601ms 178.13453ms 178.561304ms 178.749829ms 178.961175ms 179.220802ms 179.453563ms 179.4972ms 179.684027ms 180.066018ms 180.867417ms 181.440122ms 182.2098ms 182.219692ms 182.380128ms 182.631294ms 182.644367ms 182.846313ms 183.145038ms 183.255026ms 183.401485ms 183.427974ms 183.450014ms 184.34442ms 184.470274ms 184.626522ms 184.78148ms 185.381328ms 185.433575ms 185.48749ms 186.040665ms 186.583329ms 187.097432ms 187.175313ms 187.297259ms 187.559453ms 187.566731ms 187.794818ms 188.200385ms 188.412145ms 188.617227ms 188.869491ms 188.959426ms 189.063339ms 189.479416ms 190.445393ms 190.900965ms 191.101306ms 191.517846ms 191.871869ms 192.017035ms 192.182337ms 192.186724ms 193.943754ms 194.096819ms 194.105347ms 194.296332ms 194.481119ms 194.54443ms 194.793621ms 194.954379ms 195.545604ms 195.597108ms 195.607626ms 195.66951ms 196.693475ms 197.284905ms 197.486858ms 197.561625ms 198.372921ms 198.670791ms 198.675097ms 199.090616ms 199.198983ms 199.683133ms 200.64554ms 200.732975ms 201.466007ms 201.744349ms 202.196603ms 202.698632ms 202.702646ms 202.944585ms 203.854199ms 204.257466ms 204.319536ms 204.344728ms 204.467306ms 204.603211ms 205.64168ms 206.213184ms 206.409201ms 206.416131ms 206.672097ms 206.981699ms 208.059896ms 209.303617ms 209.47279ms 210.395743ms 210.658676ms 211.072221ms 211.79348ms 211.822757ms 212.380665ms 212.420295ms 212.45657ms 212.878676ms 213.284177ms 214.347891ms 214.428707ms 215.05686ms 215.595642ms 215.752239ms 215.948839ms 216.02633ms 217.205102ms 217.954308ms 218.140963ms 218.285945ms 219.510757ms 219.934935ms 220.000716ms 220.09521ms 221.38858ms 222.51776ms 222.77513ms 223.432456ms 223.535609ms 225.425848ms 225.960241ms 227.128304ms 228.862873ms 229.789921ms 230.366052ms 230.671002ms 231.42395ms 231.717967ms 238.603346ms 239.39366ms 243.053555ms 247.233485ms 262.833207ms 269.55504ms 276.095102ms 287.430505ms 296.002276ms 301.092284ms 322.395682ms]
May 16 14:14:30.549: INFO: 50 %ile: 191.517846ms
May 16 14:14:30.549: INFO: 90 %ile: 225.425848ms
May 16 14:14:30.549: INFO: 99 %ile: 301.092284ms
May 16 14:14:30.549: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
May 16 14:14:30.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-3607" for this suite. 05/16/23 14:14:30.565
------------------------------
â€¢ [4.839 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:14:25.733
    May 16 14:14:25.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename svc-latency 05/16/23 14:14:25.734
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:14:25.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:14:25.763
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    May 16 14:14:25.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-3607 05/16/23 14:14:25.766
    W0516 14:14:25.773940      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "svc-latency-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "svc-latency-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "svc-latency-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "svc-latency-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0516 14:14:25.774071      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3607, replica count: 1
    I0516 14:14:26.825257      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0516 14:14:27.826001      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 16 14:14:27.936: INFO: Created: latency-svc-m5js9
    May 16 14:14:27.947: INFO: Got endpoints: latency-svc-m5js9 [20.927971ms]
    May 16 14:14:27.968: INFO: Created: latency-svc-p4j2q
    May 16 14:14:27.981: INFO: Got endpoints: latency-svc-p4j2q [34.190162ms]
    May 16 14:14:27.982: INFO: Created: latency-svc-6sdvq
    May 16 14:14:27.993: INFO: Got endpoints: latency-svc-6sdvq [45.505247ms]
    May 16 14:14:27.994: INFO: Created: latency-svc-zmp2k
    May 16 14:14:28.001: INFO: Got endpoints: latency-svc-zmp2k [54.228832ms]
    May 16 14:14:28.008: INFO: Created: latency-svc-jxqnb
    May 16 14:14:28.024: INFO: Created: latency-svc-mf97k
    May 16 14:14:28.024: INFO: Got endpoints: latency-svc-jxqnb [76.34595ms]
    May 16 14:14:28.031: INFO: Created: latency-svc-zjhcf
    May 16 14:14:28.031: INFO: Got endpoints: latency-svc-mf97k [83.999025ms]
    May 16 14:14:28.040: INFO: Got endpoints: latency-svc-zjhcf [92.6449ms]
    May 16 14:14:28.055: INFO: Created: latency-svc-mxfjw
    May 16 14:14:28.060: INFO: Created: latency-svc-5fcnk
    May 16 14:14:28.085: INFO: Got endpoints: latency-svc-mxfjw [137.517621ms]
    May 16 14:14:28.085: INFO: Got endpoints: latency-svc-5fcnk [137.948584ms]
    May 16 14:14:28.090: INFO: Created: latency-svc-xn62z
    May 16 14:14:28.098: INFO: Created: latency-svc-nrzcb
    May 16 14:14:28.107: INFO: Got endpoints: latency-svc-xn62z [159.023234ms]
    May 16 14:14:28.112: INFO: Got endpoints: latency-svc-nrzcb [164.217276ms]
    May 16 14:14:28.117: INFO: Created: latency-svc-4gghv
    May 16 14:14:28.126: INFO: Created: latency-svc-sv5sp
    May 16 14:14:28.128: INFO: Got endpoints: latency-svc-4gghv [180.066018ms]
    May 16 14:14:28.144: INFO: Created: latency-svc-tzfnb
    May 16 14:14:28.148: INFO: Got endpoints: latency-svc-sv5sp [200.732975ms]
    May 16 14:14:28.156: INFO: Created: latency-svc-kgz2q
    May 16 14:14:28.165: INFO: Got endpoints: latency-svc-kgz2q [218.140963ms]
    May 16 14:14:28.166: INFO: Got endpoints: latency-svc-tzfnb [217.954308ms]
    May 16 14:14:28.173: INFO: Created: latency-svc-dz2fd
    May 16 14:14:28.187: INFO: Got endpoints: latency-svc-dz2fd [239.39366ms]
    May 16 14:14:28.190: INFO: Created: latency-svc-4jgb6
    May 16 14:14:28.197: INFO: Got endpoints: latency-svc-4jgb6 [215.948839ms]
    May 16 14:14:28.200: INFO: Created: latency-svc-kxvkq
    May 16 14:14:28.209: INFO: Created: latency-svc-6qmwc
    May 16 14:14:28.213: INFO: Got endpoints: latency-svc-kxvkq [220.000716ms]
    May 16 14:14:28.224: INFO: Got endpoints: latency-svc-6qmwc [222.77513ms]
    May 16 14:14:28.225: INFO: Created: latency-svc-m6628
    May 16 14:14:28.230: INFO: Created: latency-svc-pv6ck
    May 16 14:14:28.234: INFO: Got endpoints: latency-svc-m6628 [210.395743ms]
    May 16 14:14:28.239: INFO: Got endpoints: latency-svc-pv6ck [208.059896ms]
    May 16 14:14:28.242: INFO: Created: latency-svc-gs7gc
    May 16 14:14:28.251: INFO: Got endpoints: latency-svc-gs7gc [211.072221ms]
    May 16 14:14:28.252: INFO: Created: latency-svc-mtv8n
    May 16 14:14:28.259: INFO: Got endpoints: latency-svc-mtv8n [174.099449ms]
    May 16 14:14:28.272: INFO: Created: latency-svc-4v5bk
    May 16 14:14:28.276: INFO: Created: latency-svc-cfxrj
    May 16 14:14:28.280: INFO: Got endpoints: latency-svc-4v5bk [194.481119ms]
    May 16 14:14:28.282: INFO: Got endpoints: latency-svc-cfxrj [175.736177ms]
    May 16 14:14:28.287: INFO: Created: latency-svc-64jcn
    May 16 14:14:28.295: INFO: Got endpoints: latency-svc-64jcn [182.846313ms]
    May 16 14:14:28.296: INFO: Created: latency-svc-n46mt
    May 16 14:14:28.311: INFO: Got endpoints: latency-svc-n46mt [183.427974ms]
    May 16 14:14:28.313: INFO: Created: latency-svc-nf5j6
    May 16 14:14:28.323: INFO: Got endpoints: latency-svc-nf5j6 [174.713313ms]
    May 16 14:14:28.327: INFO: Created: latency-svc-2h9xw
    May 16 14:14:28.335: INFO: Got endpoints: latency-svc-2h9xw [169.835387ms]
    May 16 14:14:28.339: INFO: Created: latency-svc-q9ll9
    May 16 14:14:28.348: INFO: Created: latency-svc-bq74g
    May 16 14:14:28.348: INFO: Got endpoints: latency-svc-q9ll9 [182.219692ms]
    May 16 14:14:28.357: INFO: Got endpoints: latency-svc-bq74g [170.26423ms]
    May 16 14:14:28.364: INFO: Created: latency-svc-qwk2h
    May 16 14:14:28.368: INFO: Created: latency-svc-jpc6p
    May 16 14:14:28.376: INFO: Got endpoints: latency-svc-qwk2h [178.13453ms]
    May 16 14:14:28.377: INFO: Got endpoints: latency-svc-jpc6p [164.149038ms]
    May 16 14:14:28.380: INFO: Created: latency-svc-pqrqw
    May 16 14:14:28.388: INFO: Created: latency-svc-c7vdp
    May 16 14:14:28.390: INFO: Got endpoints: latency-svc-pqrqw [165.865605ms]
    May 16 14:14:28.400: INFO: Got endpoints: latency-svc-c7vdp [166.275805ms]
    May 16 14:14:28.406: INFO: Created: latency-svc-n4jxl
    May 16 14:14:28.412: INFO: Created: latency-svc-k7rkb
    May 16 14:14:28.419: INFO: Got endpoints: latency-svc-n4jxl [179.4972ms]
    May 16 14:14:28.421: INFO: Got endpoints: latency-svc-k7rkb [170.040394ms]
    May 16 14:14:28.425: INFO: Created: latency-svc-57rs6
    May 16 14:14:28.432: INFO: Created: latency-svc-7fc9n
    May 16 14:14:28.434: INFO: Got endpoints: latency-svc-57rs6 [175.12411ms]
    May 16 14:14:28.440: INFO: Got endpoints: latency-svc-7fc9n [160.787193ms]
    May 16 14:14:28.442: INFO: Created: latency-svc-74vxr
    May 16 14:14:28.449: INFO: Got endpoints: latency-svc-74vxr [166.861677ms]
    May 16 14:14:28.452: INFO: Created: latency-svc-vktzt
    May 16 14:14:28.461: INFO: Got endpoints: latency-svc-vktzt [166.014008ms]
    May 16 14:14:28.467: INFO: Created: latency-svc-xxgdm
    May 16 14:14:28.476: INFO: Got endpoints: latency-svc-xxgdm [165.396534ms]
    May 16 14:14:28.477: INFO: Created: latency-svc-8cpxj
    May 16 14:14:28.487: INFO: Got endpoints: latency-svc-8cpxj [164.172691ms]
    May 16 14:14:28.492: INFO: Created: latency-svc-wpr8n
    May 16 14:14:28.503: INFO: Created: latency-svc-vpp5q
    May 16 14:14:28.504: INFO: Got endpoints: latency-svc-wpr8n [168.191472ms]
    May 16 14:14:28.511: INFO: Got endpoints: latency-svc-vpp5q [163.363047ms]
    May 16 14:14:28.513: INFO: Created: latency-svc-g7666
    May 16 14:14:28.525: INFO: Got endpoints: latency-svc-g7666 [167.362847ms]
    May 16 14:14:28.525: INFO: Created: latency-svc-knkrm
    May 16 14:14:28.537: INFO: Got endpoints: latency-svc-knkrm [161.761466ms]
    May 16 14:14:28.544: INFO: Created: latency-svc-h6wq7
    May 16 14:14:28.548: INFO: Created: latency-svc-npb7b
    May 16 14:14:28.552: INFO: Got endpoints: latency-svc-h6wq7 [175.040425ms]
    May 16 14:14:28.561: INFO: Got endpoints: latency-svc-npb7b [171.253428ms]
    May 16 14:14:28.562: INFO: Created: latency-svc-642f4
    May 16 14:14:28.571: INFO: Got endpoints: latency-svc-642f4 [170.317111ms]
    May 16 14:14:28.572: INFO: Created: latency-svc-4bfbj
    May 16 14:14:28.582: INFO: Got endpoints: latency-svc-4bfbj [163.479955ms]
    May 16 14:14:28.585: INFO: Created: latency-svc-sh22r
    May 16 14:14:28.596: INFO: Created: latency-svc-llpgq
    May 16 14:14:28.600: INFO: Got endpoints: latency-svc-sh22r [178.561304ms]
    May 16 14:14:28.603: INFO: Got endpoints: latency-svc-llpgq [169.160143ms]
    May 16 14:14:28.609: INFO: Created: latency-svc-ff8k4
    May 16 14:14:28.616: INFO: Got endpoints: latency-svc-ff8k4 [176.132237ms]
    May 16 14:14:28.617: INFO: Created: latency-svc-fdz5g
    May 16 14:14:28.627: INFO: Got endpoints: latency-svc-fdz5g [178.04601ms]
    May 16 14:14:28.633: INFO: Created: latency-svc-hzhjl
    May 16 14:14:28.639: INFO: Got endpoints: latency-svc-hzhjl [178.749829ms]
    May 16 14:14:28.644: INFO: Created: latency-svc-7ctft
    May 16 14:14:28.654: INFO: Got endpoints: latency-svc-7ctft [177.142794ms]
    May 16 14:14:28.654: INFO: Created: latency-svc-798hs
    May 16 14:14:28.661: INFO: Created: latency-svc-hr4qt
    May 16 14:14:28.663: INFO: Got endpoints: latency-svc-798hs [175.819988ms]
    May 16 14:14:28.670: INFO: Got endpoints: latency-svc-hr4qt [165.946131ms]
    May 16 14:14:28.681: INFO: Created: latency-svc-l7f86
    May 16 14:14:28.695: INFO: Got endpoints: latency-svc-l7f86 [183.401485ms]
    May 16 14:14:28.699: INFO: Created: latency-svc-gxj7p
    May 16 14:14:28.707: INFO: Got endpoints: latency-svc-gxj7p [182.631294ms]
    May 16 14:14:28.711: INFO: Created: latency-svc-vkkjt
    May 16 14:14:28.719: INFO: Got endpoints: latency-svc-vkkjt [181.440122ms]
    May 16 14:14:28.719: INFO: Created: latency-svc-qwqbl
    May 16 14:14:28.732: INFO: Got endpoints: latency-svc-qwqbl [179.453563ms]
    May 16 14:14:28.740: INFO: Created: latency-svc-z6g4z
    May 16 14:14:28.741: INFO: Created: latency-svc-ml4jv
    May 16 14:14:28.747: INFO: Got endpoints: latency-svc-ml4jv [185.48749ms]
    May 16 14:14:28.750: INFO: Got endpoints: latency-svc-z6g4z [179.220802ms]
    May 16 14:14:28.758: INFO: Created: latency-svc-gcmbw
    May 16 14:14:28.768: INFO: Created: latency-svc-whdn6
    May 16 14:14:28.768: INFO: Got endpoints: latency-svc-gcmbw [186.040665ms]
    May 16 14:14:28.780: INFO: Got endpoints: latency-svc-whdn6 [179.684027ms]
    May 16 14:14:28.786: INFO: Created: latency-svc-hpvlz
    May 16 14:14:28.792: INFO: Got endpoints: latency-svc-hpvlz [188.200385ms]
    May 16 14:14:28.792: INFO: Created: latency-svc-wg7bq
    May 16 14:14:28.800: INFO: Got endpoints: latency-svc-wg7bq [183.145038ms]
    May 16 14:14:28.804: INFO: Created: latency-svc-zgxm2
    May 16 14:14:28.811: INFO: Got endpoints: latency-svc-zgxm2 [183.450014ms]
    May 16 14:14:28.813: INFO: Created: latency-svc-tvfdf
    May 16 14:14:28.822: INFO: Got endpoints: latency-svc-tvfdf [182.2098ms]
    May 16 14:14:28.827: INFO: Created: latency-svc-sk7sq
    May 16 14:14:28.837: INFO: Created: latency-svc-r527b
    May 16 14:14:28.841: INFO: Got endpoints: latency-svc-sk7sq [187.297259ms]
    May 16 14:14:28.850: INFO: Got endpoints: latency-svc-r527b [187.097432ms]
    May 16 14:14:28.858: INFO: Created: latency-svc-6q4w9
    May 16 14:14:28.861: INFO: Created: latency-svc-dmzng
    May 16 14:14:28.874: INFO: Got endpoints: latency-svc-6q4w9 [204.603211ms]
    May 16 14:14:28.882: INFO: Got endpoints: latency-svc-dmzng [187.559453ms]
    May 16 14:14:28.888: INFO: Created: latency-svc-wmvwb
    May 16 14:14:28.899: INFO: Created: latency-svc-9nrbd
    May 16 14:14:28.902: INFO: Got endpoints: latency-svc-wmvwb [194.793621ms]
    May 16 14:14:28.908: INFO: Created: latency-svc-s7f5l
    May 16 14:14:28.914: INFO: Got endpoints: latency-svc-9nrbd [195.597108ms]
    May 16 14:14:28.916: INFO: Created: latency-svc-ttf29
    May 16 14:14:28.921: INFO: Got endpoints: latency-svc-s7f5l [188.959426ms]
    May 16 14:14:28.932: INFO: Created: latency-svc-nh6wr
    May 16 14:14:28.936: INFO: Got endpoints: latency-svc-ttf29 [188.617227ms]
    May 16 14:14:28.939: INFO: Created: latency-svc-gncbx
    May 16 14:14:28.952: INFO: Created: latency-svc-lgh9d
    May 16 14:14:28.952: INFO: Got endpoints: latency-svc-nh6wr [202.196603ms]
    May 16 14:14:28.953: INFO: Got endpoints: latency-svc-gncbx [184.470274ms]
    May 16 14:14:28.989: INFO: Got endpoints: latency-svc-lgh9d [209.303617ms]
    May 16 14:14:28.993: INFO: Created: latency-svc-cjh4s
    May 16 14:14:29.013: INFO: Got endpoints: latency-svc-cjh4s [221.38858ms]
    May 16 14:14:29.024: INFO: Created: latency-svc-wf8sn
    May 16 14:14:29.027: INFO: Created: latency-svc-nqxp9
    May 16 14:14:29.031: INFO: Got endpoints: latency-svc-wf8sn [231.42395ms]
    May 16 14:14:29.038: INFO: Got endpoints: latency-svc-nqxp9 [227.128304ms]
    May 16 14:14:29.042: INFO: Created: latency-svc-wzsgk
    May 16 14:14:29.052: INFO: Got endpoints: latency-svc-wzsgk [229.789921ms]
    May 16 14:14:29.052: INFO: Created: latency-svc-tkqk8
    May 16 14:14:29.061: INFO: Got endpoints: latency-svc-tkqk8 [220.09521ms]
    May 16 14:14:29.064: INFO: Created: latency-svc-j62qj
    May 16 14:14:29.076: INFO: Got endpoints: latency-svc-j62qj [225.425848ms]
    May 16 14:14:29.087: INFO: Created: latency-svc-rnmdm
    May 16 14:14:29.097: INFO: Got endpoints: latency-svc-rnmdm [222.51776ms]
    May 16 14:14:29.097: INFO: Created: latency-svc-knnfs
    May 16 14:14:29.105: INFO: Created: latency-svc-xz96z
    May 16 14:14:29.106: INFO: Got endpoints: latency-svc-knnfs [223.535609ms]
    May 16 14:14:29.112: INFO: Got endpoints: latency-svc-xz96z [209.47279ms]
    May 16 14:14:29.117: INFO: Created: latency-svc-rjppg
    May 16 14:14:29.126: INFO: Created: latency-svc-l6nwg
    May 16 14:14:29.126: INFO: Got endpoints: latency-svc-rjppg [211.79348ms]
    May 16 14:14:29.144: INFO: Got endpoints: latency-svc-l6nwg [223.432456ms]
    May 16 14:14:29.151: INFO: Created: latency-svc-7c46p
    May 16 14:14:29.160: INFO: Created: latency-svc-b4vzh
    May 16 14:14:29.167: INFO: Got endpoints: latency-svc-7c46p [231.717967ms]
    May 16 14:14:29.171: INFO: Got endpoints: latency-svc-b4vzh [218.285945ms]
    May 16 14:14:29.175: INFO: Created: latency-svc-5w85d
    May 16 14:14:29.183: INFO: Got endpoints: latency-svc-5w85d [230.671002ms]
    May 16 14:14:29.184: INFO: Created: latency-svc-45t6x
    May 16 14:14:29.195: INFO: Got endpoints: latency-svc-45t6x [205.64168ms]
    May 16 14:14:29.197: INFO: Created: latency-svc-hhf8b
    May 16 14:14:29.206: INFO: Created: latency-svc-gztsv
    May 16 14:14:29.210: INFO: Got endpoints: latency-svc-hhf8b [196.693475ms]
    May 16 14:14:29.220: INFO: Got endpoints: latency-svc-gztsv [188.869491ms]
    May 16 14:14:29.228: INFO: Created: latency-svc-xt5kl
    May 16 14:14:29.239: INFO: Got endpoints: latency-svc-xt5kl [200.64554ms]
    May 16 14:14:29.239: INFO: Created: latency-svc-f6psp
    May 16 14:14:29.250: INFO: Got endpoints: latency-svc-f6psp [198.675097ms]
    May 16 14:14:29.254: INFO: Created: latency-svc-tz8ln
    May 16 14:14:29.262: INFO: Created: latency-svc-fnm62
    May 16 14:14:29.265: INFO: Got endpoints: latency-svc-tz8ln [203.854199ms]
    May 16 14:14:29.275: INFO: Got endpoints: latency-svc-fnm62 [199.683133ms]
    May 16 14:14:29.278: INFO: Created: latency-svc-xx9ps
    May 16 14:14:29.292: INFO: Created: latency-svc-bx2mg
    May 16 14:14:29.294: INFO: Got endpoints: latency-svc-xx9ps [197.284905ms]
    May 16 14:14:29.301: INFO: Got endpoints: latency-svc-bx2mg [195.66951ms]
    May 16 14:14:29.304: INFO: Created: latency-svc-j9dqg
    May 16 14:14:29.313: INFO: Got endpoints: latency-svc-j9dqg [201.466007ms]
    May 16 14:14:29.316: INFO: Created: latency-svc-pkwzj
    May 16 14:14:29.325: INFO: Got endpoints: latency-svc-pkwzj [198.670791ms]
    May 16 14:14:29.336: INFO: Created: latency-svc-9gpx4
    May 16 14:14:29.346: INFO: Created: latency-svc-mhk26
    May 16 14:14:29.350: INFO: Created: latency-svc-kztr7
    May 16 14:14:29.350: INFO: Got endpoints: latency-svc-9gpx4 [206.409201ms]
    May 16 14:14:29.358: INFO: Got endpoints: latency-svc-mhk26 [190.445393ms]
    May 16 14:14:29.363: INFO: Created: latency-svc-jchwq
    May 16 14:14:29.365: INFO: Got endpoints: latency-svc-kztr7 [194.105347ms]
    May 16 14:14:29.375: INFO: Got endpoints: latency-svc-jchwq [191.517846ms]
    May 16 14:14:29.380: INFO: Created: latency-svc-x5tsz
    May 16 14:14:29.393: INFO: Got endpoints: latency-svc-x5tsz [198.372921ms]
    May 16 14:14:29.394: INFO: Created: latency-svc-6zkgv
    May 16 14:14:29.402: INFO: Got endpoints: latency-svc-6zkgv [192.182337ms]
    May 16 14:14:29.403: INFO: Created: latency-svc-2r2nh
    May 16 14:14:29.412: INFO: Got endpoints: latency-svc-2r2nh [192.017035ms]
    May 16 14:14:29.416: INFO: Created: latency-svc-sz8kh
    May 16 14:14:29.430: INFO: Got endpoints: latency-svc-sz8kh [191.871869ms]
    May 16 14:14:29.438: INFO: Created: latency-svc-nzbw6
    May 16 14:14:29.447: INFO: Created: latency-svc-9lnx2
    May 16 14:14:29.453: INFO: Got endpoints: latency-svc-nzbw6 [202.702646ms]
    May 16 14:14:29.459: INFO: Got endpoints: latency-svc-9lnx2 [194.54443ms]
    May 16 14:14:29.462: INFO: Created: latency-svc-znvr7
    May 16 14:14:29.469: INFO: Got endpoints: latency-svc-znvr7 [193.943754ms]
    May 16 14:14:29.470: INFO: Created: latency-svc-xfkw4
    May 16 14:14:29.481: INFO: Got endpoints: latency-svc-xfkw4 [186.583329ms]
    May 16 14:14:29.497: INFO: Created: latency-svc-w4qjq
    May 16 14:14:29.508: INFO: Created: latency-svc-w5d65
    May 16 14:14:29.514: INFO: Got endpoints: latency-svc-w4qjq [212.420295ms]
    May 16 14:14:29.517: INFO: Got endpoints: latency-svc-w5d65 [204.344728ms]
    May 16 14:14:29.530: INFO: Created: latency-svc-znlxl
    May 16 14:14:29.537: INFO: Created: latency-svc-grss2
    May 16 14:14:29.537: INFO: Got endpoints: latency-svc-znlxl [212.45657ms]
    May 16 14:14:29.542: INFO: Got endpoints: latency-svc-grss2 [191.101306ms]
    May 16 14:14:29.553: INFO: Created: latency-svc-96q9x
    May 16 14:14:29.565: INFO: Got endpoints: latency-svc-96q9x [206.672097ms]
    May 16 14:14:29.566: INFO: Created: latency-svc-xmwxr
    May 16 14:14:29.577: INFO: Got endpoints: latency-svc-xmwxr [211.822757ms]
    May 16 14:14:29.581: INFO: Created: latency-svc-7ftj7
    May 16 14:14:29.590: INFO: Created: latency-svc-k7rmm
    May 16 14:14:29.590: INFO: Got endpoints: latency-svc-7ftj7 [215.05686ms]
    May 16 14:14:29.597: INFO: Got endpoints: latency-svc-k7rmm [204.467306ms]
    May 16 14:14:29.608: INFO: Created: latency-svc-p9n7k
    May 16 14:14:29.617: INFO: Created: latency-svc-q8w9c
    May 16 14:14:29.618: INFO: Got endpoints: latency-svc-p9n7k [215.595642ms]
    May 16 14:14:29.631: INFO: Created: latency-svc-hnrl8
    May 16 14:14:29.632: INFO: Got endpoints: latency-svc-q8w9c [219.934935ms]
    May 16 14:14:29.645: INFO: Created: latency-svc-gp4qs
    May 16 14:14:29.645: INFO: Got endpoints: latency-svc-hnrl8 [214.347891ms]
    May 16 14:14:29.651: INFO: Got endpoints: latency-svc-gp4qs [197.561625ms]
    May 16 14:14:29.658: INFO: Created: latency-svc-44p9w
    May 16 14:14:29.661: INFO: Created: latency-svc-6b8fd
    May 16 14:14:29.666: INFO: Got endpoints: latency-svc-44p9w [206.981699ms]
    May 16 14:14:29.675: INFO: Created: latency-svc-mhbnh
    May 16 14:14:29.676: INFO: Got endpoints: latency-svc-6b8fd [206.416131ms]
    May 16 14:14:29.685: INFO: Got endpoints: latency-svc-mhbnh [204.319536ms]
    May 16 14:14:29.686: INFO: Created: latency-svc-knw9q
    May 16 14:14:29.693: INFO: Got endpoints: latency-svc-knw9q [178.961175ms]
    May 16 14:14:29.700: INFO: Created: latency-svc-92wcs
    May 16 14:14:29.711: INFO: Created: latency-svc-5klhw
    May 16 14:14:29.712: INFO: Got endpoints: latency-svc-92wcs [194.096819ms]
    May 16 14:14:29.723: INFO: Got endpoints: latency-svc-5klhw [185.433575ms]
    May 16 14:14:29.727: INFO: Created: latency-svc-49dvr
    May 16 14:14:29.736: INFO: Got endpoints: latency-svc-49dvr [194.954379ms]
    May 16 14:14:29.737: INFO: Created: latency-svc-bsjd7
    May 16 14:14:29.749: INFO: Got endpoints: latency-svc-bsjd7 [184.34442ms]
    May 16 14:14:29.757: INFO: Created: latency-svc-r9x47
    May 16 14:14:29.769: INFO: Got endpoints: latency-svc-r9x47 [192.186724ms]
    May 16 14:14:29.769: INFO: Created: latency-svc-5nvgd
    May 16 14:14:29.777: INFO: Got endpoints: latency-svc-5nvgd [187.175313ms]
    May 16 14:14:29.783: INFO: Created: latency-svc-9htfc
    May 16 14:14:29.786: INFO: Created: latency-svc-w2wfz
    May 16 14:14:29.795: INFO: Got endpoints: latency-svc-9htfc [197.486858ms]
    May 16 14:14:29.802: INFO: Got endpoints: latency-svc-w2wfz [184.626522ms]
    May 16 14:14:29.803: INFO: Created: latency-svc-spmqq
    May 16 14:14:29.823: INFO: Created: latency-svc-ks5vx
    May 16 14:14:29.823: INFO: Got endpoints: latency-svc-spmqq [190.900965ms]
    May 16 14:14:29.834: INFO: Created: latency-svc-vql2q
    May 16 14:14:29.834: INFO: Got endpoints: latency-svc-ks5vx [189.063339ms]
    May 16 14:14:29.846: INFO: Got endpoints: latency-svc-vql2q [195.607626ms]
    May 16 14:14:29.949: INFO: Created: latency-svc-h7s7b
    May 16 14:14:29.950: INFO: Created: latency-svc-m29f2
    May 16 14:14:29.951: INFO: Created: latency-svc-fk4xm
    May 16 14:14:29.951: INFO: Created: latency-svc-l5xsp
    May 16 14:14:29.951: INFO: Created: latency-svc-n4cgp
    May 16 14:14:29.951: INFO: Created: latency-svc-hckmp
    May 16 14:14:29.951: INFO: Created: latency-svc-8l544
    May 16 14:14:29.951: INFO: Created: latency-svc-ghr7c
    May 16 14:14:29.951: INFO: Created: latency-svc-8gmrx
    May 16 14:14:29.951: INFO: Created: latency-svc-mf6ph
    May 16 14:14:29.951: INFO: Created: latency-svc-2f2pj
    May 16 14:14:29.953: INFO: Created: latency-svc-mmkt4
    May 16 14:14:29.958: INFO: Created: latency-svc-ftclz
    May 16 14:14:29.958: INFO: Created: latency-svc-j9srt
    May 16 14:14:29.958: INFO: Created: latency-svc-tbwg4
    May 16 14:14:29.961: INFO: Got endpoints: latency-svc-ghr7c [276.095102ms]
    May 16 14:14:29.968: INFO: Got endpoints: latency-svc-m29f2 [301.092284ms]
    May 16 14:14:29.971: INFO: Got endpoints: latency-svc-l5xsp [175.9504ms]
    May 16 14:14:29.971: INFO: Got endpoints: latency-svc-h7s7b [124.794885ms]
    May 16 14:14:29.972: INFO: Got endpoints: latency-svc-8l544 [296.002276ms]
    May 16 14:14:29.976: INFO: Got endpoints: latency-svc-mf6ph [199.090616ms]
    May 16 14:14:29.988: INFO: Got endpoints: latency-svc-hckmp [238.603346ms]
    May 16 14:14:29.992: INFO: Got endpoints: latency-svc-8gmrx [269.55504ms]
    May 16 14:14:29.999: INFO: Got endpoints: latency-svc-mmkt4 [175.92047ms]
    May 16 14:14:29.999: INFO: Got endpoints: latency-svc-tbwg4 [230.366052ms]
    May 16 14:14:29.999: INFO: Got endpoints: latency-svc-fk4xm [287.430505ms]
    May 16 14:14:29.999: INFO: Got endpoints: latency-svc-n4cgp [262.833207ms]
    May 16 14:14:30.004: INFO: Got endpoints: latency-svc-2f2pj [201.744349ms]
    May 16 14:14:30.014: INFO: Created: latency-svc-fbl7f
    May 16 14:14:30.015: INFO: Got endpoints: latency-svc-ftclz [322.395682ms]
    May 16 14:14:30.018: INFO: Created: latency-svc-k4c5k
    May 16 14:14:30.021: INFO: Got endpoints: latency-svc-j9srt [187.566731ms]
    May 16 14:14:30.024: INFO: Got endpoints: latency-svc-fbl7f [62.923462ms]
    May 16 14:14:30.026: INFO: Got endpoints: latency-svc-k4c5k [58.860096ms]
    May 16 14:14:30.054: INFO: Created: latency-svc-x5cpg
    May 16 14:14:30.055: INFO: Created: latency-svc-slxzv
    May 16 14:14:30.061: INFO: Got endpoints: latency-svc-x5cpg [89.861351ms]
    May 16 14:14:30.070: INFO: Created: latency-svc-plj96
    May 16 14:14:30.076: INFO: Got endpoints: latency-svc-slxzv [105.104812ms]
    May 16 14:14:30.082: INFO: Created: latency-svc-wjmms
    May 16 14:14:30.092: INFO: Got endpoints: latency-svc-plj96 [119.965951ms]
    May 16 14:14:30.093: INFO: Got endpoints: latency-svc-wjmms [116.232728ms]
    May 16 14:14:30.114: INFO: Created: latency-svc-h8kgs
    May 16 14:14:30.123: INFO: Created: latency-svc-rkfdr
    May 16 14:14:30.124: INFO: Got endpoints: latency-svc-h8kgs [131.311221ms]
    May 16 14:14:30.138: INFO: Got endpoints: latency-svc-rkfdr [150.870082ms]
    May 16 14:14:30.142: INFO: Created: latency-svc-vh2h7
    May 16 14:14:30.150: INFO: Got endpoints: latency-svc-vh2h7 [151.013446ms]
    May 16 14:14:30.155: INFO: Created: latency-svc-wbqbj
    May 16 14:14:30.168: INFO: Got endpoints: latency-svc-wbqbj [168.179516ms]
    May 16 14:14:30.170: INFO: Created: latency-svc-8v67q
    May 16 14:14:30.182: INFO: Got endpoints: latency-svc-8v67q [182.380128ms]
    May 16 14:14:30.182: INFO: Created: latency-svc-xkw42
    May 16 14:14:30.187: INFO: Got endpoints: latency-svc-xkw42 [188.412145ms]
    May 16 14:14:30.197: INFO: Created: latency-svc-2kz72
    May 16 14:14:30.207: INFO: Got endpoints: latency-svc-2kz72 [202.698632ms]
    May 16 14:14:30.209: INFO: Created: latency-svc-k9dt7
    May 16 14:14:30.214: INFO: Got endpoints: latency-svc-k9dt7 [199.198983ms]
    May 16 14:14:30.227: INFO: Created: latency-svc-t42rl
    May 16 14:14:30.236: INFO: Created: latency-svc-rf4d4
    May 16 14:14:30.236: INFO: Got endpoints: latency-svc-t42rl [214.428707ms]
    May 16 14:14:30.252: INFO: Created: latency-svc-xcn7q
    May 16 14:14:30.267: INFO: Got endpoints: latency-svc-rf4d4 [243.053555ms]
    May 16 14:14:30.273: INFO: Created: latency-svc-kfjwz
    May 16 14:14:30.274: INFO: Got endpoints: latency-svc-xcn7q [247.233485ms]
    May 16 14:14:30.280: INFO: Got endpoints: latency-svc-kfjwz [219.510757ms]
    May 16 14:14:30.292: INFO: Created: latency-svc-bhsxq
    May 16 14:14:30.299: INFO: Created: latency-svc-qdb97
    May 16 14:14:30.302: INFO: Got endpoints: latency-svc-bhsxq [225.960241ms]
    May 16 14:14:30.307: INFO: Got endpoints: latency-svc-qdb97 [215.752239ms]
    May 16 14:14:30.312: INFO: Created: latency-svc-fv25w
    May 16 14:14:30.321: INFO: Got endpoints: latency-svc-fv25w [228.862873ms]
    May 16 14:14:30.327: INFO: Created: latency-svc-n4dmh
    May 16 14:14:30.341: INFO: Got endpoints: latency-svc-n4dmh [217.205102ms]
    May 16 14:14:30.342: INFO: Created: latency-svc-nhnqs
    May 16 14:14:30.345: INFO: Created: latency-svc-k9g5n
    May 16 14:14:30.351: INFO: Got endpoints: latency-svc-nhnqs [212.380665ms]
    May 16 14:14:30.363: INFO: Got endpoints: latency-svc-k9g5n [212.878676ms]
    May 16 14:14:30.368: INFO: Created: latency-svc-znl7p
    May 16 14:14:30.372: INFO: Got endpoints: latency-svc-znl7p [204.257466ms]
    May 16 14:14:30.376: INFO: Created: latency-svc-mhpz7
    May 16 14:14:30.388: INFO: Got endpoints: latency-svc-mhpz7 [206.213184ms]
    May 16 14:14:30.395: INFO: Created: latency-svc-vxzbv
    May 16 14:14:30.400: INFO: Created: latency-svc-dmrlt
    May 16 14:14:30.401: INFO: Got endpoints: latency-svc-vxzbv [213.284177ms]
    May 16 14:14:30.417: INFO: Got endpoints: latency-svc-dmrlt [210.658676ms]
    May 16 14:14:30.421: INFO: Created: latency-svc-qp47c
    May 16 14:14:30.430: INFO: Got endpoints: latency-svc-qp47c [216.02633ms]
    May 16 14:14:30.433: INFO: Created: latency-svc-dr7tc
    May 16 14:14:30.439: INFO: Got endpoints: latency-svc-dr7tc [202.944585ms]
    May 16 14:14:30.442: INFO: Created: latency-svc-s7fdq
    May 16 14:14:30.451: INFO: Created: latency-svc-zdwz8
    May 16 14:14:30.452: INFO: Got endpoints: latency-svc-s7fdq [185.381328ms]
    May 16 14:14:30.461: INFO: Got endpoints: latency-svc-zdwz8 [187.794818ms]
    May 16 14:14:30.466: INFO: Created: latency-svc-jcvts
    May 16 14:14:30.476: INFO: Got endpoints: latency-svc-jcvts [195.545604ms]
    May 16 14:14:30.478: INFO: Created: latency-svc-p66zq
    May 16 14:14:30.487: INFO: Got endpoints: latency-svc-p66zq [184.78148ms]
    May 16 14:14:30.492: INFO: Created: latency-svc-wfxp8
    May 16 14:14:30.495: INFO: Created: latency-svc-qgk5l
    May 16 14:14:30.502: INFO: Got endpoints: latency-svc-wfxp8 [194.296332ms]
    May 16 14:14:30.511: INFO: Got endpoints: latency-svc-qgk5l [189.479416ms]
    May 16 14:14:30.512: INFO: Created: latency-svc-gphvj
    May 16 14:14:30.522: INFO: Got endpoints: latency-svc-gphvj [180.867417ms]
    May 16 14:14:30.526: INFO: Created: latency-svc-64ww6
    May 16 14:14:30.533: INFO: Got endpoints: latency-svc-64ww6 [182.644367ms]
    May 16 14:14:30.540: INFO: Created: latency-svc-6n942
    May 16 14:14:30.541: INFO: Created: latency-svc-zb2fq
    May 16 14:14:30.547: INFO: Got endpoints: latency-svc-6n942 [183.255026ms]
    May 16 14:14:30.549: INFO: Got endpoints: latency-svc-zb2fq [177.431254ms]
    May 16 14:14:30.549: INFO: Latencies: [34.190162ms 45.505247ms 54.228832ms 58.860096ms 62.923462ms 76.34595ms 83.999025ms 89.861351ms 92.6449ms 105.104812ms 116.232728ms 119.965951ms 124.794885ms 131.311221ms 137.517621ms 137.948584ms 150.870082ms 151.013446ms 159.023234ms 160.787193ms 161.761466ms 163.363047ms 163.479955ms 164.149038ms 164.172691ms 164.217276ms 165.396534ms 165.865605ms 165.946131ms 166.014008ms 166.275805ms 166.861677ms 167.362847ms 168.179516ms 168.191472ms 169.160143ms 169.835387ms 170.040394ms 170.26423ms 170.317111ms 171.253428ms 174.099449ms 174.713313ms 175.040425ms 175.12411ms 175.736177ms 175.819988ms 175.92047ms 175.9504ms 176.132237ms 177.142794ms 177.431254ms 178.04601ms 178.13453ms 178.561304ms 178.749829ms 178.961175ms 179.220802ms 179.453563ms 179.4972ms 179.684027ms 180.066018ms 180.867417ms 181.440122ms 182.2098ms 182.219692ms 182.380128ms 182.631294ms 182.644367ms 182.846313ms 183.145038ms 183.255026ms 183.401485ms 183.427974ms 183.450014ms 184.34442ms 184.470274ms 184.626522ms 184.78148ms 185.381328ms 185.433575ms 185.48749ms 186.040665ms 186.583329ms 187.097432ms 187.175313ms 187.297259ms 187.559453ms 187.566731ms 187.794818ms 188.200385ms 188.412145ms 188.617227ms 188.869491ms 188.959426ms 189.063339ms 189.479416ms 190.445393ms 190.900965ms 191.101306ms 191.517846ms 191.871869ms 192.017035ms 192.182337ms 192.186724ms 193.943754ms 194.096819ms 194.105347ms 194.296332ms 194.481119ms 194.54443ms 194.793621ms 194.954379ms 195.545604ms 195.597108ms 195.607626ms 195.66951ms 196.693475ms 197.284905ms 197.486858ms 197.561625ms 198.372921ms 198.670791ms 198.675097ms 199.090616ms 199.198983ms 199.683133ms 200.64554ms 200.732975ms 201.466007ms 201.744349ms 202.196603ms 202.698632ms 202.702646ms 202.944585ms 203.854199ms 204.257466ms 204.319536ms 204.344728ms 204.467306ms 204.603211ms 205.64168ms 206.213184ms 206.409201ms 206.416131ms 206.672097ms 206.981699ms 208.059896ms 209.303617ms 209.47279ms 210.395743ms 210.658676ms 211.072221ms 211.79348ms 211.822757ms 212.380665ms 212.420295ms 212.45657ms 212.878676ms 213.284177ms 214.347891ms 214.428707ms 215.05686ms 215.595642ms 215.752239ms 215.948839ms 216.02633ms 217.205102ms 217.954308ms 218.140963ms 218.285945ms 219.510757ms 219.934935ms 220.000716ms 220.09521ms 221.38858ms 222.51776ms 222.77513ms 223.432456ms 223.535609ms 225.425848ms 225.960241ms 227.128304ms 228.862873ms 229.789921ms 230.366052ms 230.671002ms 231.42395ms 231.717967ms 238.603346ms 239.39366ms 243.053555ms 247.233485ms 262.833207ms 269.55504ms 276.095102ms 287.430505ms 296.002276ms 301.092284ms 322.395682ms]
    May 16 14:14:30.549: INFO: 50 %ile: 191.517846ms
    May 16 14:14:30.549: INFO: 90 %ile: 225.425848ms
    May 16 14:14:30.549: INFO: 99 %ile: 301.092284ms
    May 16 14:14:30.549: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    May 16 14:14:30.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-3607" for this suite. 05/16/23 14:14:30.565
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:14:30.572
May 16 14:14:30.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename deployment 05/16/23 14:14:30.573
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:14:30.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:14:30.606
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 05/16/23 14:14:30.622
STEP: waiting for Deployment to be created 05/16/23 14:14:30.632
STEP: waiting for all Replicas to be Ready 05/16/23 14:14:30.636
May 16 14:14:30.638: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 16 14:14:30.638: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 16 14:14:30.656: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 16 14:14:30.656: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 16 14:14:30.667: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 16 14:14:30.667: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 16 14:14:30.718: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 16 14:14:30.718: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 16 14:14:31.552: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 16 14:14:31.552: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 16 14:14:31.634: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 05/16/23 14:14:31.634
W0516 14:14:31.645547      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May 16 14:14:31.647: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 05/16/23 14:14:31.647
May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0
May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0
May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0
May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0
May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0
May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0
May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0
May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0
May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
May 16 14:14:31.659: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
May 16 14:14:31.659: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
May 16 14:14:31.675: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
May 16 14:14:31.675: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
May 16 14:14:31.696: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
May 16 14:14:31.696: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
May 16 14:14:31.710: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
May 16 14:14:31.710: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
May 16 14:14:34.055: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
May 16 14:14:34.055: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
May 16 14:14:34.085: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
STEP: listing Deployments 05/16/23 14:14:34.085
May 16 14:14:34.091: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 05/16/23 14:14:34.091
May 16 14:14:34.103: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 05/16/23 14:14:34.103
May 16 14:14:34.110: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 16 14:14:34.115: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 16 14:14:34.136: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 16 14:14:34.157: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 16 14:14:34.174: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 16 14:14:34.188: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 16 14:14:35.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 16 14:14:38.085: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
May 16 14:14:38.148: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 16 14:14:38.166: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 16 14:14:43.629: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 05/16/23 14:14:43.668
STEP: fetching the DeploymentStatus 05/16/23 14:14:43.685
May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 3
May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 3
STEP: deleting the Deployment 05/16/23 14:14:43.691
May 16 14:14:43.711: INFO: observed event type MODIFIED
May 16 14:14:43.711: INFO: observed event type MODIFIED
May 16 14:14:43.711: INFO: observed event type MODIFIED
May 16 14:14:43.711: INFO: observed event type MODIFIED
May 16 14:14:43.711: INFO: observed event type MODIFIED
May 16 14:14:43.712: INFO: observed event type MODIFIED
May 16 14:14:43.712: INFO: observed event type MODIFIED
May 16 14:14:43.712: INFO: observed event type MODIFIED
May 16 14:14:43.712: INFO: observed event type MODIFIED
May 16 14:14:43.712: INFO: observed event type MODIFIED
May 16 14:14:43.712: INFO: observed event type MODIFIED
May 16 14:14:43.712: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 16 14:14:43.718: INFO: Log out all the ReplicaSets if there is no deployment created
May 16 14:14:43.724: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-5102  4d5384e0-4c3c-4beb-8f1a-4ed956df1aa0 37859 2 2023-05-16 14:14:34 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 5edf8a21-72e5-4cc4-9ea7-2254144c84e9 0xc0041989a7 0xc0041989a8}] [] [{kube-controller-manager Update apps/v1 2023-05-16 14:14:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5edf8a21-72e5-4cc4-9ea7-2254144c84e9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 14:14:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004198a30 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

May 16 14:14:43.731: INFO: pod: "test-deployment-7b7876f9d6-44pq9":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-44pq9 test-deployment-7b7876f9d6- deployment-5102  e76bc77f-76a6-462a-9413-423f2940467b 37858 0 2023-05-16 14:14:38 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.20/23"],"mac_address":"0a:58:0a:81:02:14","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.20/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.20"
    ],
    "mac": "0a:58:0a:81:02:14",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 4d5384e0-4c3c-4beb-8f1a-4ed956df1aa0 0xc00408a2b7 0xc00408a2b8}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:14:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:14:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d5384e0-4c3c-4beb-8f1a-4ed956df1aa0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-05-16 14:14:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-05-16 14:14:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v46pf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v46pf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-212-246.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c30,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9wl24,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.212.246,PodIP:10.129.2.20,StartTime:2023-05-16 14:14:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:14:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://a98ee9d8f5f8f2963db39051a669a8274c4d640b516e4df74f2ba75cc788fd52,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 16 14:14:43.731: INFO: pod: "test-deployment-7b7876f9d6-85gs2":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-85gs2 test-deployment-7b7876f9d6- deployment-5102  a381888c-90e4-46dc-8df0-1d94bdfbd398 37319 0 2023-05-16 14:14:34 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.20/23"],"mac_address":"0a:58:0a:80:02:14","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.20/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.20"
    ],
    "mac": "0a:58:0a:80:02:14",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 4d5384e0-4c3c-4beb-8f1a-4ed956df1aa0 0xc00408a527 0xc00408a528}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:14:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:14:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d5384e0-4c3c-4beb-8f1a-4ed956df1aa0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-05-16 14:14:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-05-16 14:14:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fv8bd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fv8bd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c30,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9wl24,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.142,PodIP:10.128.2.20,StartTime:2023-05-16 14:14:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:14:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://0d63b76d582ddb75216e8d33405f97d3cc44015c97947dce849ecddd2ca0d490,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 16 14:14:43.731: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-5102  2852f392-1978-427c-8504-c32a47b471a5 37867 4 2023-05-16 14:14:31 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 5edf8a21-72e5-4cc4-9ea7-2254144c84e9 0xc004198a97 0xc004198a98}] [] [{kube-controller-manager Update apps/v1 2023-05-16 14:14:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5edf8a21-72e5-4cc4-9ea7-2254144c84e9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 14:14:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004198b20 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May 16 14:14:43.738: INFO: pod: "test-deployment-7df74c55ff-xlfjx":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-xlfjx test-deployment-7df74c55ff- deployment-5102  1aa4168e-4c72-40ad-888f-1fe9d5ca2de3 37863 0 2023-05-16 14:14:34 +0000 UTC 2023-05-16 14:14:44 +0000 UTC 0xc004198ea8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.30/23"],"mac_address":"0a:58:0a:83:00:1e","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.30/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.30"
    ],
    "mac": "0a:58:0a:83:00:1e",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 2852f392-1978-427c-8504-c32a47b471a5 0xc004198ee7 0xc004198ee8}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:14:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:14:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2852f392-1978-427c-8504-c32a47b471a5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-05-16 14:14:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-05-16 14:14:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vj7vg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vj7vg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c30,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9wl24,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:10.131.0.30,StartTime:2023-05-16 14:14:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:14:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://5136cb7922b026e046b454dd9beeedd1094d0cfd0ab2f3677e3948da375beca7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 16 14:14:43.738: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-5102  2b90b6c4-2372-4a54-9e1d-707c0253e427 36898 3 2023-05-16 14:14:30 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 5edf8a21-72e5-4cc4-9ea7-2254144c84e9 0xc004198b87 0xc004198b88}] [] [{kube-controller-manager Update apps/v1 2023-05-16 14:14:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5edf8a21-72e5-4cc4-9ea7-2254144c84e9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 14:14:34 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004198c10 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May 16 14:14:43.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5102" for this suite. 05/16/23 14:14:43.746
------------------------------
â€¢ [SLOW TEST] [13.184 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:14:30.572
    May 16 14:14:30.572: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename deployment 05/16/23 14:14:30.573
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:14:30.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:14:30.606
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 05/16/23 14:14:30.622
    STEP: waiting for Deployment to be created 05/16/23 14:14:30.632
    STEP: waiting for all Replicas to be Ready 05/16/23 14:14:30.636
    May 16 14:14:30.638: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 16 14:14:30.638: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 16 14:14:30.656: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 16 14:14:30.656: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 16 14:14:30.667: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 16 14:14:30.667: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 16 14:14:30.718: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 16 14:14:30.718: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    May 16 14:14:31.552: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    May 16 14:14:31.552: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    May 16 14:14:31.634: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 05/16/23 14:14:31.634
    W0516 14:14:31.645547      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May 16 14:14:31.647: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 05/16/23 14:14:31.647
    May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0
    May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0
    May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0
    May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0
    May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0
    May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0
    May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0
    May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 0
    May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
    May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
    May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
    May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
    May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
    May 16 14:14:31.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
    May 16 14:14:31.659: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
    May 16 14:14:31.659: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
    May 16 14:14:31.675: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
    May 16 14:14:31.675: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
    May 16 14:14:31.696: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
    May 16 14:14:31.696: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
    May 16 14:14:31.710: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
    May 16 14:14:31.710: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
    May 16 14:14:34.055: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
    May 16 14:14:34.055: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
    May 16 14:14:34.085: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
    STEP: listing Deployments 05/16/23 14:14:34.085
    May 16 14:14:34.091: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 05/16/23 14:14:34.091
    May 16 14:14:34.103: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 05/16/23 14:14:34.103
    May 16 14:14:34.110: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May 16 14:14:34.115: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May 16 14:14:34.136: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May 16 14:14:34.157: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May 16 14:14:34.174: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May 16 14:14:34.188: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    May 16 14:14:35.648: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May 16 14:14:38.085: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    May 16 14:14:38.148: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May 16 14:14:38.166: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    May 16 14:14:43.629: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 05/16/23 14:14:43.668
    STEP: fetching the DeploymentStatus 05/16/23 14:14:43.685
    May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
    May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
    May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
    May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
    May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
    May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 1
    May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
    May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 3
    May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
    May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 2
    May 16 14:14:43.691: INFO: observed Deployment test-deployment in namespace deployment-5102 with ReadyReplicas 3
    STEP: deleting the Deployment 05/16/23 14:14:43.691
    May 16 14:14:43.711: INFO: observed event type MODIFIED
    May 16 14:14:43.711: INFO: observed event type MODIFIED
    May 16 14:14:43.711: INFO: observed event type MODIFIED
    May 16 14:14:43.711: INFO: observed event type MODIFIED
    May 16 14:14:43.711: INFO: observed event type MODIFIED
    May 16 14:14:43.712: INFO: observed event type MODIFIED
    May 16 14:14:43.712: INFO: observed event type MODIFIED
    May 16 14:14:43.712: INFO: observed event type MODIFIED
    May 16 14:14:43.712: INFO: observed event type MODIFIED
    May 16 14:14:43.712: INFO: observed event type MODIFIED
    May 16 14:14:43.712: INFO: observed event type MODIFIED
    May 16 14:14:43.712: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 16 14:14:43.718: INFO: Log out all the ReplicaSets if there is no deployment created
    May 16 14:14:43.724: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-5102  4d5384e0-4c3c-4beb-8f1a-4ed956df1aa0 37859 2 2023-05-16 14:14:34 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 5edf8a21-72e5-4cc4-9ea7-2254144c84e9 0xc0041989a7 0xc0041989a8}] [] [{kube-controller-manager Update apps/v1 2023-05-16 14:14:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5edf8a21-72e5-4cc4-9ea7-2254144c84e9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 14:14:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004198a30 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    May 16 14:14:43.731: INFO: pod: "test-deployment-7b7876f9d6-44pq9":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-44pq9 test-deployment-7b7876f9d6- deployment-5102  e76bc77f-76a6-462a-9413-423f2940467b 37858 0 2023-05-16 14:14:38 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.20/23"],"mac_address":"0a:58:0a:81:02:14","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.20/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.20"
        ],
        "mac": "0a:58:0a:81:02:14",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 4d5384e0-4c3c-4beb-8f1a-4ed956df1aa0 0xc00408a2b7 0xc00408a2b8}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:14:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:14:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d5384e0-4c3c-4beb-8f1a-4ed956df1aa0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-05-16 14:14:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-05-16 14:14:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v46pf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v46pf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-212-246.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c30,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9wl24,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.212.246,PodIP:10.129.2.20,StartTime:2023-05-16 14:14:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:14:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://a98ee9d8f5f8f2963db39051a669a8274c4d640b516e4df74f2ba75cc788fd52,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May 16 14:14:43.731: INFO: pod: "test-deployment-7b7876f9d6-85gs2":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-85gs2 test-deployment-7b7876f9d6- deployment-5102  a381888c-90e4-46dc-8df0-1d94bdfbd398 37319 0 2023-05-16 14:14:34 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.20/23"],"mac_address":"0a:58:0a:80:02:14","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.20/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.20"
        ],
        "mac": "0a:58:0a:80:02:14",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 4d5384e0-4c3c-4beb-8f1a-4ed956df1aa0 0xc00408a527 0xc00408a528}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:14:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:14:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d5384e0-4c3c-4beb-8f1a-4ed956df1aa0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-05-16 14:14:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-05-16 14:14:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fv8bd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fv8bd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c30,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9wl24,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.142,PodIP:10.128.2.20,StartTime:2023-05-16 14:14:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:14:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://0d63b76d582ddb75216e8d33405f97d3cc44015c97947dce849ecddd2ca0d490,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May 16 14:14:43.731: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-5102  2852f392-1978-427c-8504-c32a47b471a5 37867 4 2023-05-16 14:14:31 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 5edf8a21-72e5-4cc4-9ea7-2254144c84e9 0xc004198a97 0xc004198a98}] [] [{kube-controller-manager Update apps/v1 2023-05-16 14:14:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5edf8a21-72e5-4cc4-9ea7-2254144c84e9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 14:14:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004198b20 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    May 16 14:14:43.738: INFO: pod: "test-deployment-7df74c55ff-xlfjx":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-xlfjx test-deployment-7df74c55ff- deployment-5102  1aa4168e-4c72-40ad-888f-1fe9d5ca2de3 37863 0 2023-05-16 14:14:34 +0000 UTC 2023-05-16 14:14:44 +0000 UTC 0xc004198ea8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.30/23"],"mac_address":"0a:58:0a:83:00:1e","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.30/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.30"
        ],
        "mac": "0a:58:0a:83:00:1e",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 2852f392-1978-427c-8504-c32a47b471a5 0xc004198ee7 0xc004198ee8}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:14:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:14:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2852f392-1978-427c-8504-c32a47b471a5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-05-16 14:14:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-05-16 14:14:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vj7vg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vj7vg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c30,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9wl24,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:14:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:10.131.0.30,StartTime:2023-05-16 14:14:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:14:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://5136cb7922b026e046b454dd9beeedd1094d0cfd0ab2f3677e3948da375beca7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    May 16 14:14:43.738: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-5102  2b90b6c4-2372-4a54-9e1d-707c0253e427 36898 3 2023-05-16 14:14:30 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 5edf8a21-72e5-4cc4-9ea7-2254144c84e9 0xc004198b87 0xc004198b88}] [] [{kube-controller-manager Update apps/v1 2023-05-16 14:14:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5edf8a21-72e5-4cc4-9ea7-2254144c84e9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 14:14:34 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004198c10 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May 16 14:14:43.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5102" for this suite. 05/16/23 14:14:43.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:14:43.757
May 16 14:14:43.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename security-context-test 05/16/23 14:14:43.758
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:14:43.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:14:43.792
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
May 16 14:14:43.836: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-35c92be6-2eae-426e-8a0d-10b55130cc1c" in namespace "security-context-test-3226" to be "Succeeded or Failed"
May 16 14:14:43.859: INFO: Pod "busybox-readonly-false-35c92be6-2eae-426e-8a0d-10b55130cc1c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.459221ms
May 16 14:14:45.863: INFO: Pod "busybox-readonly-false-35c92be6-2eae-426e-8a0d-10b55130cc1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027383701s
May 16 14:14:47.865: INFO: Pod "busybox-readonly-false-35c92be6-2eae-426e-8a0d-10b55130cc1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028393582s
May 16 14:14:47.865: INFO: Pod "busybox-readonly-false-35c92be6-2eae-426e-8a0d-10b55130cc1c" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May 16 14:14:47.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3226" for this suite. 05/16/23 14:14:47.869
------------------------------
â€¢ [4.118 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:14:43.757
    May 16 14:14:43.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename security-context-test 05/16/23 14:14:43.758
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:14:43.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:14:43.792
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    May 16 14:14:43.836: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-35c92be6-2eae-426e-8a0d-10b55130cc1c" in namespace "security-context-test-3226" to be "Succeeded or Failed"
    May 16 14:14:43.859: INFO: Pod "busybox-readonly-false-35c92be6-2eae-426e-8a0d-10b55130cc1c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.459221ms
    May 16 14:14:45.863: INFO: Pod "busybox-readonly-false-35c92be6-2eae-426e-8a0d-10b55130cc1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027383701s
    May 16 14:14:47.865: INFO: Pod "busybox-readonly-false-35c92be6-2eae-426e-8a0d-10b55130cc1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028393582s
    May 16 14:14:47.865: INFO: Pod "busybox-readonly-false-35c92be6-2eae-426e-8a0d-10b55130cc1c" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May 16 14:14:47.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3226" for this suite. 05/16/23 14:14:47.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:14:47.876
May 16 14:14:47.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubectl 05/16/23 14:14:47.876
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:14:47.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:14:47.898
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/16/23 14:14:47.901
May 16 14:14:47.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6364 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May 16 14:14:47.965: INFO: stderr: ""
May 16 14:14:47.965: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 05/16/23 14:14:47.965
STEP: verifying the pod e2e-test-httpd-pod was created 05/16/23 14:14:53.018
May 16 14:14:53.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6364 get pod e2e-test-httpd-pod -o json'
May 16 14:14:53.066: INFO: stderr: ""
May 16 14:14:53.066: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.ovn.org/pod-networks\": \"{\\\"default\\\":{\\\"ip_addresses\\\":[\\\"10.131.0.32/23\\\"],\\\"mac_address\\\":\\\"0a:58:0a:83:00:20\\\",\\\"gateway_ips\\\":[\\\"10.131.0.1\\\"],\\\"ip_address\\\":\\\"10.131.0.32/23\\\",\\\"gateway_ip\\\":\\\"10.131.0.1\\\"}}\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"ovn-kubernetes\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.131.0.32\\\"\\n    ],\\n    \\\"mac\\\": \\\"0a:58:0a:83:00:20\\\",\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-05-16T14:14:47Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6364\",\n        \"resourceVersion\": \"38080\",\n        \"uid\": \"edcf2b6f-9844-4133-8d27-209a5637b879\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-lbf8p\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-161-164.eu-central-1.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c30,c10\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-lbf8p\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-16T14:14:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-16T14:14:49Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-16T14:14:49Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-16T14:14:47Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://6c0ac0eac879e8ac370047ef45d62ce4caad85dba89c0571b213be9bbcc83691\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-16T14:14:48Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.161.164\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.131.0.32\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.131.0.32\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-16T14:14:47Z\"\n    }\n}\n"
STEP: replace the image in the pod 05/16/23 14:14:53.066
May 16 14:14:53.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6364 replace -f -'
May 16 14:14:54.670: INFO: stderr: ""
May 16 14:14:54.670: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 05/16/23 14:14:54.67
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
May 16 14:14:54.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6364 delete pods e2e-test-httpd-pod'
May 16 14:14:56.695: INFO: stderr: ""
May 16 14:14:56.695: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 16 14:14:56.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6364" for this suite. 05/16/23 14:14:56.702
------------------------------
â€¢ [SLOW TEST] [8.834 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:14:47.876
    May 16 14:14:47.876: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubectl 05/16/23 14:14:47.876
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:14:47.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:14:47.898
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/16/23 14:14:47.901
    May 16 14:14:47.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6364 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    May 16 14:14:47.965: INFO: stderr: ""
    May 16 14:14:47.965: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 05/16/23 14:14:47.965
    STEP: verifying the pod e2e-test-httpd-pod was created 05/16/23 14:14:53.018
    May 16 14:14:53.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6364 get pod e2e-test-httpd-pod -o json'
    May 16 14:14:53.066: INFO: stderr: ""
    May 16 14:14:53.066: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.ovn.org/pod-networks\": \"{\\\"default\\\":{\\\"ip_addresses\\\":[\\\"10.131.0.32/23\\\"],\\\"mac_address\\\":\\\"0a:58:0a:83:00:20\\\",\\\"gateway_ips\\\":[\\\"10.131.0.1\\\"],\\\"ip_address\\\":\\\"10.131.0.32/23\\\",\\\"gateway_ip\\\":\\\"10.131.0.1\\\"}}\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"ovn-kubernetes\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.131.0.32\\\"\\n    ],\\n    \\\"mac\\\": \\\"0a:58:0a:83:00:20\\\",\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-05-16T14:14:47Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6364\",\n        \"resourceVersion\": \"38080\",\n        \"uid\": \"edcf2b6f-9844-4133-8d27-209a5637b879\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-lbf8p\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-161-164.eu-central-1.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c30,c10\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-lbf8p\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-16T14:14:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-16T14:14:49Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-16T14:14:49Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-16T14:14:47Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://6c0ac0eac879e8ac370047ef45d62ce4caad85dba89c0571b213be9bbcc83691\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-16T14:14:48Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.161.164\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.131.0.32\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.131.0.32\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-16T14:14:47Z\"\n    }\n}\n"
    STEP: replace the image in the pod 05/16/23 14:14:53.066
    May 16 14:14:53.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6364 replace -f -'
    May 16 14:14:54.670: INFO: stderr: ""
    May 16 14:14:54.670: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 05/16/23 14:14:54.67
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    May 16 14:14:54.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6364 delete pods e2e-test-httpd-pod'
    May 16 14:14:56.695: INFO: stderr: ""
    May 16 14:14:56.695: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 16 14:14:56.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6364" for this suite. 05/16/23 14:14:56.702
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:14:56.71
May 16 14:14:56.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename services 05/16/23 14:14:56.71
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:14:56.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:14:56.741
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 05/16/23 14:14:56.76
STEP: waiting for available Endpoint 05/16/23 14:14:56.765
STEP: listing all Endpoints 05/16/23 14:14:56.767
STEP: updating the Endpoint 05/16/23 14:14:56.771
STEP: fetching the Endpoint 05/16/23 14:14:56.797
STEP: patching the Endpoint 05/16/23 14:14:56.804
STEP: fetching the Endpoint 05/16/23 14:14:56.81
STEP: deleting the Endpoint by Collection 05/16/23 14:14:56.814
STEP: waiting for Endpoint deletion 05/16/23 14:14:56.851
STEP: fetching the Endpoint 05/16/23 14:14:56.853
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 16 14:14:56.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4581" for this suite. 05/16/23 14:14:56.859
------------------------------
â€¢ [0.165 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:14:56.71
    May 16 14:14:56.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename services 05/16/23 14:14:56.71
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:14:56.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:14:56.741
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 05/16/23 14:14:56.76
    STEP: waiting for available Endpoint 05/16/23 14:14:56.765
    STEP: listing all Endpoints 05/16/23 14:14:56.767
    STEP: updating the Endpoint 05/16/23 14:14:56.771
    STEP: fetching the Endpoint 05/16/23 14:14:56.797
    STEP: patching the Endpoint 05/16/23 14:14:56.804
    STEP: fetching the Endpoint 05/16/23 14:14:56.81
    STEP: deleting the Endpoint by Collection 05/16/23 14:14:56.814
    STEP: waiting for Endpoint deletion 05/16/23 14:14:56.851
    STEP: fetching the Endpoint 05/16/23 14:14:56.853
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 16 14:14:56.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4581" for this suite. 05/16/23 14:14:56.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:14:56.875
May 16 14:14:56.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename daemonsets 05/16/23 14:14:56.876
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:14:56.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:14:56.918
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
May 16 14:14:56.984: INFO: Create a RollingUpdate DaemonSet
May 16 14:14:56.991: INFO: Check that daemon pods launch on every node of the cluster
May 16 14:14:56.994: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:14:56.994: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:14:56.994: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:14:57.001: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 14:14:57.001: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 14:14:58.007: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:14:58.007: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:14:58.007: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:14:58.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 14:14:58.010: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 14:14:59.007: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:14:59.007: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:14:59.007: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:14:59.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 16 14:14:59.011: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
May 16 14:14:59.011: INFO: Update the DaemonSet to trigger a rollout
May 16 14:14:59.035: INFO: Updating DaemonSet daemon-set
May 16 14:15:02.060: INFO: Roll back the DaemonSet before rollout is complete
May 16 14:15:02.068: INFO: Updating DaemonSet daemon-set
May 16 14:15:02.068: INFO: Make sure DaemonSet rollback is complete
May 16 14:15:02.071: INFO: Wrong image for pod: daemon-set-pf9gv. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
May 16 14:15:02.071: INFO: Pod daemon-set-pf9gv is not available
May 16 14:15:02.075: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:15:02.075: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:15:02.075: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:15:03.085: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:15:03.085: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:15:03.085: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:15:04.084: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:15:04.084: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:15:04.084: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:15:05.084: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:15:05.084: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:15:05.085: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:15:06.084: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:15:06.084: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:15:06.084: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:15:07.079: INFO: Pod daemon-set-l2rkn is not available
May 16 14:15:07.084: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:15:07.084: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:15:07.084: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/16/23 14:15:07.091
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9508, will wait for the garbage collector to delete the pods 05/16/23 14:15:07.091
May 16 14:15:07.153: INFO: Deleting DaemonSet.extensions daemon-set took: 5.231375ms
May 16 14:15:07.254: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.681006ms
May 16 14:15:10.157: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 14:15:10.157: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 16 14:15:10.161: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38476"},"items":null}

May 16 14:15:10.164: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38476"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 14:15:10.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9508" for this suite. 05/16/23 14:15:10.182
------------------------------
â€¢ [SLOW TEST] [13.313 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:14:56.875
    May 16 14:14:56.875: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename daemonsets 05/16/23 14:14:56.876
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:14:56.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:14:56.918
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    May 16 14:14:56.984: INFO: Create a RollingUpdate DaemonSet
    May 16 14:14:56.991: INFO: Check that daemon pods launch on every node of the cluster
    May 16 14:14:56.994: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:14:56.994: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:14:56.994: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:14:57.001: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 14:14:57.001: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 14:14:58.007: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:14:58.007: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:14:58.007: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:14:58.010: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 14:14:58.010: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 14:14:59.007: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:14:59.007: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:14:59.007: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:14:59.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May 16 14:14:59.011: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    May 16 14:14:59.011: INFO: Update the DaemonSet to trigger a rollout
    May 16 14:14:59.035: INFO: Updating DaemonSet daemon-set
    May 16 14:15:02.060: INFO: Roll back the DaemonSet before rollout is complete
    May 16 14:15:02.068: INFO: Updating DaemonSet daemon-set
    May 16 14:15:02.068: INFO: Make sure DaemonSet rollback is complete
    May 16 14:15:02.071: INFO: Wrong image for pod: daemon-set-pf9gv. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    May 16 14:15:02.071: INFO: Pod daemon-set-pf9gv is not available
    May 16 14:15:02.075: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:15:02.075: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:15:02.075: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:15:03.085: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:15:03.085: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:15:03.085: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:15:04.084: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:15:04.084: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:15:04.084: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:15:05.084: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:15:05.084: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:15:05.085: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:15:06.084: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:15:06.084: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:15:06.084: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:15:07.079: INFO: Pod daemon-set-l2rkn is not available
    May 16 14:15:07.084: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:15:07.084: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:15:07.084: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/16/23 14:15:07.091
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9508, will wait for the garbage collector to delete the pods 05/16/23 14:15:07.091
    May 16 14:15:07.153: INFO: Deleting DaemonSet.extensions daemon-set took: 5.231375ms
    May 16 14:15:07.254: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.681006ms
    May 16 14:15:10.157: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 14:15:10.157: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May 16 14:15:10.161: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38476"},"items":null}

    May 16 14:15:10.164: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38476"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 14:15:10.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9508" for this suite. 05/16/23 14:15:10.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:15:10.189
May 16 14:15:10.189: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename resourcequota 05/16/23 14:15:10.19
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:15:10.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:15:10.21
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 05/16/23 14:15:10.213
STEP: Counting existing ResourceQuota 05/16/23 14:15:16.218
STEP: Creating a ResourceQuota 05/16/23 14:15:21.222
STEP: Ensuring resource quota status is calculated 05/16/23 14:15:21.227
STEP: Creating a Secret 05/16/23 14:15:23.231
STEP: Ensuring resource quota status captures secret creation 05/16/23 14:15:23.241
STEP: Deleting a secret 05/16/23 14:15:25.247
STEP: Ensuring resource quota status released usage 05/16/23 14:15:25.254
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 16 14:15:27.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4039" for this suite. 05/16/23 14:15:27.261
------------------------------
â€¢ [SLOW TEST] [17.077 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:15:10.189
    May 16 14:15:10.189: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename resourcequota 05/16/23 14:15:10.19
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:15:10.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:15:10.21
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 05/16/23 14:15:10.213
    STEP: Counting existing ResourceQuota 05/16/23 14:15:16.218
    STEP: Creating a ResourceQuota 05/16/23 14:15:21.222
    STEP: Ensuring resource quota status is calculated 05/16/23 14:15:21.227
    STEP: Creating a Secret 05/16/23 14:15:23.231
    STEP: Ensuring resource quota status captures secret creation 05/16/23 14:15:23.241
    STEP: Deleting a secret 05/16/23 14:15:25.247
    STEP: Ensuring resource quota status released usage 05/16/23 14:15:25.254
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 16 14:15:27.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4039" for this suite. 05/16/23 14:15:27.261
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:15:27.267
May 16 14:15:27.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 14:15:27.268
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:15:27.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:15:27.29
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 05/16/23 14:15:27.293
May 16 14:15:27.310: INFO: Waiting up to 5m0s for pod "downwardapi-volume-32f138d7-49e8-4bcb-bc77-f6b4e8ee3457" in namespace "projected-3268" to be "Succeeded or Failed"
May 16 14:15:27.316: INFO: Pod "downwardapi-volume-32f138d7-49e8-4bcb-bc77-f6b4e8ee3457": Phase="Pending", Reason="", readiness=false. Elapsed: 5.778139ms
May 16 14:15:29.320: INFO: Pod "downwardapi-volume-32f138d7-49e8-4bcb-bc77-f6b4e8ee3457": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009693263s
May 16 14:15:31.320: INFO: Pod "downwardapi-volume-32f138d7-49e8-4bcb-bc77-f6b4e8ee3457": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010077459s
STEP: Saw pod success 05/16/23 14:15:31.32
May 16 14:15:31.320: INFO: Pod "downwardapi-volume-32f138d7-49e8-4bcb-bc77-f6b4e8ee3457" satisfied condition "Succeeded or Failed"
May 16 14:15:31.323: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-32f138d7-49e8-4bcb-bc77-f6b4e8ee3457 container client-container: <nil>
STEP: delete the pod 05/16/23 14:15:31.333
May 16 14:15:31.345: INFO: Waiting for pod downwardapi-volume-32f138d7-49e8-4bcb-bc77-f6b4e8ee3457 to disappear
May 16 14:15:31.348: INFO: Pod downwardapi-volume-32f138d7-49e8-4bcb-bc77-f6b4e8ee3457 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 16 14:15:31.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3268" for this suite. 05/16/23 14:15:31.354
------------------------------
â€¢ [4.093 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:15:27.267
    May 16 14:15:27.267: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 14:15:27.268
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:15:27.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:15:27.29
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 05/16/23 14:15:27.293
    May 16 14:15:27.310: INFO: Waiting up to 5m0s for pod "downwardapi-volume-32f138d7-49e8-4bcb-bc77-f6b4e8ee3457" in namespace "projected-3268" to be "Succeeded or Failed"
    May 16 14:15:27.316: INFO: Pod "downwardapi-volume-32f138d7-49e8-4bcb-bc77-f6b4e8ee3457": Phase="Pending", Reason="", readiness=false. Elapsed: 5.778139ms
    May 16 14:15:29.320: INFO: Pod "downwardapi-volume-32f138d7-49e8-4bcb-bc77-f6b4e8ee3457": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009693263s
    May 16 14:15:31.320: INFO: Pod "downwardapi-volume-32f138d7-49e8-4bcb-bc77-f6b4e8ee3457": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010077459s
    STEP: Saw pod success 05/16/23 14:15:31.32
    May 16 14:15:31.320: INFO: Pod "downwardapi-volume-32f138d7-49e8-4bcb-bc77-f6b4e8ee3457" satisfied condition "Succeeded or Failed"
    May 16 14:15:31.323: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-32f138d7-49e8-4bcb-bc77-f6b4e8ee3457 container client-container: <nil>
    STEP: delete the pod 05/16/23 14:15:31.333
    May 16 14:15:31.345: INFO: Waiting for pod downwardapi-volume-32f138d7-49e8-4bcb-bc77-f6b4e8ee3457 to disappear
    May 16 14:15:31.348: INFO: Pod downwardapi-volume-32f138d7-49e8-4bcb-bc77-f6b4e8ee3457 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 16 14:15:31.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3268" for this suite. 05/16/23 14:15:31.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:15:31.361
May 16 14:15:31.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename cronjob 05/16/23 14:15:31.361
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:15:31.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:15:31.381
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 05/16/23 14:15:31.384
STEP: creating 05/16/23 14:15:31.384
W0516 14:15:31.392012      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: getting 05/16/23 14:15:31.392
STEP: listing 05/16/23 14:15:31.397
STEP: watching 05/16/23 14:15:31.401
May 16 14:15:31.401: INFO: starting watch
STEP: cluster-wide listing 05/16/23 14:15:31.402
STEP: cluster-wide watching 05/16/23 14:15:31.407
May 16 14:15:31.407: INFO: starting watch
STEP: patching 05/16/23 14:15:31.408
STEP: updating 05/16/23 14:15:31.416
May 16 14:15:31.432: INFO: waiting for watch events with expected annotations
May 16 14:15:31.432: INFO: saw patched and updated annotations
STEP: patching /status 05/16/23 14:15:31.432
STEP: updating /status 05/16/23 14:15:31.439
STEP: get /status 05/16/23 14:15:31.446
STEP: deleting 05/16/23 14:15:31.45
STEP: deleting a collection 05/16/23 14:15:31.463
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May 16 14:15:31.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3863" for this suite. 05/16/23 14:15:31.478
------------------------------
â€¢ [0.124 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:15:31.361
    May 16 14:15:31.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename cronjob 05/16/23 14:15:31.361
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:15:31.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:15:31.381
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 05/16/23 14:15:31.384
    STEP: creating 05/16/23 14:15:31.384
    W0516 14:15:31.392012      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: getting 05/16/23 14:15:31.392
    STEP: listing 05/16/23 14:15:31.397
    STEP: watching 05/16/23 14:15:31.401
    May 16 14:15:31.401: INFO: starting watch
    STEP: cluster-wide listing 05/16/23 14:15:31.402
    STEP: cluster-wide watching 05/16/23 14:15:31.407
    May 16 14:15:31.407: INFO: starting watch
    STEP: patching 05/16/23 14:15:31.408
    STEP: updating 05/16/23 14:15:31.416
    May 16 14:15:31.432: INFO: waiting for watch events with expected annotations
    May 16 14:15:31.432: INFO: saw patched and updated annotations
    STEP: patching /status 05/16/23 14:15:31.432
    STEP: updating /status 05/16/23 14:15:31.439
    STEP: get /status 05/16/23 14:15:31.446
    STEP: deleting 05/16/23 14:15:31.45
    STEP: deleting a collection 05/16/23 14:15:31.463
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May 16 14:15:31.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3863" for this suite. 05/16/23 14:15:31.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:15:31.486
May 16 14:15:31.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename statefulset 05/16/23 14:15:31.486
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:15:31.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:15:31.507
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-50 05/16/23 14:15:31.51
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 05/16/23 14:15:31.517
STEP: Creating pod with conflicting port in namespace statefulset-50 05/16/23 14:15:31.522
STEP: Waiting until pod test-pod will start running in namespace statefulset-50 05/16/23 14:15:31.536
May 16 14:15:31.536: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-50" to be "running"
May 16 14:15:31.541: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.130658ms
May 16 14:15:33.545: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=false. Elapsed: 2.00859948s
May 16 14:15:33.545: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-50 05/16/23 14:15:33.545
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-50 05/16/23 14:15:33.551
May 16 14:15:33.585: INFO: Observed stateful pod in namespace: statefulset-50, name: ss-0, uid: 913ba729-8daf-4b05-86cb-68167c0b2777, status phase: Pending. Waiting for statefulset controller to delete.
May 16 14:15:33.594: INFO: Observed stateful pod in namespace: statefulset-50, name: ss-0, uid: 913ba729-8daf-4b05-86cb-68167c0b2777, status phase: Pending. Waiting for statefulset controller to delete.
May 16 14:15:33.600: INFO: Observed stateful pod in namespace: statefulset-50, name: ss-0, uid: 913ba729-8daf-4b05-86cb-68167c0b2777, status phase: Failed. Waiting for statefulset controller to delete.
May 16 14:15:33.610: INFO: Observed stateful pod in namespace: statefulset-50, name: ss-0, uid: 913ba729-8daf-4b05-86cb-68167c0b2777, status phase: Failed. Waiting for statefulset controller to delete.
May 16 14:15:33.624: INFO: Observed stateful pod in namespace: statefulset-50, name: ss-0, uid: 913ba729-8daf-4b05-86cb-68167c0b2777, status phase: Failed. Waiting for statefulset controller to delete.
May 16 14:15:33.631: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-50
STEP: Removing pod with conflicting port in namespace statefulset-50 05/16/23 14:15:33.631
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-50 and will be in running state 05/16/23 14:15:33.644
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May 16 14:15:45.676: INFO: Deleting all statefulset in ns statefulset-50
May 16 14:15:45.680: INFO: Scaling statefulset ss to 0
May 16 14:15:55.698: INFO: Waiting for statefulset status.replicas updated to 0
May 16 14:15:55.701: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May 16 14:15:55.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-50" for this suite. 05/16/23 14:15:55.719
------------------------------
â€¢ [SLOW TEST] [24.239 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:15:31.486
    May 16 14:15:31.486: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename statefulset 05/16/23 14:15:31.486
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:15:31.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:15:31.507
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-50 05/16/23 14:15:31.51
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 05/16/23 14:15:31.517
    STEP: Creating pod with conflicting port in namespace statefulset-50 05/16/23 14:15:31.522
    STEP: Waiting until pod test-pod will start running in namespace statefulset-50 05/16/23 14:15:31.536
    May 16 14:15:31.536: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-50" to be "running"
    May 16 14:15:31.541: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.130658ms
    May 16 14:15:33.545: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=false. Elapsed: 2.00859948s
    May 16 14:15:33.545: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-50 05/16/23 14:15:33.545
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-50 05/16/23 14:15:33.551
    May 16 14:15:33.585: INFO: Observed stateful pod in namespace: statefulset-50, name: ss-0, uid: 913ba729-8daf-4b05-86cb-68167c0b2777, status phase: Pending. Waiting for statefulset controller to delete.
    May 16 14:15:33.594: INFO: Observed stateful pod in namespace: statefulset-50, name: ss-0, uid: 913ba729-8daf-4b05-86cb-68167c0b2777, status phase: Pending. Waiting for statefulset controller to delete.
    May 16 14:15:33.600: INFO: Observed stateful pod in namespace: statefulset-50, name: ss-0, uid: 913ba729-8daf-4b05-86cb-68167c0b2777, status phase: Failed. Waiting for statefulset controller to delete.
    May 16 14:15:33.610: INFO: Observed stateful pod in namespace: statefulset-50, name: ss-0, uid: 913ba729-8daf-4b05-86cb-68167c0b2777, status phase: Failed. Waiting for statefulset controller to delete.
    May 16 14:15:33.624: INFO: Observed stateful pod in namespace: statefulset-50, name: ss-0, uid: 913ba729-8daf-4b05-86cb-68167c0b2777, status phase: Failed. Waiting for statefulset controller to delete.
    May 16 14:15:33.631: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-50
    STEP: Removing pod with conflicting port in namespace statefulset-50 05/16/23 14:15:33.631
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-50 and will be in running state 05/16/23 14:15:33.644
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May 16 14:15:45.676: INFO: Deleting all statefulset in ns statefulset-50
    May 16 14:15:45.680: INFO: Scaling statefulset ss to 0
    May 16 14:15:55.698: INFO: Waiting for statefulset status.replicas updated to 0
    May 16 14:15:55.701: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May 16 14:15:55.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-50" for this suite. 05/16/23 14:15:55.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:15:55.725
May 16 14:15:55.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename replication-controller 05/16/23 14:15:55.726
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:15:55.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:15:55.747
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 05/16/23 14:15:55.757
STEP: waiting for RC to be added 05/16/23 14:15:55.763
STEP: waiting for available Replicas 05/16/23 14:15:55.763
STEP: patching ReplicationController 05/16/23 14:15:57.825
STEP: waiting for RC to be modified 05/16/23 14:15:57.83
STEP: patching ReplicationController status 05/16/23 14:15:57.83
STEP: waiting for RC to be modified 05/16/23 14:15:57.837
STEP: waiting for available Replicas 05/16/23 14:15:57.837
STEP: fetching ReplicationController status 05/16/23 14:15:57.842
STEP: patching ReplicationController scale 05/16/23 14:15:57.846
STEP: waiting for RC to be modified 05/16/23 14:15:57.85
STEP: waiting for ReplicationController's scale to be the max amount 05/16/23 14:15:57.85
STEP: fetching ReplicationController; ensuring that it's patched 05/16/23 14:16:00.222
STEP: updating ReplicationController status 05/16/23 14:16:00.225
STEP: waiting for RC to be modified 05/16/23 14:16:00.231
STEP: listing all ReplicationControllers 05/16/23 14:16:00.231
STEP: checking that ReplicationController has expected values 05/16/23 14:16:00.235
STEP: deleting ReplicationControllers by collection 05/16/23 14:16:00.235
STEP: waiting for ReplicationController to have a DELETED watchEvent 05/16/23 14:16:00.241
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May 16 14:16:00.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9488" for this suite. 05/16/23 14:16:00.272
------------------------------
â€¢ [4.552 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:15:55.725
    May 16 14:15:55.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename replication-controller 05/16/23 14:15:55.726
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:15:55.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:15:55.747
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 05/16/23 14:15:55.757
    STEP: waiting for RC to be added 05/16/23 14:15:55.763
    STEP: waiting for available Replicas 05/16/23 14:15:55.763
    STEP: patching ReplicationController 05/16/23 14:15:57.825
    STEP: waiting for RC to be modified 05/16/23 14:15:57.83
    STEP: patching ReplicationController status 05/16/23 14:15:57.83
    STEP: waiting for RC to be modified 05/16/23 14:15:57.837
    STEP: waiting for available Replicas 05/16/23 14:15:57.837
    STEP: fetching ReplicationController status 05/16/23 14:15:57.842
    STEP: patching ReplicationController scale 05/16/23 14:15:57.846
    STEP: waiting for RC to be modified 05/16/23 14:15:57.85
    STEP: waiting for ReplicationController's scale to be the max amount 05/16/23 14:15:57.85
    STEP: fetching ReplicationController; ensuring that it's patched 05/16/23 14:16:00.222
    STEP: updating ReplicationController status 05/16/23 14:16:00.225
    STEP: waiting for RC to be modified 05/16/23 14:16:00.231
    STEP: listing all ReplicationControllers 05/16/23 14:16:00.231
    STEP: checking that ReplicationController has expected values 05/16/23 14:16:00.235
    STEP: deleting ReplicationControllers by collection 05/16/23 14:16:00.235
    STEP: waiting for ReplicationController to have a DELETED watchEvent 05/16/23 14:16:00.241
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May 16 14:16:00.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9488" for this suite. 05/16/23 14:16:00.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:16:00.277
May 16 14:16:00.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename endpointslice 05/16/23 14:16:00.278
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:00.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:00.297
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 05/16/23 14:16:05.405
STEP: referencing matching pods with named port 05/16/23 14:16:10.412
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 05/16/23 14:16:15.42
STEP: recreating EndpointSlices after they've been deleted 05/16/23 14:16:20.431
May 16 14:16:20.457: INFO: EndpointSlice for Service endpointslice-5442/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
May 16 14:16:30.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5442" for this suite. 05/16/23 14:16:30.469
------------------------------
â€¢ [SLOW TEST] [30.198 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:16:00.277
    May 16 14:16:00.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename endpointslice 05/16/23 14:16:00.278
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:00.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:00.297
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 05/16/23 14:16:05.405
    STEP: referencing matching pods with named port 05/16/23 14:16:10.412
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 05/16/23 14:16:15.42
    STEP: recreating EndpointSlices after they've been deleted 05/16/23 14:16:20.431
    May 16 14:16:20.457: INFO: EndpointSlice for Service endpointslice-5442/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    May 16 14:16:30.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5442" for this suite. 05/16/23 14:16:30.469
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:16:30.476
May 16 14:16:30.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename pods 05/16/23 14:16:30.476
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:30.497
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:30.5
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
May 16 14:16:30.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: creating the pod 05/16/23 14:16:30.503
STEP: submitting the pod to kubernetes 05/16/23 14:16:30.503
May 16 14:16:30.522: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-8f97c927-c02b-4c4e-9497-f223a22366a5" in namespace "pods-2399" to be "running and ready"
May 16 14:16:30.532: INFO: Pod "pod-exec-websocket-8f97c927-c02b-4c4e-9497-f223a22366a5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.207685ms
May 16 14:16:30.532: INFO: The phase of Pod pod-exec-websocket-8f97c927-c02b-4c4e-9497-f223a22366a5 is Pending, waiting for it to be Running (with Ready = true)
May 16 14:16:32.536: INFO: Pod "pod-exec-websocket-8f97c927-c02b-4c4e-9497-f223a22366a5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013661524s
May 16 14:16:32.536: INFO: The phase of Pod pod-exec-websocket-8f97c927-c02b-4c4e-9497-f223a22366a5 is Running (Ready = true)
May 16 14:16:32.536: INFO: Pod "pod-exec-websocket-8f97c927-c02b-4c4e-9497-f223a22366a5" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 16 14:16:32.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2399" for this suite. 05/16/23 14:16:32.654
------------------------------
â€¢ [2.183 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:16:30.476
    May 16 14:16:30.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename pods 05/16/23 14:16:30.476
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:30.497
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:30.5
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    May 16 14:16:30.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: creating the pod 05/16/23 14:16:30.503
    STEP: submitting the pod to kubernetes 05/16/23 14:16:30.503
    May 16 14:16:30.522: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-8f97c927-c02b-4c4e-9497-f223a22366a5" in namespace "pods-2399" to be "running and ready"
    May 16 14:16:30.532: INFO: Pod "pod-exec-websocket-8f97c927-c02b-4c4e-9497-f223a22366a5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.207685ms
    May 16 14:16:30.532: INFO: The phase of Pod pod-exec-websocket-8f97c927-c02b-4c4e-9497-f223a22366a5 is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:16:32.536: INFO: Pod "pod-exec-websocket-8f97c927-c02b-4c4e-9497-f223a22366a5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013661524s
    May 16 14:16:32.536: INFO: The phase of Pod pod-exec-websocket-8f97c927-c02b-4c4e-9497-f223a22366a5 is Running (Ready = true)
    May 16 14:16:32.536: INFO: Pod "pod-exec-websocket-8f97c927-c02b-4c4e-9497-f223a22366a5" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 16 14:16:32.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2399" for this suite. 05/16/23 14:16:32.654
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:16:32.659
May 16 14:16:32.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 14:16:32.66
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:32.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:32.68
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 05/16/23 14:16:32.683
May 16 14:16:32.694: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c41bfeba-76df-4c7d-930f-2ad7cf5c14fe" in namespace "projected-1928" to be "Succeeded or Failed"
May 16 14:16:32.699: INFO: Pod "downwardapi-volume-c41bfeba-76df-4c7d-930f-2ad7cf5c14fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.294211ms
May 16 14:16:34.703: INFO: Pod "downwardapi-volume-c41bfeba-76df-4c7d-930f-2ad7cf5c14fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008799049s
May 16 14:16:36.704: INFO: Pod "downwardapi-volume-c41bfeba-76df-4c7d-930f-2ad7cf5c14fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009530804s
STEP: Saw pod success 05/16/23 14:16:36.704
May 16 14:16:36.704: INFO: Pod "downwardapi-volume-c41bfeba-76df-4c7d-930f-2ad7cf5c14fe" satisfied condition "Succeeded or Failed"
May 16 14:16:36.708: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-c41bfeba-76df-4c7d-930f-2ad7cf5c14fe container client-container: <nil>
STEP: delete the pod 05/16/23 14:16:36.716
May 16 14:16:36.727: INFO: Waiting for pod downwardapi-volume-c41bfeba-76df-4c7d-930f-2ad7cf5c14fe to disappear
May 16 14:16:36.729: INFO: Pod downwardapi-volume-c41bfeba-76df-4c7d-930f-2ad7cf5c14fe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 16 14:16:36.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1928" for this suite. 05/16/23 14:16:36.733
------------------------------
â€¢ [4.081 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:16:32.659
    May 16 14:16:32.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 14:16:32.66
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:32.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:32.68
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 05/16/23 14:16:32.683
    May 16 14:16:32.694: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c41bfeba-76df-4c7d-930f-2ad7cf5c14fe" in namespace "projected-1928" to be "Succeeded or Failed"
    May 16 14:16:32.699: INFO: Pod "downwardapi-volume-c41bfeba-76df-4c7d-930f-2ad7cf5c14fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.294211ms
    May 16 14:16:34.703: INFO: Pod "downwardapi-volume-c41bfeba-76df-4c7d-930f-2ad7cf5c14fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008799049s
    May 16 14:16:36.704: INFO: Pod "downwardapi-volume-c41bfeba-76df-4c7d-930f-2ad7cf5c14fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009530804s
    STEP: Saw pod success 05/16/23 14:16:36.704
    May 16 14:16:36.704: INFO: Pod "downwardapi-volume-c41bfeba-76df-4c7d-930f-2ad7cf5c14fe" satisfied condition "Succeeded or Failed"
    May 16 14:16:36.708: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-c41bfeba-76df-4c7d-930f-2ad7cf5c14fe container client-container: <nil>
    STEP: delete the pod 05/16/23 14:16:36.716
    May 16 14:16:36.727: INFO: Waiting for pod downwardapi-volume-c41bfeba-76df-4c7d-930f-2ad7cf5c14fe to disappear
    May 16 14:16:36.729: INFO: Pod downwardapi-volume-c41bfeba-76df-4c7d-930f-2ad7cf5c14fe no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 16 14:16:36.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1928" for this suite. 05/16/23 14:16:36.733
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:16:36.74
May 16 14:16:36.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename runtimeclass 05/16/23 14:16:36.741
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:36.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:36.761
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 05/16/23 14:16:36.764
STEP: getting /apis/node.k8s.io 05/16/23 14:16:36.766
STEP: getting /apis/node.k8s.io/v1 05/16/23 14:16:36.768
STEP: creating 05/16/23 14:16:36.769
STEP: watching 05/16/23 14:16:36.789
May 16 14:16:36.789: INFO: starting watch
STEP: getting 05/16/23 14:16:36.795
STEP: listing 05/16/23 14:16:36.802
STEP: patching 05/16/23 14:16:36.81
STEP: updating 05/16/23 14:16:36.815
May 16 14:16:36.822: INFO: waiting for watch events with expected annotations
STEP: deleting 05/16/23 14:16:36.822
STEP: deleting a collection 05/16/23 14:16:36.833
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May 16 14:16:36.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2880" for this suite. 05/16/23 14:16:36.852
------------------------------
â€¢ [0.119 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:16:36.74
    May 16 14:16:36.740: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename runtimeclass 05/16/23 14:16:36.741
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:36.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:36.761
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 05/16/23 14:16:36.764
    STEP: getting /apis/node.k8s.io 05/16/23 14:16:36.766
    STEP: getting /apis/node.k8s.io/v1 05/16/23 14:16:36.768
    STEP: creating 05/16/23 14:16:36.769
    STEP: watching 05/16/23 14:16:36.789
    May 16 14:16:36.789: INFO: starting watch
    STEP: getting 05/16/23 14:16:36.795
    STEP: listing 05/16/23 14:16:36.802
    STEP: patching 05/16/23 14:16:36.81
    STEP: updating 05/16/23 14:16:36.815
    May 16 14:16:36.822: INFO: waiting for watch events with expected annotations
    STEP: deleting 05/16/23 14:16:36.822
    STEP: deleting a collection 05/16/23 14:16:36.833
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May 16 14:16:36.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2880" for this suite. 05/16/23 14:16:36.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:16:36.86
May 16 14:16:36.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename replication-controller 05/16/23 14:16:36.86
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:36.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:36.886
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-vmvj7" 05/16/23 14:16:36.889
W0516 14:16:36.895770      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:16:36.895: INFO: Get Replication Controller "e2e-rc-vmvj7" to confirm replicas
May 16 14:16:37.900: INFO: Get Replication Controller "e2e-rc-vmvj7" to confirm replicas
May 16 14:16:37.912: INFO: Found 1 replicas for "e2e-rc-vmvj7" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-vmvj7" 05/16/23 14:16:37.912
STEP: Updating a scale subresource 05/16/23 14:16:37.917
STEP: Verifying replicas where modified for replication controller "e2e-rc-vmvj7" 05/16/23 14:16:37.922
May 16 14:16:37.922: INFO: Get Replication Controller "e2e-rc-vmvj7" to confirm replicas
May 16 14:16:38.925: INFO: Get Replication Controller "e2e-rc-vmvj7" to confirm replicas
May 16 14:16:38.929: INFO: Found 2 replicas for "e2e-rc-vmvj7" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May 16 14:16:38.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5652" for this suite. 05/16/23 14:16:38.935
------------------------------
â€¢ [2.081 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:16:36.86
    May 16 14:16:36.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename replication-controller 05/16/23 14:16:36.86
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:36.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:36.886
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-vmvj7" 05/16/23 14:16:36.889
    W0516 14:16:36.895770      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:16:36.895: INFO: Get Replication Controller "e2e-rc-vmvj7" to confirm replicas
    May 16 14:16:37.900: INFO: Get Replication Controller "e2e-rc-vmvj7" to confirm replicas
    May 16 14:16:37.912: INFO: Found 1 replicas for "e2e-rc-vmvj7" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-vmvj7" 05/16/23 14:16:37.912
    STEP: Updating a scale subresource 05/16/23 14:16:37.917
    STEP: Verifying replicas where modified for replication controller "e2e-rc-vmvj7" 05/16/23 14:16:37.922
    May 16 14:16:37.922: INFO: Get Replication Controller "e2e-rc-vmvj7" to confirm replicas
    May 16 14:16:38.925: INFO: Get Replication Controller "e2e-rc-vmvj7" to confirm replicas
    May 16 14:16:38.929: INFO: Found 2 replicas for "e2e-rc-vmvj7" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May 16 14:16:38.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5652" for this suite. 05/16/23 14:16:38.935
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:16:38.941
May 16 14:16:38.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename configmap 05/16/23 14:16:38.942
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:38.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:38.962
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-fb7f6646-2277-4311-8b38-20783e86de79 05/16/23 14:16:38.964
STEP: Creating a pod to test consume configMaps 05/16/23 14:16:38.969
May 16 14:16:38.978: INFO: Waiting up to 5m0s for pod "pod-configmaps-d3a83a82-5b50-4fb2-a8e9-b854f5ef63a4" in namespace "configmap-8557" to be "Succeeded or Failed"
May 16 14:16:38.982: INFO: Pod "pod-configmaps-d3a83a82-5b50-4fb2-a8e9-b854f5ef63a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.966071ms
May 16 14:16:40.986: INFO: Pod "pod-configmaps-d3a83a82-5b50-4fb2-a8e9-b854f5ef63a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007977355s
May 16 14:16:42.988: INFO: Pod "pod-configmaps-d3a83a82-5b50-4fb2-a8e9-b854f5ef63a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009204648s
STEP: Saw pod success 05/16/23 14:16:42.988
May 16 14:16:42.988: INFO: Pod "pod-configmaps-d3a83a82-5b50-4fb2-a8e9-b854f5ef63a4" satisfied condition "Succeeded or Failed"
May 16 14:16:42.992: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-configmaps-d3a83a82-5b50-4fb2-a8e9-b854f5ef63a4 container agnhost-container: <nil>
STEP: delete the pod 05/16/23 14:16:42.998
May 16 14:16:43.008: INFO: Waiting for pod pod-configmaps-d3a83a82-5b50-4fb2-a8e9-b854f5ef63a4 to disappear
May 16 14:16:43.011: INFO: Pod pod-configmaps-d3a83a82-5b50-4fb2-a8e9-b854f5ef63a4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 16 14:16:43.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8557" for this suite. 05/16/23 14:16:43.015
------------------------------
â€¢ [4.080 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:16:38.941
    May 16 14:16:38.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename configmap 05/16/23 14:16:38.942
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:38.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:38.962
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-fb7f6646-2277-4311-8b38-20783e86de79 05/16/23 14:16:38.964
    STEP: Creating a pod to test consume configMaps 05/16/23 14:16:38.969
    May 16 14:16:38.978: INFO: Waiting up to 5m0s for pod "pod-configmaps-d3a83a82-5b50-4fb2-a8e9-b854f5ef63a4" in namespace "configmap-8557" to be "Succeeded or Failed"
    May 16 14:16:38.982: INFO: Pod "pod-configmaps-d3a83a82-5b50-4fb2-a8e9-b854f5ef63a4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.966071ms
    May 16 14:16:40.986: INFO: Pod "pod-configmaps-d3a83a82-5b50-4fb2-a8e9-b854f5ef63a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007977355s
    May 16 14:16:42.988: INFO: Pod "pod-configmaps-d3a83a82-5b50-4fb2-a8e9-b854f5ef63a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009204648s
    STEP: Saw pod success 05/16/23 14:16:42.988
    May 16 14:16:42.988: INFO: Pod "pod-configmaps-d3a83a82-5b50-4fb2-a8e9-b854f5ef63a4" satisfied condition "Succeeded or Failed"
    May 16 14:16:42.992: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-configmaps-d3a83a82-5b50-4fb2-a8e9-b854f5ef63a4 container agnhost-container: <nil>
    STEP: delete the pod 05/16/23 14:16:42.998
    May 16 14:16:43.008: INFO: Waiting for pod pod-configmaps-d3a83a82-5b50-4fb2-a8e9-b854f5ef63a4 to disappear
    May 16 14:16:43.011: INFO: Pod pod-configmaps-d3a83a82-5b50-4fb2-a8e9-b854f5ef63a4 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 16 14:16:43.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8557" for this suite. 05/16/23 14:16:43.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:16:43.023
May 16 14:16:43.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubectl 05/16/23 14:16:43.023
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:43.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:43.045
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 05/16/23 14:16:43.048
May 16 14:16:43.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2063 create -f -'
May 16 14:16:43.298: INFO: stderr: ""
May 16 14:16:43.298: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/16/23 14:16:43.298
May 16 14:16:44.302: INFO: Selector matched 1 pods for map[app:agnhost]
May 16 14:16:44.302: INFO: Found 1 / 1
May 16 14:16:44.302: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 05/16/23 14:16:44.302
May 16 14:16:44.304: INFO: Selector matched 1 pods for map[app:agnhost]
May 16 14:16:44.304: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 16 14:16:44.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2063 patch pod agnhost-primary-2qpqq -p {"metadata":{"annotations":{"x":"y"}}}'
May 16 14:16:44.360: INFO: stderr: ""
May 16 14:16:44.360: INFO: stdout: "pod/agnhost-primary-2qpqq patched\n"
STEP: checking annotations 05/16/23 14:16:44.36
May 16 14:16:44.364: INFO: Selector matched 1 pods for map[app:agnhost]
May 16 14:16:44.364: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 16 14:16:44.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2063" for this suite. 05/16/23 14:16:44.374
------------------------------
â€¢ [1.357 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:16:43.023
    May 16 14:16:43.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubectl 05/16/23 14:16:43.023
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:43.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:43.045
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 05/16/23 14:16:43.048
    May 16 14:16:43.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2063 create -f -'
    May 16 14:16:43.298: INFO: stderr: ""
    May 16 14:16:43.298: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/16/23 14:16:43.298
    May 16 14:16:44.302: INFO: Selector matched 1 pods for map[app:agnhost]
    May 16 14:16:44.302: INFO: Found 1 / 1
    May 16 14:16:44.302: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 05/16/23 14:16:44.302
    May 16 14:16:44.304: INFO: Selector matched 1 pods for map[app:agnhost]
    May 16 14:16:44.304: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May 16 14:16:44.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2063 patch pod agnhost-primary-2qpqq -p {"metadata":{"annotations":{"x":"y"}}}'
    May 16 14:16:44.360: INFO: stderr: ""
    May 16 14:16:44.360: INFO: stdout: "pod/agnhost-primary-2qpqq patched\n"
    STEP: checking annotations 05/16/23 14:16:44.36
    May 16 14:16:44.364: INFO: Selector matched 1 pods for map[app:agnhost]
    May 16 14:16:44.364: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 16 14:16:44.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2063" for this suite. 05/16/23 14:16:44.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:16:44.38
May 16 14:16:44.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename downward-api 05/16/23 14:16:44.381
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:44.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:44.401
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 05/16/23 14:16:44.404
May 16 14:16:44.421: INFO: Waiting up to 5m0s for pod "downward-api-9053cde1-84a6-4517-ac97-2da590186098" in namespace "downward-api-7516" to be "Succeeded or Failed"
May 16 14:16:44.425: INFO: Pod "downward-api-9053cde1-84a6-4517-ac97-2da590186098": Phase="Pending", Reason="", readiness=false. Elapsed: 3.926133ms
May 16 14:16:46.429: INFO: Pod "downward-api-9053cde1-84a6-4517-ac97-2da590186098": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008243641s
May 16 14:16:48.429: INFO: Pod "downward-api-9053cde1-84a6-4517-ac97-2da590186098": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007787809s
STEP: Saw pod success 05/16/23 14:16:48.429
May 16 14:16:48.429: INFO: Pod "downward-api-9053cde1-84a6-4517-ac97-2da590186098" satisfied condition "Succeeded or Failed"
May 16 14:16:48.432: INFO: Trying to get logs from node ip-10-0-212-246.eu-central-1.compute.internal pod downward-api-9053cde1-84a6-4517-ac97-2da590186098 container dapi-container: <nil>
STEP: delete the pod 05/16/23 14:16:48.446
May 16 14:16:48.459: INFO: Waiting for pod downward-api-9053cde1-84a6-4517-ac97-2da590186098 to disappear
May 16 14:16:48.463: INFO: Pod downward-api-9053cde1-84a6-4517-ac97-2da590186098 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May 16 14:16:48.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7516" for this suite. 05/16/23 14:16:48.468
------------------------------
â€¢ [4.095 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:16:44.38
    May 16 14:16:44.380: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename downward-api 05/16/23 14:16:44.381
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:44.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:44.401
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 05/16/23 14:16:44.404
    May 16 14:16:44.421: INFO: Waiting up to 5m0s for pod "downward-api-9053cde1-84a6-4517-ac97-2da590186098" in namespace "downward-api-7516" to be "Succeeded or Failed"
    May 16 14:16:44.425: INFO: Pod "downward-api-9053cde1-84a6-4517-ac97-2da590186098": Phase="Pending", Reason="", readiness=false. Elapsed: 3.926133ms
    May 16 14:16:46.429: INFO: Pod "downward-api-9053cde1-84a6-4517-ac97-2da590186098": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008243641s
    May 16 14:16:48.429: INFO: Pod "downward-api-9053cde1-84a6-4517-ac97-2da590186098": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007787809s
    STEP: Saw pod success 05/16/23 14:16:48.429
    May 16 14:16:48.429: INFO: Pod "downward-api-9053cde1-84a6-4517-ac97-2da590186098" satisfied condition "Succeeded or Failed"
    May 16 14:16:48.432: INFO: Trying to get logs from node ip-10-0-212-246.eu-central-1.compute.internal pod downward-api-9053cde1-84a6-4517-ac97-2da590186098 container dapi-container: <nil>
    STEP: delete the pod 05/16/23 14:16:48.446
    May 16 14:16:48.459: INFO: Waiting for pod downward-api-9053cde1-84a6-4517-ac97-2da590186098 to disappear
    May 16 14:16:48.463: INFO: Pod downward-api-9053cde1-84a6-4517-ac97-2da590186098 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May 16 14:16:48.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7516" for this suite. 05/16/23 14:16:48.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:16:48.475
May 16 14:16:48.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename emptydir 05/16/23 14:16:48.476
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:48.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:48.499
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 05/16/23 14:16:48.502
May 16 14:16:48.519: INFO: Waiting up to 5m0s for pod "pod-ebb652c1-a183-48cf-afc6-d7397f07600d" in namespace "emptydir-5716" to be "Succeeded or Failed"
May 16 14:16:48.522: INFO: Pod "pod-ebb652c1-a183-48cf-afc6-d7397f07600d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.119603ms
May 16 14:16:50.528: INFO: Pod "pod-ebb652c1-a183-48cf-afc6-d7397f07600d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008484079s
May 16 14:16:52.527: INFO: Pod "pod-ebb652c1-a183-48cf-afc6-d7397f07600d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008002241s
STEP: Saw pod success 05/16/23 14:16:52.527
May 16 14:16:52.527: INFO: Pod "pod-ebb652c1-a183-48cf-afc6-d7397f07600d" satisfied condition "Succeeded or Failed"
May 16 14:16:52.530: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-ebb652c1-a183-48cf-afc6-d7397f07600d container test-container: <nil>
STEP: delete the pod 05/16/23 14:16:52.536
May 16 14:16:52.546: INFO: Waiting for pod pod-ebb652c1-a183-48cf-afc6-d7397f07600d to disappear
May 16 14:16:52.549: INFO: Pod pod-ebb652c1-a183-48cf-afc6-d7397f07600d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 16 14:16:52.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5716" for this suite. 05/16/23 14:16:52.554
------------------------------
â€¢ [4.084 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:16:48.475
    May 16 14:16:48.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename emptydir 05/16/23 14:16:48.476
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:48.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:48.499
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 05/16/23 14:16:48.502
    May 16 14:16:48.519: INFO: Waiting up to 5m0s for pod "pod-ebb652c1-a183-48cf-afc6-d7397f07600d" in namespace "emptydir-5716" to be "Succeeded or Failed"
    May 16 14:16:48.522: INFO: Pod "pod-ebb652c1-a183-48cf-afc6-d7397f07600d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.119603ms
    May 16 14:16:50.528: INFO: Pod "pod-ebb652c1-a183-48cf-afc6-d7397f07600d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008484079s
    May 16 14:16:52.527: INFO: Pod "pod-ebb652c1-a183-48cf-afc6-d7397f07600d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008002241s
    STEP: Saw pod success 05/16/23 14:16:52.527
    May 16 14:16:52.527: INFO: Pod "pod-ebb652c1-a183-48cf-afc6-d7397f07600d" satisfied condition "Succeeded or Failed"
    May 16 14:16:52.530: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-ebb652c1-a183-48cf-afc6-d7397f07600d container test-container: <nil>
    STEP: delete the pod 05/16/23 14:16:52.536
    May 16 14:16:52.546: INFO: Waiting for pod pod-ebb652c1-a183-48cf-afc6-d7397f07600d to disappear
    May 16 14:16:52.549: INFO: Pod pod-ebb652c1-a183-48cf-afc6-d7397f07600d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 16 14:16:52.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5716" for this suite. 05/16/23 14:16:52.554
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:16:52.56
May 16 14:16:52.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename emptydir 05/16/23 14:16:52.561
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:52.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:52.583
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 05/16/23 14:16:52.586
W0516 14:16:52.599389      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:16:52.599: INFO: Waiting up to 5m0s for pod "pod-5b6b0d02-57ca-4fde-845f-713054b0e06d" in namespace "emptydir-1875" to be "Succeeded or Failed"
May 16 14:16:52.602: INFO: Pod "pod-5b6b0d02-57ca-4fde-845f-713054b0e06d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.324383ms
May 16 14:16:54.606: INFO: Pod "pod-5b6b0d02-57ca-4fde-845f-713054b0e06d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00738938s
May 16 14:16:56.607: INFO: Pod "pod-5b6b0d02-57ca-4fde-845f-713054b0e06d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00759261s
STEP: Saw pod success 05/16/23 14:16:56.607
May 16 14:16:56.607: INFO: Pod "pod-5b6b0d02-57ca-4fde-845f-713054b0e06d" satisfied condition "Succeeded or Failed"
May 16 14:16:56.610: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-5b6b0d02-57ca-4fde-845f-713054b0e06d container test-container: <nil>
STEP: delete the pod 05/16/23 14:16:56.616
May 16 14:16:56.626: INFO: Waiting for pod pod-5b6b0d02-57ca-4fde-845f-713054b0e06d to disappear
May 16 14:16:56.629: INFO: Pod pod-5b6b0d02-57ca-4fde-845f-713054b0e06d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 16 14:16:56.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1875" for this suite. 05/16/23 14:16:56.634
------------------------------
â€¢ [4.081 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:16:52.56
    May 16 14:16:52.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename emptydir 05/16/23 14:16:52.561
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:52.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:52.583
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 05/16/23 14:16:52.586
    W0516 14:16:52.599389      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:16:52.599: INFO: Waiting up to 5m0s for pod "pod-5b6b0d02-57ca-4fde-845f-713054b0e06d" in namespace "emptydir-1875" to be "Succeeded or Failed"
    May 16 14:16:52.602: INFO: Pod "pod-5b6b0d02-57ca-4fde-845f-713054b0e06d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.324383ms
    May 16 14:16:54.606: INFO: Pod "pod-5b6b0d02-57ca-4fde-845f-713054b0e06d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00738938s
    May 16 14:16:56.607: INFO: Pod "pod-5b6b0d02-57ca-4fde-845f-713054b0e06d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00759261s
    STEP: Saw pod success 05/16/23 14:16:56.607
    May 16 14:16:56.607: INFO: Pod "pod-5b6b0d02-57ca-4fde-845f-713054b0e06d" satisfied condition "Succeeded or Failed"
    May 16 14:16:56.610: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-5b6b0d02-57ca-4fde-845f-713054b0e06d container test-container: <nil>
    STEP: delete the pod 05/16/23 14:16:56.616
    May 16 14:16:56.626: INFO: Waiting for pod pod-5b6b0d02-57ca-4fde-845f-713054b0e06d to disappear
    May 16 14:16:56.629: INFO: Pod pod-5b6b0d02-57ca-4fde-845f-713054b0e06d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 16 14:16:56.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1875" for this suite. 05/16/23 14:16:56.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:16:56.642
May 16 14:16:56.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename resourcequota 05/16/23 14:16:56.642
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:56.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:56.662
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-4wcc4" 05/16/23 14:16:56.667
May 16 14:16:56.676: INFO: Resource quota "e2e-rq-status-4wcc4" reports spec: hard cpu limit of 500m
May 16 14:16:56.676: INFO: Resource quota "e2e-rq-status-4wcc4" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-4wcc4" /status 05/16/23 14:16:56.676
STEP: Confirm /status for "e2e-rq-status-4wcc4" resourceQuota via watch 05/16/23 14:16:56.687
May 16 14:16:56.689: INFO: observed resourceQuota "e2e-rq-status-4wcc4" in namespace "resourcequota-3041" with hard status: v1.ResourceList(nil)
May 16 14:16:56.689: INFO: Found resourceQuota "e2e-rq-status-4wcc4" in namespace "resourcequota-3041" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
May 16 14:16:56.689: INFO: ResourceQuota "e2e-rq-status-4wcc4" /status was updated
STEP: Patching hard spec values for cpu & memory 05/16/23 14:16:56.692
May 16 14:16:56.699: INFO: Resource quota "e2e-rq-status-4wcc4" reports spec: hard cpu limit of 1
May 16 14:16:56.699: INFO: Resource quota "e2e-rq-status-4wcc4" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-4wcc4" /status 05/16/23 14:16:56.699
STEP: Confirm /status for "e2e-rq-status-4wcc4" resourceQuota via watch 05/16/23 14:16:56.709
May 16 14:16:56.710: INFO: observed resourceQuota "e2e-rq-status-4wcc4" in namespace "resourcequota-3041" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
May 16 14:16:56.710: INFO: Found resourceQuota "e2e-rq-status-4wcc4" in namespace "resourcequota-3041" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
May 16 14:16:56.710: INFO: ResourceQuota "e2e-rq-status-4wcc4" /status was patched
STEP: Get "e2e-rq-status-4wcc4" /status 05/16/23 14:16:56.71
May 16 14:16:56.715: INFO: Resourcequota "e2e-rq-status-4wcc4" reports status: hard cpu of 1
May 16 14:16:56.715: INFO: Resourcequota "e2e-rq-status-4wcc4" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-4wcc4" /status before checking Spec is unchanged 05/16/23 14:16:56.717
May 16 14:16:56.727: INFO: Resourcequota "e2e-rq-status-4wcc4" reports status: hard cpu of 2
May 16 14:16:56.727: INFO: Resourcequota "e2e-rq-status-4wcc4" reports status: hard memory of 2Gi
May 16 14:16:56.729: INFO: observed resourceQuota "e2e-rq-status-4wcc4" in namespace "resourcequota-3041" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
May 16 14:16:56.729: INFO: Found resourceQuota "e2e-rq-status-4wcc4" in namespace "resourcequota-3041" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
May 16 14:19:41.736: INFO: ResourceQuota "e2e-rq-status-4wcc4" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 16 14:19:41.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3041" for this suite. 05/16/23 14:19:41.741
------------------------------
â€¢ [SLOW TEST] [165.105 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:16:56.642
    May 16 14:16:56.642: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename resourcequota 05/16/23 14:16:56.642
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:16:56.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:16:56.662
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-4wcc4" 05/16/23 14:16:56.667
    May 16 14:16:56.676: INFO: Resource quota "e2e-rq-status-4wcc4" reports spec: hard cpu limit of 500m
    May 16 14:16:56.676: INFO: Resource quota "e2e-rq-status-4wcc4" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-4wcc4" /status 05/16/23 14:16:56.676
    STEP: Confirm /status for "e2e-rq-status-4wcc4" resourceQuota via watch 05/16/23 14:16:56.687
    May 16 14:16:56.689: INFO: observed resourceQuota "e2e-rq-status-4wcc4" in namespace "resourcequota-3041" with hard status: v1.ResourceList(nil)
    May 16 14:16:56.689: INFO: Found resourceQuota "e2e-rq-status-4wcc4" in namespace "resourcequota-3041" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    May 16 14:16:56.689: INFO: ResourceQuota "e2e-rq-status-4wcc4" /status was updated
    STEP: Patching hard spec values for cpu & memory 05/16/23 14:16:56.692
    May 16 14:16:56.699: INFO: Resource quota "e2e-rq-status-4wcc4" reports spec: hard cpu limit of 1
    May 16 14:16:56.699: INFO: Resource quota "e2e-rq-status-4wcc4" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-4wcc4" /status 05/16/23 14:16:56.699
    STEP: Confirm /status for "e2e-rq-status-4wcc4" resourceQuota via watch 05/16/23 14:16:56.709
    May 16 14:16:56.710: INFO: observed resourceQuota "e2e-rq-status-4wcc4" in namespace "resourcequota-3041" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    May 16 14:16:56.710: INFO: Found resourceQuota "e2e-rq-status-4wcc4" in namespace "resourcequota-3041" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    May 16 14:16:56.710: INFO: ResourceQuota "e2e-rq-status-4wcc4" /status was patched
    STEP: Get "e2e-rq-status-4wcc4" /status 05/16/23 14:16:56.71
    May 16 14:16:56.715: INFO: Resourcequota "e2e-rq-status-4wcc4" reports status: hard cpu of 1
    May 16 14:16:56.715: INFO: Resourcequota "e2e-rq-status-4wcc4" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-4wcc4" /status before checking Spec is unchanged 05/16/23 14:16:56.717
    May 16 14:16:56.727: INFO: Resourcequota "e2e-rq-status-4wcc4" reports status: hard cpu of 2
    May 16 14:16:56.727: INFO: Resourcequota "e2e-rq-status-4wcc4" reports status: hard memory of 2Gi
    May 16 14:16:56.729: INFO: observed resourceQuota "e2e-rq-status-4wcc4" in namespace "resourcequota-3041" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    May 16 14:16:56.729: INFO: Found resourceQuota "e2e-rq-status-4wcc4" in namespace "resourcequota-3041" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    May 16 14:19:41.736: INFO: ResourceQuota "e2e-rq-status-4wcc4" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 16 14:19:41.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3041" for this suite. 05/16/23 14:19:41.741
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:19:41.747
May 16 14:19:41.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename var-expansion 05/16/23 14:19:41.748
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:19:41.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:19:41.77
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 05/16/23 14:19:41.772
STEP: waiting for pod running 05/16/23 14:19:41.788
May 16 14:19:41.788: INFO: Waiting up to 2m0s for pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b" in namespace "var-expansion-5294" to be "running"
May 16 14:19:41.792: INFO: Pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.80702ms
May 16 14:19:43.796: INFO: Pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b": Phase="Running", Reason="", readiness=true. Elapsed: 2.007756218s
May 16 14:19:43.796: INFO: Pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b" satisfied condition "running"
STEP: creating a file in subpath 05/16/23 14:19:43.796
May 16 14:19:43.802: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5294 PodName:var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 14:19:43.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 14:19:43.802: INFO: ExecWithOptions: Clientset creation
May 16 14:19:43.802: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/var-expansion-5294/pods/var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 05/16/23 14:19:43.865
May 16 14:19:43.868: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5294 PodName:var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 14:19:43.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 14:19:43.868: INFO: ExecWithOptions: Clientset creation
May 16 14:19:43.868: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/var-expansion-5294/pods/var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 05/16/23 14:19:43.925
May 16 14:19:44.437: INFO: Successfully updated pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b"
STEP: waiting for annotated pod running 05/16/23 14:19:44.437
May 16 14:19:44.437: INFO: Waiting up to 2m0s for pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b" in namespace "var-expansion-5294" to be "running"
May 16 14:19:44.439: INFO: Pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b": Phase="Running", Reason="", readiness=true. Elapsed: 2.428755ms
May 16 14:19:44.439: INFO: Pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b" satisfied condition "running"
STEP: deleting the pod gracefully 05/16/23 14:19:44.439
May 16 14:19:44.439: INFO: Deleting pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b" in namespace "var-expansion-5294"
May 16 14:19:44.447: INFO: Wait up to 5m0s for pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May 16 14:20:18.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5294" for this suite. 05/16/23 14:20:18.458
------------------------------
â€¢ [SLOW TEST] [36.715 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:19:41.747
    May 16 14:19:41.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename var-expansion 05/16/23 14:19:41.748
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:19:41.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:19:41.77
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 05/16/23 14:19:41.772
    STEP: waiting for pod running 05/16/23 14:19:41.788
    May 16 14:19:41.788: INFO: Waiting up to 2m0s for pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b" in namespace "var-expansion-5294" to be "running"
    May 16 14:19:41.792: INFO: Pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.80702ms
    May 16 14:19:43.796: INFO: Pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b": Phase="Running", Reason="", readiness=true. Elapsed: 2.007756218s
    May 16 14:19:43.796: INFO: Pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b" satisfied condition "running"
    STEP: creating a file in subpath 05/16/23 14:19:43.796
    May 16 14:19:43.802: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5294 PodName:var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 14:19:43.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 14:19:43.802: INFO: ExecWithOptions: Clientset creation
    May 16 14:19:43.802: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/var-expansion-5294/pods/var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 05/16/23 14:19:43.865
    May 16 14:19:43.868: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5294 PodName:var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 14:19:43.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 14:19:43.868: INFO: ExecWithOptions: Clientset creation
    May 16 14:19:43.868: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/var-expansion-5294/pods/var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 05/16/23 14:19:43.925
    May 16 14:19:44.437: INFO: Successfully updated pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b"
    STEP: waiting for annotated pod running 05/16/23 14:19:44.437
    May 16 14:19:44.437: INFO: Waiting up to 2m0s for pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b" in namespace "var-expansion-5294" to be "running"
    May 16 14:19:44.439: INFO: Pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b": Phase="Running", Reason="", readiness=true. Elapsed: 2.428755ms
    May 16 14:19:44.439: INFO: Pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b" satisfied condition "running"
    STEP: deleting the pod gracefully 05/16/23 14:19:44.439
    May 16 14:19:44.439: INFO: Deleting pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b" in namespace "var-expansion-5294"
    May 16 14:19:44.447: INFO: Wait up to 5m0s for pod "var-expansion-b699547b-76a5-4836-8847-5e1f0646d09b" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May 16 14:20:18.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5294" for this suite. 05/16/23 14:20:18.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:20:18.464
May 16 14:20:18.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename webhook 05/16/23 14:20:18.465
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:20:18.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:20:18.486
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/16/23 14:20:18.514
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 14:20:18.933
STEP: Deploying the webhook pod 05/16/23 14:20:18.942
STEP: Wait for the deployment to be ready 05/16/23 14:20:18.952
May 16 14:20:18.966: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/16/23 14:20:20.977
STEP: Verifying the service has paired with the endpoint 05/16/23 14:20:20.986
May 16 14:20:21.986: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 05/16/23 14:20:21.99
STEP: Creating a custom resource definition that should be denied by the webhook 05/16/23 14:20:22.005
May 16 14:20:22.005: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 14:20:22.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-173" for this suite. 05/16/23 14:20:22.093
STEP: Destroying namespace "webhook-173-markers" for this suite. 05/16/23 14:20:22.102
------------------------------
â€¢ [3.644 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:20:18.464
    May 16 14:20:18.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename webhook 05/16/23 14:20:18.465
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:20:18.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:20:18.486
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/16/23 14:20:18.514
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 14:20:18.933
    STEP: Deploying the webhook pod 05/16/23 14:20:18.942
    STEP: Wait for the deployment to be ready 05/16/23 14:20:18.952
    May 16 14:20:18.966: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/16/23 14:20:20.977
    STEP: Verifying the service has paired with the endpoint 05/16/23 14:20:20.986
    May 16 14:20:21.986: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 05/16/23 14:20:21.99
    STEP: Creating a custom resource definition that should be denied by the webhook 05/16/23 14:20:22.005
    May 16 14:20:22.005: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 14:20:22.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-173" for this suite. 05/16/23 14:20:22.093
    STEP: Destroying namespace "webhook-173-markers" for this suite. 05/16/23 14:20:22.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:20:22.11
May 16 14:20:22.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename svcaccounts 05/16/23 14:20:22.11
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:20:22.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:20:22.151
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-fhpdg"  05/16/23 14:20:22.154
May 16 14:20:22.159: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-fhpdg"  05/16/23 14:20:22.159
May 16 14:20:22.208: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May 16 14:20:22.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1689" for this suite. 05/16/23 14:20:22.221
------------------------------
â€¢ [0.118 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:20:22.11
    May 16 14:20:22.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename svcaccounts 05/16/23 14:20:22.11
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:20:22.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:20:22.151
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-fhpdg"  05/16/23 14:20:22.154
    May 16 14:20:22.159: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-fhpdg"  05/16/23 14:20:22.159
    May 16 14:20:22.208: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May 16 14:20:22.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1689" for this suite. 05/16/23 14:20:22.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:20:22.228
May 16 14:20:22.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename subpath 05/16/23 14:20:22.228
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:20:22.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:20:22.249
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/16/23 14:20:22.252
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-hdc2 05/16/23 14:20:22.259
STEP: Creating a pod to test atomic-volume-subpath 05/16/23 14:20:22.259
May 16 14:20:22.269: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hdc2" in namespace "subpath-4917" to be "Succeeded or Failed"
May 16 14:20:22.273: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.776244ms
May 16 14:20:24.278: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008232014s
May 16 14:20:26.277: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 4.008044453s
May 16 14:20:28.278: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 6.009150921s
May 16 14:20:30.278: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 8.008268543s
May 16 14:20:32.278: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 10.008386853s
May 16 14:20:34.276: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 12.006773319s
May 16 14:20:36.279: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 14.009288522s
May 16 14:20:38.278: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 16.009104495s
May 16 14:20:40.278: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 18.009169624s
May 16 14:20:42.278: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 20.008593699s
May 16 14:20:44.277: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=false. Elapsed: 22.007979985s
May 16 14:20:46.277: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007951311s
STEP: Saw pod success 05/16/23 14:20:46.277
May 16 14:20:46.277: INFO: Pod "pod-subpath-test-configmap-hdc2" satisfied condition "Succeeded or Failed"
May 16 14:20:46.281: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-subpath-test-configmap-hdc2 container test-container-subpath-configmap-hdc2: <nil>
STEP: delete the pod 05/16/23 14:20:46.29
May 16 14:20:46.300: INFO: Waiting for pod pod-subpath-test-configmap-hdc2 to disappear
May 16 14:20:46.302: INFO: Pod pod-subpath-test-configmap-hdc2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hdc2 05/16/23 14:20:46.302
May 16 14:20:46.302: INFO: Deleting pod "pod-subpath-test-configmap-hdc2" in namespace "subpath-4917"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May 16 14:20:46.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4917" for this suite. 05/16/23 14:20:46.307
------------------------------
â€¢ [SLOW TEST] [24.086 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:20:22.228
    May 16 14:20:22.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename subpath 05/16/23 14:20:22.228
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:20:22.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:20:22.249
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/16/23 14:20:22.252
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-hdc2 05/16/23 14:20:22.259
    STEP: Creating a pod to test atomic-volume-subpath 05/16/23 14:20:22.259
    May 16 14:20:22.269: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hdc2" in namespace "subpath-4917" to be "Succeeded or Failed"
    May 16 14:20:22.273: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.776244ms
    May 16 14:20:24.278: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008232014s
    May 16 14:20:26.277: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 4.008044453s
    May 16 14:20:28.278: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 6.009150921s
    May 16 14:20:30.278: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 8.008268543s
    May 16 14:20:32.278: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 10.008386853s
    May 16 14:20:34.276: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 12.006773319s
    May 16 14:20:36.279: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 14.009288522s
    May 16 14:20:38.278: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 16.009104495s
    May 16 14:20:40.278: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 18.009169624s
    May 16 14:20:42.278: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=true. Elapsed: 20.008593699s
    May 16 14:20:44.277: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Running", Reason="", readiness=false. Elapsed: 22.007979985s
    May 16 14:20:46.277: INFO: Pod "pod-subpath-test-configmap-hdc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007951311s
    STEP: Saw pod success 05/16/23 14:20:46.277
    May 16 14:20:46.277: INFO: Pod "pod-subpath-test-configmap-hdc2" satisfied condition "Succeeded or Failed"
    May 16 14:20:46.281: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-subpath-test-configmap-hdc2 container test-container-subpath-configmap-hdc2: <nil>
    STEP: delete the pod 05/16/23 14:20:46.29
    May 16 14:20:46.300: INFO: Waiting for pod pod-subpath-test-configmap-hdc2 to disappear
    May 16 14:20:46.302: INFO: Pod pod-subpath-test-configmap-hdc2 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-hdc2 05/16/23 14:20:46.302
    May 16 14:20:46.302: INFO: Deleting pod "pod-subpath-test-configmap-hdc2" in namespace "subpath-4917"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May 16 14:20:46.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4917" for this suite. 05/16/23 14:20:46.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:20:46.314
May 16 14:20:46.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename configmap 05/16/23 14:20:46.315
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:20:46.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:20:46.336
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-6771a326-4037-4df1-bfa0-73ca81aa8518 05/16/23 14:20:46.339
STEP: Creating a pod to test consume configMaps 05/16/23 14:20:46.343
May 16 14:20:46.357: INFO: Waiting up to 5m0s for pod "pod-configmaps-2972fe9b-2dda-48b0-9202-cacf39494545" in namespace "configmap-6961" to be "Succeeded or Failed"
May 16 14:20:46.377: INFO: Pod "pod-configmaps-2972fe9b-2dda-48b0-9202-cacf39494545": Phase="Pending", Reason="", readiness=false. Elapsed: 20.510068ms
May 16 14:20:48.381: INFO: Pod "pod-configmaps-2972fe9b-2dda-48b0-9202-cacf39494545": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024246907s
May 16 14:20:50.383: INFO: Pod "pod-configmaps-2972fe9b-2dda-48b0-9202-cacf39494545": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025776047s
STEP: Saw pod success 05/16/23 14:20:50.383
May 16 14:20:50.383: INFO: Pod "pod-configmaps-2972fe9b-2dda-48b0-9202-cacf39494545" satisfied condition "Succeeded or Failed"
May 16 14:20:50.385: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-configmaps-2972fe9b-2dda-48b0-9202-cacf39494545 container agnhost-container: <nil>
STEP: delete the pod 05/16/23 14:20:50.39
May 16 14:20:50.403: INFO: Waiting for pod pod-configmaps-2972fe9b-2dda-48b0-9202-cacf39494545 to disappear
May 16 14:20:50.405: INFO: Pod pod-configmaps-2972fe9b-2dda-48b0-9202-cacf39494545 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 16 14:20:50.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6961" for this suite. 05/16/23 14:20:50.411
------------------------------
â€¢ [4.102 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:20:46.314
    May 16 14:20:46.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename configmap 05/16/23 14:20:46.315
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:20:46.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:20:46.336
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-6771a326-4037-4df1-bfa0-73ca81aa8518 05/16/23 14:20:46.339
    STEP: Creating a pod to test consume configMaps 05/16/23 14:20:46.343
    May 16 14:20:46.357: INFO: Waiting up to 5m0s for pod "pod-configmaps-2972fe9b-2dda-48b0-9202-cacf39494545" in namespace "configmap-6961" to be "Succeeded or Failed"
    May 16 14:20:46.377: INFO: Pod "pod-configmaps-2972fe9b-2dda-48b0-9202-cacf39494545": Phase="Pending", Reason="", readiness=false. Elapsed: 20.510068ms
    May 16 14:20:48.381: INFO: Pod "pod-configmaps-2972fe9b-2dda-48b0-9202-cacf39494545": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024246907s
    May 16 14:20:50.383: INFO: Pod "pod-configmaps-2972fe9b-2dda-48b0-9202-cacf39494545": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025776047s
    STEP: Saw pod success 05/16/23 14:20:50.383
    May 16 14:20:50.383: INFO: Pod "pod-configmaps-2972fe9b-2dda-48b0-9202-cacf39494545" satisfied condition "Succeeded or Failed"
    May 16 14:20:50.385: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-configmaps-2972fe9b-2dda-48b0-9202-cacf39494545 container agnhost-container: <nil>
    STEP: delete the pod 05/16/23 14:20:50.39
    May 16 14:20:50.403: INFO: Waiting for pod pod-configmaps-2972fe9b-2dda-48b0-9202-cacf39494545 to disappear
    May 16 14:20:50.405: INFO: Pod pod-configmaps-2972fe9b-2dda-48b0-9202-cacf39494545 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 16 14:20:50.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6961" for this suite. 05/16/23 14:20:50.411
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:20:50.417
May 16 14:20:50.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename init-container 05/16/23 14:20:50.417
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:20:50.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:20:50.437
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 05/16/23 14:20:50.439
May 16 14:20:50.439: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May 16 14:20:55.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4440" for this suite. 05/16/23 14:20:55.461
------------------------------
â€¢ [SLOW TEST] [5.053 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:20:50.417
    May 16 14:20:50.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename init-container 05/16/23 14:20:50.417
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:20:50.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:20:50.437
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 05/16/23 14:20:50.439
    May 16 14:20:50.439: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May 16 14:20:55.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4440" for this suite. 05/16/23 14:20:55.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:20:55.471
May 16 14:20:55.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename secrets 05/16/23 14:20:55.471
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:20:55.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:20:55.545
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-4b47c506-b77d-46b7-9f96-cd47f422d5cf 05/16/23 14:20:55.548
STEP: Creating a pod to test consume secrets 05/16/23 14:20:55.554
May 16 14:20:55.562: INFO: Waiting up to 5m0s for pod "pod-secrets-48301624-a067-4200-9e82-f046aa5b8ecc" in namespace "secrets-8400" to be "Succeeded or Failed"
May 16 14:20:55.565: INFO: Pod "pod-secrets-48301624-a067-4200-9e82-f046aa5b8ecc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.596137ms
May 16 14:20:57.571: INFO: Pod "pod-secrets-48301624-a067-4200-9e82-f046aa5b8ecc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008344834s
May 16 14:20:59.570: INFO: Pod "pod-secrets-48301624-a067-4200-9e82-f046aa5b8ecc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007836868s
STEP: Saw pod success 05/16/23 14:20:59.57
May 16 14:20:59.570: INFO: Pod "pod-secrets-48301624-a067-4200-9e82-f046aa5b8ecc" satisfied condition "Succeeded or Failed"
May 16 14:20:59.573: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-secrets-48301624-a067-4200-9e82-f046aa5b8ecc container secret-volume-test: <nil>
STEP: delete the pod 05/16/23 14:20:59.577
May 16 14:20:59.586: INFO: Waiting for pod pod-secrets-48301624-a067-4200-9e82-f046aa5b8ecc to disappear
May 16 14:20:59.588: INFO: Pod pod-secrets-48301624-a067-4200-9e82-f046aa5b8ecc no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May 16 14:20:59.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8400" for this suite. 05/16/23 14:20:59.593
------------------------------
â€¢ [4.128 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:20:55.471
    May 16 14:20:55.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename secrets 05/16/23 14:20:55.471
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:20:55.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:20:55.545
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-4b47c506-b77d-46b7-9f96-cd47f422d5cf 05/16/23 14:20:55.548
    STEP: Creating a pod to test consume secrets 05/16/23 14:20:55.554
    May 16 14:20:55.562: INFO: Waiting up to 5m0s for pod "pod-secrets-48301624-a067-4200-9e82-f046aa5b8ecc" in namespace "secrets-8400" to be "Succeeded or Failed"
    May 16 14:20:55.565: INFO: Pod "pod-secrets-48301624-a067-4200-9e82-f046aa5b8ecc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.596137ms
    May 16 14:20:57.571: INFO: Pod "pod-secrets-48301624-a067-4200-9e82-f046aa5b8ecc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008344834s
    May 16 14:20:59.570: INFO: Pod "pod-secrets-48301624-a067-4200-9e82-f046aa5b8ecc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007836868s
    STEP: Saw pod success 05/16/23 14:20:59.57
    May 16 14:20:59.570: INFO: Pod "pod-secrets-48301624-a067-4200-9e82-f046aa5b8ecc" satisfied condition "Succeeded or Failed"
    May 16 14:20:59.573: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-secrets-48301624-a067-4200-9e82-f046aa5b8ecc container secret-volume-test: <nil>
    STEP: delete the pod 05/16/23 14:20:59.577
    May 16 14:20:59.586: INFO: Waiting for pod pod-secrets-48301624-a067-4200-9e82-f046aa5b8ecc to disappear
    May 16 14:20:59.588: INFO: Pod pod-secrets-48301624-a067-4200-9e82-f046aa5b8ecc no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May 16 14:20:59.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8400" for this suite. 05/16/23 14:20:59.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:20:59.599
May 16 14:20:59.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename security-context 05/16/23 14:20:59.6
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:20:59.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:20:59.619
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/16/23 14:20:59.622
W0516 14:20:59.635199      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:20:59.635: INFO: Waiting up to 5m0s for pod "security-context-308d903c-6cb0-4f9c-9594-ac1a253411d8" in namespace "security-context-5560" to be "Succeeded or Failed"
May 16 14:20:59.640: INFO: Pod "security-context-308d903c-6cb0-4f9c-9594-ac1a253411d8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.47765ms
May 16 14:21:01.644: INFO: Pod "security-context-308d903c-6cb0-4f9c-9594-ac1a253411d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009623406s
May 16 14:21:03.644: INFO: Pod "security-context-308d903c-6cb0-4f9c-9594-ac1a253411d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009484333s
STEP: Saw pod success 05/16/23 14:21:03.644
May 16 14:21:03.644: INFO: Pod "security-context-308d903c-6cb0-4f9c-9594-ac1a253411d8" satisfied condition "Succeeded or Failed"
May 16 14:21:03.647: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod security-context-308d903c-6cb0-4f9c-9594-ac1a253411d8 container test-container: <nil>
STEP: delete the pod 05/16/23 14:21:03.654
May 16 14:21:03.666: INFO: Waiting for pod security-context-308d903c-6cb0-4f9c-9594-ac1a253411d8 to disappear
May 16 14:21:03.669: INFO: Pod security-context-308d903c-6cb0-4f9c-9594-ac1a253411d8 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May 16 14:21:03.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-5560" for this suite. 05/16/23 14:21:03.673
------------------------------
â€¢ [4.079 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:20:59.599
    May 16 14:20:59.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename security-context 05/16/23 14:20:59.6
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:20:59.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:20:59.619
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/16/23 14:20:59.622
    W0516 14:20:59.635199      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:20:59.635: INFO: Waiting up to 5m0s for pod "security-context-308d903c-6cb0-4f9c-9594-ac1a253411d8" in namespace "security-context-5560" to be "Succeeded or Failed"
    May 16 14:20:59.640: INFO: Pod "security-context-308d903c-6cb0-4f9c-9594-ac1a253411d8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.47765ms
    May 16 14:21:01.644: INFO: Pod "security-context-308d903c-6cb0-4f9c-9594-ac1a253411d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009623406s
    May 16 14:21:03.644: INFO: Pod "security-context-308d903c-6cb0-4f9c-9594-ac1a253411d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009484333s
    STEP: Saw pod success 05/16/23 14:21:03.644
    May 16 14:21:03.644: INFO: Pod "security-context-308d903c-6cb0-4f9c-9594-ac1a253411d8" satisfied condition "Succeeded or Failed"
    May 16 14:21:03.647: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod security-context-308d903c-6cb0-4f9c-9594-ac1a253411d8 container test-container: <nil>
    STEP: delete the pod 05/16/23 14:21:03.654
    May 16 14:21:03.666: INFO: Waiting for pod security-context-308d903c-6cb0-4f9c-9594-ac1a253411d8 to disappear
    May 16 14:21:03.669: INFO: Pod security-context-308d903c-6cb0-4f9c-9594-ac1a253411d8 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May 16 14:21:03.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-5560" for this suite. 05/16/23 14:21:03.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:21:03.678
May 16 14:21:03.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename custom-resource-definition 05/16/23 14:21:03.679
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:03.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:03.702
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
May 16 14:21:03.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 14:21:06.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1001" for this suite. 05/16/23 14:21:06.817
------------------------------
â€¢ [3.145 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:21:03.678
    May 16 14:21:03.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename custom-resource-definition 05/16/23 14:21:03.679
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:03.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:03.702
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    May 16 14:21:03.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 14:21:06.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1001" for this suite. 05/16/23 14:21:06.817
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:21:06.824
May 16 14:21:06.824: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename conformance-tests 05/16/23 14:21:06.825
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:06.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:06.846
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 05/16/23 14:21:06.891
May 16 14:21:06.891: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
May 16 14:21:06.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-4419" for this suite. 05/16/23 14:21:06.907
------------------------------
â€¢ [0.091 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:21:06.824
    May 16 14:21:06.824: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename conformance-tests 05/16/23 14:21:06.825
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:06.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:06.846
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 05/16/23 14:21:06.891
    May 16 14:21:06.891: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    May 16 14:21:06.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-4419" for this suite. 05/16/23 14:21:06.907
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:21:06.915
May 16 14:21:06.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename secrets 05/16/23 14:21:06.916
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:06.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:06.94
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
May 16 14:21:06.949: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-56e72f14-c7f2-4d97-9e74-a7ec84bcf1f3 05/16/23 14:21:06.949
STEP: Creating secret with name s-test-opt-upd-4cab837a-94c3-4894-b284-77da61524003 05/16/23 14:21:06.954
STEP: Creating the pod 05/16/23 14:21:06.959
May 16 14:21:06.975: INFO: Waiting up to 5m0s for pod "pod-secrets-e040c560-6ad2-4855-a3b7-fdf1e7d3188d" in namespace "secrets-4612" to be "running and ready"
May 16 14:21:06.982: INFO: Pod "pod-secrets-e040c560-6ad2-4855-a3b7-fdf1e7d3188d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.237438ms
May 16 14:21:06.982: INFO: The phase of Pod pod-secrets-e040c560-6ad2-4855-a3b7-fdf1e7d3188d is Pending, waiting for it to be Running (with Ready = true)
May 16 14:21:08.985: INFO: Pod "pod-secrets-e040c560-6ad2-4855-a3b7-fdf1e7d3188d": Phase="Running", Reason="", readiness=true. Elapsed: 2.009746018s
May 16 14:21:08.985: INFO: The phase of Pod pod-secrets-e040c560-6ad2-4855-a3b7-fdf1e7d3188d is Running (Ready = true)
May 16 14:21:08.985: INFO: Pod "pod-secrets-e040c560-6ad2-4855-a3b7-fdf1e7d3188d" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-56e72f14-c7f2-4d97-9e74-a7ec84bcf1f3 05/16/23 14:21:09.004
STEP: Updating secret s-test-opt-upd-4cab837a-94c3-4894-b284-77da61524003 05/16/23 14:21:09.009
STEP: Creating secret with name s-test-opt-create-bfcb142b-1805-4392-a811-80b20a6f9953 05/16/23 14:21:09.013
STEP: waiting to observe update in volume 05/16/23 14:21:09.018
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May 16 14:21:13.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4612" for this suite. 05/16/23 14:21:13.051
------------------------------
â€¢ [SLOW TEST] [6.141 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:21:06.915
    May 16 14:21:06.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename secrets 05/16/23 14:21:06.916
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:06.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:06.94
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    May 16 14:21:06.949: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating secret with name s-test-opt-del-56e72f14-c7f2-4d97-9e74-a7ec84bcf1f3 05/16/23 14:21:06.949
    STEP: Creating secret with name s-test-opt-upd-4cab837a-94c3-4894-b284-77da61524003 05/16/23 14:21:06.954
    STEP: Creating the pod 05/16/23 14:21:06.959
    May 16 14:21:06.975: INFO: Waiting up to 5m0s for pod "pod-secrets-e040c560-6ad2-4855-a3b7-fdf1e7d3188d" in namespace "secrets-4612" to be "running and ready"
    May 16 14:21:06.982: INFO: Pod "pod-secrets-e040c560-6ad2-4855-a3b7-fdf1e7d3188d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.237438ms
    May 16 14:21:06.982: INFO: The phase of Pod pod-secrets-e040c560-6ad2-4855-a3b7-fdf1e7d3188d is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:21:08.985: INFO: Pod "pod-secrets-e040c560-6ad2-4855-a3b7-fdf1e7d3188d": Phase="Running", Reason="", readiness=true. Elapsed: 2.009746018s
    May 16 14:21:08.985: INFO: The phase of Pod pod-secrets-e040c560-6ad2-4855-a3b7-fdf1e7d3188d is Running (Ready = true)
    May 16 14:21:08.985: INFO: Pod "pod-secrets-e040c560-6ad2-4855-a3b7-fdf1e7d3188d" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-56e72f14-c7f2-4d97-9e74-a7ec84bcf1f3 05/16/23 14:21:09.004
    STEP: Updating secret s-test-opt-upd-4cab837a-94c3-4894-b284-77da61524003 05/16/23 14:21:09.009
    STEP: Creating secret with name s-test-opt-create-bfcb142b-1805-4392-a811-80b20a6f9953 05/16/23 14:21:09.013
    STEP: waiting to observe update in volume 05/16/23 14:21:09.018
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May 16 14:21:13.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4612" for this suite. 05/16/23 14:21:13.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:21:13.057
May 16 14:21:13.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename limitrange 05/16/23 14:21:13.058
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:13.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:13.078
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-qt585" in namespace "limitrange-8461" 05/16/23 14:21:13.081
STEP: Creating another limitRange in another namespace 05/16/23 14:21:13.087
May 16 14:21:13.111: INFO: Namespace "e2e-limitrange-qt585-116" created
May 16 14:21:13.111: INFO: Creating LimitRange "e2e-limitrange-qt585" in namespace "e2e-limitrange-qt585-116"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-qt585" 05/16/23 14:21:13.125
May 16 14:21:13.130: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-qt585" in "limitrange-8461" namespace 05/16/23 14:21:13.13
May 16 14:21:13.138: INFO: LimitRange "e2e-limitrange-qt585" has been patched
STEP: Delete LimitRange "e2e-limitrange-qt585" by Collection with labelSelector: "e2e-limitrange-qt585=patched" 05/16/23 14:21:13.138
STEP: Confirm that the limitRange "e2e-limitrange-qt585" has been deleted 05/16/23 14:21:13.153
May 16 14:21:13.153: INFO: Requesting list of LimitRange to confirm quantity
May 16 14:21:13.157: INFO: Found 0 LimitRange with label "e2e-limitrange-qt585=patched"
May 16 14:21:13.157: INFO: LimitRange "e2e-limitrange-qt585" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-qt585" 05/16/23 14:21:13.157
May 16 14:21:13.168: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
May 16 14:21:13.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-8461" for this suite. 05/16/23 14:21:13.173
STEP: Destroying namespace "e2e-limitrange-qt585-116" for this suite. 05/16/23 14:21:13.179
------------------------------
â€¢ [0.128 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:21:13.057
    May 16 14:21:13.057: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename limitrange 05/16/23 14:21:13.058
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:13.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:13.078
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-qt585" in namespace "limitrange-8461" 05/16/23 14:21:13.081
    STEP: Creating another limitRange in another namespace 05/16/23 14:21:13.087
    May 16 14:21:13.111: INFO: Namespace "e2e-limitrange-qt585-116" created
    May 16 14:21:13.111: INFO: Creating LimitRange "e2e-limitrange-qt585" in namespace "e2e-limitrange-qt585-116"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-qt585" 05/16/23 14:21:13.125
    May 16 14:21:13.130: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-qt585" in "limitrange-8461" namespace 05/16/23 14:21:13.13
    May 16 14:21:13.138: INFO: LimitRange "e2e-limitrange-qt585" has been patched
    STEP: Delete LimitRange "e2e-limitrange-qt585" by Collection with labelSelector: "e2e-limitrange-qt585=patched" 05/16/23 14:21:13.138
    STEP: Confirm that the limitRange "e2e-limitrange-qt585" has been deleted 05/16/23 14:21:13.153
    May 16 14:21:13.153: INFO: Requesting list of LimitRange to confirm quantity
    May 16 14:21:13.157: INFO: Found 0 LimitRange with label "e2e-limitrange-qt585=patched"
    May 16 14:21:13.157: INFO: LimitRange "e2e-limitrange-qt585" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-qt585" 05/16/23 14:21:13.157
    May 16 14:21:13.168: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    May 16 14:21:13.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-8461" for this suite. 05/16/23 14:21:13.173
    STEP: Destroying namespace "e2e-limitrange-qt585-116" for this suite. 05/16/23 14:21:13.179
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:21:13.185
May 16 14:21:13.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 14:21:13.186
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:13.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:13.205
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-223e2d68-49d8-432e-848f-3ef7470b1f08 05/16/23 14:21:13.208
STEP: Creating a pod to test consume configMaps 05/16/23 14:21:13.217
May 16 14:21:13.243: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0b20f9b4-81e5-4822-9b82-dcee5f0bfb7e" in namespace "projected-6570" to be "Succeeded or Failed"
May 16 14:21:13.246: INFO: Pod "pod-projected-configmaps-0b20f9b4-81e5-4822-9b82-dcee5f0bfb7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.282016ms
May 16 14:21:15.250: INFO: Pod "pod-projected-configmaps-0b20f9b4-81e5-4822-9b82-dcee5f0bfb7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006374677s
May 16 14:21:17.250: INFO: Pod "pod-projected-configmaps-0b20f9b4-81e5-4822-9b82-dcee5f0bfb7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00637714s
STEP: Saw pod success 05/16/23 14:21:17.25
May 16 14:21:17.250: INFO: Pod "pod-projected-configmaps-0b20f9b4-81e5-4822-9b82-dcee5f0bfb7e" satisfied condition "Succeeded or Failed"
May 16 14:21:17.252: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod pod-projected-configmaps-0b20f9b4-81e5-4822-9b82-dcee5f0bfb7e container agnhost-container: <nil>
STEP: delete the pod 05/16/23 14:21:17.262
May 16 14:21:17.272: INFO: Waiting for pod pod-projected-configmaps-0b20f9b4-81e5-4822-9b82-dcee5f0bfb7e to disappear
May 16 14:21:17.274: INFO: Pod pod-projected-configmaps-0b20f9b4-81e5-4822-9b82-dcee5f0bfb7e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May 16 14:21:17.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6570" for this suite. 05/16/23 14:21:17.277
------------------------------
â€¢ [4.099 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:21:13.185
    May 16 14:21:13.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 14:21:13.186
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:13.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:13.205
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-223e2d68-49d8-432e-848f-3ef7470b1f08 05/16/23 14:21:13.208
    STEP: Creating a pod to test consume configMaps 05/16/23 14:21:13.217
    May 16 14:21:13.243: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0b20f9b4-81e5-4822-9b82-dcee5f0bfb7e" in namespace "projected-6570" to be "Succeeded or Failed"
    May 16 14:21:13.246: INFO: Pod "pod-projected-configmaps-0b20f9b4-81e5-4822-9b82-dcee5f0bfb7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.282016ms
    May 16 14:21:15.250: INFO: Pod "pod-projected-configmaps-0b20f9b4-81e5-4822-9b82-dcee5f0bfb7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006374677s
    May 16 14:21:17.250: INFO: Pod "pod-projected-configmaps-0b20f9b4-81e5-4822-9b82-dcee5f0bfb7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00637714s
    STEP: Saw pod success 05/16/23 14:21:17.25
    May 16 14:21:17.250: INFO: Pod "pod-projected-configmaps-0b20f9b4-81e5-4822-9b82-dcee5f0bfb7e" satisfied condition "Succeeded or Failed"
    May 16 14:21:17.252: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod pod-projected-configmaps-0b20f9b4-81e5-4822-9b82-dcee5f0bfb7e container agnhost-container: <nil>
    STEP: delete the pod 05/16/23 14:21:17.262
    May 16 14:21:17.272: INFO: Waiting for pod pod-projected-configmaps-0b20f9b4-81e5-4822-9b82-dcee5f0bfb7e to disappear
    May 16 14:21:17.274: INFO: Pod pod-projected-configmaps-0b20f9b4-81e5-4822-9b82-dcee5f0bfb7e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May 16 14:21:17.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6570" for this suite. 05/16/23 14:21:17.277
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:21:17.284
May 16 14:21:17.284: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename downward-api 05/16/23 14:21:17.285
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:17.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:17.303
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 05/16/23 14:21:17.306
May 16 14:21:17.321: INFO: Waiting up to 5m0s for pod "downward-api-7ff39b2e-7e43-4263-961a-4394ad858985" in namespace "downward-api-6227" to be "Succeeded or Failed"
May 16 14:21:17.327: INFO: Pod "downward-api-7ff39b2e-7e43-4263-961a-4394ad858985": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082499ms
May 16 14:21:19.332: INFO: Pod "downward-api-7ff39b2e-7e43-4263-961a-4394ad858985": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011442569s
May 16 14:21:21.330: INFO: Pod "downward-api-7ff39b2e-7e43-4263-961a-4394ad858985": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00942833s
STEP: Saw pod success 05/16/23 14:21:21.33
May 16 14:21:21.330: INFO: Pod "downward-api-7ff39b2e-7e43-4263-961a-4394ad858985" satisfied condition "Succeeded or Failed"
May 16 14:21:21.334: INFO: Trying to get logs from node ip-10-0-212-246.eu-central-1.compute.internal pod downward-api-7ff39b2e-7e43-4263-961a-4394ad858985 container dapi-container: <nil>
STEP: delete the pod 05/16/23 14:21:21.345
May 16 14:21:21.353: INFO: Waiting for pod downward-api-7ff39b2e-7e43-4263-961a-4394ad858985 to disappear
May 16 14:21:21.356: INFO: Pod downward-api-7ff39b2e-7e43-4263-961a-4394ad858985 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May 16 14:21:21.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6227" for this suite. 05/16/23 14:21:21.361
------------------------------
â€¢ [4.082 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:21:17.284
    May 16 14:21:17.284: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename downward-api 05/16/23 14:21:17.285
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:17.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:17.303
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 05/16/23 14:21:17.306
    May 16 14:21:17.321: INFO: Waiting up to 5m0s for pod "downward-api-7ff39b2e-7e43-4263-961a-4394ad858985" in namespace "downward-api-6227" to be "Succeeded or Failed"
    May 16 14:21:17.327: INFO: Pod "downward-api-7ff39b2e-7e43-4263-961a-4394ad858985": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082499ms
    May 16 14:21:19.332: INFO: Pod "downward-api-7ff39b2e-7e43-4263-961a-4394ad858985": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011442569s
    May 16 14:21:21.330: INFO: Pod "downward-api-7ff39b2e-7e43-4263-961a-4394ad858985": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00942833s
    STEP: Saw pod success 05/16/23 14:21:21.33
    May 16 14:21:21.330: INFO: Pod "downward-api-7ff39b2e-7e43-4263-961a-4394ad858985" satisfied condition "Succeeded or Failed"
    May 16 14:21:21.334: INFO: Trying to get logs from node ip-10-0-212-246.eu-central-1.compute.internal pod downward-api-7ff39b2e-7e43-4263-961a-4394ad858985 container dapi-container: <nil>
    STEP: delete the pod 05/16/23 14:21:21.345
    May 16 14:21:21.353: INFO: Waiting for pod downward-api-7ff39b2e-7e43-4263-961a-4394ad858985 to disappear
    May 16 14:21:21.356: INFO: Pod downward-api-7ff39b2e-7e43-4263-961a-4394ad858985 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May 16 14:21:21.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6227" for this suite. 05/16/23 14:21:21.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:21:21.367
May 16 14:21:21.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 14:21:21.368
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:21.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:21.387
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-837f1999-327c-47a7-a272-e98f854b0aca 05/16/23 14:21:21.389
STEP: Creating a pod to test consume secrets 05/16/23 14:21:21.395
May 16 14:21:21.412: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6ea22e82-b68e-445c-a0b6-473a396fbf7a" in namespace "projected-7" to be "Succeeded or Failed"
May 16 14:21:21.416: INFO: Pod "pod-projected-secrets-6ea22e82-b68e-445c-a0b6-473a396fbf7a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.436017ms
May 16 14:21:23.421: INFO: Pod "pod-projected-secrets-6ea22e82-b68e-445c-a0b6-473a396fbf7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009047392s
May 16 14:21:25.420: INFO: Pod "pod-projected-secrets-6ea22e82-b68e-445c-a0b6-473a396fbf7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008361615s
STEP: Saw pod success 05/16/23 14:21:25.42
May 16 14:21:25.420: INFO: Pod "pod-projected-secrets-6ea22e82-b68e-445c-a0b6-473a396fbf7a" satisfied condition "Succeeded or Failed"
May 16 14:21:25.422: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-secrets-6ea22e82-b68e-445c-a0b6-473a396fbf7a container projected-secret-volume-test: <nil>
STEP: delete the pod 05/16/23 14:21:25.428
May 16 14:21:25.436: INFO: Waiting for pod pod-projected-secrets-6ea22e82-b68e-445c-a0b6-473a396fbf7a to disappear
May 16 14:21:25.440: INFO: Pod pod-projected-secrets-6ea22e82-b68e-445c-a0b6-473a396fbf7a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May 16 14:21:25.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7" for this suite. 05/16/23 14:21:25.443
------------------------------
â€¢ [4.083 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:21:21.367
    May 16 14:21:21.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 14:21:21.368
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:21.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:21.387
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-837f1999-327c-47a7-a272-e98f854b0aca 05/16/23 14:21:21.389
    STEP: Creating a pod to test consume secrets 05/16/23 14:21:21.395
    May 16 14:21:21.412: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6ea22e82-b68e-445c-a0b6-473a396fbf7a" in namespace "projected-7" to be "Succeeded or Failed"
    May 16 14:21:21.416: INFO: Pod "pod-projected-secrets-6ea22e82-b68e-445c-a0b6-473a396fbf7a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.436017ms
    May 16 14:21:23.421: INFO: Pod "pod-projected-secrets-6ea22e82-b68e-445c-a0b6-473a396fbf7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009047392s
    May 16 14:21:25.420: INFO: Pod "pod-projected-secrets-6ea22e82-b68e-445c-a0b6-473a396fbf7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008361615s
    STEP: Saw pod success 05/16/23 14:21:25.42
    May 16 14:21:25.420: INFO: Pod "pod-projected-secrets-6ea22e82-b68e-445c-a0b6-473a396fbf7a" satisfied condition "Succeeded or Failed"
    May 16 14:21:25.422: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-secrets-6ea22e82-b68e-445c-a0b6-473a396fbf7a container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/16/23 14:21:25.428
    May 16 14:21:25.436: INFO: Waiting for pod pod-projected-secrets-6ea22e82-b68e-445c-a0b6-473a396fbf7a to disappear
    May 16 14:21:25.440: INFO: Pod pod-projected-secrets-6ea22e82-b68e-445c-a0b6-473a396fbf7a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May 16 14:21:25.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7" for this suite. 05/16/23 14:21:25.443
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:21:25.45
May 16 14:21:25.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename services 05/16/23 14:21:25.451
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:25.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:25.474
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 05/16/23 14:21:25.477
May 16 14:21:25.477: INFO: Creating e2e-svc-a-psqb5
May 16 14:21:25.495: INFO: Creating e2e-svc-b-fgfzg
May 16 14:21:25.520: INFO: Creating e2e-svc-c-vppv9
STEP: deleting service collection 05/16/23 14:21:25.561
May 16 14:21:25.616: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 16 14:21:25.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3546" for this suite. 05/16/23 14:21:25.626
------------------------------
â€¢ [0.189 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:21:25.45
    May 16 14:21:25.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename services 05/16/23 14:21:25.451
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:25.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:25.474
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 05/16/23 14:21:25.477
    May 16 14:21:25.477: INFO: Creating e2e-svc-a-psqb5
    May 16 14:21:25.495: INFO: Creating e2e-svc-b-fgfzg
    May 16 14:21:25.520: INFO: Creating e2e-svc-c-vppv9
    STEP: deleting service collection 05/16/23 14:21:25.561
    May 16 14:21:25.616: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 16 14:21:25.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3546" for this suite. 05/16/23 14:21:25.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:21:25.64
May 16 14:21:25.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename pod-network-test 05/16/23 14:21:25.64
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:25.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:25.668
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-889 05/16/23 14:21:25.671
STEP: creating a selector 05/16/23 14:21:25.671
STEP: Creating the service pods in kubernetes 05/16/23 14:21:25.671
May 16 14:21:25.672: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
W0516 14:21:25.701074      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:21:25.748: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-889" to be "running and ready"
May 16 14:21:25.752: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.154756ms
May 16 14:21:25.752: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 16 14:21:27.756: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.007722722s
May 16 14:21:27.756: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 14:21:29.758: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009819343s
May 16 14:21:29.758: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 14:21:31.756: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008528705s
May 16 14:21:31.756: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 14:21:33.756: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008341045s
May 16 14:21:33.756: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 14:21:35.755: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007507242s
May 16 14:21:35.755: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 14:21:37.756: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.008279303s
May 16 14:21:37.756: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May 16 14:21:37.756: INFO: Pod "netserver-0" satisfied condition "running and ready"
May 16 14:21:37.760: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-889" to be "running and ready"
May 16 14:21:37.763: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.037811ms
May 16 14:21:37.763: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May 16 14:21:37.763: INFO: Pod "netserver-1" satisfied condition "running and ready"
May 16 14:21:37.765: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-889" to be "running and ready"
May 16 14:21:37.767: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.069437ms
May 16 14:21:37.767: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May 16 14:21:37.767: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/16/23 14:21:37.77
May 16 14:21:37.784: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-889" to be "running"
May 16 14:21:37.786: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.197865ms
May 16 14:21:39.791: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006899964s
May 16 14:21:39.791: INFO: Pod "test-container-pod" satisfied condition "running"
May 16 14:21:39.794: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-889" to be "running"
May 16 14:21:39.796: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.30131ms
May 16 14:21:39.796: INFO: Pod "host-test-container-pod" satisfied condition "running"
May 16 14:21:39.799: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 16 14:21:39.799: INFO: Going to poll 10.128.2.27 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 16 14:21:39.802: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.2.27 8081 | grep -v '^\s*$'] Namespace:pod-network-test-889 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 14:21:39.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 14:21:39.802: INFO: ExecWithOptions: Clientset creation
May 16 14:21:39.802: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-889/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.2.27+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 16 14:21:40.900: INFO: Found all 1 expected endpoints: [netserver-0]
May 16 14:21:40.900: INFO: Going to poll 10.131.0.53 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 16 14:21:40.903: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.131.0.53 8081 | grep -v '^\s*$'] Namespace:pod-network-test-889 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 14:21:40.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 14:21:40.903: INFO: ExecWithOptions: Clientset creation
May 16 14:21:40.903: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-889/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.131.0.53+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 16 14:21:42.006: INFO: Found all 1 expected endpoints: [netserver-1]
May 16 14:21:42.006: INFO: Going to poll 10.129.2.30 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 16 14:21:42.009: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.129.2.30 8081 | grep -v '^\s*$'] Namespace:pod-network-test-889 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 14:21:42.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 14:21:42.010: INFO: ExecWithOptions: Clientset creation
May 16 14:21:42.010: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-889/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.129.2.30+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 16 14:21:43.114: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
May 16 14:21:43.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-889" for this suite. 05/16/23 14:21:43.12
------------------------------
â€¢ [SLOW TEST] [17.487 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:21:25.64
    May 16 14:21:25.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename pod-network-test 05/16/23 14:21:25.64
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:25.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:25.668
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-889 05/16/23 14:21:25.671
    STEP: creating a selector 05/16/23 14:21:25.671
    STEP: Creating the service pods in kubernetes 05/16/23 14:21:25.671
    May 16 14:21:25.672: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    W0516 14:21:25.701074      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:21:25.748: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-889" to be "running and ready"
    May 16 14:21:25.752: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.154756ms
    May 16 14:21:25.752: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:21:27.756: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.007722722s
    May 16 14:21:27.756: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 14:21:29.758: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009819343s
    May 16 14:21:29.758: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 14:21:31.756: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008528705s
    May 16 14:21:31.756: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 14:21:33.756: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008341045s
    May 16 14:21:33.756: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 14:21:35.755: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007507242s
    May 16 14:21:35.755: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 14:21:37.756: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.008279303s
    May 16 14:21:37.756: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May 16 14:21:37.756: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May 16 14:21:37.760: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-889" to be "running and ready"
    May 16 14:21:37.763: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.037811ms
    May 16 14:21:37.763: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May 16 14:21:37.763: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May 16 14:21:37.765: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-889" to be "running and ready"
    May 16 14:21:37.767: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.069437ms
    May 16 14:21:37.767: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May 16 14:21:37.767: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/16/23 14:21:37.77
    May 16 14:21:37.784: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-889" to be "running"
    May 16 14:21:37.786: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.197865ms
    May 16 14:21:39.791: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006899964s
    May 16 14:21:39.791: INFO: Pod "test-container-pod" satisfied condition "running"
    May 16 14:21:39.794: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-889" to be "running"
    May 16 14:21:39.796: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.30131ms
    May 16 14:21:39.796: INFO: Pod "host-test-container-pod" satisfied condition "running"
    May 16 14:21:39.799: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May 16 14:21:39.799: INFO: Going to poll 10.128.2.27 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    May 16 14:21:39.802: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.2.27 8081 | grep -v '^\s*$'] Namespace:pod-network-test-889 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 14:21:39.802: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 14:21:39.802: INFO: ExecWithOptions: Clientset creation
    May 16 14:21:39.802: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-889/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.2.27+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 16 14:21:40.900: INFO: Found all 1 expected endpoints: [netserver-0]
    May 16 14:21:40.900: INFO: Going to poll 10.131.0.53 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    May 16 14:21:40.903: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.131.0.53 8081 | grep -v '^\s*$'] Namespace:pod-network-test-889 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 14:21:40.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 14:21:40.903: INFO: ExecWithOptions: Clientset creation
    May 16 14:21:40.903: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-889/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.131.0.53+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 16 14:21:42.006: INFO: Found all 1 expected endpoints: [netserver-1]
    May 16 14:21:42.006: INFO: Going to poll 10.129.2.30 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    May 16 14:21:42.009: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.129.2.30 8081 | grep -v '^\s*$'] Namespace:pod-network-test-889 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 14:21:42.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 14:21:42.010: INFO: ExecWithOptions: Clientset creation
    May 16 14:21:42.010: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-889/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.129.2.30+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 16 14:21:43.114: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    May 16 14:21:43.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-889" for this suite. 05/16/23 14:21:43.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:21:43.128
May 16 14:21:43.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename services 05/16/23 14:21:43.129
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:43.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:43.147
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8296 05/16/23 14:21:43.149
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/16/23 14:21:43.187
STEP: creating service externalsvc in namespace services-8296 05/16/23 14:21:43.187
STEP: creating replication controller externalsvc in namespace services-8296 05/16/23 14:21:43.206
I0516 14:21:43.214751      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8296, replica count: 2
I0516 14:21:46.265816      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 05/16/23 14:21:46.269
May 16 14:21:46.287: INFO: Creating new exec pod
May 16 14:21:46.297: INFO: Waiting up to 5m0s for pod "execpodmv5v4" in namespace "services-8296" to be "running"
May 16 14:21:46.300: INFO: Pod "execpodmv5v4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.56489ms
May 16 14:21:48.304: INFO: Pod "execpodmv5v4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007477327s
May 16 14:21:48.304: INFO: Pod "execpodmv5v4" satisfied condition "running"
May 16 14:21:48.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-8296 exec execpodmv5v4 -- /bin/sh -x -c nslookup nodeport-service.services-8296.svc.cluster.local'
May 16 14:21:48.424: INFO: stderr: "+ nslookup nodeport-service.services-8296.svc.cluster.local\n"
May 16 14:21:48.424: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nnodeport-service.services-8296.svc.cluster.local\tcanonical name = externalsvc.services-8296.svc.cluster.local.\nName:\texternalsvc.services-8296.svc.cluster.local\nAddress: 172.30.114.83\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8296, will wait for the garbage collector to delete the pods 05/16/23 14:21:48.424
May 16 14:21:48.483: INFO: Deleting ReplicationController externalsvc took: 4.676617ms
May 16 14:21:48.583: INFO: Terminating ReplicationController externalsvc pods took: 100.692546ms
May 16 14:21:50.708: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 16 14:21:50.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8296" for this suite. 05/16/23 14:21:50.729
------------------------------
â€¢ [SLOW TEST] [7.610 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:21:43.128
    May 16 14:21:43.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename services 05/16/23 14:21:43.129
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:43.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:43.147
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-8296 05/16/23 14:21:43.149
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/16/23 14:21:43.187
    STEP: creating service externalsvc in namespace services-8296 05/16/23 14:21:43.187
    STEP: creating replication controller externalsvc in namespace services-8296 05/16/23 14:21:43.206
    I0516 14:21:43.214751      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8296, replica count: 2
    I0516 14:21:46.265816      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 05/16/23 14:21:46.269
    May 16 14:21:46.287: INFO: Creating new exec pod
    May 16 14:21:46.297: INFO: Waiting up to 5m0s for pod "execpodmv5v4" in namespace "services-8296" to be "running"
    May 16 14:21:46.300: INFO: Pod "execpodmv5v4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.56489ms
    May 16 14:21:48.304: INFO: Pod "execpodmv5v4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007477327s
    May 16 14:21:48.304: INFO: Pod "execpodmv5v4" satisfied condition "running"
    May 16 14:21:48.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-8296 exec execpodmv5v4 -- /bin/sh -x -c nslookup nodeport-service.services-8296.svc.cluster.local'
    May 16 14:21:48.424: INFO: stderr: "+ nslookup nodeport-service.services-8296.svc.cluster.local\n"
    May 16 14:21:48.424: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nnodeport-service.services-8296.svc.cluster.local\tcanonical name = externalsvc.services-8296.svc.cluster.local.\nName:\texternalsvc.services-8296.svc.cluster.local\nAddress: 172.30.114.83\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8296, will wait for the garbage collector to delete the pods 05/16/23 14:21:48.424
    May 16 14:21:48.483: INFO: Deleting ReplicationController externalsvc took: 4.676617ms
    May 16 14:21:48.583: INFO: Terminating ReplicationController externalsvc pods took: 100.692546ms
    May 16 14:21:50.708: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 16 14:21:50.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8296" for this suite. 05/16/23 14:21:50.729
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:21:50.739
May 16 14:21:50.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename lease-test 05/16/23 14:21:50.739
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:50.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:50.769
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
May 16 14:21:50.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-7755" for this suite. 05/16/23 14:21:50.847
------------------------------
â€¢ [0.115 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:21:50.739
    May 16 14:21:50.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename lease-test 05/16/23 14:21:50.739
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:50.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:50.769
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    May 16 14:21:50.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-7755" for this suite. 05/16/23 14:21:50.847
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:21:50.854
May 16 14:21:50.854: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename resourcequota 05/16/23 14:21:50.855
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:50.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:50.873
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 05/16/23 14:21:50.876
STEP: Getting a ResourceQuota 05/16/23 14:21:50.881
STEP: Listing all ResourceQuotas with LabelSelector 05/16/23 14:21:50.886
STEP: Patching the ResourceQuota 05/16/23 14:21:50.891
STEP: Deleting a Collection of ResourceQuotas 05/16/23 14:21:50.897
STEP: Verifying the deleted ResourceQuota 05/16/23 14:21:50.911
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 16 14:21:50.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5561" for this suite. 05/16/23 14:21:50.922
------------------------------
â€¢ [0.074 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:21:50.854
    May 16 14:21:50.854: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename resourcequota 05/16/23 14:21:50.855
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:50.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:50.873
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 05/16/23 14:21:50.876
    STEP: Getting a ResourceQuota 05/16/23 14:21:50.881
    STEP: Listing all ResourceQuotas with LabelSelector 05/16/23 14:21:50.886
    STEP: Patching the ResourceQuota 05/16/23 14:21:50.891
    STEP: Deleting a Collection of ResourceQuotas 05/16/23 14:21:50.897
    STEP: Verifying the deleted ResourceQuota 05/16/23 14:21:50.911
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 16 14:21:50.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5561" for this suite. 05/16/23 14:21:50.922
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:21:50.928
May 16 14:21:50.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename crd-publish-openapi 05/16/23 14:21:50.929
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:50.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:50.96
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 05/16/23 14:21:50.962
May 16 14:21:50.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 14:21:54.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 14:22:06.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4498" for this suite. 05/16/23 14:22:06.619
------------------------------
â€¢ [SLOW TEST] [15.696 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:21:50.928
    May 16 14:21:50.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename crd-publish-openapi 05/16/23 14:21:50.929
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:21:50.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:21:50.96
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 05/16/23 14:21:50.962
    May 16 14:21:50.963: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 14:21:54.448: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 14:22:06.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4498" for this suite. 05/16/23 14:22:06.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:22:06.624
May 16 14:22:06.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename dns 05/16/23 14:22:06.625
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:06.638
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:06.641
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 05/16/23 14:22:06.642
May 16 14:22:07.655: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1187  b28e3e17-7792-4bf5-a1ee-079178d699a5 43835 0 2023-05-16 14:22:07 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] [] [{e2e.test Update v1 2023-05-16 14:22:06 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9mt5f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9mt5f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c36,c25,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 16 14:22:07.656: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1187" to be "running and ready"
May 16 14:22:07.658: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.759517ms
May 16 14:22:07.658: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May 16 14:22:09.663: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.006900428s
May 16 14:22:09.663: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
May 16 14:22:09.663: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 05/16/23 14:22:09.663
May 16 14:22:09.663: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1187 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 14:22:09.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 14:22:09.663: INFO: ExecWithOptions: Clientset creation
May 16 14:22:09.663: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/dns-1187/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 05/16/23 14:22:09.742
May 16 14:22:09.742: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1187 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 14:22:09.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 14:22:09.742: INFO: ExecWithOptions: Clientset creation
May 16 14:22:09.742: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/dns-1187/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 16 14:22:09.812: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May 16 14:22:09.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1187" for this suite. 05/16/23 14:22:09.828
------------------------------
â€¢ [3.209 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:22:06.624
    May 16 14:22:06.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename dns 05/16/23 14:22:06.625
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:06.638
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:06.641
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 05/16/23 14:22:06.642
    May 16 14:22:07.655: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1187  b28e3e17-7792-4bf5-a1ee-079178d699a5 43835 0 2023-05-16 14:22:07 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] [] [{e2e.test Update v1 2023-05-16 14:22:06 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9mt5f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9mt5f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c36,c25,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 16 14:22:07.656: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1187" to be "running and ready"
    May 16 14:22:07.658: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.759517ms
    May 16 14:22:07.658: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:22:09.663: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.006900428s
    May 16 14:22:09.663: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    May 16 14:22:09.663: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 05/16/23 14:22:09.663
    May 16 14:22:09.663: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1187 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 14:22:09.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 14:22:09.663: INFO: ExecWithOptions: Clientset creation
    May 16 14:22:09.663: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/dns-1187/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 05/16/23 14:22:09.742
    May 16 14:22:09.742: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1187 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 14:22:09.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 14:22:09.742: INFO: ExecWithOptions: Clientset creation
    May 16 14:22:09.742: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/dns-1187/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 16 14:22:09.812: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May 16 14:22:09.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1187" for this suite. 05/16/23 14:22:09.828
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:22:09.834
May 16 14:22:09.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename configmap 05/16/23 14:22:09.835
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:09.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:09.853
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
May 16 14:22:09.861: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-0ec8a5d7-637a-46ad-8d84-0fb1373d3bab 05/16/23 14:22:09.861
STEP: Creating configMap with name cm-test-opt-upd-f3102782-cb37-4240-80fa-e2f0d2612d3b 05/16/23 14:22:09.866
STEP: Creating the pod 05/16/23 14:22:09.871
May 16 14:22:09.883: INFO: Waiting up to 5m0s for pod "pod-configmaps-71401f03-2a8e-489f-9f53-2986d7fbb5ba" in namespace "configmap-681" to be "running and ready"
May 16 14:22:09.890: INFO: Pod "pod-configmaps-71401f03-2a8e-489f-9f53-2986d7fbb5ba": Phase="Pending", Reason="", readiness=false. Elapsed: 6.654712ms
May 16 14:22:09.890: INFO: The phase of Pod pod-configmaps-71401f03-2a8e-489f-9f53-2986d7fbb5ba is Pending, waiting for it to be Running (with Ready = true)
May 16 14:22:11.894: INFO: Pod "pod-configmaps-71401f03-2a8e-489f-9f53-2986d7fbb5ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.010319163s
May 16 14:22:11.894: INFO: The phase of Pod pod-configmaps-71401f03-2a8e-489f-9f53-2986d7fbb5ba is Running (Ready = true)
May 16 14:22:11.894: INFO: Pod "pod-configmaps-71401f03-2a8e-489f-9f53-2986d7fbb5ba" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-0ec8a5d7-637a-46ad-8d84-0fb1373d3bab 05/16/23 14:22:11.917
STEP: Updating configmap cm-test-opt-upd-f3102782-cb37-4240-80fa-e2f0d2612d3b 05/16/23 14:22:11.922
STEP: Creating configMap with name cm-test-opt-create-422530c9-69b1-457c-8af8-eaf52af304f3 05/16/23 14:22:11.926
STEP: waiting to observe update in volume 05/16/23 14:22:11.93
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 16 14:22:15.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-681" for this suite. 05/16/23 14:22:15.963
------------------------------
â€¢ [SLOW TEST] [6.134 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:22:09.834
    May 16 14:22:09.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename configmap 05/16/23 14:22:09.835
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:09.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:09.853
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    May 16 14:22:09.861: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name cm-test-opt-del-0ec8a5d7-637a-46ad-8d84-0fb1373d3bab 05/16/23 14:22:09.861
    STEP: Creating configMap with name cm-test-opt-upd-f3102782-cb37-4240-80fa-e2f0d2612d3b 05/16/23 14:22:09.866
    STEP: Creating the pod 05/16/23 14:22:09.871
    May 16 14:22:09.883: INFO: Waiting up to 5m0s for pod "pod-configmaps-71401f03-2a8e-489f-9f53-2986d7fbb5ba" in namespace "configmap-681" to be "running and ready"
    May 16 14:22:09.890: INFO: Pod "pod-configmaps-71401f03-2a8e-489f-9f53-2986d7fbb5ba": Phase="Pending", Reason="", readiness=false. Elapsed: 6.654712ms
    May 16 14:22:09.890: INFO: The phase of Pod pod-configmaps-71401f03-2a8e-489f-9f53-2986d7fbb5ba is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:22:11.894: INFO: Pod "pod-configmaps-71401f03-2a8e-489f-9f53-2986d7fbb5ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.010319163s
    May 16 14:22:11.894: INFO: The phase of Pod pod-configmaps-71401f03-2a8e-489f-9f53-2986d7fbb5ba is Running (Ready = true)
    May 16 14:22:11.894: INFO: Pod "pod-configmaps-71401f03-2a8e-489f-9f53-2986d7fbb5ba" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-0ec8a5d7-637a-46ad-8d84-0fb1373d3bab 05/16/23 14:22:11.917
    STEP: Updating configmap cm-test-opt-upd-f3102782-cb37-4240-80fa-e2f0d2612d3b 05/16/23 14:22:11.922
    STEP: Creating configMap with name cm-test-opt-create-422530c9-69b1-457c-8af8-eaf52af304f3 05/16/23 14:22:11.926
    STEP: waiting to observe update in volume 05/16/23 14:22:11.93
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 16 14:22:15.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-681" for this suite. 05/16/23 14:22:15.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:22:15.968
May 16 14:22:15.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename replicaset 05/16/23 14:22:15.969
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:15.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:15.988
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 05/16/23 14:22:15.99
W0516 14:22:16.995938      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:22:16.998: INFO: Pod name sample-pod: Found 0 pods out of 1
May 16 14:22:22.003: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/16/23 14:22:22.003
STEP: getting scale subresource 05/16/23 14:22:22.003
STEP: updating a scale subresource 05/16/23 14:22:22.005
STEP: verifying the replicaset Spec.Replicas was modified 05/16/23 14:22:22.013
STEP: Patch a scale subresource 05/16/23 14:22:22.027
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May 16 14:22:22.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7605" for this suite. 05/16/23 14:22:22.057
------------------------------
â€¢ [SLOW TEST] [6.096 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:22:15.968
    May 16 14:22:15.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename replicaset 05/16/23 14:22:15.969
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:15.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:15.988
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 05/16/23 14:22:15.99
    W0516 14:22:16.995938      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:22:16.998: INFO: Pod name sample-pod: Found 0 pods out of 1
    May 16 14:22:22.003: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/16/23 14:22:22.003
    STEP: getting scale subresource 05/16/23 14:22:22.003
    STEP: updating a scale subresource 05/16/23 14:22:22.005
    STEP: verifying the replicaset Spec.Replicas was modified 05/16/23 14:22:22.013
    STEP: Patch a scale subresource 05/16/23 14:22:22.027
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May 16 14:22:22.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7605" for this suite. 05/16/23 14:22:22.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:22:22.065
May 16 14:22:22.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename resourcequota 05/16/23 14:22:22.065
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:22.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:22.091
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 05/16/23 14:22:22.093
STEP: Getting a ResourceQuota 05/16/23 14:22:22.102
STEP: Updating a ResourceQuota 05/16/23 14:22:22.108
STEP: Verifying a ResourceQuota was modified 05/16/23 14:22:22.115
STEP: Deleting a ResourceQuota 05/16/23 14:22:22.118
STEP: Verifying the deleted ResourceQuota 05/16/23 14:22:22.127
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 16 14:22:22.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9518" for this suite. 05/16/23 14:22:22.135
------------------------------
â€¢ [0.077 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:22:22.065
    May 16 14:22:22.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename resourcequota 05/16/23 14:22:22.065
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:22.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:22.091
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 05/16/23 14:22:22.093
    STEP: Getting a ResourceQuota 05/16/23 14:22:22.102
    STEP: Updating a ResourceQuota 05/16/23 14:22:22.108
    STEP: Verifying a ResourceQuota was modified 05/16/23 14:22:22.115
    STEP: Deleting a ResourceQuota 05/16/23 14:22:22.118
    STEP: Verifying the deleted ResourceQuota 05/16/23 14:22:22.127
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 16 14:22:22.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9518" for this suite. 05/16/23 14:22:22.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:22:22.142
May 16 14:22:22.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename runtimeclass 05/16/23 14:22:22.143
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:22.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:22.16
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
May 16 14:22:22.182: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7221 to be scheduled
May 16 14:22:22.186: INFO: 1 pods are not scheduled: [runtimeclass-7221/test-runtimeclass-runtimeclass-7221-preconfigured-handler-cnn6m(aef8c0b1-f459-4b15-98b1-88600f6b49cd)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May 16 14:22:24.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7221" for this suite. 05/16/23 14:22:24.199
------------------------------
â€¢ [2.061 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:22:22.142
    May 16 14:22:22.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename runtimeclass 05/16/23 14:22:22.143
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:22.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:22.16
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    May 16 14:22:22.182: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7221 to be scheduled
    May 16 14:22:22.186: INFO: 1 pods are not scheduled: [runtimeclass-7221/test-runtimeclass-runtimeclass-7221-preconfigured-handler-cnn6m(aef8c0b1-f459-4b15-98b1-88600f6b49cd)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May 16 14:22:24.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7221" for this suite. 05/16/23 14:22:24.199
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:22:24.204
May 16 14:22:24.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename replicaset 05/16/23 14:22:24.205
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:24.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:24.222
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 05/16/23 14:22:24.224
W0516 14:22:24.237852      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-adoption-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:22:24.237: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-390" to be "running and ready"
May 16 14:22:24.244: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 6.124938ms
May 16 14:22:24.244: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May 16 14:22:26.247: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.009655764s
May 16 14:22:26.247: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
May 16 14:22:26.247: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 05/16/23 14:22:26.25
STEP: Then the orphan pod is adopted 05/16/23 14:22:26.254
STEP: When the matched label of one of its pods change 05/16/23 14:22:27.259
May 16 14:22:27.262: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 05/16/23 14:22:27.273
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May 16 14:22:28.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-390" for this suite. 05/16/23 14:22:28.283
------------------------------
â€¢ [4.084 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:22:24.204
    May 16 14:22:24.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename replicaset 05/16/23 14:22:24.205
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:24.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:24.222
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 05/16/23 14:22:24.224
    W0516 14:22:24.237852      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-adoption-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:22:24.237: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-390" to be "running and ready"
    May 16 14:22:24.244: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 6.124938ms
    May 16 14:22:24.244: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:22:26.247: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.009655764s
    May 16 14:22:26.247: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    May 16 14:22:26.247: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 05/16/23 14:22:26.25
    STEP: Then the orphan pod is adopted 05/16/23 14:22:26.254
    STEP: When the matched label of one of its pods change 05/16/23 14:22:27.259
    May 16 14:22:27.262: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 05/16/23 14:22:27.273
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May 16 14:22:28.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-390" for this suite. 05/16/23 14:22:28.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:22:28.29
May 16 14:22:28.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename tables 05/16/23 14:22:28.29
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:28.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:28.304
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
May 16 14:22:28.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-4172" for this suite. 05/16/23 14:22:28.313
------------------------------
â€¢ [0.036 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:22:28.29
    May 16 14:22:28.290: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename tables 05/16/23 14:22:28.29
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:28.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:28.304
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    May 16 14:22:28.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-4172" for this suite. 05/16/23 14:22:28.313
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:22:28.326
May 16 14:22:28.326: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename watch 05/16/23 14:22:28.327
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:28.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:28.344
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 05/16/23 14:22:28.346
STEP: creating a watch on configmaps with label B 05/16/23 14:22:28.348
STEP: creating a watch on configmaps with label A or B 05/16/23 14:22:28.349
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 05/16/23 14:22:28.351
May 16 14:22:28.358: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9257  9ed1b355-bfbd-4246-8950-a917f4f506c7 44365 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 16 14:22:28.358: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9257  9ed1b355-bfbd-4246-8950-a917f4f506c7 44365 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 05/16/23 14:22:28.358
May 16 14:22:28.366: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9257  9ed1b355-bfbd-4246-8950-a917f4f506c7 44370 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 16 14:22:28.366: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9257  9ed1b355-bfbd-4246-8950-a917f4f506c7 44370 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 05/16/23 14:22:28.366
May 16 14:22:28.372: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9257  9ed1b355-bfbd-4246-8950-a917f4f506c7 44371 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 16 14:22:28.372: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9257  9ed1b355-bfbd-4246-8950-a917f4f506c7 44371 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 05/16/23 14:22:28.372
May 16 14:22:28.378: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9257  9ed1b355-bfbd-4246-8950-a917f4f506c7 44372 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 16 14:22:28.378: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9257  9ed1b355-bfbd-4246-8950-a917f4f506c7 44372 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 05/16/23 14:22:28.379
May 16 14:22:28.385: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9257  2eddf98f-c3a6-4e53-a2e6-d860b278c3e4 44374 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 16 14:22:28.385: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9257  2eddf98f-c3a6-4e53-a2e6-d860b278c3e4 44374 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 05/16/23 14:22:38.386
May 16 14:22:38.392: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9257  2eddf98f-c3a6-4e53-a2e6-d860b278c3e4 44511 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 16 14:22:38.392: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9257  2eddf98f-c3a6-4e53-a2e6-d860b278c3e4 44511 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May 16 14:22:48.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9257" for this suite. 05/16/23 14:22:48.4
------------------------------
â€¢ [SLOW TEST] [20.081 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:22:28.326
    May 16 14:22:28.326: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename watch 05/16/23 14:22:28.327
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:28.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:28.344
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 05/16/23 14:22:28.346
    STEP: creating a watch on configmaps with label B 05/16/23 14:22:28.348
    STEP: creating a watch on configmaps with label A or B 05/16/23 14:22:28.349
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 05/16/23 14:22:28.351
    May 16 14:22:28.358: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9257  9ed1b355-bfbd-4246-8950-a917f4f506c7 44365 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May 16 14:22:28.358: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9257  9ed1b355-bfbd-4246-8950-a917f4f506c7 44365 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 05/16/23 14:22:28.358
    May 16 14:22:28.366: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9257  9ed1b355-bfbd-4246-8950-a917f4f506c7 44370 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 16 14:22:28.366: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9257  9ed1b355-bfbd-4246-8950-a917f4f506c7 44370 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 05/16/23 14:22:28.366
    May 16 14:22:28.372: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9257  9ed1b355-bfbd-4246-8950-a917f4f506c7 44371 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 16 14:22:28.372: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9257  9ed1b355-bfbd-4246-8950-a917f4f506c7 44371 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 05/16/23 14:22:28.372
    May 16 14:22:28.378: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9257  9ed1b355-bfbd-4246-8950-a917f4f506c7 44372 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 16 14:22:28.378: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9257  9ed1b355-bfbd-4246-8950-a917f4f506c7 44372 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 05/16/23 14:22:28.379
    May 16 14:22:28.385: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9257  2eddf98f-c3a6-4e53-a2e6-d860b278c3e4 44374 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May 16 14:22:28.385: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9257  2eddf98f-c3a6-4e53-a2e6-d860b278c3e4 44374 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 05/16/23 14:22:38.386
    May 16 14:22:38.392: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9257  2eddf98f-c3a6-4e53-a2e6-d860b278c3e4 44511 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May 16 14:22:38.392: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9257  2eddf98f-c3a6-4e53-a2e6-d860b278c3e4 44511 0 2023-05-16 14:22:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-16 14:22:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May 16 14:22:48.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9257" for this suite. 05/16/23 14:22:48.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:22:48.407
May 16 14:22:48.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename pods 05/16/23 14:22:48.408
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:48.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:48.423
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 05/16/23 14:22:48.425
STEP: submitting the pod to kubernetes 05/16/23 14:22:48.425
W0516 14:22:48.439914      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: verifying QOS class is set on the pod 05/16/23 14:22:48.439
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
May 16 14:22:48.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4722" for this suite. 05/16/23 14:22:48.45
------------------------------
â€¢ [0.059 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:22:48.407
    May 16 14:22:48.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename pods 05/16/23 14:22:48.408
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:48.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:48.423
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 05/16/23 14:22:48.425
    STEP: submitting the pod to kubernetes 05/16/23 14:22:48.425
    W0516 14:22:48.439914      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: verifying QOS class is set on the pod 05/16/23 14:22:48.439
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    May 16 14:22:48.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4722" for this suite. 05/16/23 14:22:48.45
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:22:48.467
May 16 14:22:48.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename webhook 05/16/23 14:22:48.468
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:48.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:48.49
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/16/23 14:22:48.506
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 14:22:48.735
STEP: Deploying the webhook pod 05/16/23 14:22:48.744
STEP: Wait for the deployment to be ready 05/16/23 14:22:48.754
May 16 14:22:48.760: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/16/23 14:22:50.768
STEP: Verifying the service has paired with the endpoint 05/16/23 14:22:50.778
May 16 14:22:51.778: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 05/16/23 14:22:51.781
STEP: Creating a configMap that does not comply to the validation webhook rules 05/16/23 14:22:51.795
STEP: Updating a validating webhook configuration's rules to not include the create operation 05/16/23 14:22:51.8
STEP: Creating a configMap that does not comply to the validation webhook rules 05/16/23 14:22:51.808
STEP: Patching a validating webhook configuration's rules to include the create operation 05/16/23 14:22:51.816
STEP: Creating a configMap that does not comply to the validation webhook rules 05/16/23 14:22:51.821
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 14:22:51.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3083" for this suite. 05/16/23 14:22:51.875
STEP: Destroying namespace "webhook-3083-markers" for this suite. 05/16/23 14:22:51.882
------------------------------
â€¢ [3.423 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:22:48.467
    May 16 14:22:48.467: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename webhook 05/16/23 14:22:48.468
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:48.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:48.49
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/16/23 14:22:48.506
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 14:22:48.735
    STEP: Deploying the webhook pod 05/16/23 14:22:48.744
    STEP: Wait for the deployment to be ready 05/16/23 14:22:48.754
    May 16 14:22:48.760: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/16/23 14:22:50.768
    STEP: Verifying the service has paired with the endpoint 05/16/23 14:22:50.778
    May 16 14:22:51.778: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 05/16/23 14:22:51.781
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/16/23 14:22:51.795
    STEP: Updating a validating webhook configuration's rules to not include the create operation 05/16/23 14:22:51.8
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/16/23 14:22:51.808
    STEP: Patching a validating webhook configuration's rules to include the create operation 05/16/23 14:22:51.816
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/16/23 14:22:51.821
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 14:22:51.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3083" for this suite. 05/16/23 14:22:51.875
    STEP: Destroying namespace "webhook-3083-markers" for this suite. 05/16/23 14:22:51.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:22:51.891
May 16 14:22:51.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename services 05/16/23 14:22:51.891
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:51.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:51.912
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-5851 05/16/23 14:22:51.914
STEP: creating replication controller nodeport-test in namespace services-5851 05/16/23 14:22:51.95
I0516 14:22:51.962966      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-5851, replica count: 2
I0516 14:22:55.014931      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 16 14:22:55.014: INFO: Creating new exec pod
May 16 14:22:55.024: INFO: Waiting up to 5m0s for pod "execpodz5zs7" in namespace "services-5851" to be "running"
May 16 14:22:55.026: INFO: Pod "execpodz5zs7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.426803ms
May 16 14:22:57.035: INFO: Pod "execpodz5zs7": Phase="Running", Reason="", readiness=true. Elapsed: 2.011458758s
May 16 14:22:57.035: INFO: Pod "execpodz5zs7" satisfied condition "running"
May 16 14:22:58.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5851 exec execpodz5zs7 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
May 16 14:22:58.164: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 16 14:22:58.164: INFO: stdout: ""
May 16 14:22:58.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5851 exec execpodz5zs7 -- /bin/sh -x -c nc -v -z -w 2 172.30.94.229 80'
May 16 14:22:58.282: INFO: stderr: "+ nc -v -z -w 2 172.30.94.229 80\nConnection to 172.30.94.229 80 port [tcp/http] succeeded!\n"
May 16 14:22:58.282: INFO: stdout: ""
May 16 14:22:58.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5851 exec execpodz5zs7 -- /bin/sh -x -c nc -v -z -w 2 10.0.132.142 32672'
May 16 14:22:58.387: INFO: stderr: "+ nc -v -z -w 2 10.0.132.142 32672\nConnection to 10.0.132.142 32672 port [tcp/*] succeeded!\n"
May 16 14:22:58.387: INFO: stdout: ""
May 16 14:22:58.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5851 exec execpodz5zs7 -- /bin/sh -x -c nc -v -z -w 2 10.0.161.164 32672'
May 16 14:22:58.519: INFO: stderr: "+ nc -v -z -w 2 10.0.161.164 32672\nConnection to 10.0.161.164 32672 port [tcp/*] succeeded!\n"
May 16 14:22:58.519: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 16 14:22:58.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5851" for this suite. 05/16/23 14:22:58.525
------------------------------
â€¢ [SLOW TEST] [6.650 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:22:51.891
    May 16 14:22:51.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename services 05/16/23 14:22:51.891
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:51.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:51.912
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-5851 05/16/23 14:22:51.914
    STEP: creating replication controller nodeport-test in namespace services-5851 05/16/23 14:22:51.95
    I0516 14:22:51.962966      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-5851, replica count: 2
    I0516 14:22:55.014931      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 16 14:22:55.014: INFO: Creating new exec pod
    May 16 14:22:55.024: INFO: Waiting up to 5m0s for pod "execpodz5zs7" in namespace "services-5851" to be "running"
    May 16 14:22:55.026: INFO: Pod "execpodz5zs7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.426803ms
    May 16 14:22:57.035: INFO: Pod "execpodz5zs7": Phase="Running", Reason="", readiness=true. Elapsed: 2.011458758s
    May 16 14:22:57.035: INFO: Pod "execpodz5zs7" satisfied condition "running"
    May 16 14:22:58.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5851 exec execpodz5zs7 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    May 16 14:22:58.164: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    May 16 14:22:58.164: INFO: stdout: ""
    May 16 14:22:58.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5851 exec execpodz5zs7 -- /bin/sh -x -c nc -v -z -w 2 172.30.94.229 80'
    May 16 14:22:58.282: INFO: stderr: "+ nc -v -z -w 2 172.30.94.229 80\nConnection to 172.30.94.229 80 port [tcp/http] succeeded!\n"
    May 16 14:22:58.282: INFO: stdout: ""
    May 16 14:22:58.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5851 exec execpodz5zs7 -- /bin/sh -x -c nc -v -z -w 2 10.0.132.142 32672'
    May 16 14:22:58.387: INFO: stderr: "+ nc -v -z -w 2 10.0.132.142 32672\nConnection to 10.0.132.142 32672 port [tcp/*] succeeded!\n"
    May 16 14:22:58.387: INFO: stdout: ""
    May 16 14:22:58.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5851 exec execpodz5zs7 -- /bin/sh -x -c nc -v -z -w 2 10.0.161.164 32672'
    May 16 14:22:58.519: INFO: stderr: "+ nc -v -z -w 2 10.0.161.164 32672\nConnection to 10.0.161.164 32672 port [tcp/*] succeeded!\n"
    May 16 14:22:58.519: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 16 14:22:58.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5851" for this suite. 05/16/23 14:22:58.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:22:58.542
May 16 14:22:58.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename pod-network-test 05/16/23 14:22:58.542
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:58.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:58.565
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-7565 05/16/23 14:22:58.572
STEP: creating a selector 05/16/23 14:22:58.572
STEP: Creating the service pods in kubernetes 05/16/23 14:22:58.572
May 16 14:22:58.572: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 16 14:22:58.628: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7565" to be "running and ready"
May 16 14:22:58.635: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.874203ms
May 16 14:22:58.635: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 16 14:23:00.659: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.030414752s
May 16 14:23:00.659: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 14:23:02.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010688614s
May 16 14:23:02.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 14:23:04.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01107531s
May 16 14:23:04.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 14:23:06.645: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016767837s
May 16 14:23:06.645: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 14:23:08.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011340353s
May 16 14:23:08.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 14:23:10.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011377756s
May 16 14:23:10.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 14:23:12.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011582089s
May 16 14:23:12.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 14:23:14.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.011562901s
May 16 14:23:14.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 14:23:16.638: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009827948s
May 16 14:23:16.638: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 14:23:18.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011802014s
May 16 14:23:18.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 14:23:20.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011089281s
May 16 14:23:20.639: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May 16 14:23:20.639: INFO: Pod "netserver-0" satisfied condition "running and ready"
May 16 14:23:20.642: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7565" to be "running and ready"
May 16 14:23:20.644: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.316695ms
May 16 14:23:20.644: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May 16 14:23:20.644: INFO: Pod "netserver-1" satisfied condition "running and ready"
May 16 14:23:20.646: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7565" to be "running and ready"
May 16 14:23:20.649: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.535971ms
May 16 14:23:20.649: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May 16 14:23:20.649: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/16/23 14:23:20.651
May 16 14:23:20.657: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7565" to be "running"
May 16 14:23:20.660: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.812183ms
May 16 14:23:22.665: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007339553s
May 16 14:23:22.665: INFO: Pod "test-container-pod" satisfied condition "running"
May 16 14:23:22.667: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 16 14:23:22.667: INFO: Breadth first check of 10.128.2.30 on host 10.0.132.142...
May 16 14:23:22.670: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.67:9080/dial?request=hostname&protocol=udp&host=10.128.2.30&port=8081&tries=1'] Namespace:pod-network-test-7565 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 14:23:22.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 14:23:22.670: INFO: ExecWithOptions: Clientset creation
May 16 14:23:22.671: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-7565/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.0.67%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.2.30%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May 16 14:23:22.744: INFO: Waiting for responses: map[]
May 16 14:23:22.744: INFO: reached 10.128.2.30 after 0/1 tries
May 16 14:23:22.744: INFO: Breadth first check of 10.131.0.66 on host 10.0.161.164...
May 16 14:23:22.747: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.67:9080/dial?request=hostname&protocol=udp&host=10.131.0.66&port=8081&tries=1'] Namespace:pod-network-test-7565 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 14:23:22.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 14:23:22.747: INFO: ExecWithOptions: Clientset creation
May 16 14:23:22.747: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-7565/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.0.67%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.131.0.66%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May 16 14:23:22.798: INFO: Waiting for responses: map[]
May 16 14:23:22.798: INFO: reached 10.131.0.66 after 0/1 tries
May 16 14:23:22.798: INFO: Breadth first check of 10.129.2.33 on host 10.0.212.246...
May 16 14:23:22.801: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.67:9080/dial?request=hostname&protocol=udp&host=10.129.2.33&port=8081&tries=1'] Namespace:pod-network-test-7565 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 14:23:22.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 14:23:22.801: INFO: ExecWithOptions: Clientset creation
May 16 14:23:22.801: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-7565/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.0.67%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.129.2.33%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May 16 14:23:22.850: INFO: Waiting for responses: map[]
May 16 14:23:22.850: INFO: reached 10.129.2.33 after 0/1 tries
May 16 14:23:22.850: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
May 16 14:23:22.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7565" for this suite. 05/16/23 14:23:22.854
------------------------------
â€¢ [SLOW TEST] [24.317 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:22:58.542
    May 16 14:22:58.542: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename pod-network-test 05/16/23 14:22:58.542
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:22:58.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:22:58.565
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-7565 05/16/23 14:22:58.572
    STEP: creating a selector 05/16/23 14:22:58.572
    STEP: Creating the service pods in kubernetes 05/16/23 14:22:58.572
    May 16 14:22:58.572: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May 16 14:22:58.628: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7565" to be "running and ready"
    May 16 14:22:58.635: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.874203ms
    May 16 14:22:58.635: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:23:00.659: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.030414752s
    May 16 14:23:00.659: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 14:23:02.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010688614s
    May 16 14:23:02.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 14:23:04.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01107531s
    May 16 14:23:04.639: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 14:23:06.645: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016767837s
    May 16 14:23:06.645: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 14:23:08.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011340353s
    May 16 14:23:08.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 14:23:10.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011377756s
    May 16 14:23:10.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 14:23:12.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011582089s
    May 16 14:23:12.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 14:23:14.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.011562901s
    May 16 14:23:14.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 14:23:16.638: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009827948s
    May 16 14:23:16.638: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 14:23:18.640: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011802014s
    May 16 14:23:18.640: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 14:23:20.639: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011089281s
    May 16 14:23:20.639: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May 16 14:23:20.639: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May 16 14:23:20.642: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7565" to be "running and ready"
    May 16 14:23:20.644: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.316695ms
    May 16 14:23:20.644: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May 16 14:23:20.644: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May 16 14:23:20.646: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7565" to be "running and ready"
    May 16 14:23:20.649: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.535971ms
    May 16 14:23:20.649: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May 16 14:23:20.649: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/16/23 14:23:20.651
    May 16 14:23:20.657: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7565" to be "running"
    May 16 14:23:20.660: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.812183ms
    May 16 14:23:22.665: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007339553s
    May 16 14:23:22.665: INFO: Pod "test-container-pod" satisfied condition "running"
    May 16 14:23:22.667: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May 16 14:23:22.667: INFO: Breadth first check of 10.128.2.30 on host 10.0.132.142...
    May 16 14:23:22.670: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.67:9080/dial?request=hostname&protocol=udp&host=10.128.2.30&port=8081&tries=1'] Namespace:pod-network-test-7565 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 14:23:22.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 14:23:22.670: INFO: ExecWithOptions: Clientset creation
    May 16 14:23:22.671: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-7565/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.0.67%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.2.30%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May 16 14:23:22.744: INFO: Waiting for responses: map[]
    May 16 14:23:22.744: INFO: reached 10.128.2.30 after 0/1 tries
    May 16 14:23:22.744: INFO: Breadth first check of 10.131.0.66 on host 10.0.161.164...
    May 16 14:23:22.747: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.67:9080/dial?request=hostname&protocol=udp&host=10.131.0.66&port=8081&tries=1'] Namespace:pod-network-test-7565 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 14:23:22.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 14:23:22.747: INFO: ExecWithOptions: Clientset creation
    May 16 14:23:22.747: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-7565/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.0.67%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.131.0.66%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May 16 14:23:22.798: INFO: Waiting for responses: map[]
    May 16 14:23:22.798: INFO: reached 10.131.0.66 after 0/1 tries
    May 16 14:23:22.798: INFO: Breadth first check of 10.129.2.33 on host 10.0.212.246...
    May 16 14:23:22.801: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.0.67:9080/dial?request=hostname&protocol=udp&host=10.129.2.33&port=8081&tries=1'] Namespace:pod-network-test-7565 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 14:23:22.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 14:23:22.801: INFO: ExecWithOptions: Clientset creation
    May 16 14:23:22.801: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-7565/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.0.67%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.129.2.33%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May 16 14:23:22.850: INFO: Waiting for responses: map[]
    May 16 14:23:22.850: INFO: reached 10.129.2.33 after 0/1 tries
    May 16 14:23:22.850: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    May 16 14:23:22.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7565" for this suite. 05/16/23 14:23:22.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:23:22.86
May 16 14:23:22.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename disruption 05/16/23 14:23:22.86
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:23:22.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:23:22.878
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 05/16/23 14:23:22.885
STEP: Waiting for all pods to be running 05/16/23 14:23:22.922
May 16 14:23:22.926: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May 16 14:23:24.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6735" for this suite. 05/16/23 14:23:24.944
------------------------------
â€¢ [2.089 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:23:22.86
    May 16 14:23:22.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename disruption 05/16/23 14:23:22.86
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:23:22.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:23:22.878
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 05/16/23 14:23:22.885
    STEP: Waiting for all pods to be running 05/16/23 14:23:22.922
    May 16 14:23:22.926: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May 16 14:23:24.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6735" for this suite. 05/16/23 14:23:24.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:23:24.95
May 16 14:23:24.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename sched-pred 05/16/23 14:23:24.95
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:23:24.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:23:24.973
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
May 16 14:23:24.975: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 16 14:23:24.986: INFO: Waiting for terminating namespaces to be deleted...
May 16 14:23:24.990: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-132-142.eu-central-1.compute.internal before test
May 16 14:23:25.018: INFO: pod-0 from disruption-6735 started at 2023-05-16 14:23:22 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container donothing ready: true, restart count 0
May 16 14:23:25.018: INFO: aws-ebs-csi-driver-node-27d5z from openshift-cluster-csi-drivers started at 2023-05-16 13:52:17 +0000 UTC (3 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container csi-driver ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 16 14:23:25.018: INFO: tuned-j9bjl from openshift-cluster-node-tuning-operator started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container tuned ready: true, restart count 0
May 16 14:23:25.018: INFO: dns-default-9lcb7 from openshift-dns started at 2023-05-16 13:53:17 +0000 UTC (2 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container dns ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.018: INFO: node-resolver-b47gh from openshift-dns started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 16 14:23:25.018: INFO: image-registry-b549d4f89-4ck4v from openshift-image-registry started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container registry ready: true, restart count 0
May 16 14:23:25.018: INFO: node-ca-px455 from openshift-image-registry started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container node-ca ready: true, restart count 0
May 16 14:23:25.018: INFO: ingress-canary-4hv85 from openshift-ingress-canary started at 2023-05-16 13:53:17 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May 16 14:23:25.018: INFO: router-default-8f95c5-srcbl from openshift-ingress started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container router ready: true, restart count 0
May 16 14:23:25.018: INFO: machine-config-daemon-l6bpj from openshift-machine-config-operator started at 2023-05-16 13:52:17 +0000 UTC (2 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 14:23:25.018: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (6 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container alertmanager ready: true, restart count 1
May 16 14:23:25.018: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container config-reloader ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 16 14:23:25.018: INFO: node-exporter-pltn9 from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container node-exporter ready: true, restart count 0
May 16 14:23:25.018: INFO: prometheus-adapter-7767c6bd47-bfq4d from openshift-monitoring started at 2023-05-16 13:53:50 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 16 14:23:25.018: INFO: telemeter-client-859dbffbb4-mt92v from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (3 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container reload ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container telemeter-client ready: true, restart count 0
May 16 14:23:25.018: INFO: thanos-querier-5c9d876b5c-xdt6m from openshift-monitoring started at 2023-05-16 13:53:54 +0000 UTC (6 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container thanos-query ready: true, restart count 0
May 16 14:23:25.018: INFO: multus-2wjqr from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container kube-multus ready: true, restart count 0
May 16 14:23:25.018: INFO: multus-additional-cni-plugins-z2h4l from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May 16 14:23:25.018: INFO: network-metrics-daemon-rqdtf from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (2 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 16 14:23:25.018: INFO: network-check-target-28wts from openshift-network-diagnostics started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container network-check-target-container ready: true, restart count 0
May 16 14:23:25.018: INFO: ovnkube-node-npwgq from openshift-ovn-kubernetes started at 2023-05-16 13:52:17 +0000 UTC (5 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container ovn-acl-logging ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container ovn-controller ready: true, restart count 1
May 16 14:23:25.018: INFO: 	Container ovnkube-node ready: true, restart count 0
May 16 14:23:25.018: INFO: netserver-0 from pod-network-test-7565 started at 2023-05-16 14:22:58 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container webserver ready: true, restart count 0
May 16 14:23:25.018: INFO: sonobuoy from sonobuoy started at 2023-05-16 14:11:07 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 16 14:23:25.018: INFO: sonobuoy-e2e-job-840dd79630d44353 from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container e2e ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 14:23:25.018: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-7zsf6 from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
May 16 14:23:25.018: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 14:23:25.018: INFO: 	Container systemd-logs ready: true, restart count 0
May 16 14:23:25.018: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-161-164.eu-central-1.compute.internal before test
May 16 14:23:25.039: INFO: pod-1 from disruption-6735 started at 2023-05-16 14:23:22 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container donothing ready: true, restart count 0
May 16 14:23:25.039: INFO: aws-ebs-csi-driver-node-9m6vn from openshift-cluster-csi-drivers started at 2023-05-16 13:44:25 +0000 UTC (3 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container csi-driver ready: true, restart count 0
May 16 14:23:25.039: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 16 14:23:25.039: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 16 14:23:25.039: INFO: tuned-6n5b7 from openshift-cluster-node-tuning-operator started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container tuned ready: true, restart count 0
May 16 14:23:25.039: INFO: dns-default-6jx29 from openshift-dns started at 2023-05-16 13:46:22 +0000 UTC (2 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container dns ready: true, restart count 0
May 16 14:23:25.039: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.039: INFO: node-resolver-jbmqq from openshift-dns started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 16 14:23:25.039: INFO: node-ca-gw7rz from openshift-image-registry started at 2023-05-16 13:50:21 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container node-ca ready: true, restart count 0
May 16 14:23:25.039: INFO: ingress-canary-sxs5c from openshift-ingress-canary started at 2023-05-16 13:46:22 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May 16 14:23:25.039: INFO: router-default-8f95c5-q46gb from openshift-ingress started at 2023-05-16 13:46:22 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container router ready: true, restart count 0
May 16 14:23:25.039: INFO: machine-config-daemon-qhdsv from openshift-machine-config-operator started at 2023-05-16 13:44:25 +0000 UTC (2 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 16 14:23:25.039: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 14:23:25.039: INFO: node-exporter-l2zvs from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.039: INFO: 	Container node-exporter ready: true, restart count 0
May 16 14:23:25.039: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-05-16 13:53:57 +0000 UTC (6 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container config-reloader ready: true, restart count 0
May 16 14:23:25.039: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.039: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May 16 14:23:25.039: INFO: 	Container prometheus ready: true, restart count 0
May 16 14:23:25.039: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 16 14:23:25.039: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 16 14:23:25.039: INFO: prometheus-operator-admission-webhook-5d679565bb-sc62w from openshift-monitoring started at 2023-05-16 13:46:22 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
May 16 14:23:25.039: INFO: multus-additional-cni-plugins-jkn7l from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May 16 14:23:25.039: INFO: multus-mc6qb from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container kube-multus ready: true, restart count 0
May 16 14:23:25.039: INFO: network-metrics-daemon-n4p75 from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (2 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.039: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 16 14:23:25.039: INFO: network-check-source-7f6b75fdb6-r7bwb from openshift-network-diagnostics started at 2023-05-16 13:46:22 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container check-endpoints ready: true, restart count 1
May 16 14:23:25.039: INFO: network-check-target-bprb5 from openshift-network-diagnostics started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container network-check-target-container ready: true, restart count 0
May 16 14:23:25.039: INFO: collect-profiles-28070745-bvc57 from openshift-operator-lifecycle-manager started at 2023-05-16 13:46:22 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container collect-profiles ready: false, restart count 0
May 16 14:23:25.039: INFO: collect-profiles-28070745-nqg55 from openshift-operator-lifecycle-manager started at 2023-05-16 13:47:55 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container collect-profiles ready: false, restart count 0
May 16 14:23:25.039: INFO: collect-profiles-28070760-nvgx7 from openshift-operator-lifecycle-manager started at 2023-05-16 14:00:00 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container collect-profiles ready: false, restart count 0
May 16 14:23:25.039: INFO: collect-profiles-28070775-wpbwl from openshift-operator-lifecycle-manager started at 2023-05-16 14:15:00 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container collect-profiles ready: false, restart count 0
May 16 14:23:25.039: INFO: ovnkube-node-w8qn8 from openshift-ovn-kubernetes started at 2023-05-16 13:44:25 +0000 UTC (5 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.039: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
May 16 14:23:25.039: INFO: 	Container ovn-acl-logging ready: true, restart count 0
May 16 14:23:25.039: INFO: 	Container ovn-controller ready: true, restart count 1
May 16 14:23:25.039: INFO: 	Container ovnkube-node ready: true, restart count 0
May 16 14:23:25.039: INFO: netserver-1 from pod-network-test-7565 started at 2023-05-16 14:22:58 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container webserver ready: true, restart count 0
May 16 14:23:25.039: INFO: test-container-pod from pod-network-test-7565 started at 2023-05-16 14:23:20 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container webserver ready: true, restart count 0
May 16 14:23:25.039: INFO: pod-qos-class-177c1cda-c74c-4b3a-94cc-8018f27421eb from pods-4722 started at 2023-05-16 14:22:48 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container agnhost ready: false, restart count 0
May 16 14:23:25.039: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-5r4tm from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
May 16 14:23:25.039: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 14:23:25.039: INFO: 	Container systemd-logs ready: true, restart count 0
May 16 14:23:25.039: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-212-246.eu-central-1.compute.internal before test
May 16 14:23:25.061: INFO: pod-2 from disruption-6735 started at 2023-05-16 14:23:22 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container donothing ready: true, restart count 0
May 16 14:23:25.061: INFO: aws-ebs-csi-driver-node-b4s28 from openshift-cluster-csi-drivers started at 2023-05-16 13:52:19 +0000 UTC (3 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container csi-driver ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 16 14:23:25.061: INFO: tuned-8x9rx from openshift-cluster-node-tuning-operator started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container tuned ready: true, restart count 0
May 16 14:23:25.061: INFO: dns-default-dbkcs from openshift-dns started at 2023-05-16 13:53:17 +0000 UTC (2 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container dns ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.061: INFO: node-resolver-gdzz2 from openshift-dns started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 16 14:23:25.061: INFO: image-registry-b549d4f89-hs4qn from openshift-image-registry started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container registry ready: true, restart count 0
May 16 14:23:25.061: INFO: node-ca-nsxtf from openshift-image-registry started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container node-ca ready: true, restart count 0
May 16 14:23:25.061: INFO: ingress-canary-wphz5 from openshift-ingress-canary started at 2023-05-16 13:53:17 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May 16 14:23:25.061: INFO: machine-config-daemon-z7sw6 from openshift-machine-config-operator started at 2023-05-16 13:52:19 +0000 UTC (2 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 14:23:25.061: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (6 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container alertmanager ready: true, restart count 1
May 16 14:23:25.061: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container config-reloader ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 16 14:23:25.061: INFO: kube-state-metrics-6ccfb58dc4-wgwts from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (3 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 16 14:23:25.061: INFO: node-exporter-jrknj from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container node-exporter ready: true, restart count 0
May 16 14:23:25.061: INFO: openshift-state-metrics-7d7f8b4cf8-9lbg2 from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (3 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container openshift-state-metrics ready: true, restart count 0
May 16 14:23:25.061: INFO: prometheus-adapter-7767c6bd47-bqlnh from openshift-monitoring started at 2023-05-16 13:53:50 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 16 14:23:25.061: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-05-16 13:53:57 +0000 UTC (6 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container config-reloader ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container prometheus ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 16 14:23:25.061: INFO: prometheus-operator-admission-webhook-5d679565bb-96qsx from openshift-monitoring started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
May 16 14:23:25.061: INFO: thanos-querier-5c9d876b5c-wwd7q from openshift-monitoring started at 2023-05-16 13:53:54 +0000 UTC (6 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container thanos-query ready: true, restart count 0
May 16 14:23:25.061: INFO: multus-additional-cni-plugins-wxcnn from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May 16 14:23:25.061: INFO: multus-rpn9n from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container kube-multus ready: true, restart count 0
May 16 14:23:25.061: INFO: network-metrics-daemon-7sqls from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (2 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 16 14:23:25.061: INFO: network-check-target-8pcqz from openshift-network-diagnostics started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container network-check-target-container ready: true, restart count 0
May 16 14:23:25.061: INFO: ovnkube-node-kqvdm from openshift-ovn-kubernetes started at 2023-05-16 13:52:19 +0000 UTC (5 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container ovn-acl-logging ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container ovn-controller ready: true, restart count 1
May 16 14:23:25.061: INFO: 	Container ovnkube-node ready: true, restart count 0
May 16 14:23:25.061: INFO: netserver-2 from pod-network-test-7565 started at 2023-05-16 14:22:58 +0000 UTC (1 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container webserver ready: true, restart count 0
May 16 14:23:25.061: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-gsqlr from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
May 16 14:23:25.061: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 14:23:25.061: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 05/16/23 14:23:25.061
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.175fa5ba463d0642], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling..] 05/16/23 14:23:25.125
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 14:23:26.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8828" for this suite. 05/16/23 14:23:26.126
------------------------------
â€¢ [1.182 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:23:24.95
    May 16 14:23:24.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename sched-pred 05/16/23 14:23:24.95
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:23:24.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:23:24.973
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    May 16 14:23:24.975: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May 16 14:23:24.986: INFO: Waiting for terminating namespaces to be deleted...
    May 16 14:23:24.990: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-132-142.eu-central-1.compute.internal before test
    May 16 14:23:25.018: INFO: pod-0 from disruption-6735 started at 2023-05-16 14:23:22 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container donothing ready: true, restart count 0
    May 16 14:23:25.018: INFO: aws-ebs-csi-driver-node-27d5z from openshift-cluster-csi-drivers started at 2023-05-16 13:52:17 +0000 UTC (3 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container csi-driver ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    May 16 14:23:25.018: INFO: tuned-j9bjl from openshift-cluster-node-tuning-operator started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container tuned ready: true, restart count 0
    May 16 14:23:25.018: INFO: dns-default-9lcb7 from openshift-dns started at 2023-05-16 13:53:17 +0000 UTC (2 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container dns ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.018: INFO: node-resolver-b47gh from openshift-dns started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May 16 14:23:25.018: INFO: image-registry-b549d4f89-4ck4v from openshift-image-registry started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container registry ready: true, restart count 0
    May 16 14:23:25.018: INFO: node-ca-px455 from openshift-image-registry started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container node-ca ready: true, restart count 0
    May 16 14:23:25.018: INFO: ingress-canary-4hv85 from openshift-ingress-canary started at 2023-05-16 13:53:17 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May 16 14:23:25.018: INFO: router-default-8f95c5-srcbl from openshift-ingress started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container router ready: true, restart count 0
    May 16 14:23:25.018: INFO: machine-config-daemon-l6bpj from openshift-machine-config-operator started at 2023-05-16 13:52:17 +0000 UTC (2 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container machine-config-daemon ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 14:23:25.018: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (6 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container alertmanager ready: true, restart count 1
    May 16 14:23:25.018: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container config-reloader ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May 16 14:23:25.018: INFO: node-exporter-pltn9 from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container node-exporter ready: true, restart count 0
    May 16 14:23:25.018: INFO: prometheus-adapter-7767c6bd47-bfq4d from openshift-monitoring started at 2023-05-16 13:53:50 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container prometheus-adapter ready: true, restart count 0
    May 16 14:23:25.018: INFO: telemeter-client-859dbffbb4-mt92v from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (3 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container reload ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container telemeter-client ready: true, restart count 0
    May 16 14:23:25.018: INFO: thanos-querier-5c9d876b5c-xdt6m from openshift-monitoring started at 2023-05-16 13:53:54 +0000 UTC (6 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container thanos-query ready: true, restart count 0
    May 16 14:23:25.018: INFO: multus-2wjqr from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container kube-multus ready: true, restart count 0
    May 16 14:23:25.018: INFO: multus-additional-cni-plugins-z2h4l from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May 16 14:23:25.018: INFO: network-metrics-daemon-rqdtf from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (2 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May 16 14:23:25.018: INFO: network-check-target-28wts from openshift-network-diagnostics started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container network-check-target-container ready: true, restart count 0
    May 16 14:23:25.018: INFO: ovnkube-node-npwgq from openshift-ovn-kubernetes started at 2023-05-16 13:52:17 +0000 UTC (5 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container ovn-controller ready: true, restart count 1
    May 16 14:23:25.018: INFO: 	Container ovnkube-node ready: true, restart count 0
    May 16 14:23:25.018: INFO: netserver-0 from pod-network-test-7565 started at 2023-05-16 14:22:58 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container webserver ready: true, restart count 0
    May 16 14:23:25.018: INFO: sonobuoy from sonobuoy started at 2023-05-16 14:11:07 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May 16 14:23:25.018: INFO: sonobuoy-e2e-job-840dd79630d44353 from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container e2e ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 16 14:23:25.018: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-7zsf6 from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
    May 16 14:23:25.018: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 16 14:23:25.018: INFO: 	Container systemd-logs ready: true, restart count 0
    May 16 14:23:25.018: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-161-164.eu-central-1.compute.internal before test
    May 16 14:23:25.039: INFO: pod-1 from disruption-6735 started at 2023-05-16 14:23:22 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container donothing ready: true, restart count 0
    May 16 14:23:25.039: INFO: aws-ebs-csi-driver-node-9m6vn from openshift-cluster-csi-drivers started at 2023-05-16 13:44:25 +0000 UTC (3 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container csi-driver ready: true, restart count 0
    May 16 14:23:25.039: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    May 16 14:23:25.039: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    May 16 14:23:25.039: INFO: tuned-6n5b7 from openshift-cluster-node-tuning-operator started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container tuned ready: true, restart count 0
    May 16 14:23:25.039: INFO: dns-default-6jx29 from openshift-dns started at 2023-05-16 13:46:22 +0000 UTC (2 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container dns ready: true, restart count 0
    May 16 14:23:25.039: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.039: INFO: node-resolver-jbmqq from openshift-dns started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May 16 14:23:25.039: INFO: node-ca-gw7rz from openshift-image-registry started at 2023-05-16 13:50:21 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container node-ca ready: true, restart count 0
    May 16 14:23:25.039: INFO: ingress-canary-sxs5c from openshift-ingress-canary started at 2023-05-16 13:46:22 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May 16 14:23:25.039: INFO: router-default-8f95c5-q46gb from openshift-ingress started at 2023-05-16 13:46:22 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container router ready: true, restart count 0
    May 16 14:23:25.039: INFO: machine-config-daemon-qhdsv from openshift-machine-config-operator started at 2023-05-16 13:44:25 +0000 UTC (2 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container machine-config-daemon ready: true, restart count 0
    May 16 14:23:25.039: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 14:23:25.039: INFO: node-exporter-l2zvs from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.039: INFO: 	Container node-exporter ready: true, restart count 0
    May 16 14:23:25.039: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-05-16 13:53:57 +0000 UTC (6 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container config-reloader ready: true, restart count 0
    May 16 14:23:25.039: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.039: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    May 16 14:23:25.039: INFO: 	Container prometheus ready: true, restart count 0
    May 16 14:23:25.039: INFO: 	Container prometheus-proxy ready: true, restart count 0
    May 16 14:23:25.039: INFO: 	Container thanos-sidecar ready: true, restart count 0
    May 16 14:23:25.039: INFO: prometheus-operator-admission-webhook-5d679565bb-sc62w from openshift-monitoring started at 2023-05-16 13:46:22 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    May 16 14:23:25.039: INFO: multus-additional-cni-plugins-jkn7l from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May 16 14:23:25.039: INFO: multus-mc6qb from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container kube-multus ready: true, restart count 0
    May 16 14:23:25.039: INFO: network-metrics-daemon-n4p75 from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (2 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.039: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May 16 14:23:25.039: INFO: network-check-source-7f6b75fdb6-r7bwb from openshift-network-diagnostics started at 2023-05-16 13:46:22 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container check-endpoints ready: true, restart count 1
    May 16 14:23:25.039: INFO: network-check-target-bprb5 from openshift-network-diagnostics started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container network-check-target-container ready: true, restart count 0
    May 16 14:23:25.039: INFO: collect-profiles-28070745-bvc57 from openshift-operator-lifecycle-manager started at 2023-05-16 13:46:22 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container collect-profiles ready: false, restart count 0
    May 16 14:23:25.039: INFO: collect-profiles-28070745-nqg55 from openshift-operator-lifecycle-manager started at 2023-05-16 13:47:55 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container collect-profiles ready: false, restart count 0
    May 16 14:23:25.039: INFO: collect-profiles-28070760-nvgx7 from openshift-operator-lifecycle-manager started at 2023-05-16 14:00:00 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container collect-profiles ready: false, restart count 0
    May 16 14:23:25.039: INFO: collect-profiles-28070775-wpbwl from openshift-operator-lifecycle-manager started at 2023-05-16 14:15:00 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container collect-profiles ready: false, restart count 0
    May 16 14:23:25.039: INFO: ovnkube-node-w8qn8 from openshift-ovn-kubernetes started at 2023-05-16 13:44:25 +0000 UTC (5 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.039: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    May 16 14:23:25.039: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    May 16 14:23:25.039: INFO: 	Container ovn-controller ready: true, restart count 1
    May 16 14:23:25.039: INFO: 	Container ovnkube-node ready: true, restart count 0
    May 16 14:23:25.039: INFO: netserver-1 from pod-network-test-7565 started at 2023-05-16 14:22:58 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container webserver ready: true, restart count 0
    May 16 14:23:25.039: INFO: test-container-pod from pod-network-test-7565 started at 2023-05-16 14:23:20 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container webserver ready: true, restart count 0
    May 16 14:23:25.039: INFO: pod-qos-class-177c1cda-c74c-4b3a-94cc-8018f27421eb from pods-4722 started at 2023-05-16 14:22:48 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container agnhost ready: false, restart count 0
    May 16 14:23:25.039: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-5r4tm from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
    May 16 14:23:25.039: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 16 14:23:25.039: INFO: 	Container systemd-logs ready: true, restart count 0
    May 16 14:23:25.039: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-212-246.eu-central-1.compute.internal before test
    May 16 14:23:25.061: INFO: pod-2 from disruption-6735 started at 2023-05-16 14:23:22 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container donothing ready: true, restart count 0
    May 16 14:23:25.061: INFO: aws-ebs-csi-driver-node-b4s28 from openshift-cluster-csi-drivers started at 2023-05-16 13:52:19 +0000 UTC (3 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container csi-driver ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    May 16 14:23:25.061: INFO: tuned-8x9rx from openshift-cluster-node-tuning-operator started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container tuned ready: true, restart count 0
    May 16 14:23:25.061: INFO: dns-default-dbkcs from openshift-dns started at 2023-05-16 13:53:17 +0000 UTC (2 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container dns ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.061: INFO: node-resolver-gdzz2 from openshift-dns started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May 16 14:23:25.061: INFO: image-registry-b549d4f89-hs4qn from openshift-image-registry started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container registry ready: true, restart count 0
    May 16 14:23:25.061: INFO: node-ca-nsxtf from openshift-image-registry started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container node-ca ready: true, restart count 0
    May 16 14:23:25.061: INFO: ingress-canary-wphz5 from openshift-ingress-canary started at 2023-05-16 13:53:17 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May 16 14:23:25.061: INFO: machine-config-daemon-z7sw6 from openshift-machine-config-operator started at 2023-05-16 13:52:19 +0000 UTC (2 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container machine-config-daemon ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 14:23:25.061: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (6 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container alertmanager ready: true, restart count 1
    May 16 14:23:25.061: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container config-reloader ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May 16 14:23:25.061: INFO: kube-state-metrics-6ccfb58dc4-wgwts from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (3 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container kube-state-metrics ready: true, restart count 0
    May 16 14:23:25.061: INFO: node-exporter-jrknj from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container node-exporter ready: true, restart count 0
    May 16 14:23:25.061: INFO: openshift-state-metrics-7d7f8b4cf8-9lbg2 from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (3 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    May 16 14:23:25.061: INFO: prometheus-adapter-7767c6bd47-bqlnh from openshift-monitoring started at 2023-05-16 13:53:50 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container prometheus-adapter ready: true, restart count 0
    May 16 14:23:25.061: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-05-16 13:53:57 +0000 UTC (6 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container config-reloader ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container prometheus ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container prometheus-proxy ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container thanos-sidecar ready: true, restart count 0
    May 16 14:23:25.061: INFO: prometheus-operator-admission-webhook-5d679565bb-96qsx from openshift-monitoring started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    May 16 14:23:25.061: INFO: thanos-querier-5c9d876b5c-wwd7q from openshift-monitoring started at 2023-05-16 13:53:54 +0000 UTC (6 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container thanos-query ready: true, restart count 0
    May 16 14:23:25.061: INFO: multus-additional-cni-plugins-wxcnn from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May 16 14:23:25.061: INFO: multus-rpn9n from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container kube-multus ready: true, restart count 0
    May 16 14:23:25.061: INFO: network-metrics-daemon-7sqls from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (2 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May 16 14:23:25.061: INFO: network-check-target-8pcqz from openshift-network-diagnostics started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container network-check-target-container ready: true, restart count 0
    May 16 14:23:25.061: INFO: ovnkube-node-kqvdm from openshift-ovn-kubernetes started at 2023-05-16 13:52:19 +0000 UTC (5 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container ovn-controller ready: true, restart count 1
    May 16 14:23:25.061: INFO: 	Container ovnkube-node ready: true, restart count 0
    May 16 14:23:25.061: INFO: netserver-2 from pod-network-test-7565 started at 2023-05-16 14:22:58 +0000 UTC (1 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container webserver ready: true, restart count 0
    May 16 14:23:25.061: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-gsqlr from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
    May 16 14:23:25.061: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 16 14:23:25.061: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 05/16/23 14:23:25.061
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.175fa5ba463d0642], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling..] 05/16/23 14:23:25.125
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 14:23:26.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8828" for this suite. 05/16/23 14:23:26.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:23:26.132
May 16 14:23:26.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename security-context-test 05/16/23 14:23:26.133
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:23:26.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:23:26.149
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
May 16 14:23:27.161: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-815f9686-41d9-43bd-84e5-29efbd02e036" in namespace "security-context-test-7091" to be "Succeeded or Failed"
May 16 14:23:27.163: INFO: Pod "busybox-privileged-false-815f9686-41d9-43bd-84e5-29efbd02e036": Phase="Pending", Reason="", readiness=false. Elapsed: 2.288012ms
May 16 14:23:29.167: INFO: Pod "busybox-privileged-false-815f9686-41d9-43bd-84e5-29efbd02e036": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006287713s
May 16 14:23:31.167: INFO: Pod "busybox-privileged-false-815f9686-41d9-43bd-84e5-29efbd02e036": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006232921s
May 16 14:23:31.167: INFO: Pod "busybox-privileged-false-815f9686-41d9-43bd-84e5-29efbd02e036" satisfied condition "Succeeded or Failed"
May 16 14:23:31.173: INFO: Got logs for pod "busybox-privileged-false-815f9686-41d9-43bd-84e5-29efbd02e036": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May 16 14:23:31.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7091" for this suite. 05/16/23 14:23:31.177
------------------------------
â€¢ [SLOW TEST] [5.051 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:23:26.132
    May 16 14:23:26.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename security-context-test 05/16/23 14:23:26.133
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:23:26.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:23:26.149
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    May 16 14:23:27.161: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-815f9686-41d9-43bd-84e5-29efbd02e036" in namespace "security-context-test-7091" to be "Succeeded or Failed"
    May 16 14:23:27.163: INFO: Pod "busybox-privileged-false-815f9686-41d9-43bd-84e5-29efbd02e036": Phase="Pending", Reason="", readiness=false. Elapsed: 2.288012ms
    May 16 14:23:29.167: INFO: Pod "busybox-privileged-false-815f9686-41d9-43bd-84e5-29efbd02e036": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006287713s
    May 16 14:23:31.167: INFO: Pod "busybox-privileged-false-815f9686-41d9-43bd-84e5-29efbd02e036": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006232921s
    May 16 14:23:31.167: INFO: Pod "busybox-privileged-false-815f9686-41d9-43bd-84e5-29efbd02e036" satisfied condition "Succeeded or Failed"
    May 16 14:23:31.173: INFO: Got logs for pod "busybox-privileged-false-815f9686-41d9-43bd-84e5-29efbd02e036": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May 16 14:23:31.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7091" for this suite. 05/16/23 14:23:31.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:23:31.183
May 16 14:23:31.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename deployment 05/16/23 14:23:31.184
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:23:31.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:23:31.204
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
May 16 14:23:31.206: INFO: Creating simple deployment test-new-deployment
W0516 14:23:32.212127      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:23:32.217: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 05/16/23 14:23:34.229
STEP: updating a scale subresource 05/16/23 14:23:34.232
STEP: verifying the deployment Spec.Replicas was modified 05/16/23 14:23:34.237
STEP: Patch a scale subresource 05/16/23 14:23:34.239
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 16 14:23:34.254: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-8695  919317a1-dfc9-4fa0-b21d-33f2fa0c7e35 45595 3 2023-05-16 14:23:31 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-16 14:23:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 14:23:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00cf602d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-16 14:23:33 +0000 UTC,LastTransitionTime:2023-05-16 14:23:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-05-16 14:23:33 +0000 UTC,LastTransitionTime:2023-05-16 14:23:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 16 14:23:34.260: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-8695  e6bf5542-80ec-4d6c-b364-7f2d5c4697a2 45599 3 2023-05-16 14:23:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 919317a1-dfc9-4fa0-b21d-33f2fa0c7e35 0xc00cf60707 0xc00cf60708}] [] [{kube-controller-manager Update apps/v1 2023-05-16 14:23:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-16 14:23:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"919317a1-dfc9-4fa0-b21d-33f2fa0c7e35\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00cf60798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 16 14:23:34.267: INFO: Pod "test-new-deployment-7f5969cbc7-cdjzd" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-cdjzd test-new-deployment-7f5969cbc7- deployment-8695  c69cc191-57cf-4e13-a27e-3f14f29e24ee 45582 0 2023-05-16 14:23:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.70/23"],"mac_address":"0a:58:0a:83:00:46","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.70/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.70"
    ],
    "mac": "0a:58:0a:83:00:46",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 e6bf5542-80ec-4d6c-b364-7f2d5c4697a2 0xc00cf60b87 0xc00cf60b88}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6bf5542-80ec-4d6c-b364-7f2d5c4697a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-05-16 14:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-05-16 14:23:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.70\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wm6cz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wm6cz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c37,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dbmzl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:23:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:23:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:23:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:10.131.0.70,StartTime:2023-05-16 14:23:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:23:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://1b85485502c2f661b62b18957634e5725ced87b85ef3d2f0692041ab7d1bff99,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 16 14:23:34.267: INFO: Pod "test-new-deployment-7f5969cbc7-kp5nw" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-kp5nw test-new-deployment-7f5969cbc7- deployment-8695  cfa465ff-9aef-4aa9-bc4e-c89bfbaaffb4 45600 0 2023-05-16 14:23:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 e6bf5542-80ec-4d6c-b364-7f2d5c4697a2 0xc00cf60de7 0xc00cf60de8}] [] [{kube-controller-manager Update v1 2023-05-16 14:23:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6bf5542-80ec-4d6c-b364-7f2d5c4697a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w8s8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w8s8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c37,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dbmzl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:23:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May 16 14:23:34.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8695" for this suite. 05/16/23 14:23:34.273
------------------------------
â€¢ [3.099 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:23:31.183
    May 16 14:23:31.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename deployment 05/16/23 14:23:31.184
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:23:31.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:23:31.204
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    May 16 14:23:31.206: INFO: Creating simple deployment test-new-deployment
    W0516 14:23:32.212127      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:23:32.217: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 05/16/23 14:23:34.229
    STEP: updating a scale subresource 05/16/23 14:23:34.232
    STEP: verifying the deployment Spec.Replicas was modified 05/16/23 14:23:34.237
    STEP: Patch a scale subresource 05/16/23 14:23:34.239
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 16 14:23:34.254: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-8695  919317a1-dfc9-4fa0-b21d-33f2fa0c7e35 45595 3 2023-05-16 14:23:31 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-16 14:23:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 14:23:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00cf602d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-16 14:23:33 +0000 UTC,LastTransitionTime:2023-05-16 14:23:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-05-16 14:23:33 +0000 UTC,LastTransitionTime:2023-05-16 14:23:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May 16 14:23:34.260: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-8695  e6bf5542-80ec-4d6c-b364-7f2d5c4697a2 45599 3 2023-05-16 14:23:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 919317a1-dfc9-4fa0-b21d-33f2fa0c7e35 0xc00cf60707 0xc00cf60708}] [] [{kube-controller-manager Update apps/v1 2023-05-16 14:23:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-16 14:23:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"919317a1-dfc9-4fa0-b21d-33f2fa0c7e35\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00cf60798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May 16 14:23:34.267: INFO: Pod "test-new-deployment-7f5969cbc7-cdjzd" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-cdjzd test-new-deployment-7f5969cbc7- deployment-8695  c69cc191-57cf-4e13-a27e-3f14f29e24ee 45582 0 2023-05-16 14:23:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.70/23"],"mac_address":"0a:58:0a:83:00:46","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.70/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.70"
        ],
        "mac": "0a:58:0a:83:00:46",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 e6bf5542-80ec-4d6c-b364-7f2d5c4697a2 0xc00cf60b87 0xc00cf60b88}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6bf5542-80ec-4d6c-b364-7f2d5c4697a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-05-16 14:23:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-05-16 14:23:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.70\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wm6cz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wm6cz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c37,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dbmzl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:23:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:23:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:23:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:23:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:10.131.0.70,StartTime:2023-05-16 14:23:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:23:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://1b85485502c2f661b62b18957634e5725ced87b85ef3d2f0692041ab7d1bff99,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 16 14:23:34.267: INFO: Pod "test-new-deployment-7f5969cbc7-kp5nw" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-kp5nw test-new-deployment-7f5969cbc7- deployment-8695  cfa465ff-9aef-4aa9-bc4e-c89bfbaaffb4 45600 0 2023-05-16 14:23:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 e6bf5542-80ec-4d6c-b364-7f2d5c4697a2 0xc00cf60de7 0xc00cf60de8}] [] [{kube-controller-manager Update v1 2023-05-16 14:23:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6bf5542-80ec-4d6c-b364-7f2d5c4697a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w8s8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w8s8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c37,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dbmzl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:23:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May 16 14:23:34.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8695" for this suite. 05/16/23 14:23:34.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:23:34.283
May 16 14:23:34.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename watch 05/16/23 14:23:34.284
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:23:34.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:23:34.306
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 05/16/23 14:23:34.309
STEP: modifying the configmap once 05/16/23 14:23:34.315
STEP: modifying the configmap a second time 05/16/23 14:23:34.322
STEP: deleting the configmap 05/16/23 14:23:34.337
STEP: creating a watch on configmaps from the resource version returned by the first update 05/16/23 14:23:34.351
STEP: Expecting to observe notifications for all changes to the configmap after the first update 05/16/23 14:23:34.352
May 16 14:23:34.352: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9434  2422ad0d-b135-44e4-a77b-b0330df9694f 45628 0 2023-05-16 14:23:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-16 14:23:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 16 14:23:34.352: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9434  2422ad0d-b135-44e4-a77b-b0330df9694f 45633 0 2023-05-16 14:23:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-16 14:23:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May 16 14:23:34.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9434" for this suite. 05/16/23 14:23:34.36
------------------------------
â€¢ [0.089 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:23:34.283
    May 16 14:23:34.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename watch 05/16/23 14:23:34.284
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:23:34.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:23:34.306
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 05/16/23 14:23:34.309
    STEP: modifying the configmap once 05/16/23 14:23:34.315
    STEP: modifying the configmap a second time 05/16/23 14:23:34.322
    STEP: deleting the configmap 05/16/23 14:23:34.337
    STEP: creating a watch on configmaps from the resource version returned by the first update 05/16/23 14:23:34.351
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 05/16/23 14:23:34.352
    May 16 14:23:34.352: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9434  2422ad0d-b135-44e4-a77b-b0330df9694f 45628 0 2023-05-16 14:23:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-16 14:23:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 16 14:23:34.352: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9434  2422ad0d-b135-44e4-a77b-b0330df9694f 45633 0 2023-05-16 14:23:34 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-16 14:23:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May 16 14:23:34.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9434" for this suite. 05/16/23 14:23:34.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:23:34.373
May 16 14:23:34.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename cronjob 05/16/23 14:23:34.374
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:23:34.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:23:34.399
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 05/16/23 14:23:34.4
W0516 14:23:35.406453      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring no jobs are scheduled 05/16/23 14:23:35.406
STEP: Ensuring no job exists by listing jobs explicitly 05/16/23 14:28:35.413
STEP: Removing cronjob 05/16/23 14:28:35.416
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May 16 14:28:35.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8772" for this suite. 05/16/23 14:28:35.425
------------------------------
â€¢ [SLOW TEST] [301.056 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:23:34.373
    May 16 14:23:34.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename cronjob 05/16/23 14:23:34.374
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:23:34.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:23:34.399
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 05/16/23 14:23:34.4
    W0516 14:23:35.406453      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring no jobs are scheduled 05/16/23 14:23:35.406
    STEP: Ensuring no job exists by listing jobs explicitly 05/16/23 14:28:35.413
    STEP: Removing cronjob 05/16/23 14:28:35.416
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May 16 14:28:35.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8772" for this suite. 05/16/23 14:28:35.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:28:35.431
May 16 14:28:35.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename configmap 05/16/23 14:28:35.432
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:28:35.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:28:35.448
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-3580c79d-c175-4bfd-9513-eb60bdabba91 05/16/23 14:28:35.45
STEP: Creating a pod to test consume configMaps 05/16/23 14:28:35.456
W0516 14:28:35.473348      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:28:35.473: INFO: Waiting up to 5m0s for pod "pod-configmaps-31320e7d-2804-474b-894d-ffd18e5af143" in namespace "configmap-9942" to be "Succeeded or Failed"
May 16 14:28:35.479: INFO: Pod "pod-configmaps-31320e7d-2804-474b-894d-ffd18e5af143": Phase="Pending", Reason="", readiness=false. Elapsed: 6.288103ms
May 16 14:28:37.483: INFO: Pod "pod-configmaps-31320e7d-2804-474b-894d-ffd18e5af143": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010325767s
May 16 14:28:39.485: INFO: Pod "pod-configmaps-31320e7d-2804-474b-894d-ffd18e5af143": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01156822s
STEP: Saw pod success 05/16/23 14:28:39.485
May 16 14:28:39.485: INFO: Pod "pod-configmaps-31320e7d-2804-474b-894d-ffd18e5af143" satisfied condition "Succeeded or Failed"
May 16 14:28:39.488: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-configmaps-31320e7d-2804-474b-894d-ffd18e5af143 container agnhost-container: <nil>
STEP: delete the pod 05/16/23 14:28:39.499
May 16 14:28:39.510: INFO: Waiting for pod pod-configmaps-31320e7d-2804-474b-894d-ffd18e5af143 to disappear
May 16 14:28:39.513: INFO: Pod pod-configmaps-31320e7d-2804-474b-894d-ffd18e5af143 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 16 14:28:39.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9942" for this suite. 05/16/23 14:28:39.516
------------------------------
â€¢ [4.091 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:28:35.431
    May 16 14:28:35.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename configmap 05/16/23 14:28:35.432
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:28:35.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:28:35.448
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-3580c79d-c175-4bfd-9513-eb60bdabba91 05/16/23 14:28:35.45
    STEP: Creating a pod to test consume configMaps 05/16/23 14:28:35.456
    W0516 14:28:35.473348      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:28:35.473: INFO: Waiting up to 5m0s for pod "pod-configmaps-31320e7d-2804-474b-894d-ffd18e5af143" in namespace "configmap-9942" to be "Succeeded or Failed"
    May 16 14:28:35.479: INFO: Pod "pod-configmaps-31320e7d-2804-474b-894d-ffd18e5af143": Phase="Pending", Reason="", readiness=false. Elapsed: 6.288103ms
    May 16 14:28:37.483: INFO: Pod "pod-configmaps-31320e7d-2804-474b-894d-ffd18e5af143": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010325767s
    May 16 14:28:39.485: INFO: Pod "pod-configmaps-31320e7d-2804-474b-894d-ffd18e5af143": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01156822s
    STEP: Saw pod success 05/16/23 14:28:39.485
    May 16 14:28:39.485: INFO: Pod "pod-configmaps-31320e7d-2804-474b-894d-ffd18e5af143" satisfied condition "Succeeded or Failed"
    May 16 14:28:39.488: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-configmaps-31320e7d-2804-474b-894d-ffd18e5af143 container agnhost-container: <nil>
    STEP: delete the pod 05/16/23 14:28:39.499
    May 16 14:28:39.510: INFO: Waiting for pod pod-configmaps-31320e7d-2804-474b-894d-ffd18e5af143 to disappear
    May 16 14:28:39.513: INFO: Pod pod-configmaps-31320e7d-2804-474b-894d-ffd18e5af143 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 16 14:28:39.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9942" for this suite. 05/16/23 14:28:39.516
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:28:39.522
May 16 14:28:39.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename pods 05/16/23 14:28:39.523
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:28:39.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:28:39.54
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
May 16 14:28:39.555: INFO: Waiting up to 5m0s for pod "server-envvars-74794d54-3818-45a4-ae36-bd3169d2b7ed" in namespace "pods-8349" to be "running and ready"
May 16 14:28:39.560: INFO: Pod "server-envvars-74794d54-3818-45a4-ae36-bd3169d2b7ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.968485ms
May 16 14:28:39.560: INFO: The phase of Pod server-envvars-74794d54-3818-45a4-ae36-bd3169d2b7ed is Pending, waiting for it to be Running (with Ready = true)
May 16 14:28:41.564: INFO: Pod "server-envvars-74794d54-3818-45a4-ae36-bd3169d2b7ed": Phase="Running", Reason="", readiness=true. Elapsed: 2.009158005s
May 16 14:28:41.564: INFO: The phase of Pod server-envvars-74794d54-3818-45a4-ae36-bd3169d2b7ed is Running (Ready = true)
May 16 14:28:41.564: INFO: Pod "server-envvars-74794d54-3818-45a4-ae36-bd3169d2b7ed" satisfied condition "running and ready"
May 16 14:28:41.584: INFO: Waiting up to 5m0s for pod "client-envvars-ab738ef4-33fb-4020-b4d3-ff0b3f6b7171" in namespace "pods-8349" to be "Succeeded or Failed"
May 16 14:28:41.588: INFO: Pod "client-envvars-ab738ef4-33fb-4020-b4d3-ff0b3f6b7171": Phase="Pending", Reason="", readiness=false. Elapsed: 4.617502ms
May 16 14:28:43.592: INFO: Pod "client-envvars-ab738ef4-33fb-4020-b4d3-ff0b3f6b7171": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008402711s
May 16 14:28:45.592: INFO: Pod "client-envvars-ab738ef4-33fb-4020-b4d3-ff0b3f6b7171": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008448109s
STEP: Saw pod success 05/16/23 14:28:45.592
May 16 14:28:45.592: INFO: Pod "client-envvars-ab738ef4-33fb-4020-b4d3-ff0b3f6b7171" satisfied condition "Succeeded or Failed"
May 16 14:28:45.595: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod client-envvars-ab738ef4-33fb-4020-b4d3-ff0b3f6b7171 container env3cont: <nil>
STEP: delete the pod 05/16/23 14:28:45.605
May 16 14:28:45.619: INFO: Waiting for pod client-envvars-ab738ef4-33fb-4020-b4d3-ff0b3f6b7171 to disappear
May 16 14:28:45.621: INFO: Pod client-envvars-ab738ef4-33fb-4020-b4d3-ff0b3f6b7171 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 16 14:28:45.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8349" for this suite. 05/16/23 14:28:45.625
------------------------------
â€¢ [SLOW TEST] [6.108 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:28:39.522
    May 16 14:28:39.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename pods 05/16/23 14:28:39.523
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:28:39.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:28:39.54
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    May 16 14:28:39.555: INFO: Waiting up to 5m0s for pod "server-envvars-74794d54-3818-45a4-ae36-bd3169d2b7ed" in namespace "pods-8349" to be "running and ready"
    May 16 14:28:39.560: INFO: Pod "server-envvars-74794d54-3818-45a4-ae36-bd3169d2b7ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.968485ms
    May 16 14:28:39.560: INFO: The phase of Pod server-envvars-74794d54-3818-45a4-ae36-bd3169d2b7ed is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:28:41.564: INFO: Pod "server-envvars-74794d54-3818-45a4-ae36-bd3169d2b7ed": Phase="Running", Reason="", readiness=true. Elapsed: 2.009158005s
    May 16 14:28:41.564: INFO: The phase of Pod server-envvars-74794d54-3818-45a4-ae36-bd3169d2b7ed is Running (Ready = true)
    May 16 14:28:41.564: INFO: Pod "server-envvars-74794d54-3818-45a4-ae36-bd3169d2b7ed" satisfied condition "running and ready"
    May 16 14:28:41.584: INFO: Waiting up to 5m0s for pod "client-envvars-ab738ef4-33fb-4020-b4d3-ff0b3f6b7171" in namespace "pods-8349" to be "Succeeded or Failed"
    May 16 14:28:41.588: INFO: Pod "client-envvars-ab738ef4-33fb-4020-b4d3-ff0b3f6b7171": Phase="Pending", Reason="", readiness=false. Elapsed: 4.617502ms
    May 16 14:28:43.592: INFO: Pod "client-envvars-ab738ef4-33fb-4020-b4d3-ff0b3f6b7171": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008402711s
    May 16 14:28:45.592: INFO: Pod "client-envvars-ab738ef4-33fb-4020-b4d3-ff0b3f6b7171": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008448109s
    STEP: Saw pod success 05/16/23 14:28:45.592
    May 16 14:28:45.592: INFO: Pod "client-envvars-ab738ef4-33fb-4020-b4d3-ff0b3f6b7171" satisfied condition "Succeeded or Failed"
    May 16 14:28:45.595: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod client-envvars-ab738ef4-33fb-4020-b4d3-ff0b3f6b7171 container env3cont: <nil>
    STEP: delete the pod 05/16/23 14:28:45.605
    May 16 14:28:45.619: INFO: Waiting for pod client-envvars-ab738ef4-33fb-4020-b4d3-ff0b3f6b7171 to disappear
    May 16 14:28:45.621: INFO: Pod client-envvars-ab738ef4-33fb-4020-b4d3-ff0b3f6b7171 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 16 14:28:45.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8349" for this suite. 05/16/23 14:28:45.625
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:28:45.63
May 16 14:28:45.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename secrets 05/16/23 14:28:45.631
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:28:45.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:28:45.646
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-b1eb7153-f4d3-4b27-beba-c153f6225d99 05/16/23 14:28:45.648
STEP: Creating a pod to test consume secrets 05/16/23 14:28:45.659
May 16 14:28:45.677: INFO: Waiting up to 5m0s for pod "pod-secrets-119a70b9-062b-4289-a6a8-0f7398486b62" in namespace "secrets-8245" to be "Succeeded or Failed"
May 16 14:28:45.679: INFO: Pod "pod-secrets-119a70b9-062b-4289-a6a8-0f7398486b62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.777035ms
May 16 14:28:47.684: INFO: Pod "pod-secrets-119a70b9-062b-4289-a6a8-0f7398486b62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006981119s
May 16 14:28:49.683: INFO: Pod "pod-secrets-119a70b9-062b-4289-a6a8-0f7398486b62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005914168s
STEP: Saw pod success 05/16/23 14:28:49.683
May 16 14:28:49.683: INFO: Pod "pod-secrets-119a70b9-062b-4289-a6a8-0f7398486b62" satisfied condition "Succeeded or Failed"
May 16 14:28:49.685: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod pod-secrets-119a70b9-062b-4289-a6a8-0f7398486b62 container secret-volume-test: <nil>
STEP: delete the pod 05/16/23 14:28:49.691
May 16 14:28:49.702: INFO: Waiting for pod pod-secrets-119a70b9-062b-4289-a6a8-0f7398486b62 to disappear
May 16 14:28:49.704: INFO: Pod pod-secrets-119a70b9-062b-4289-a6a8-0f7398486b62 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May 16 14:28:49.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8245" for this suite. 05/16/23 14:28:49.709
------------------------------
â€¢ [4.084 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:28:45.63
    May 16 14:28:45.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename secrets 05/16/23 14:28:45.631
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:28:45.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:28:45.646
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-b1eb7153-f4d3-4b27-beba-c153f6225d99 05/16/23 14:28:45.648
    STEP: Creating a pod to test consume secrets 05/16/23 14:28:45.659
    May 16 14:28:45.677: INFO: Waiting up to 5m0s for pod "pod-secrets-119a70b9-062b-4289-a6a8-0f7398486b62" in namespace "secrets-8245" to be "Succeeded or Failed"
    May 16 14:28:45.679: INFO: Pod "pod-secrets-119a70b9-062b-4289-a6a8-0f7398486b62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.777035ms
    May 16 14:28:47.684: INFO: Pod "pod-secrets-119a70b9-062b-4289-a6a8-0f7398486b62": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006981119s
    May 16 14:28:49.683: INFO: Pod "pod-secrets-119a70b9-062b-4289-a6a8-0f7398486b62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005914168s
    STEP: Saw pod success 05/16/23 14:28:49.683
    May 16 14:28:49.683: INFO: Pod "pod-secrets-119a70b9-062b-4289-a6a8-0f7398486b62" satisfied condition "Succeeded or Failed"
    May 16 14:28:49.685: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod pod-secrets-119a70b9-062b-4289-a6a8-0f7398486b62 container secret-volume-test: <nil>
    STEP: delete the pod 05/16/23 14:28:49.691
    May 16 14:28:49.702: INFO: Waiting for pod pod-secrets-119a70b9-062b-4289-a6a8-0f7398486b62 to disappear
    May 16 14:28:49.704: INFO: Pod pod-secrets-119a70b9-062b-4289-a6a8-0f7398486b62 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May 16 14:28:49.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8245" for this suite. 05/16/23 14:28:49.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:28:49.715
May 16 14:28:49.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename job 05/16/23 14:28:49.716
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:28:49.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:28:49.732
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 05/16/23 14:28:49.737
W0516 14:28:49.746625      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Patching the Job 05/16/23 14:28:49.746
STEP: Watching for Job to be patched 05/16/23 14:28:49.761
May 16 14:28:49.761: INFO: Event ADDED observed for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn] and annotations: map[batch.kubernetes.io/job-tracking:]
May 16 14:28:49.761: INFO: Event MODIFIED observed for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn] and annotations: map[batch.kubernetes.io/job-tracking:]
May 16 14:28:49.761: INFO: Event MODIFIED found for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn e2e-rwqbn:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 05/16/23 14:28:49.761
STEP: Watching for Job to be updated 05/16/23 14:28:49.775
May 16 14:28:49.776: INFO: Event MODIFIED found for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn e2e-rwqbn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May 16 14:28:49.776: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 05/16/23 14:28:49.776
May 16 14:28:49.781: INFO: Job: e2e-rwqbn as labels: map[e2e-job-label:e2e-rwqbn e2e-rwqbn:patched]
STEP: Waiting for job to complete 05/16/23 14:28:49.781
STEP: Delete a job collection with a labelselector 05/16/23 14:28:57.786
STEP: Watching for Job to be deleted 05/16/23 14:28:57.793
May 16 14:28:57.794: INFO: Event MODIFIED observed for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn e2e-rwqbn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May 16 14:28:57.794: INFO: Event MODIFIED observed for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn e2e-rwqbn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May 16 14:28:57.794: INFO: Event MODIFIED observed for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn e2e-rwqbn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May 16 14:28:57.794: INFO: Event MODIFIED observed for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn e2e-rwqbn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May 16 14:28:57.794: INFO: Event MODIFIED observed for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn e2e-rwqbn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
May 16 14:28:57.794: INFO: Event DELETED found for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn e2e-rwqbn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 05/16/23 14:28:57.794
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May 16 14:28:57.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5148" for this suite. 05/16/23 14:28:57.802
------------------------------
â€¢ [SLOW TEST] [8.103 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:28:49.715
    May 16 14:28:49.715: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename job 05/16/23 14:28:49.716
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:28:49.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:28:49.732
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 05/16/23 14:28:49.737
    W0516 14:28:49.746625      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Patching the Job 05/16/23 14:28:49.746
    STEP: Watching for Job to be patched 05/16/23 14:28:49.761
    May 16 14:28:49.761: INFO: Event ADDED observed for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn] and annotations: map[batch.kubernetes.io/job-tracking:]
    May 16 14:28:49.761: INFO: Event MODIFIED observed for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn] and annotations: map[batch.kubernetes.io/job-tracking:]
    May 16 14:28:49.761: INFO: Event MODIFIED found for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn e2e-rwqbn:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 05/16/23 14:28:49.761
    STEP: Watching for Job to be updated 05/16/23 14:28:49.775
    May 16 14:28:49.776: INFO: Event MODIFIED found for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn e2e-rwqbn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May 16 14:28:49.776: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 05/16/23 14:28:49.776
    May 16 14:28:49.781: INFO: Job: e2e-rwqbn as labels: map[e2e-job-label:e2e-rwqbn e2e-rwqbn:patched]
    STEP: Waiting for job to complete 05/16/23 14:28:49.781
    STEP: Delete a job collection with a labelselector 05/16/23 14:28:57.786
    STEP: Watching for Job to be deleted 05/16/23 14:28:57.793
    May 16 14:28:57.794: INFO: Event MODIFIED observed for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn e2e-rwqbn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May 16 14:28:57.794: INFO: Event MODIFIED observed for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn e2e-rwqbn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May 16 14:28:57.794: INFO: Event MODIFIED observed for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn e2e-rwqbn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May 16 14:28:57.794: INFO: Event MODIFIED observed for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn e2e-rwqbn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May 16 14:28:57.794: INFO: Event MODIFIED observed for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn e2e-rwqbn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    May 16 14:28:57.794: INFO: Event DELETED found for Job e2e-rwqbn in namespace job-5148 with labels: map[e2e-job-label:e2e-rwqbn e2e-rwqbn:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 05/16/23 14:28:57.794
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May 16 14:28:57.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5148" for this suite. 05/16/23 14:28:57.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:28:57.819
May 16 14:28:57.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename emptydir 05/16/23 14:28:57.82
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:28:57.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:28:57.837
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 05/16/23 14:28:57.84
W0516 14:28:57.857002      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:28:57.857: INFO: Waiting up to 5m0s for pod "pod-ba5f1b8c-8dd6-4eda-b951-0e8ae01d7ebf" in namespace "emptydir-2152" to be "Succeeded or Failed"
May 16 14:28:57.859: INFO: Pod "pod-ba5f1b8c-8dd6-4eda-b951-0e8ae01d7ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.557464ms
May 16 14:28:59.863: INFO: Pod "pod-ba5f1b8c-8dd6-4eda-b951-0e8ae01d7ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006565301s
May 16 14:29:01.864: INFO: Pod "pod-ba5f1b8c-8dd6-4eda-b951-0e8ae01d7ebf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007152086s
STEP: Saw pod success 05/16/23 14:29:01.864
May 16 14:29:01.864: INFO: Pod "pod-ba5f1b8c-8dd6-4eda-b951-0e8ae01d7ebf" satisfied condition "Succeeded or Failed"
May 16 14:29:01.867: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-ba5f1b8c-8dd6-4eda-b951-0e8ae01d7ebf container test-container: <nil>
STEP: delete the pod 05/16/23 14:29:01.872
May 16 14:29:01.884: INFO: Waiting for pod pod-ba5f1b8c-8dd6-4eda-b951-0e8ae01d7ebf to disappear
May 16 14:29:01.886: INFO: Pod pod-ba5f1b8c-8dd6-4eda-b951-0e8ae01d7ebf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 16 14:29:01.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2152" for this suite. 05/16/23 14:29:01.89
------------------------------
â€¢ [4.077 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:28:57.819
    May 16 14:28:57.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename emptydir 05/16/23 14:28:57.82
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:28:57.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:28:57.837
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 05/16/23 14:28:57.84
    W0516 14:28:57.857002      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:28:57.857: INFO: Waiting up to 5m0s for pod "pod-ba5f1b8c-8dd6-4eda-b951-0e8ae01d7ebf" in namespace "emptydir-2152" to be "Succeeded or Failed"
    May 16 14:28:57.859: INFO: Pod "pod-ba5f1b8c-8dd6-4eda-b951-0e8ae01d7ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.557464ms
    May 16 14:28:59.863: INFO: Pod "pod-ba5f1b8c-8dd6-4eda-b951-0e8ae01d7ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006565301s
    May 16 14:29:01.864: INFO: Pod "pod-ba5f1b8c-8dd6-4eda-b951-0e8ae01d7ebf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007152086s
    STEP: Saw pod success 05/16/23 14:29:01.864
    May 16 14:29:01.864: INFO: Pod "pod-ba5f1b8c-8dd6-4eda-b951-0e8ae01d7ebf" satisfied condition "Succeeded or Failed"
    May 16 14:29:01.867: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-ba5f1b8c-8dd6-4eda-b951-0e8ae01d7ebf container test-container: <nil>
    STEP: delete the pod 05/16/23 14:29:01.872
    May 16 14:29:01.884: INFO: Waiting for pod pod-ba5f1b8c-8dd6-4eda-b951-0e8ae01d7ebf to disappear
    May 16 14:29:01.886: INFO: Pod pod-ba5f1b8c-8dd6-4eda-b951-0e8ae01d7ebf no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 16 14:29:01.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2152" for this suite. 05/16/23 14:29:01.89
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:29:01.897
May 16 14:29:01.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 14:29:01.897
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:29:01.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:29:01.911
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-e251a94f-eab8-4886-b444-6ded22f1e5c5 05/16/23 14:29:01.913
STEP: Creating a pod to test consume configMaps 05/16/23 14:29:01.919
W0516 14:29:01.931969      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:29:01.932: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cb627446-8871-4db3-a77c-2b95004519d5" in namespace "projected-2490" to be "Succeeded or Failed"
May 16 14:29:01.934: INFO: Pod "pod-projected-configmaps-cb627446-8871-4db3-a77c-2b95004519d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.935361ms
May 16 14:29:03.939: INFO: Pod "pod-projected-configmaps-cb627446-8871-4db3-a77c-2b95004519d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006992653s
May 16 14:29:05.938: INFO: Pod "pod-projected-configmaps-cb627446-8871-4db3-a77c-2b95004519d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006137835s
STEP: Saw pod success 05/16/23 14:29:05.938
May 16 14:29:05.938: INFO: Pod "pod-projected-configmaps-cb627446-8871-4db3-a77c-2b95004519d5" satisfied condition "Succeeded or Failed"
May 16 14:29:05.940: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-configmaps-cb627446-8871-4db3-a77c-2b95004519d5 container agnhost-container: <nil>
STEP: delete the pod 05/16/23 14:29:05.946
May 16 14:29:05.957: INFO: Waiting for pod pod-projected-configmaps-cb627446-8871-4db3-a77c-2b95004519d5 to disappear
May 16 14:29:05.962: INFO: Pod pod-projected-configmaps-cb627446-8871-4db3-a77c-2b95004519d5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May 16 14:29:05.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2490" for this suite. 05/16/23 14:29:05.965
------------------------------
â€¢ [4.074 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:29:01.897
    May 16 14:29:01.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 14:29:01.897
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:29:01.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:29:01.911
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-e251a94f-eab8-4886-b444-6ded22f1e5c5 05/16/23 14:29:01.913
    STEP: Creating a pod to test consume configMaps 05/16/23 14:29:01.919
    W0516 14:29:01.931969      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:29:01.932: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cb627446-8871-4db3-a77c-2b95004519d5" in namespace "projected-2490" to be "Succeeded or Failed"
    May 16 14:29:01.934: INFO: Pod "pod-projected-configmaps-cb627446-8871-4db3-a77c-2b95004519d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.935361ms
    May 16 14:29:03.939: INFO: Pod "pod-projected-configmaps-cb627446-8871-4db3-a77c-2b95004519d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006992653s
    May 16 14:29:05.938: INFO: Pod "pod-projected-configmaps-cb627446-8871-4db3-a77c-2b95004519d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006137835s
    STEP: Saw pod success 05/16/23 14:29:05.938
    May 16 14:29:05.938: INFO: Pod "pod-projected-configmaps-cb627446-8871-4db3-a77c-2b95004519d5" satisfied condition "Succeeded or Failed"
    May 16 14:29:05.940: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-configmaps-cb627446-8871-4db3-a77c-2b95004519d5 container agnhost-container: <nil>
    STEP: delete the pod 05/16/23 14:29:05.946
    May 16 14:29:05.957: INFO: Waiting for pod pod-projected-configmaps-cb627446-8871-4db3-a77c-2b95004519d5 to disappear
    May 16 14:29:05.962: INFO: Pod pod-projected-configmaps-cb627446-8871-4db3-a77c-2b95004519d5 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May 16 14:29:05.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2490" for this suite. 05/16/23 14:29:05.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:29:05.971
May 16 14:29:05.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 14:29:05.972
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:29:05.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:29:05.992
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 05/16/23 14:29:05.994
W0516 14:29:06.008212      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:29:06.008: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f6877153-3980-4933-90cb-57a204c561db" in namespace "projected-9482" to be "Succeeded or Failed"
May 16 14:29:06.015: INFO: Pod "downwardapi-volume-f6877153-3980-4933-90cb-57a204c561db": Phase="Pending", Reason="", readiness=false. Elapsed: 7.507033ms
May 16 14:29:08.019: INFO: Pod "downwardapi-volume-f6877153-3980-4933-90cb-57a204c561db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01094386s
May 16 14:29:10.020: INFO: Pod "downwardapi-volume-f6877153-3980-4933-90cb-57a204c561db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012508742s
STEP: Saw pod success 05/16/23 14:29:10.02
May 16 14:29:10.020: INFO: Pod "downwardapi-volume-f6877153-3980-4933-90cb-57a204c561db" satisfied condition "Succeeded or Failed"
May 16 14:29:10.023: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-f6877153-3980-4933-90cb-57a204c561db container client-container: <nil>
STEP: delete the pod 05/16/23 14:29:10.028
May 16 14:29:10.037: INFO: Waiting for pod downwardapi-volume-f6877153-3980-4933-90cb-57a204c561db to disappear
May 16 14:29:10.040: INFO: Pod downwardapi-volume-f6877153-3980-4933-90cb-57a204c561db no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 16 14:29:10.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9482" for this suite. 05/16/23 14:29:10.043
------------------------------
â€¢ [4.078 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:29:05.971
    May 16 14:29:05.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 14:29:05.972
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:29:05.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:29:05.992
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 05/16/23 14:29:05.994
    W0516 14:29:06.008212      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:29:06.008: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f6877153-3980-4933-90cb-57a204c561db" in namespace "projected-9482" to be "Succeeded or Failed"
    May 16 14:29:06.015: INFO: Pod "downwardapi-volume-f6877153-3980-4933-90cb-57a204c561db": Phase="Pending", Reason="", readiness=false. Elapsed: 7.507033ms
    May 16 14:29:08.019: INFO: Pod "downwardapi-volume-f6877153-3980-4933-90cb-57a204c561db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01094386s
    May 16 14:29:10.020: INFO: Pod "downwardapi-volume-f6877153-3980-4933-90cb-57a204c561db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012508742s
    STEP: Saw pod success 05/16/23 14:29:10.02
    May 16 14:29:10.020: INFO: Pod "downwardapi-volume-f6877153-3980-4933-90cb-57a204c561db" satisfied condition "Succeeded or Failed"
    May 16 14:29:10.023: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-f6877153-3980-4933-90cb-57a204c561db container client-container: <nil>
    STEP: delete the pod 05/16/23 14:29:10.028
    May 16 14:29:10.037: INFO: Waiting for pod downwardapi-volume-f6877153-3980-4933-90cb-57a204c561db to disappear
    May 16 14:29:10.040: INFO: Pod downwardapi-volume-f6877153-3980-4933-90cb-57a204c561db no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 16 14:29:10.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9482" for this suite. 05/16/23 14:29:10.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:29:10.049
May 16 14:29:10.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename deployment 05/16/23 14:29:10.05
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:29:10.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:29:10.066
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
May 16 14:29:10.069: INFO: Creating deployment "test-recreate-deployment"
W0516 14:29:11.074796      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:29:11.074: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 16 14:29:11.082: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 16 14:29:13.088: INFO: Waiting deployment "test-recreate-deployment" to complete
May 16 14:29:13.091: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 16 14:29:13.098: INFO: Updating deployment test-recreate-deployment
May 16 14:29:13.098: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 16 14:29:13.189: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2388  14d42780-73df-43cb-9715-33c1628300fb 48387 2 2023-05-16 14:29:10 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-16 14:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 14:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000282458 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-16 14:29:13 +0000 UTC,LastTransitionTime:2023-05-16 14:29:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-05-16 14:29:13 +0000 UTC,LastTransitionTime:2023-05-16 14:29:11 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 16 14:29:13.192: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-2388  4f741265-7eb3-45f9-97f5-b432bd36e1e1 48386 1 2023-05-16 14:29:13 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 14d42780-73df-43cb-9715-33c1628300fb 0xc004e9dad0 0xc004e9dad1}] [] [{kube-controller-manager Update apps/v1 2023-05-16 14:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14d42780-73df-43cb-9715-33c1628300fb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 14:29:13 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e9db68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 16 14:29:13.192: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 16 14:29:13.192: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-2388  fdca4de3-6f62-4e27-b4d1-56ab84771f31 48374 2 2023-05-16 14:29:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 14d42780-73df-43cb-9715-33c1628300fb 0xc004e9d9b7 0xc004e9d9b8}] [] [{kube-controller-manager Update apps/v1 2023-05-16 14:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14d42780-73df-43cb-9715-33c1628300fb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 14:29:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e9da68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 16 14:29:13.195: INFO: Pod "test-recreate-deployment-cff6dc657-94qgl" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-94qgl test-recreate-deployment-cff6dc657- deployment-2388  719ff69f-1281-4a30-8b49-b0b3fe5fefcb 48385 0 2023-05-16 14:29:13 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.80/23"],"mac_address":"0a:58:0a:83:00:50","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.80/23","gateway_ip":"10.131.0.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 4f741265-7eb3-45f9-97f5-b432bd36e1e1 0xc004fdca77 0xc004fdca78}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f741265-7eb3-45f9-97f5-b432bd36e1e1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:29:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tdsvk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tdsvk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c40,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rhnnv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:29:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:29:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:29:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:29:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:,StartTime:2023-05-16 14:29:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May 16 14:29:13.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2388" for this suite. 05/16/23 14:29:13.199
------------------------------
â€¢ [3.155 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:29:10.049
    May 16 14:29:10.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename deployment 05/16/23 14:29:10.05
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:29:10.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:29:10.066
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    May 16 14:29:10.069: INFO: Creating deployment "test-recreate-deployment"
    W0516 14:29:11.074796      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:29:11.074: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    May 16 14:29:11.082: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    May 16 14:29:13.088: INFO: Waiting deployment "test-recreate-deployment" to complete
    May 16 14:29:13.091: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    May 16 14:29:13.098: INFO: Updating deployment test-recreate-deployment
    May 16 14:29:13.098: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 16 14:29:13.189: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-2388  14d42780-73df-43cb-9715-33c1628300fb 48387 2 2023-05-16 14:29:10 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-16 14:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 14:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000282458 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-16 14:29:13 +0000 UTC,LastTransitionTime:2023-05-16 14:29:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-05-16 14:29:13 +0000 UTC,LastTransitionTime:2023-05-16 14:29:11 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    May 16 14:29:13.192: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-2388  4f741265-7eb3-45f9-97f5-b432bd36e1e1 48386 1 2023-05-16 14:29:13 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 14d42780-73df-43cb-9715-33c1628300fb 0xc004e9dad0 0xc004e9dad1}] [] [{kube-controller-manager Update apps/v1 2023-05-16 14:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14d42780-73df-43cb-9715-33c1628300fb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 14:29:13 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e9db68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May 16 14:29:13.192: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    May 16 14:29:13.192: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-2388  fdca4de3-6f62-4e27-b4d1-56ab84771f31 48374 2 2023-05-16 14:29:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 14d42780-73df-43cb-9715-33c1628300fb 0xc004e9d9b7 0xc004e9d9b8}] [] [{kube-controller-manager Update apps/v1 2023-05-16 14:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14d42780-73df-43cb-9715-33c1628300fb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 14:29:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e9da68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May 16 14:29:13.195: INFO: Pod "test-recreate-deployment-cff6dc657-94qgl" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-94qgl test-recreate-deployment-cff6dc657- deployment-2388  719ff69f-1281-4a30-8b49-b0b3fe5fefcb 48385 0 2023-05-16 14:29:13 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.80/23"],"mac_address":"0a:58:0a:83:00:50","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.80/23","gateway_ip":"10.131.0.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 4f741265-7eb3-45f9-97f5-b432bd36e1e1 0xc004fdca77 0xc004fdca78}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f741265-7eb3-45f9-97f5-b432bd36e1e1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:29:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tdsvk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tdsvk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c40,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rhnnv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:29:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:29:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:29:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:29:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:,StartTime:2023-05-16 14:29:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May 16 14:29:13.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2388" for this suite. 05/16/23 14:29:13.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:29:13.205
May 16 14:29:13.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename svcaccounts 05/16/23 14:29:13.205
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:29:13.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:29:13.219
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
W0516 14:29:13.237115      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "oidc-discovery-validator" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "oidc-discovery-validator" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "oidc-discovery-validator" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "oidc-discovery-validator" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:29:13.237: INFO: created pod
May 16 14:29:13.237: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8083" to be "Succeeded or Failed"
May 16 14:29:13.241: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.525882ms
May 16 14:29:15.244: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007256222s
May 16 14:29:17.245: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007844904s
STEP: Saw pod success 05/16/23 14:29:17.245
May 16 14:29:17.245: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
May 16 14:29:47.245: INFO: polling logs
May 16 14:29:47.252: INFO: Pod logs: 
I0516 14:29:14.052180       1 log.go:198] OK: Got token
I0516 14:29:14.052204       1 log.go:198] validating with in-cluster discovery
I0516 14:29:14.052498       1 log.go:198] OK: got issuer https://kubernetes.default.svc
I0516 14:29:14.052515       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-8083:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1684247953, NotBefore:1684247353, IssuedAt:1684247353, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8083", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"14ae1fe7-8c6e-45d0-82f0-4a5c9db74967"}}}
I0516 14:29:14.060005       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0516 14:29:14.070293       1 log.go:198] OK: Validated signature on JWT
I0516 14:29:14.070358       1 log.go:198] OK: Got valid claims from token!
I0516 14:29:14.070382       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-8083:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1684247953, NotBefore:1684247353, IssuedAt:1684247353, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8083", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"14ae1fe7-8c6e-45d0-82f0-4a5c9db74967"}}}

May 16 14:29:47.252: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May 16 14:29:47.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8083" for this suite. 05/16/23 14:29:47.262
------------------------------
â€¢ [SLOW TEST] [34.064 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:29:13.205
    May 16 14:29:13.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename svcaccounts 05/16/23 14:29:13.205
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:29:13.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:29:13.219
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    W0516 14:29:13.237115      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "oidc-discovery-validator" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "oidc-discovery-validator" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "oidc-discovery-validator" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "oidc-discovery-validator" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:29:13.237: INFO: created pod
    May 16 14:29:13.237: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8083" to be "Succeeded or Failed"
    May 16 14:29:13.241: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.525882ms
    May 16 14:29:15.244: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007256222s
    May 16 14:29:17.245: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007844904s
    STEP: Saw pod success 05/16/23 14:29:17.245
    May 16 14:29:17.245: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    May 16 14:29:47.245: INFO: polling logs
    May 16 14:29:47.252: INFO: Pod logs: 
    I0516 14:29:14.052180       1 log.go:198] OK: Got token
    I0516 14:29:14.052204       1 log.go:198] validating with in-cluster discovery
    I0516 14:29:14.052498       1 log.go:198] OK: got issuer https://kubernetes.default.svc
    I0516 14:29:14.052515       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-8083:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1684247953, NotBefore:1684247353, IssuedAt:1684247353, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8083", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"14ae1fe7-8c6e-45d0-82f0-4a5c9db74967"}}}
    I0516 14:29:14.060005       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I0516 14:29:14.070293       1 log.go:198] OK: Validated signature on JWT
    I0516 14:29:14.070358       1 log.go:198] OK: Got valid claims from token!
    I0516 14:29:14.070382       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-8083:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1684247953, NotBefore:1684247353, IssuedAt:1684247353, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8083", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"14ae1fe7-8c6e-45d0-82f0-4a5c9db74967"}}}

    May 16 14:29:47.252: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May 16 14:29:47.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8083" for this suite. 05/16/23 14:29:47.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:29:47.27
May 16 14:29:47.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename container-runtime 05/16/23 14:29:47.27
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:29:47.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:29:47.298
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 05/16/23 14:29:47.3
STEP: wait for the container to reach Succeeded 05/16/23 14:29:47.325
STEP: get the container status 05/16/23 14:29:51.343
STEP: the container should be terminated 05/16/23 14:29:51.346
STEP: the termination message should be set 05/16/23 14:29:51.346
May 16 14:29:51.346: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 05/16/23 14:29:51.346
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May 16 14:29:51.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6572" for this suite. 05/16/23 14:29:51.365
------------------------------
â€¢ [4.101 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:29:47.27
    May 16 14:29:47.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename container-runtime 05/16/23 14:29:47.27
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:29:47.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:29:47.298
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 05/16/23 14:29:47.3
    STEP: wait for the container to reach Succeeded 05/16/23 14:29:47.325
    STEP: get the container status 05/16/23 14:29:51.343
    STEP: the container should be terminated 05/16/23 14:29:51.346
    STEP: the termination message should be set 05/16/23 14:29:51.346
    May 16 14:29:51.346: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 05/16/23 14:29:51.346
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May 16 14:29:51.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6572" for this suite. 05/16/23 14:29:51.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:29:51.371
May 16 14:29:51.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename security-context-test 05/16/23 14:29:51.372
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:29:51.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:29:51.391
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
W0516 14:29:51.404177      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-user-65534-6d095693-2752-4c5f-adba-b6f04c3e7c3b" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-user-65534-6d095693-2752-4c5f-adba-b6f04c3e7c3b" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "busybox-user-65534-6d095693-2752-4c5f-adba-b6f04c3e7c3b" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:29:51.404: INFO: Waiting up to 5m0s for pod "busybox-user-65534-6d095693-2752-4c5f-adba-b6f04c3e7c3b" in namespace "security-context-test-3990" to be "Succeeded or Failed"
May 16 14:29:51.407: INFO: Pod "busybox-user-65534-6d095693-2752-4c5f-adba-b6f04c3e7c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.042023ms
May 16 14:29:53.413: INFO: Pod "busybox-user-65534-6d095693-2752-4c5f-adba-b6f04c3e7c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009575051s
May 16 14:29:55.411: INFO: Pod "busybox-user-65534-6d095693-2752-4c5f-adba-b6f04c3e7c3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007104215s
May 16 14:29:55.411: INFO: Pod "busybox-user-65534-6d095693-2752-4c5f-adba-b6f04c3e7c3b" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May 16 14:29:55.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3990" for this suite. 05/16/23 14:29:55.415
------------------------------
â€¢ [4.050 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:29:51.371
    May 16 14:29:51.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename security-context-test 05/16/23 14:29:51.372
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:29:51.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:29:51.391
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    W0516 14:29:51.404177      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-user-65534-6d095693-2752-4c5f-adba-b6f04c3e7c3b" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-user-65534-6d095693-2752-4c5f-adba-b6f04c3e7c3b" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "busybox-user-65534-6d095693-2752-4c5f-adba-b6f04c3e7c3b" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:29:51.404: INFO: Waiting up to 5m0s for pod "busybox-user-65534-6d095693-2752-4c5f-adba-b6f04c3e7c3b" in namespace "security-context-test-3990" to be "Succeeded or Failed"
    May 16 14:29:51.407: INFO: Pod "busybox-user-65534-6d095693-2752-4c5f-adba-b6f04c3e7c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.042023ms
    May 16 14:29:53.413: INFO: Pod "busybox-user-65534-6d095693-2752-4c5f-adba-b6f04c3e7c3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009575051s
    May 16 14:29:55.411: INFO: Pod "busybox-user-65534-6d095693-2752-4c5f-adba-b6f04c3e7c3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007104215s
    May 16 14:29:55.411: INFO: Pod "busybox-user-65534-6d095693-2752-4c5f-adba-b6f04c3e7c3b" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May 16 14:29:55.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3990" for this suite. 05/16/23 14:29:55.415
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:29:55.421
May 16 14:29:55.421: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename pods 05/16/23 14:29:55.422
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:29:55.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:29:55.435
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 05/16/23 14:29:55.437
STEP: submitting the pod to kubernetes 05/16/23 14:29:55.437
May 16 14:29:55.462: INFO: Waiting up to 5m0s for pod "pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375" in namespace "pods-5001" to be "running and ready"
May 16 14:29:55.467: INFO: Pod "pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375": Phase="Pending", Reason="", readiness=false. Elapsed: 5.243945ms
May 16 14:29:55.467: INFO: The phase of Pod pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375 is Pending, waiting for it to be Running (with Ready = true)
May 16 14:29:57.470: INFO: Pod "pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375": Phase="Running", Reason="", readiness=true. Elapsed: 2.008782549s
May 16 14:29:57.470: INFO: The phase of Pod pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375 is Running (Ready = true)
May 16 14:29:57.470: INFO: Pod "pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 05/16/23 14:29:57.473
STEP: updating the pod 05/16/23 14:29:57.476
May 16 14:29:57.987: INFO: Successfully updated pod "pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375"
May 16 14:29:57.987: INFO: Waiting up to 5m0s for pod "pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375" in namespace "pods-5001" to be "running"
May 16 14:29:57.990: INFO: Pod "pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375": Phase="Running", Reason="", readiness=true. Elapsed: 3.19808ms
May 16 14:29:57.990: INFO: Pod "pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 05/16/23 14:29:57.99
May 16 14:29:57.993: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 16 14:29:57.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5001" for this suite. 05/16/23 14:29:57.997
------------------------------
â€¢ [2.580 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:29:55.421
    May 16 14:29:55.421: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename pods 05/16/23 14:29:55.422
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:29:55.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:29:55.435
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 05/16/23 14:29:55.437
    STEP: submitting the pod to kubernetes 05/16/23 14:29:55.437
    May 16 14:29:55.462: INFO: Waiting up to 5m0s for pod "pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375" in namespace "pods-5001" to be "running and ready"
    May 16 14:29:55.467: INFO: Pod "pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375": Phase="Pending", Reason="", readiness=false. Elapsed: 5.243945ms
    May 16 14:29:55.467: INFO: The phase of Pod pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375 is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:29:57.470: INFO: Pod "pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375": Phase="Running", Reason="", readiness=true. Elapsed: 2.008782549s
    May 16 14:29:57.470: INFO: The phase of Pod pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375 is Running (Ready = true)
    May 16 14:29:57.470: INFO: Pod "pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 05/16/23 14:29:57.473
    STEP: updating the pod 05/16/23 14:29:57.476
    May 16 14:29:57.987: INFO: Successfully updated pod "pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375"
    May 16 14:29:57.987: INFO: Waiting up to 5m0s for pod "pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375" in namespace "pods-5001" to be "running"
    May 16 14:29:57.990: INFO: Pod "pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375": Phase="Running", Reason="", readiness=true. Elapsed: 3.19808ms
    May 16 14:29:57.990: INFO: Pod "pod-update-e40072e2-6e47-4e5d-93ea-0dd220c10375" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 05/16/23 14:29:57.99
    May 16 14:29:57.993: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 16 14:29:57.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5001" for this suite. 05/16/23 14:29:57.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:29:58.002
May 16 14:29:58.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename container-runtime 05/16/23 14:29:58.003
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:29:58.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:29:58.018
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 05/16/23 14:29:58.02
STEP: wait for the container to reach Succeeded 05/16/23 14:29:59.031
STEP: get the container status 05/16/23 14:30:02.044
STEP: the container should be terminated 05/16/23 14:30:02.046
STEP: the termination message should be set 05/16/23 14:30:02.046
May 16 14:30:02.047: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 05/16/23 14:30:02.047
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May 16 14:30:02.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1768" for this suite. 05/16/23 14:30:02.064
------------------------------
â€¢ [4.067 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:29:58.002
    May 16 14:29:58.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename container-runtime 05/16/23 14:29:58.003
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:29:58.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:29:58.018
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 05/16/23 14:29:58.02
    STEP: wait for the container to reach Succeeded 05/16/23 14:29:59.031
    STEP: get the container status 05/16/23 14:30:02.044
    STEP: the container should be terminated 05/16/23 14:30:02.046
    STEP: the termination message should be set 05/16/23 14:30:02.046
    May 16 14:30:02.047: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 05/16/23 14:30:02.047
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May 16 14:30:02.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1768" for this suite. 05/16/23 14:30:02.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:30:02.069
May 16 14:30:02.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 14:30:02.07
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:30:02.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:30:02.088
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 05/16/23 14:30:02.092
W0516 14:30:02.103761      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:30:02.103: INFO: Waiting up to 5m0s for pod "downwardapi-volume-86c4bdc9-8bab-4406-bf9c-03d8eb82bcd5" in namespace "projected-430" to be "Succeeded or Failed"
May 16 14:30:02.107: INFO: Pod "downwardapi-volume-86c4bdc9-8bab-4406-bf9c-03d8eb82bcd5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065159ms
May 16 14:30:04.112: INFO: Pod "downwardapi-volume-86c4bdc9-8bab-4406-bf9c-03d8eb82bcd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00826599s
May 16 14:30:06.111: INFO: Pod "downwardapi-volume-86c4bdc9-8bab-4406-bf9c-03d8eb82bcd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007372408s
STEP: Saw pod success 05/16/23 14:30:06.111
May 16 14:30:06.111: INFO: Pod "downwardapi-volume-86c4bdc9-8bab-4406-bf9c-03d8eb82bcd5" satisfied condition "Succeeded or Failed"
May 16 14:30:06.114: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod downwardapi-volume-86c4bdc9-8bab-4406-bf9c-03d8eb82bcd5 container client-container: <nil>
STEP: delete the pod 05/16/23 14:30:06.119
May 16 14:30:06.130: INFO: Waiting for pod downwardapi-volume-86c4bdc9-8bab-4406-bf9c-03d8eb82bcd5 to disappear
May 16 14:30:06.133: INFO: Pod downwardapi-volume-86c4bdc9-8bab-4406-bf9c-03d8eb82bcd5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 16 14:30:06.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-430" for this suite. 05/16/23 14:30:06.137
------------------------------
â€¢ [4.073 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:30:02.069
    May 16 14:30:02.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 14:30:02.07
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:30:02.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:30:02.088
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 05/16/23 14:30:02.092
    W0516 14:30:02.103761      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:30:02.103: INFO: Waiting up to 5m0s for pod "downwardapi-volume-86c4bdc9-8bab-4406-bf9c-03d8eb82bcd5" in namespace "projected-430" to be "Succeeded or Failed"
    May 16 14:30:02.107: INFO: Pod "downwardapi-volume-86c4bdc9-8bab-4406-bf9c-03d8eb82bcd5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065159ms
    May 16 14:30:04.112: INFO: Pod "downwardapi-volume-86c4bdc9-8bab-4406-bf9c-03d8eb82bcd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00826599s
    May 16 14:30:06.111: INFO: Pod "downwardapi-volume-86c4bdc9-8bab-4406-bf9c-03d8eb82bcd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007372408s
    STEP: Saw pod success 05/16/23 14:30:06.111
    May 16 14:30:06.111: INFO: Pod "downwardapi-volume-86c4bdc9-8bab-4406-bf9c-03d8eb82bcd5" satisfied condition "Succeeded or Failed"
    May 16 14:30:06.114: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod downwardapi-volume-86c4bdc9-8bab-4406-bf9c-03d8eb82bcd5 container client-container: <nil>
    STEP: delete the pod 05/16/23 14:30:06.119
    May 16 14:30:06.130: INFO: Waiting for pod downwardapi-volume-86c4bdc9-8bab-4406-bf9c-03d8eb82bcd5 to disappear
    May 16 14:30:06.133: INFO: Pod downwardapi-volume-86c4bdc9-8bab-4406-bf9c-03d8eb82bcd5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 16 14:30:06.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-430" for this suite. 05/16/23 14:30:06.137
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:30:06.143
May 16 14:30:06.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename gc 05/16/23 14:30:06.144
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:30:06.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:30:06.177
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 05/16/23 14:30:06.184
STEP: delete the rc 05/16/23 14:30:11.195
STEP: wait for the rc to be deleted 05/16/23 14:30:11.202
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 05/16/23 14:30:16.205
STEP: Gathering metrics 05/16/23 14:30:46.215
W0516 14:30:46.217613      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0516 14:30:46.217626      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May 16 14:30:46.217: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 16 14:30:46.217: INFO: Deleting pod "simpletest.rc-24t2n" in namespace "gc-8020"
May 16 14:30:46.229: INFO: Deleting pod "simpletest.rc-2wtf8" in namespace "gc-8020"
May 16 14:30:46.243: INFO: Deleting pod "simpletest.rc-45x2x" in namespace "gc-8020"
May 16 14:30:46.252: INFO: Deleting pod "simpletest.rc-4lmks" in namespace "gc-8020"
May 16 14:30:46.267: INFO: Deleting pod "simpletest.rc-4p79s" in namespace "gc-8020"
May 16 14:30:46.287: INFO: Deleting pod "simpletest.rc-4qt29" in namespace "gc-8020"
May 16 14:30:46.302: INFO: Deleting pod "simpletest.rc-4tqqf" in namespace "gc-8020"
May 16 14:30:46.315: INFO: Deleting pod "simpletest.rc-5dkgg" in namespace "gc-8020"
May 16 14:30:46.327: INFO: Deleting pod "simpletest.rc-5tlpc" in namespace "gc-8020"
May 16 14:30:46.339: INFO: Deleting pod "simpletest.rc-6bpk6" in namespace "gc-8020"
May 16 14:30:46.362: INFO: Deleting pod "simpletest.rc-6nvv9" in namespace "gc-8020"
May 16 14:30:46.380: INFO: Deleting pod "simpletest.rc-6tjhm" in namespace "gc-8020"
May 16 14:30:46.395: INFO: Deleting pod "simpletest.rc-6v7nn" in namespace "gc-8020"
May 16 14:30:46.413: INFO: Deleting pod "simpletest.rc-6zb59" in namespace "gc-8020"
May 16 14:30:46.431: INFO: Deleting pod "simpletest.rc-7vk58" in namespace "gc-8020"
May 16 14:30:46.446: INFO: Deleting pod "simpletest.rc-8whzx" in namespace "gc-8020"
May 16 14:30:46.461: INFO: Deleting pod "simpletest.rc-9bc28" in namespace "gc-8020"
May 16 14:30:46.478: INFO: Deleting pod "simpletest.rc-9smwh" in namespace "gc-8020"
May 16 14:30:46.498: INFO: Deleting pod "simpletest.rc-9v9rt" in namespace "gc-8020"
May 16 14:30:46.512: INFO: Deleting pod "simpletest.rc-9x2pc" in namespace "gc-8020"
May 16 14:30:46.526: INFO: Deleting pod "simpletest.rc-b74kt" in namespace "gc-8020"
May 16 14:30:46.559: INFO: Deleting pod "simpletest.rc-bgfqz" in namespace "gc-8020"
May 16 14:30:46.575: INFO: Deleting pod "simpletest.rc-bpc54" in namespace "gc-8020"
May 16 14:30:46.587: INFO: Deleting pod "simpletest.rc-c9cqq" in namespace "gc-8020"
May 16 14:30:46.610: INFO: Deleting pod "simpletest.rc-cdt69" in namespace "gc-8020"
May 16 14:30:46.625: INFO: Deleting pod "simpletest.rc-d6f7q" in namespace "gc-8020"
May 16 14:30:46.645: INFO: Deleting pod "simpletest.rc-dcp8s" in namespace "gc-8020"
May 16 14:30:46.661: INFO: Deleting pod "simpletest.rc-dgdck" in namespace "gc-8020"
May 16 14:30:46.675: INFO: Deleting pod "simpletest.rc-dthtn" in namespace "gc-8020"
May 16 14:30:46.694: INFO: Deleting pod "simpletest.rc-f2vws" in namespace "gc-8020"
May 16 14:30:46.711: INFO: Deleting pod "simpletest.rc-f6xz9" in namespace "gc-8020"
May 16 14:30:46.737: INFO: Deleting pod "simpletest.rc-f9gxw" in namespace "gc-8020"
May 16 14:30:46.755: INFO: Deleting pod "simpletest.rc-fgvt2" in namespace "gc-8020"
May 16 14:30:46.776: INFO: Deleting pod "simpletest.rc-gjhbf" in namespace "gc-8020"
May 16 14:30:46.804: INFO: Deleting pod "simpletest.rc-h845b" in namespace "gc-8020"
May 16 14:30:46.820: INFO: Deleting pod "simpletest.rc-hvp5l" in namespace "gc-8020"
May 16 14:30:46.834: INFO: Deleting pod "simpletest.rc-hvw6h" in namespace "gc-8020"
May 16 14:30:46.846: INFO: Deleting pod "simpletest.rc-j54jn" in namespace "gc-8020"
May 16 14:30:46.864: INFO: Deleting pod "simpletest.rc-jpwq2" in namespace "gc-8020"
May 16 14:30:46.880: INFO: Deleting pod "simpletest.rc-jvln6" in namespace "gc-8020"
May 16 14:30:46.895: INFO: Deleting pod "simpletest.rc-jzqdd" in namespace "gc-8020"
May 16 14:30:46.917: INFO: Deleting pod "simpletest.rc-khlwk" in namespace "gc-8020"
May 16 14:30:46.931: INFO: Deleting pod "simpletest.rc-khp2v" in namespace "gc-8020"
May 16 14:30:46.947: INFO: Deleting pod "simpletest.rc-kkth7" in namespace "gc-8020"
May 16 14:30:46.970: INFO: Deleting pod "simpletest.rc-ktjbh" in namespace "gc-8020"
May 16 14:30:46.986: INFO: Deleting pod "simpletest.rc-kxgsm" in namespace "gc-8020"
May 16 14:30:47.000: INFO: Deleting pod "simpletest.rc-kxxk7" in namespace "gc-8020"
May 16 14:30:47.014: INFO: Deleting pod "simpletest.rc-l8lt8" in namespace "gc-8020"
May 16 14:30:47.035: INFO: Deleting pod "simpletest.rc-lcm2l" in namespace "gc-8020"
May 16 14:30:47.049: INFO: Deleting pod "simpletest.rc-m8kp5" in namespace "gc-8020"
May 16 14:30:47.065: INFO: Deleting pod "simpletest.rc-m99c7" in namespace "gc-8020"
May 16 14:30:47.082: INFO: Deleting pod "simpletest.rc-mk6m5" in namespace "gc-8020"
May 16 14:30:47.102: INFO: Deleting pod "simpletest.rc-ml5wv" in namespace "gc-8020"
May 16 14:30:47.114: INFO: Deleting pod "simpletest.rc-n68bg" in namespace "gc-8020"
May 16 14:30:47.127: INFO: Deleting pod "simpletest.rc-ncdvc" in namespace "gc-8020"
May 16 14:30:47.144: INFO: Deleting pod "simpletest.rc-nffcb" in namespace "gc-8020"
May 16 14:30:47.176: INFO: Deleting pod "simpletest.rc-nrqxg" in namespace "gc-8020"
May 16 14:30:47.189: INFO: Deleting pod "simpletest.rc-p2l87" in namespace "gc-8020"
May 16 14:30:47.207: INFO: Deleting pod "simpletest.rc-p4t76" in namespace "gc-8020"
May 16 14:30:47.220: INFO: Deleting pod "simpletest.rc-p8tdv" in namespace "gc-8020"
May 16 14:30:47.236: INFO: Deleting pod "simpletest.rc-pb6hk" in namespace "gc-8020"
May 16 14:30:47.254: INFO: Deleting pod "simpletest.rc-pgh4c" in namespace "gc-8020"
May 16 14:30:47.274: INFO: Deleting pod "simpletest.rc-pjllb" in namespace "gc-8020"
May 16 14:30:47.293: INFO: Deleting pod "simpletest.rc-psprl" in namespace "gc-8020"
May 16 14:30:47.306: INFO: Deleting pod "simpletest.rc-pvkms" in namespace "gc-8020"
May 16 14:30:47.338: INFO: Deleting pod "simpletest.rc-pw9fw" in namespace "gc-8020"
May 16 14:30:47.378: INFO: Deleting pod "simpletest.rc-pwn8w" in namespace "gc-8020"
May 16 14:30:47.466: INFO: Deleting pod "simpletest.rc-q28h9" in namespace "gc-8020"
May 16 14:30:47.503: INFO: Deleting pod "simpletest.rc-qhnfs" in namespace "gc-8020"
May 16 14:30:47.530: INFO: Deleting pod "simpletest.rc-qq5mx" in namespace "gc-8020"
May 16 14:30:47.552: INFO: Deleting pod "simpletest.rc-qs54w" in namespace "gc-8020"
May 16 14:30:47.569: INFO: Deleting pod "simpletest.rc-qwzmd" in namespace "gc-8020"
May 16 14:30:47.580: INFO: Deleting pod "simpletest.rc-r8rmd" in namespace "gc-8020"
May 16 14:30:47.600: INFO: Deleting pod "simpletest.rc-rvzxd" in namespace "gc-8020"
May 16 14:30:47.618: INFO: Deleting pod "simpletest.rc-rwvxm" in namespace "gc-8020"
May 16 14:30:47.635: INFO: Deleting pod "simpletest.rc-sgzmf" in namespace "gc-8020"
May 16 14:30:47.653: INFO: Deleting pod "simpletest.rc-srjsg" in namespace "gc-8020"
May 16 14:30:47.671: INFO: Deleting pod "simpletest.rc-t4rnz" in namespace "gc-8020"
May 16 14:30:47.727: INFO: Deleting pod "simpletest.rc-t5d8t" in namespace "gc-8020"
May 16 14:30:47.769: INFO: Deleting pod "simpletest.rc-tpcxl" in namespace "gc-8020"
May 16 14:30:47.819: INFO: Deleting pod "simpletest.rc-tshg2" in namespace "gc-8020"
May 16 14:30:47.869: INFO: Deleting pod "simpletest.rc-txk5v" in namespace "gc-8020"
May 16 14:30:47.918: INFO: Deleting pod "simpletest.rc-v27ch" in namespace "gc-8020"
May 16 14:30:47.979: INFO: Deleting pod "simpletest.rc-v59ks" in namespace "gc-8020"
May 16 14:30:48.083: INFO: Deleting pod "simpletest.rc-v748g" in namespace "gc-8020"
May 16 14:30:48.126: INFO: Deleting pod "simpletest.rc-v9hrd" in namespace "gc-8020"
May 16 14:30:48.166: INFO: Deleting pod "simpletest.rc-vc2m5" in namespace "gc-8020"
May 16 14:30:48.184: INFO: Deleting pod "simpletest.rc-vvrw7" in namespace "gc-8020"
May 16 14:30:48.223: INFO: Deleting pod "simpletest.rc-wb7ll" in namespace "gc-8020"
May 16 14:30:48.281: INFO: Deleting pod "simpletest.rc-wl2rr" in namespace "gc-8020"
May 16 14:30:48.319: INFO: Deleting pod "simpletest.rc-x74nb" in namespace "gc-8020"
May 16 14:30:48.373: INFO: Deleting pod "simpletest.rc-x8kck" in namespace "gc-8020"
May 16 14:30:48.416: INFO: Deleting pod "simpletest.rc-xblmh" in namespace "gc-8020"
May 16 14:30:48.485: INFO: Deleting pod "simpletest.rc-xgl5n" in namespace "gc-8020"
May 16 14:30:48.535: INFO: Deleting pod "simpletest.rc-xgz6w" in namespace "gc-8020"
May 16 14:30:48.575: INFO: Deleting pod "simpletest.rc-xvrfj" in namespace "gc-8020"
May 16 14:30:48.619: INFO: Deleting pod "simpletest.rc-xzl5m" in namespace "gc-8020"
May 16 14:30:48.671: INFO: Deleting pod "simpletest.rc-z69r5" in namespace "gc-8020"
May 16 14:30:48.727: INFO: Deleting pod "simpletest.rc-zvpfp" in namespace "gc-8020"
May 16 14:30:48.782: INFO: Deleting pod "simpletest.rc-zxw4r" in namespace "gc-8020"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May 16 14:30:48.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8020" for this suite. 05/16/23 14:30:48.864
------------------------------
â€¢ [SLOW TEST] [42.772 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:30:06.143
    May 16 14:30:06.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename gc 05/16/23 14:30:06.144
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:30:06.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:30:06.177
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 05/16/23 14:30:06.184
    STEP: delete the rc 05/16/23 14:30:11.195
    STEP: wait for the rc to be deleted 05/16/23 14:30:11.202
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 05/16/23 14:30:16.205
    STEP: Gathering metrics 05/16/23 14:30:46.215
    W0516 14:30:46.217613      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
    W0516 14:30:46.217626      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May 16 14:30:46.217: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    May 16 14:30:46.217: INFO: Deleting pod "simpletest.rc-24t2n" in namespace "gc-8020"
    May 16 14:30:46.229: INFO: Deleting pod "simpletest.rc-2wtf8" in namespace "gc-8020"
    May 16 14:30:46.243: INFO: Deleting pod "simpletest.rc-45x2x" in namespace "gc-8020"
    May 16 14:30:46.252: INFO: Deleting pod "simpletest.rc-4lmks" in namespace "gc-8020"
    May 16 14:30:46.267: INFO: Deleting pod "simpletest.rc-4p79s" in namespace "gc-8020"
    May 16 14:30:46.287: INFO: Deleting pod "simpletest.rc-4qt29" in namespace "gc-8020"
    May 16 14:30:46.302: INFO: Deleting pod "simpletest.rc-4tqqf" in namespace "gc-8020"
    May 16 14:30:46.315: INFO: Deleting pod "simpletest.rc-5dkgg" in namespace "gc-8020"
    May 16 14:30:46.327: INFO: Deleting pod "simpletest.rc-5tlpc" in namespace "gc-8020"
    May 16 14:30:46.339: INFO: Deleting pod "simpletest.rc-6bpk6" in namespace "gc-8020"
    May 16 14:30:46.362: INFO: Deleting pod "simpletest.rc-6nvv9" in namespace "gc-8020"
    May 16 14:30:46.380: INFO: Deleting pod "simpletest.rc-6tjhm" in namespace "gc-8020"
    May 16 14:30:46.395: INFO: Deleting pod "simpletest.rc-6v7nn" in namespace "gc-8020"
    May 16 14:30:46.413: INFO: Deleting pod "simpletest.rc-6zb59" in namespace "gc-8020"
    May 16 14:30:46.431: INFO: Deleting pod "simpletest.rc-7vk58" in namespace "gc-8020"
    May 16 14:30:46.446: INFO: Deleting pod "simpletest.rc-8whzx" in namespace "gc-8020"
    May 16 14:30:46.461: INFO: Deleting pod "simpletest.rc-9bc28" in namespace "gc-8020"
    May 16 14:30:46.478: INFO: Deleting pod "simpletest.rc-9smwh" in namespace "gc-8020"
    May 16 14:30:46.498: INFO: Deleting pod "simpletest.rc-9v9rt" in namespace "gc-8020"
    May 16 14:30:46.512: INFO: Deleting pod "simpletest.rc-9x2pc" in namespace "gc-8020"
    May 16 14:30:46.526: INFO: Deleting pod "simpletest.rc-b74kt" in namespace "gc-8020"
    May 16 14:30:46.559: INFO: Deleting pod "simpletest.rc-bgfqz" in namespace "gc-8020"
    May 16 14:30:46.575: INFO: Deleting pod "simpletest.rc-bpc54" in namespace "gc-8020"
    May 16 14:30:46.587: INFO: Deleting pod "simpletest.rc-c9cqq" in namespace "gc-8020"
    May 16 14:30:46.610: INFO: Deleting pod "simpletest.rc-cdt69" in namespace "gc-8020"
    May 16 14:30:46.625: INFO: Deleting pod "simpletest.rc-d6f7q" in namespace "gc-8020"
    May 16 14:30:46.645: INFO: Deleting pod "simpletest.rc-dcp8s" in namespace "gc-8020"
    May 16 14:30:46.661: INFO: Deleting pod "simpletest.rc-dgdck" in namespace "gc-8020"
    May 16 14:30:46.675: INFO: Deleting pod "simpletest.rc-dthtn" in namespace "gc-8020"
    May 16 14:30:46.694: INFO: Deleting pod "simpletest.rc-f2vws" in namespace "gc-8020"
    May 16 14:30:46.711: INFO: Deleting pod "simpletest.rc-f6xz9" in namespace "gc-8020"
    May 16 14:30:46.737: INFO: Deleting pod "simpletest.rc-f9gxw" in namespace "gc-8020"
    May 16 14:30:46.755: INFO: Deleting pod "simpletest.rc-fgvt2" in namespace "gc-8020"
    May 16 14:30:46.776: INFO: Deleting pod "simpletest.rc-gjhbf" in namespace "gc-8020"
    May 16 14:30:46.804: INFO: Deleting pod "simpletest.rc-h845b" in namespace "gc-8020"
    May 16 14:30:46.820: INFO: Deleting pod "simpletest.rc-hvp5l" in namespace "gc-8020"
    May 16 14:30:46.834: INFO: Deleting pod "simpletest.rc-hvw6h" in namespace "gc-8020"
    May 16 14:30:46.846: INFO: Deleting pod "simpletest.rc-j54jn" in namespace "gc-8020"
    May 16 14:30:46.864: INFO: Deleting pod "simpletest.rc-jpwq2" in namespace "gc-8020"
    May 16 14:30:46.880: INFO: Deleting pod "simpletest.rc-jvln6" in namespace "gc-8020"
    May 16 14:30:46.895: INFO: Deleting pod "simpletest.rc-jzqdd" in namespace "gc-8020"
    May 16 14:30:46.917: INFO: Deleting pod "simpletest.rc-khlwk" in namespace "gc-8020"
    May 16 14:30:46.931: INFO: Deleting pod "simpletest.rc-khp2v" in namespace "gc-8020"
    May 16 14:30:46.947: INFO: Deleting pod "simpletest.rc-kkth7" in namespace "gc-8020"
    May 16 14:30:46.970: INFO: Deleting pod "simpletest.rc-ktjbh" in namespace "gc-8020"
    May 16 14:30:46.986: INFO: Deleting pod "simpletest.rc-kxgsm" in namespace "gc-8020"
    May 16 14:30:47.000: INFO: Deleting pod "simpletest.rc-kxxk7" in namespace "gc-8020"
    May 16 14:30:47.014: INFO: Deleting pod "simpletest.rc-l8lt8" in namespace "gc-8020"
    May 16 14:30:47.035: INFO: Deleting pod "simpletest.rc-lcm2l" in namespace "gc-8020"
    May 16 14:30:47.049: INFO: Deleting pod "simpletest.rc-m8kp5" in namespace "gc-8020"
    May 16 14:30:47.065: INFO: Deleting pod "simpletest.rc-m99c7" in namespace "gc-8020"
    May 16 14:30:47.082: INFO: Deleting pod "simpletest.rc-mk6m5" in namespace "gc-8020"
    May 16 14:30:47.102: INFO: Deleting pod "simpletest.rc-ml5wv" in namespace "gc-8020"
    May 16 14:30:47.114: INFO: Deleting pod "simpletest.rc-n68bg" in namespace "gc-8020"
    May 16 14:30:47.127: INFO: Deleting pod "simpletest.rc-ncdvc" in namespace "gc-8020"
    May 16 14:30:47.144: INFO: Deleting pod "simpletest.rc-nffcb" in namespace "gc-8020"
    May 16 14:30:47.176: INFO: Deleting pod "simpletest.rc-nrqxg" in namespace "gc-8020"
    May 16 14:30:47.189: INFO: Deleting pod "simpletest.rc-p2l87" in namespace "gc-8020"
    May 16 14:30:47.207: INFO: Deleting pod "simpletest.rc-p4t76" in namespace "gc-8020"
    May 16 14:30:47.220: INFO: Deleting pod "simpletest.rc-p8tdv" in namespace "gc-8020"
    May 16 14:30:47.236: INFO: Deleting pod "simpletest.rc-pb6hk" in namespace "gc-8020"
    May 16 14:30:47.254: INFO: Deleting pod "simpletest.rc-pgh4c" in namespace "gc-8020"
    May 16 14:30:47.274: INFO: Deleting pod "simpletest.rc-pjllb" in namespace "gc-8020"
    May 16 14:30:47.293: INFO: Deleting pod "simpletest.rc-psprl" in namespace "gc-8020"
    May 16 14:30:47.306: INFO: Deleting pod "simpletest.rc-pvkms" in namespace "gc-8020"
    May 16 14:30:47.338: INFO: Deleting pod "simpletest.rc-pw9fw" in namespace "gc-8020"
    May 16 14:30:47.378: INFO: Deleting pod "simpletest.rc-pwn8w" in namespace "gc-8020"
    May 16 14:30:47.466: INFO: Deleting pod "simpletest.rc-q28h9" in namespace "gc-8020"
    May 16 14:30:47.503: INFO: Deleting pod "simpletest.rc-qhnfs" in namespace "gc-8020"
    May 16 14:30:47.530: INFO: Deleting pod "simpletest.rc-qq5mx" in namespace "gc-8020"
    May 16 14:30:47.552: INFO: Deleting pod "simpletest.rc-qs54w" in namespace "gc-8020"
    May 16 14:30:47.569: INFO: Deleting pod "simpletest.rc-qwzmd" in namespace "gc-8020"
    May 16 14:30:47.580: INFO: Deleting pod "simpletest.rc-r8rmd" in namespace "gc-8020"
    May 16 14:30:47.600: INFO: Deleting pod "simpletest.rc-rvzxd" in namespace "gc-8020"
    May 16 14:30:47.618: INFO: Deleting pod "simpletest.rc-rwvxm" in namespace "gc-8020"
    May 16 14:30:47.635: INFO: Deleting pod "simpletest.rc-sgzmf" in namespace "gc-8020"
    May 16 14:30:47.653: INFO: Deleting pod "simpletest.rc-srjsg" in namespace "gc-8020"
    May 16 14:30:47.671: INFO: Deleting pod "simpletest.rc-t4rnz" in namespace "gc-8020"
    May 16 14:30:47.727: INFO: Deleting pod "simpletest.rc-t5d8t" in namespace "gc-8020"
    May 16 14:30:47.769: INFO: Deleting pod "simpletest.rc-tpcxl" in namespace "gc-8020"
    May 16 14:30:47.819: INFO: Deleting pod "simpletest.rc-tshg2" in namespace "gc-8020"
    May 16 14:30:47.869: INFO: Deleting pod "simpletest.rc-txk5v" in namespace "gc-8020"
    May 16 14:30:47.918: INFO: Deleting pod "simpletest.rc-v27ch" in namespace "gc-8020"
    May 16 14:30:47.979: INFO: Deleting pod "simpletest.rc-v59ks" in namespace "gc-8020"
    May 16 14:30:48.083: INFO: Deleting pod "simpletest.rc-v748g" in namespace "gc-8020"
    May 16 14:30:48.126: INFO: Deleting pod "simpletest.rc-v9hrd" in namespace "gc-8020"
    May 16 14:30:48.166: INFO: Deleting pod "simpletest.rc-vc2m5" in namespace "gc-8020"
    May 16 14:30:48.184: INFO: Deleting pod "simpletest.rc-vvrw7" in namespace "gc-8020"
    May 16 14:30:48.223: INFO: Deleting pod "simpletest.rc-wb7ll" in namespace "gc-8020"
    May 16 14:30:48.281: INFO: Deleting pod "simpletest.rc-wl2rr" in namespace "gc-8020"
    May 16 14:30:48.319: INFO: Deleting pod "simpletest.rc-x74nb" in namespace "gc-8020"
    May 16 14:30:48.373: INFO: Deleting pod "simpletest.rc-x8kck" in namespace "gc-8020"
    May 16 14:30:48.416: INFO: Deleting pod "simpletest.rc-xblmh" in namespace "gc-8020"
    May 16 14:30:48.485: INFO: Deleting pod "simpletest.rc-xgl5n" in namespace "gc-8020"
    May 16 14:30:48.535: INFO: Deleting pod "simpletest.rc-xgz6w" in namespace "gc-8020"
    May 16 14:30:48.575: INFO: Deleting pod "simpletest.rc-xvrfj" in namespace "gc-8020"
    May 16 14:30:48.619: INFO: Deleting pod "simpletest.rc-xzl5m" in namespace "gc-8020"
    May 16 14:30:48.671: INFO: Deleting pod "simpletest.rc-z69r5" in namespace "gc-8020"
    May 16 14:30:48.727: INFO: Deleting pod "simpletest.rc-zvpfp" in namespace "gc-8020"
    May 16 14:30:48.782: INFO: Deleting pod "simpletest.rc-zxw4r" in namespace "gc-8020"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May 16 14:30:48.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8020" for this suite. 05/16/23 14:30:48.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:30:48.917
May 16 14:30:48.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename configmap 05/16/23 14:30:48.918
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:30:48.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:30:48.943
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-7c5d257b-78e5-4550-8de9-ca96f3aa8c49 05/16/23 14:30:48.949
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 16 14:30:48.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2272" for this suite. 05/16/23 14:30:48.968
------------------------------
â€¢ [0.082 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:30:48.917
    May 16 14:30:48.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename configmap 05/16/23 14:30:48.918
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:30:48.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:30:48.943
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-7c5d257b-78e5-4550-8de9-ca96f3aa8c49 05/16/23 14:30:48.949
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 16 14:30:48.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2272" for this suite. 05/16/23 14:30:48.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:30:49
May 16 14:30:49.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename webhook 05/16/23 14:30:49.001
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:30:49.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:30:49.042
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/16/23 14:30:49.073
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 14:30:49.463
STEP: Deploying the webhook pod 05/16/23 14:30:49.476
STEP: Wait for the deployment to be ready 05/16/23 14:30:49.486
May 16 14:30:49.495: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/16/23 14:30:51.504
STEP: Verifying the service has paired with the endpoint 05/16/23 14:30:51.514
May 16 14:30:52.515: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
May 16 14:30:52.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8659-crds.webhook.example.com via the AdmissionRegistration API 05/16/23 14:30:53.027
STEP: Creating a custom resource while v1 is storage version 05/16/23 14:30:53.04
STEP: Patching Custom Resource Definition to set v2 as storage 05/16/23 14:30:55.08
STEP: Patching the custom resource while v2 is storage version 05/16/23 14:30:55.092
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 14:30:55.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6531" for this suite. 05/16/23 14:30:55.725
STEP: Destroying namespace "webhook-6531-markers" for this suite. 05/16/23 14:30:55.732
------------------------------
â€¢ [SLOW TEST] [6.744 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:30:49
    May 16 14:30:49.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename webhook 05/16/23 14:30:49.001
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:30:49.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:30:49.042
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/16/23 14:30:49.073
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 14:30:49.463
    STEP: Deploying the webhook pod 05/16/23 14:30:49.476
    STEP: Wait for the deployment to be ready 05/16/23 14:30:49.486
    May 16 14:30:49.495: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/16/23 14:30:51.504
    STEP: Verifying the service has paired with the endpoint 05/16/23 14:30:51.514
    May 16 14:30:52.515: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    May 16 14:30:52.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8659-crds.webhook.example.com via the AdmissionRegistration API 05/16/23 14:30:53.027
    STEP: Creating a custom resource while v1 is storage version 05/16/23 14:30:53.04
    STEP: Patching Custom Resource Definition to set v2 as storage 05/16/23 14:30:55.08
    STEP: Patching the custom resource while v2 is storage version 05/16/23 14:30:55.092
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 14:30:55.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6531" for this suite. 05/16/23 14:30:55.725
    STEP: Destroying namespace "webhook-6531-markers" for this suite. 05/16/23 14:30:55.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:30:55.744
May 16 14:30:55.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename crd-publish-openapi 05/16/23 14:30:55.745
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:30:55.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:30:55.765
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
May 16 14:30:55.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/16/23 14:31:00.277
May 16 14:31:00.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-880 --namespace=crd-publish-openapi-880 create -f -'
May 16 14:31:01.399: INFO: stderr: ""
May 16 14:31:01.399: INFO: stdout: "e2e-test-crd-publish-openapi-5799-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 16 14:31:01.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-880 --namespace=crd-publish-openapi-880 delete e2e-test-crd-publish-openapi-5799-crds test-cr'
May 16 14:31:01.479: INFO: stderr: ""
May 16 14:31:01.479: INFO: stdout: "e2e-test-crd-publish-openapi-5799-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 16 14:31:01.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-880 --namespace=crd-publish-openapi-880 apply -f -'
May 16 14:31:02.417: INFO: stderr: ""
May 16 14:31:02.417: INFO: stdout: "e2e-test-crd-publish-openapi-5799-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 16 14:31:02.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-880 --namespace=crd-publish-openapi-880 delete e2e-test-crd-publish-openapi-5799-crds test-cr'
May 16 14:31:02.467: INFO: stderr: ""
May 16 14:31:02.467: INFO: stdout: "e2e-test-crd-publish-openapi-5799-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 05/16/23 14:31:02.467
May 16 14:31:02.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-880 explain e2e-test-crd-publish-openapi-5799-crds'
May 16 14:31:03.352: INFO: stderr: ""
May 16 14:31:03.352: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5799-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 14:31:06.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-880" for this suite. 05/16/23 14:31:06.705
------------------------------
â€¢ [SLOW TEST] [10.969 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:30:55.744
    May 16 14:30:55.744: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename crd-publish-openapi 05/16/23 14:30:55.745
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:30:55.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:30:55.765
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    May 16 14:30:55.767: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/16/23 14:31:00.277
    May 16 14:31:00.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-880 --namespace=crd-publish-openapi-880 create -f -'
    May 16 14:31:01.399: INFO: stderr: ""
    May 16 14:31:01.399: INFO: stdout: "e2e-test-crd-publish-openapi-5799-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    May 16 14:31:01.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-880 --namespace=crd-publish-openapi-880 delete e2e-test-crd-publish-openapi-5799-crds test-cr'
    May 16 14:31:01.479: INFO: stderr: ""
    May 16 14:31:01.479: INFO: stdout: "e2e-test-crd-publish-openapi-5799-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    May 16 14:31:01.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-880 --namespace=crd-publish-openapi-880 apply -f -'
    May 16 14:31:02.417: INFO: stderr: ""
    May 16 14:31:02.417: INFO: stdout: "e2e-test-crd-publish-openapi-5799-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    May 16 14:31:02.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-880 --namespace=crd-publish-openapi-880 delete e2e-test-crd-publish-openapi-5799-crds test-cr'
    May 16 14:31:02.467: INFO: stderr: ""
    May 16 14:31:02.467: INFO: stdout: "e2e-test-crd-publish-openapi-5799-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 05/16/23 14:31:02.467
    May 16 14:31:02.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-880 explain e2e-test-crd-publish-openapi-5799-crds'
    May 16 14:31:03.352: INFO: stderr: ""
    May 16 14:31:03.352: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5799-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 14:31:06.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-880" for this suite. 05/16/23 14:31:06.705
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:31:06.713
May 16 14:31:06.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename emptydir-wrapper 05/16/23 14:31:06.714
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:31:06.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:31:06.731
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
May 16 14:31:06.759: INFO: Waiting up to 5m0s for pod "pod-secrets-843d2b2c-eefe-480a-9ea2-4c0d92a56b11" in namespace "emptydir-wrapper-1196" to be "running and ready"
May 16 14:31:06.767: INFO: Pod "pod-secrets-843d2b2c-eefe-480a-9ea2-4c0d92a56b11": Phase="Pending", Reason="", readiness=false. Elapsed: 7.447332ms
May 16 14:31:06.767: INFO: The phase of Pod pod-secrets-843d2b2c-eefe-480a-9ea2-4c0d92a56b11 is Pending, waiting for it to be Running (with Ready = true)
May 16 14:31:08.771: INFO: Pod "pod-secrets-843d2b2c-eefe-480a-9ea2-4c0d92a56b11": Phase="Running", Reason="", readiness=true. Elapsed: 2.011125517s
May 16 14:31:08.771: INFO: The phase of Pod pod-secrets-843d2b2c-eefe-480a-9ea2-4c0d92a56b11 is Running (Ready = true)
May 16 14:31:08.771: INFO: Pod "pod-secrets-843d2b2c-eefe-480a-9ea2-4c0d92a56b11" satisfied condition "running and ready"
STEP: Cleaning up the secret 05/16/23 14:31:08.773
STEP: Cleaning up the configmap 05/16/23 14:31:08.779
STEP: Cleaning up the pod 05/16/23 14:31:08.784
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
May 16 14:31:08.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1196" for this suite. 05/16/23 14:31:08.801
------------------------------
â€¢ [2.094 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:31:06.713
    May 16 14:31:06.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename emptydir-wrapper 05/16/23 14:31:06.714
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:31:06.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:31:06.731
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    May 16 14:31:06.759: INFO: Waiting up to 5m0s for pod "pod-secrets-843d2b2c-eefe-480a-9ea2-4c0d92a56b11" in namespace "emptydir-wrapper-1196" to be "running and ready"
    May 16 14:31:06.767: INFO: Pod "pod-secrets-843d2b2c-eefe-480a-9ea2-4c0d92a56b11": Phase="Pending", Reason="", readiness=false. Elapsed: 7.447332ms
    May 16 14:31:06.767: INFO: The phase of Pod pod-secrets-843d2b2c-eefe-480a-9ea2-4c0d92a56b11 is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:31:08.771: INFO: Pod "pod-secrets-843d2b2c-eefe-480a-9ea2-4c0d92a56b11": Phase="Running", Reason="", readiness=true. Elapsed: 2.011125517s
    May 16 14:31:08.771: INFO: The phase of Pod pod-secrets-843d2b2c-eefe-480a-9ea2-4c0d92a56b11 is Running (Ready = true)
    May 16 14:31:08.771: INFO: Pod "pod-secrets-843d2b2c-eefe-480a-9ea2-4c0d92a56b11" satisfied condition "running and ready"
    STEP: Cleaning up the secret 05/16/23 14:31:08.773
    STEP: Cleaning up the configmap 05/16/23 14:31:08.779
    STEP: Cleaning up the pod 05/16/23 14:31:08.784
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    May 16 14:31:08.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1196" for this suite. 05/16/23 14:31:08.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:31:08.808
May 16 14:31:08.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename crd-webhook 05/16/23 14:31:08.808
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:31:08.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:31:08.832
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 05/16/23 14:31:08.833
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/16/23 14:31:09.178
STEP: Deploying the custom resource conversion webhook pod 05/16/23 14:31:09.185
STEP: Wait for the deployment to be ready 05/16/23 14:31:09.194
May 16 14:31:09.201: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/16/23 14:31:11.21
STEP: Verifying the service has paired with the endpoint 05/16/23 14:31:11.22
May 16 14:31:12.220: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
May 16 14:31:12.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Creating a v1 custom resource 05/16/23 14:31:14.78
STEP: Create a v2 custom resource 05/16/23 14:31:14.794
STEP: List CRs in v1 05/16/23 14:31:14.831
STEP: List CRs in v2 05/16/23 14:31:14.835
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 14:31:15.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-6870" for this suite. 05/16/23 14:31:15.419
------------------------------
â€¢ [SLOW TEST] [6.621 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:31:08.808
    May 16 14:31:08.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename crd-webhook 05/16/23 14:31:08.808
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:31:08.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:31:08.832
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 05/16/23 14:31:08.833
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/16/23 14:31:09.178
    STEP: Deploying the custom resource conversion webhook pod 05/16/23 14:31:09.185
    STEP: Wait for the deployment to be ready 05/16/23 14:31:09.194
    May 16 14:31:09.201: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/16/23 14:31:11.21
    STEP: Verifying the service has paired with the endpoint 05/16/23 14:31:11.22
    May 16 14:31:12.220: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    May 16 14:31:12.223: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Creating a v1 custom resource 05/16/23 14:31:14.78
    STEP: Create a v2 custom resource 05/16/23 14:31:14.794
    STEP: List CRs in v1 05/16/23 14:31:14.831
    STEP: List CRs in v2 05/16/23 14:31:14.835
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 14:31:15.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-6870" for this suite. 05/16/23 14:31:15.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:31:15.429
May 16 14:31:15.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename webhook 05/16/23 14:31:15.43
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:31:15.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:31:15.454
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/16/23 14:31:15.547
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 14:31:15.799
STEP: Deploying the webhook pod 05/16/23 14:31:15.807
STEP: Wait for the deployment to be ready 05/16/23 14:31:15.818
May 16 14:31:15.824: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/16/23 14:31:17.833
STEP: Verifying the service has paired with the endpoint 05/16/23 14:31:17.843
May 16 14:31:18.845: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 05/16/23 14:31:18.85
STEP: create a pod that should be denied by the webhook 05/16/23 14:31:18.862
STEP: create a pod that causes the webhook to hang 05/16/23 14:31:18.871
STEP: create a configmap that should be denied by the webhook 05/16/23 14:31:28.878
STEP: create a configmap that should be admitted by the webhook 05/16/23 14:31:28.896
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 05/16/23 14:31:28.904
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 05/16/23 14:31:28.91
STEP: create a namespace that bypass the webhook 05/16/23 14:31:28.915
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 05/16/23 14:31:28.921
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 14:31:28.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4884" for this suite. 05/16/23 14:31:29.013
STEP: Destroying namespace "webhook-4884-markers" for this suite. 05/16/23 14:31:29.019
------------------------------
â€¢ [SLOW TEST] [13.595 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:31:15.429
    May 16 14:31:15.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename webhook 05/16/23 14:31:15.43
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:31:15.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:31:15.454
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/16/23 14:31:15.547
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 14:31:15.799
    STEP: Deploying the webhook pod 05/16/23 14:31:15.807
    STEP: Wait for the deployment to be ready 05/16/23 14:31:15.818
    May 16 14:31:15.824: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/16/23 14:31:17.833
    STEP: Verifying the service has paired with the endpoint 05/16/23 14:31:17.843
    May 16 14:31:18.845: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 05/16/23 14:31:18.85
    STEP: create a pod that should be denied by the webhook 05/16/23 14:31:18.862
    STEP: create a pod that causes the webhook to hang 05/16/23 14:31:18.871
    STEP: create a configmap that should be denied by the webhook 05/16/23 14:31:28.878
    STEP: create a configmap that should be admitted by the webhook 05/16/23 14:31:28.896
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 05/16/23 14:31:28.904
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 05/16/23 14:31:28.91
    STEP: create a namespace that bypass the webhook 05/16/23 14:31:28.915
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 05/16/23 14:31:28.921
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 14:31:28.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4884" for this suite. 05/16/23 14:31:29.013
    STEP: Destroying namespace "webhook-4884-markers" for this suite. 05/16/23 14:31:29.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:31:29.026
May 16 14:31:29.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename resourcequota 05/16/23 14:31:29.027
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:31:29.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:31:29.047
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 05/16/23 14:31:29.05
STEP: Ensuring ResourceQuota status is calculated 05/16/23 14:31:29.055
STEP: Creating a ResourceQuota with not terminating scope 05/16/23 14:31:31.058
STEP: Ensuring ResourceQuota status is calculated 05/16/23 14:31:31.062
STEP: Creating a long running pod 05/16/23 14:31:33.066
STEP: Ensuring resource quota with not terminating scope captures the pod usage 05/16/23 14:31:33.079
STEP: Ensuring resource quota with terminating scope ignored the pod usage 05/16/23 14:31:35.083
STEP: Deleting the pod 05/16/23 14:31:37.087
STEP: Ensuring resource quota status released the pod usage 05/16/23 14:31:37.1
STEP: Creating a terminating pod 05/16/23 14:31:39.103
STEP: Ensuring resource quota with terminating scope captures the pod usage 05/16/23 14:31:39.114
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 05/16/23 14:31:41.118
STEP: Deleting the pod 05/16/23 14:31:43.122
STEP: Ensuring resource quota status released the pod usage 05/16/23 14:31:43.135
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 16 14:31:45.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8550" for this suite. 05/16/23 14:31:45.142
------------------------------
â€¢ [SLOW TEST] [16.121 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:31:29.026
    May 16 14:31:29.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename resourcequota 05/16/23 14:31:29.027
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:31:29.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:31:29.047
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 05/16/23 14:31:29.05
    STEP: Ensuring ResourceQuota status is calculated 05/16/23 14:31:29.055
    STEP: Creating a ResourceQuota with not terminating scope 05/16/23 14:31:31.058
    STEP: Ensuring ResourceQuota status is calculated 05/16/23 14:31:31.062
    STEP: Creating a long running pod 05/16/23 14:31:33.066
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 05/16/23 14:31:33.079
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 05/16/23 14:31:35.083
    STEP: Deleting the pod 05/16/23 14:31:37.087
    STEP: Ensuring resource quota status released the pod usage 05/16/23 14:31:37.1
    STEP: Creating a terminating pod 05/16/23 14:31:39.103
    STEP: Ensuring resource quota with terminating scope captures the pod usage 05/16/23 14:31:39.114
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 05/16/23 14:31:41.118
    STEP: Deleting the pod 05/16/23 14:31:43.122
    STEP: Ensuring resource quota status released the pod usage 05/16/23 14:31:43.135
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 16 14:31:45.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8550" for this suite. 05/16/23 14:31:45.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:31:45.148
May 16 14:31:45.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubelet-test 05/16/23 14:31:45.149
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:31:45.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:31:45.167
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
W0516 14:31:45.183091      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting for pod completion 05/16/23 14:31:45.183
May 16 14:31:45.183: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases0a9348d4-558a-413d-b970-d92658ebb1f7" in namespace "kubelet-test-2514" to be "completed"
May 16 14:31:45.187: INFO: Pod "agnhost-host-aliases0a9348d4-558a-413d-b970-d92658ebb1f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.388969ms
May 16 14:31:47.191: INFO: Pod "agnhost-host-aliases0a9348d4-558a-413d-b970-d92658ebb1f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008425486s
May 16 14:31:49.191: INFO: Pod "agnhost-host-aliases0a9348d4-558a-413d-b970-d92658ebb1f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008158852s
May 16 14:31:49.191: INFO: Pod "agnhost-host-aliases0a9348d4-558a-413d-b970-d92658ebb1f7" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May 16 14:31:49.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2514" for this suite. 05/16/23 14:31:49.206
------------------------------
â€¢ [4.064 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:31:45.148
    May 16 14:31:45.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubelet-test 05/16/23 14:31:45.149
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:31:45.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:31:45.167
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    W0516 14:31:45.183091      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Waiting for pod completion 05/16/23 14:31:45.183
    May 16 14:31:45.183: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases0a9348d4-558a-413d-b970-d92658ebb1f7" in namespace "kubelet-test-2514" to be "completed"
    May 16 14:31:45.187: INFO: Pod "agnhost-host-aliases0a9348d4-558a-413d-b970-d92658ebb1f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.388969ms
    May 16 14:31:47.191: INFO: Pod "agnhost-host-aliases0a9348d4-558a-413d-b970-d92658ebb1f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008425486s
    May 16 14:31:49.191: INFO: Pod "agnhost-host-aliases0a9348d4-558a-413d-b970-d92658ebb1f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008158852s
    May 16 14:31:49.191: INFO: Pod "agnhost-host-aliases0a9348d4-558a-413d-b970-d92658ebb1f7" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May 16 14:31:49.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2514" for this suite. 05/16/23 14:31:49.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:31:49.214
May 16 14:31:49.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename dns 05/16/23 14:31:49.215
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:31:49.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:31:49.23
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 05/16/23 14:31:49.233
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9903.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9903.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9903.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9903.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9903.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9903.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 83.6.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.6.83_udp@PTR;check="$$(dig +tcp +noall +answer +search 83.6.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.6.83_tcp@PTR;sleep 1; done
 05/16/23 14:31:49.262
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9903.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9903.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9903.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9903.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9903.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9903.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 83.6.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.6.83_udp@PTR;check="$$(dig +tcp +noall +answer +search 83.6.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.6.83_tcp@PTR;sleep 1; done
 05/16/23 14:31:49.262
STEP: creating a pod to probe DNS 05/16/23 14:31:49.262
STEP: submitting the pod to kubernetes 05/16/23 14:31:49.262
May 16 14:31:49.283: INFO: Waiting up to 15m0s for pod "dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6" in namespace "dns-9903" to be "running"
May 16 14:31:49.301: INFO: Pod "dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6": Phase="Pending", Reason="", readiness=false. Elapsed: 17.137053ms
May 16 14:31:51.304: INFO: Pod "dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.020210891s
May 16 14:31:51.304: INFO: Pod "dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6" satisfied condition "running"
STEP: retrieving the pod 05/16/23 14:31:51.304
STEP: looking for the results for each expected name from probers 05/16/23 14:31:51.306
May 16 14:31:51.311: INFO: Unable to read wheezy_udp@dns-test-service.dns-9903.svc.cluster.local from pod dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6: the server could not find the requested resource (get pods dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6)
May 16 14:31:51.314: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9903.svc.cluster.local from pod dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6: the server could not find the requested resource (get pods dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6)
May 16 14:31:51.318: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local from pod dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6: the server could not find the requested resource (get pods dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6)
May 16 14:31:51.321: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local from pod dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6: the server could not find the requested resource (get pods dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6)
May 16 14:31:51.336: INFO: Unable to read jessie_udp@dns-test-service.dns-9903.svc.cluster.local from pod dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6: the server could not find the requested resource (get pods dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6)
May 16 14:31:51.339: INFO: Unable to read jessie_tcp@dns-test-service.dns-9903.svc.cluster.local from pod dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6: the server could not find the requested resource (get pods dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6)
May 16 14:31:51.342: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local from pod dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6: the server could not find the requested resource (get pods dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6)
May 16 14:31:51.345: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local from pod dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6: the server could not find the requested resource (get pods dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6)
May 16 14:31:51.357: INFO: Lookups using dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6 failed for: [wheezy_udp@dns-test-service.dns-9903.svc.cluster.local wheezy_tcp@dns-test-service.dns-9903.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local jessie_udp@dns-test-service.dns-9903.svc.cluster.local jessie_tcp@dns-test-service.dns-9903.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local]

May 16 14:31:56.405: INFO: DNS probes using dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6 succeeded

STEP: deleting the pod 05/16/23 14:31:56.405
STEP: deleting the test service 05/16/23 14:31:56.424
STEP: deleting the test headless service 05/16/23 14:31:56.448
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May 16 14:31:56.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9903" for this suite. 05/16/23 14:31:56.486
------------------------------
â€¢ [SLOW TEST] [7.281 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:31:49.214
    May 16 14:31:49.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename dns 05/16/23 14:31:49.215
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:31:49.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:31:49.23
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 05/16/23 14:31:49.233
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9903.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9903.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9903.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9903.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9903.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9903.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 83.6.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.6.83_udp@PTR;check="$$(dig +tcp +noall +answer +search 83.6.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.6.83_tcp@PTR;sleep 1; done
     05/16/23 14:31:49.262
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9903.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9903.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9903.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9903.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9903.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9903.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9903.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 83.6.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.6.83_udp@PTR;check="$$(dig +tcp +noall +answer +search 83.6.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.6.83_tcp@PTR;sleep 1; done
     05/16/23 14:31:49.262
    STEP: creating a pod to probe DNS 05/16/23 14:31:49.262
    STEP: submitting the pod to kubernetes 05/16/23 14:31:49.262
    May 16 14:31:49.283: INFO: Waiting up to 15m0s for pod "dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6" in namespace "dns-9903" to be "running"
    May 16 14:31:49.301: INFO: Pod "dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6": Phase="Pending", Reason="", readiness=false. Elapsed: 17.137053ms
    May 16 14:31:51.304: INFO: Pod "dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.020210891s
    May 16 14:31:51.304: INFO: Pod "dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6" satisfied condition "running"
    STEP: retrieving the pod 05/16/23 14:31:51.304
    STEP: looking for the results for each expected name from probers 05/16/23 14:31:51.306
    May 16 14:31:51.311: INFO: Unable to read wheezy_udp@dns-test-service.dns-9903.svc.cluster.local from pod dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6: the server could not find the requested resource (get pods dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6)
    May 16 14:31:51.314: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9903.svc.cluster.local from pod dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6: the server could not find the requested resource (get pods dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6)
    May 16 14:31:51.318: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local from pod dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6: the server could not find the requested resource (get pods dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6)
    May 16 14:31:51.321: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local from pod dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6: the server could not find the requested resource (get pods dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6)
    May 16 14:31:51.336: INFO: Unable to read jessie_udp@dns-test-service.dns-9903.svc.cluster.local from pod dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6: the server could not find the requested resource (get pods dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6)
    May 16 14:31:51.339: INFO: Unable to read jessie_tcp@dns-test-service.dns-9903.svc.cluster.local from pod dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6: the server could not find the requested resource (get pods dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6)
    May 16 14:31:51.342: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local from pod dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6: the server could not find the requested resource (get pods dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6)
    May 16 14:31:51.345: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local from pod dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6: the server could not find the requested resource (get pods dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6)
    May 16 14:31:51.357: INFO: Lookups using dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6 failed for: [wheezy_udp@dns-test-service.dns-9903.svc.cluster.local wheezy_tcp@dns-test-service.dns-9903.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local jessie_udp@dns-test-service.dns-9903.svc.cluster.local jessie_tcp@dns-test-service.dns-9903.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9903.svc.cluster.local]

    May 16 14:31:56.405: INFO: DNS probes using dns-9903/dns-test-4f956800-f0e8-428e-a099-9a5c27d865b6 succeeded

    STEP: deleting the pod 05/16/23 14:31:56.405
    STEP: deleting the test service 05/16/23 14:31:56.424
    STEP: deleting the test headless service 05/16/23 14:31:56.448
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May 16 14:31:56.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9903" for this suite. 05/16/23 14:31:56.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:31:56.496
May 16 14:31:56.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubectl 05/16/23 14:31:56.496
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:31:56.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:31:56.618
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 05/16/23 14:31:56.62
May 16 14:31:56.620: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May 16 14:31:56.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 create -f -'
May 16 14:31:58.316: INFO: stderr: ""
May 16 14:31:58.316: INFO: stdout: "service/agnhost-replica created\n"
May 16 14:31:58.316: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May 16 14:31:58.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 create -f -'
May 16 14:31:59.866: INFO: stderr: ""
May 16 14:31:59.866: INFO: stdout: "service/agnhost-primary created\n"
May 16 14:31:59.866: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 16 14:31:59.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 create -f -'
May 16 14:32:00.128: INFO: stderr: ""
May 16 14:32:00.128: INFO: stdout: "service/frontend created\n"
May 16 14:32:00.128: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May 16 14:32:00.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 create -f -'
May 16 14:32:00.389: INFO: stderr: ""
May 16 14:32:00.389: INFO: stdout: "deployment.apps/frontend created\n"
May 16 14:32:00.389: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 16 14:32:00.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 create -f -'
May 16 14:32:00.651: INFO: stderr: ""
May 16 14:32:00.651: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May 16 14:32:00.652: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 16 14:32:00.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 create -f -'
May 16 14:32:00.976: INFO: stderr: ""
May 16 14:32:00.976: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 05/16/23 14:32:00.976
May 16 14:32:00.976: INFO: Waiting for all frontend pods to be Running.
May 16 14:32:06.028: INFO: Waiting for frontend to serve content.
May 16 14:32:06.040: INFO: Trying to add a new entry to the guestbook.
May 16 14:32:06.054: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 05/16/23 14:32:06.061
May 16 14:32:06.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 delete --grace-period=0 --force -f -'
May 16 14:32:06.140: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 14:32:06.140: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 05/16/23 14:32:06.14
May 16 14:32:06.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 delete --grace-period=0 --force -f -'
May 16 14:32:06.216: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 14:32:06.216: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 05/16/23 14:32:06.216
May 16 14:32:06.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 delete --grace-period=0 --force -f -'
May 16 14:32:06.298: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 14:32:06.298: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 05/16/23 14:32:06.298
May 16 14:32:06.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 delete --grace-period=0 --force -f -'
May 16 14:32:06.346: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 14:32:06.346: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 05/16/23 14:32:06.346
May 16 14:32:06.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 delete --grace-period=0 --force -f -'
May 16 14:32:06.402: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 14:32:06.402: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 05/16/23 14:32:06.402
May 16 14:32:06.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 delete --grace-period=0 --force -f -'
May 16 14:32:06.448: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 14:32:06.448: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 16 14:32:06.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5501" for this suite. 05/16/23 14:32:06.452
------------------------------
â€¢ [SLOW TEST] [9.962 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:31:56.496
    May 16 14:31:56.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubectl 05/16/23 14:31:56.496
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:31:56.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:31:56.618
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 05/16/23 14:31:56.62
    May 16 14:31:56.620: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    May 16 14:31:56.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 create -f -'
    May 16 14:31:58.316: INFO: stderr: ""
    May 16 14:31:58.316: INFO: stdout: "service/agnhost-replica created\n"
    May 16 14:31:58.316: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    May 16 14:31:58.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 create -f -'
    May 16 14:31:59.866: INFO: stderr: ""
    May 16 14:31:59.866: INFO: stdout: "service/agnhost-primary created\n"
    May 16 14:31:59.866: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    May 16 14:31:59.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 create -f -'
    May 16 14:32:00.128: INFO: stderr: ""
    May 16 14:32:00.128: INFO: stdout: "service/frontend created\n"
    May 16 14:32:00.128: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    May 16 14:32:00.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 create -f -'
    May 16 14:32:00.389: INFO: stderr: ""
    May 16 14:32:00.389: INFO: stdout: "deployment.apps/frontend created\n"
    May 16 14:32:00.389: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    May 16 14:32:00.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 create -f -'
    May 16 14:32:00.651: INFO: stderr: ""
    May 16 14:32:00.651: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    May 16 14:32:00.652: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    May 16 14:32:00.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 create -f -'
    May 16 14:32:00.976: INFO: stderr: ""
    May 16 14:32:00.976: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 05/16/23 14:32:00.976
    May 16 14:32:00.976: INFO: Waiting for all frontend pods to be Running.
    May 16 14:32:06.028: INFO: Waiting for frontend to serve content.
    May 16 14:32:06.040: INFO: Trying to add a new entry to the guestbook.
    May 16 14:32:06.054: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 05/16/23 14:32:06.061
    May 16 14:32:06.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 delete --grace-period=0 --force -f -'
    May 16 14:32:06.140: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 16 14:32:06.140: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 05/16/23 14:32:06.14
    May 16 14:32:06.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 delete --grace-period=0 --force -f -'
    May 16 14:32:06.216: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 16 14:32:06.216: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 05/16/23 14:32:06.216
    May 16 14:32:06.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 delete --grace-period=0 --force -f -'
    May 16 14:32:06.298: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 16 14:32:06.298: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 05/16/23 14:32:06.298
    May 16 14:32:06.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 delete --grace-period=0 --force -f -'
    May 16 14:32:06.346: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 16 14:32:06.346: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 05/16/23 14:32:06.346
    May 16 14:32:06.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 delete --grace-period=0 --force -f -'
    May 16 14:32:06.402: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 16 14:32:06.402: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 05/16/23 14:32:06.402
    May 16 14:32:06.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-5501 delete --grace-period=0 --force -f -'
    May 16 14:32:06.448: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 16 14:32:06.448: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 16 14:32:06.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5501" for this suite. 05/16/23 14:32:06.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:32:06.459
May 16 14:32:06.459: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename events 05/16/23 14:32:06.46
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:32:06.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:32:06.478
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 05/16/23 14:32:06.48
STEP: listing all events in all namespaces 05/16/23 14:32:06.495
STEP: patching the test event 05/16/23 14:32:06.604
STEP: fetching the test event 05/16/23 14:32:06.61
STEP: updating the test event 05/16/23 14:32:06.613
STEP: getting the test event 05/16/23 14:32:06.621
STEP: deleting the test event 05/16/23 14:32:06.625
STEP: listing all events in all namespaces 05/16/23 14:32:06.633
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
May 16 14:32:06.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-5936" for this suite. 05/16/23 14:32:06.776
------------------------------
â€¢ [0.323 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:32:06.459
    May 16 14:32:06.459: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename events 05/16/23 14:32:06.46
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:32:06.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:32:06.478
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 05/16/23 14:32:06.48
    STEP: listing all events in all namespaces 05/16/23 14:32:06.495
    STEP: patching the test event 05/16/23 14:32:06.604
    STEP: fetching the test event 05/16/23 14:32:06.61
    STEP: updating the test event 05/16/23 14:32:06.613
    STEP: getting the test event 05/16/23 14:32:06.621
    STEP: deleting the test event 05/16/23 14:32:06.625
    STEP: listing all events in all namespaces 05/16/23 14:32:06.633
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    May 16 14:32:06.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-5936" for this suite. 05/16/23 14:32:06.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:32:06.783
May 16 14:32:06.783: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename pods 05/16/23 14:32:06.783
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:32:06.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:32:06.802
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 05/16/23 14:32:06.84
STEP: watching for Pod to be ready 05/16/23 14:32:06.851
May 16 14:32:06.852: INFO: observed Pod pod-test in namespace pods-4873 in phase Pending with labels: map[test-pod-static:true] & conditions []
May 16 14:32:06.859: INFO: observed Pod pod-test in namespace pods-4873 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC  }]
May 16 14:32:06.868: INFO: observed Pod pod-test in namespace pods-4873 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC  }]
May 16 14:32:06.886: INFO: observed Pod pod-test in namespace pods-4873 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC  }]
May 16 14:32:07.352: INFO: observed Pod pod-test in namespace pods-4873 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC  }]
May 16 14:32:07.877: INFO: Found Pod pod-test in namespace pods-4873 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 05/16/23 14:32:07.88
STEP: getting the Pod and ensuring that it's patched 05/16/23 14:32:07.892
STEP: replacing the Pod's status Ready condition to False 05/16/23 14:32:07.896
STEP: check the Pod again to ensure its Ready conditions are False 05/16/23 14:32:07.905
STEP: deleting the Pod via a Collection with a LabelSelector 05/16/23 14:32:07.905
STEP: watching for the Pod to be deleted 05/16/23 14:32:07.913
May 16 14:32:07.914: INFO: observed event type MODIFIED
May 16 14:32:09.865: INFO: observed event type MODIFIED
May 16 14:32:10.867: INFO: observed event type MODIFIED
May 16 14:32:10.874: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 16 14:32:10.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4873" for this suite. 05/16/23 14:32:10.885
------------------------------
â€¢ [4.110 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:32:06.783
    May 16 14:32:06.783: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename pods 05/16/23 14:32:06.783
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:32:06.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:32:06.802
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 05/16/23 14:32:06.84
    STEP: watching for Pod to be ready 05/16/23 14:32:06.851
    May 16 14:32:06.852: INFO: observed Pod pod-test in namespace pods-4873 in phase Pending with labels: map[test-pod-static:true] & conditions []
    May 16 14:32:06.859: INFO: observed Pod pod-test in namespace pods-4873 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC  }]
    May 16 14:32:06.868: INFO: observed Pod pod-test in namespace pods-4873 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC  }]
    May 16 14:32:06.886: INFO: observed Pod pod-test in namespace pods-4873 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC  }]
    May 16 14:32:07.352: INFO: observed Pod pod-test in namespace pods-4873 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC  }]
    May 16 14:32:07.877: INFO: Found Pod pod-test in namespace pods-4873 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:32:06 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 05/16/23 14:32:07.88
    STEP: getting the Pod and ensuring that it's patched 05/16/23 14:32:07.892
    STEP: replacing the Pod's status Ready condition to False 05/16/23 14:32:07.896
    STEP: check the Pod again to ensure its Ready conditions are False 05/16/23 14:32:07.905
    STEP: deleting the Pod via a Collection with a LabelSelector 05/16/23 14:32:07.905
    STEP: watching for the Pod to be deleted 05/16/23 14:32:07.913
    May 16 14:32:07.914: INFO: observed event type MODIFIED
    May 16 14:32:09.865: INFO: observed event type MODIFIED
    May 16 14:32:10.867: INFO: observed event type MODIFIED
    May 16 14:32:10.874: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 16 14:32:10.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4873" for this suite. 05/16/23 14:32:10.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:32:10.893
May 16 14:32:10.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename container-runtime 05/16/23 14:32:10.893
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:32:10.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:32:10.915
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
W0516 14:32:10.934212      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "terminate-cmd-rpa" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "terminate-cmd-rpa" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "terminate-cmd-rpa" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "terminate-cmd-rpa" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 05/16/23 14:32:10.934
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 05/16/23 14:32:25.989
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 05/16/23 14:32:25.992
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 05/16/23 14:32:25.996
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 05/16/23 14:32:25.997
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 05/16/23 14:32:26.02
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 05/16/23 14:32:28.03
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 05/16/23 14:32:30.04
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 05/16/23 14:32:30.045
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 05/16/23 14:32:30.045
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 05/16/23 14:32:30.065
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 05/16/23 14:32:31.072
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 05/16/23 14:32:33.081
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 05/16/23 14:32:33.086
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 05/16/23 14:32:33.086
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May 16 14:32:33.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1074" for this suite. 05/16/23 14:32:33.108
------------------------------
â€¢ [SLOW TEST] [22.220 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:32:10.893
    May 16 14:32:10.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename container-runtime 05/16/23 14:32:10.893
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:32:10.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:32:10.915
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    W0516 14:32:10.934212      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "terminate-cmd-rpa" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "terminate-cmd-rpa" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "terminate-cmd-rpa" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "terminate-cmd-rpa" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 05/16/23 14:32:10.934
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 05/16/23 14:32:25.989
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 05/16/23 14:32:25.992
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 05/16/23 14:32:25.996
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 05/16/23 14:32:25.997
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 05/16/23 14:32:26.02
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 05/16/23 14:32:28.03
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 05/16/23 14:32:30.04
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 05/16/23 14:32:30.045
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 05/16/23 14:32:30.045
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 05/16/23 14:32:30.065
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 05/16/23 14:32:31.072
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 05/16/23 14:32:33.081
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 05/16/23 14:32:33.086
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 05/16/23 14:32:33.086
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May 16 14:32:33.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1074" for this suite. 05/16/23 14:32:33.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:32:33.113
May 16 14:32:33.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename resourcequota 05/16/23 14:32:33.114
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:32:33.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:32:33.131
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 05/16/23 14:32:33.133
STEP: Ensuring ResourceQuota status is calculated 05/16/23 14:32:33.138
STEP: Creating a ResourceQuota with not best effort scope 05/16/23 14:32:35.143
STEP: Ensuring ResourceQuota status is calculated 05/16/23 14:32:35.147
STEP: Creating a best-effort pod 05/16/23 14:32:37.15
STEP: Ensuring resource quota with best effort scope captures the pod usage 05/16/23 14:32:37.162
STEP: Ensuring resource quota with not best effort ignored the pod usage 05/16/23 14:32:39.166
STEP: Deleting the pod 05/16/23 14:32:41.17
STEP: Ensuring resource quota status released the pod usage 05/16/23 14:32:41.184
STEP: Creating a not best-effort pod 05/16/23 14:32:43.188
STEP: Ensuring resource quota with not best effort scope captures the pod usage 05/16/23 14:32:43.199
STEP: Ensuring resource quota with best effort scope ignored the pod usage 05/16/23 14:32:45.203
STEP: Deleting the pod 05/16/23 14:32:47.207
STEP: Ensuring resource quota status released the pod usage 05/16/23 14:32:47.22
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 16 14:32:49.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1523" for this suite. 05/16/23 14:32:49.227
------------------------------
â€¢ [SLOW TEST] [16.120 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:32:33.113
    May 16 14:32:33.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename resourcequota 05/16/23 14:32:33.114
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:32:33.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:32:33.131
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 05/16/23 14:32:33.133
    STEP: Ensuring ResourceQuota status is calculated 05/16/23 14:32:33.138
    STEP: Creating a ResourceQuota with not best effort scope 05/16/23 14:32:35.143
    STEP: Ensuring ResourceQuota status is calculated 05/16/23 14:32:35.147
    STEP: Creating a best-effort pod 05/16/23 14:32:37.15
    STEP: Ensuring resource quota with best effort scope captures the pod usage 05/16/23 14:32:37.162
    STEP: Ensuring resource quota with not best effort ignored the pod usage 05/16/23 14:32:39.166
    STEP: Deleting the pod 05/16/23 14:32:41.17
    STEP: Ensuring resource quota status released the pod usage 05/16/23 14:32:41.184
    STEP: Creating a not best-effort pod 05/16/23 14:32:43.188
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 05/16/23 14:32:43.199
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 05/16/23 14:32:45.203
    STEP: Deleting the pod 05/16/23 14:32:47.207
    STEP: Ensuring resource quota status released the pod usage 05/16/23 14:32:47.22
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 16 14:32:49.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1523" for this suite. 05/16/23 14:32:49.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:32:49.233
May 16 14:32:49.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename endpointslicemirroring 05/16/23 14:32:49.234
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:32:49.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:32:49.253
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 05/16/23 14:32:49.271
May 16 14:32:49.282: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 05/16/23 14:32:51.285
May 16 14:32:51.292: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 05/16/23 14:32:53.295
May 16 14:32:53.303: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
May 16 14:32:55.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-7969" for this suite. 05/16/23 14:32:55.31
------------------------------
â€¢ [SLOW TEST] [6.083 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:32:49.233
    May 16 14:32:49.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename endpointslicemirroring 05/16/23 14:32:49.234
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:32:49.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:32:49.253
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 05/16/23 14:32:49.271
    May 16 14:32:49.282: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 05/16/23 14:32:51.285
    May 16 14:32:51.292: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 05/16/23 14:32:53.295
    May 16 14:32:53.303: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    May 16 14:32:55.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-7969" for this suite. 05/16/23 14:32:55.31
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:32:55.316
May 16 14:32:55.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename job 05/16/23 14:32:55.317
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:32:55.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:32:55.334
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 05/16/23 14:32:55.336
W0516 14:32:56.344115      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions 05/16/23 14:32:56.344
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May 16 14:33:06.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7352" for this suite. 05/16/23 14:33:06.351
------------------------------
â€¢ [SLOW TEST] [11.039 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:32:55.316
    May 16 14:32:55.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename job 05/16/23 14:32:55.317
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:32:55.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:32:55.334
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 05/16/23 14:32:55.336
    W0516 14:32:56.344115      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring job reaches completions 05/16/23 14:32:56.344
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May 16 14:33:06.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7352" for this suite. 05/16/23 14:33:06.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:33:06.356
May 16 14:33:06.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename gc 05/16/23 14:33:06.357
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:33:06.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:33:06.373
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
May 16 14:33:07.414: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"b25f6a2d-8cac-4276-b76e-666f45172b6c", Controller:(*bool)(0xc004d9dea2), BlockOwnerDeletion:(*bool)(0xc004d9dea3)}}
May 16 14:33:07.433: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"59b33e41-340a-4c83-bfdf-cb2493220fb0", Controller:(*bool)(0xc0032b4ae6), BlockOwnerDeletion:(*bool)(0xc0032b4ae7)}}
May 16 14:33:07.444: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c63b6301-e7a5-4ccc-9f3d-211feedf5d6b", Controller:(*bool)(0xc0032b5486), BlockOwnerDeletion:(*bool)(0xc0032b5487)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May 16 14:33:12.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1516" for this suite. 05/16/23 14:33:12.46
------------------------------
â€¢ [SLOW TEST] [6.112 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:33:06.356
    May 16 14:33:06.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename gc 05/16/23 14:33:06.357
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:33:06.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:33:06.373
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    May 16 14:33:07.414: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"b25f6a2d-8cac-4276-b76e-666f45172b6c", Controller:(*bool)(0xc004d9dea2), BlockOwnerDeletion:(*bool)(0xc004d9dea3)}}
    May 16 14:33:07.433: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"59b33e41-340a-4c83-bfdf-cb2493220fb0", Controller:(*bool)(0xc0032b4ae6), BlockOwnerDeletion:(*bool)(0xc0032b4ae7)}}
    May 16 14:33:07.444: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c63b6301-e7a5-4ccc-9f3d-211feedf5d6b", Controller:(*bool)(0xc0032b5486), BlockOwnerDeletion:(*bool)(0xc0032b5487)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May 16 14:33:12.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1516" for this suite. 05/16/23 14:33:12.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:33:12.468
May 16 14:33:12.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename resourcequota 05/16/23 14:33:12.469
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:33:12.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:33:12.49
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 05/16/23 14:33:29.495
STEP: Creating a ResourceQuota 05/16/23 14:33:34.498
STEP: Ensuring resource quota status is calculated 05/16/23 14:33:34.503
STEP: Creating a ConfigMap 05/16/23 14:33:36.506
STEP: Ensuring resource quota status captures configMap creation 05/16/23 14:33:36.515
STEP: Deleting a ConfigMap 05/16/23 14:33:38.519
STEP: Ensuring resource quota status released usage 05/16/23 14:33:38.525
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 16 14:33:40.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-283" for this suite. 05/16/23 14:33:40.533
------------------------------
â€¢ [SLOW TEST] [28.071 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:33:12.468
    May 16 14:33:12.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename resourcequota 05/16/23 14:33:12.469
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:33:12.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:33:12.49
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 05/16/23 14:33:29.495
    STEP: Creating a ResourceQuota 05/16/23 14:33:34.498
    STEP: Ensuring resource quota status is calculated 05/16/23 14:33:34.503
    STEP: Creating a ConfigMap 05/16/23 14:33:36.506
    STEP: Ensuring resource quota status captures configMap creation 05/16/23 14:33:36.515
    STEP: Deleting a ConfigMap 05/16/23 14:33:38.519
    STEP: Ensuring resource quota status released usage 05/16/23 14:33:38.525
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 16 14:33:40.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-283" for this suite. 05/16/23 14:33:40.533
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:33:40.539
May 16 14:33:40.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename job 05/16/23 14:33:40.54
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:33:40.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:33:40.558
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 05/16/23 14:33:40.559
W0516 14:33:41.565859      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism 05/16/23 14:33:41.565
STEP: Orphaning one of the Job's Pods 05/16/23 14:33:43.569
May 16 14:33:44.082: INFO: Successfully updated pod "adopt-release-ff8p8"
STEP: Checking that the Job readopts the Pod 05/16/23 14:33:44.082
May 16 14:33:44.082: INFO: Waiting up to 15m0s for pod "adopt-release-ff8p8" in namespace "job-5862" to be "adopted"
May 16 14:33:44.085: INFO: Pod "adopt-release-ff8p8": Phase="Running", Reason="", readiness=true. Elapsed: 2.814172ms
May 16 14:33:46.088: INFO: Pod "adopt-release-ff8p8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005806667s
May 16 14:33:46.088: INFO: Pod "adopt-release-ff8p8" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 05/16/23 14:33:46.088
May 16 14:33:46.599: INFO: Successfully updated pod "adopt-release-ff8p8"
STEP: Checking that the Job releases the Pod 05/16/23 14:33:46.599
May 16 14:33:46.599: INFO: Waiting up to 15m0s for pod "adopt-release-ff8p8" in namespace "job-5862" to be "released"
May 16 14:33:46.601: INFO: Pod "adopt-release-ff8p8": Phase="Running", Reason="", readiness=true. Elapsed: 2.704512ms
May 16 14:33:48.604: INFO: Pod "adopt-release-ff8p8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005615935s
May 16 14:33:48.604: INFO: Pod "adopt-release-ff8p8" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May 16 14:33:48.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5862" for this suite. 05/16/23 14:33:48.608
------------------------------
â€¢ [SLOW TEST] [8.076 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:33:40.539
    May 16 14:33:40.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename job 05/16/23 14:33:40.54
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:33:40.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:33:40.558
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 05/16/23 14:33:40.559
    W0516 14:33:41.565859      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring active pods == parallelism 05/16/23 14:33:41.565
    STEP: Orphaning one of the Job's Pods 05/16/23 14:33:43.569
    May 16 14:33:44.082: INFO: Successfully updated pod "adopt-release-ff8p8"
    STEP: Checking that the Job readopts the Pod 05/16/23 14:33:44.082
    May 16 14:33:44.082: INFO: Waiting up to 15m0s for pod "adopt-release-ff8p8" in namespace "job-5862" to be "adopted"
    May 16 14:33:44.085: INFO: Pod "adopt-release-ff8p8": Phase="Running", Reason="", readiness=true. Elapsed: 2.814172ms
    May 16 14:33:46.088: INFO: Pod "adopt-release-ff8p8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005806667s
    May 16 14:33:46.088: INFO: Pod "adopt-release-ff8p8" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 05/16/23 14:33:46.088
    May 16 14:33:46.599: INFO: Successfully updated pod "adopt-release-ff8p8"
    STEP: Checking that the Job releases the Pod 05/16/23 14:33:46.599
    May 16 14:33:46.599: INFO: Waiting up to 15m0s for pod "adopt-release-ff8p8" in namespace "job-5862" to be "released"
    May 16 14:33:46.601: INFO: Pod "adopt-release-ff8p8": Phase="Running", Reason="", readiness=true. Elapsed: 2.704512ms
    May 16 14:33:48.604: INFO: Pod "adopt-release-ff8p8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005615935s
    May 16 14:33:48.604: INFO: Pod "adopt-release-ff8p8" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May 16 14:33:48.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5862" for this suite. 05/16/23 14:33:48.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:33:48.616
May 16 14:33:48.616: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename configmap 05/16/23 14:33:48.616
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:33:48.633
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:33:48.635
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
May 16 14:33:48.644: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-1c092411-64d0-4977-96a4-1f600a5eb388 05/16/23 14:33:48.644
STEP: Creating the pod 05/16/23 14:33:48.656
May 16 14:33:48.672: INFO: Waiting up to 5m0s for pod "pod-configmaps-4e617264-4486-4e3e-adb8-a3ee76aadf76" in namespace "configmap-5062" to be "running"
May 16 14:33:48.678: INFO: Pod "pod-configmaps-4e617264-4486-4e3e-adb8-a3ee76aadf76": Phase="Pending", Reason="", readiness=false. Elapsed: 5.947128ms
May 16 14:33:50.682: INFO: Pod "pod-configmaps-4e617264-4486-4e3e-adb8-a3ee76aadf76": Phase="Running", Reason="", readiness=false. Elapsed: 2.010053516s
May 16 14:33:50.682: INFO: Pod "pod-configmaps-4e617264-4486-4e3e-adb8-a3ee76aadf76" satisfied condition "running"
STEP: Waiting for pod with text data 05/16/23 14:33:50.682
STEP: Waiting for pod with binary data 05/16/23 14:33:50.694
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 16 14:33:50.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5062" for this suite. 05/16/23 14:33:50.703
------------------------------
â€¢ [2.093 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:33:48.616
    May 16 14:33:48.616: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename configmap 05/16/23 14:33:48.616
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:33:48.633
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:33:48.635
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    May 16 14:33:48.644: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name configmap-test-upd-1c092411-64d0-4977-96a4-1f600a5eb388 05/16/23 14:33:48.644
    STEP: Creating the pod 05/16/23 14:33:48.656
    May 16 14:33:48.672: INFO: Waiting up to 5m0s for pod "pod-configmaps-4e617264-4486-4e3e-adb8-a3ee76aadf76" in namespace "configmap-5062" to be "running"
    May 16 14:33:48.678: INFO: Pod "pod-configmaps-4e617264-4486-4e3e-adb8-a3ee76aadf76": Phase="Pending", Reason="", readiness=false. Elapsed: 5.947128ms
    May 16 14:33:50.682: INFO: Pod "pod-configmaps-4e617264-4486-4e3e-adb8-a3ee76aadf76": Phase="Running", Reason="", readiness=false. Elapsed: 2.010053516s
    May 16 14:33:50.682: INFO: Pod "pod-configmaps-4e617264-4486-4e3e-adb8-a3ee76aadf76" satisfied condition "running"
    STEP: Waiting for pod with text data 05/16/23 14:33:50.682
    STEP: Waiting for pod with binary data 05/16/23 14:33:50.694
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 16 14:33:50.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5062" for this suite. 05/16/23 14:33:50.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:33:50.709
May 16 14:33:50.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename container-probe 05/16/23 14:33:50.71
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:33:50.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:33:50.724
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-c5420cce-4b37-46d1-8b90-4322fea4b2a6 in namespace container-probe-7220 05/16/23 14:33:50.727
May 16 14:33:51.746: INFO: Waiting up to 5m0s for pod "busybox-c5420cce-4b37-46d1-8b90-4322fea4b2a6" in namespace "container-probe-7220" to be "not pending"
May 16 14:33:51.749: INFO: Pod "busybox-c5420cce-4b37-46d1-8b90-4322fea4b2a6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.017899ms
May 16 14:33:53.754: INFO: Pod "busybox-c5420cce-4b37-46d1-8b90-4322fea4b2a6": Phase="Running", Reason="", readiness=true. Elapsed: 2.007326056s
May 16 14:33:53.754: INFO: Pod "busybox-c5420cce-4b37-46d1-8b90-4322fea4b2a6" satisfied condition "not pending"
May 16 14:33:53.754: INFO: Started pod busybox-c5420cce-4b37-46d1-8b90-4322fea4b2a6 in namespace container-probe-7220
STEP: checking the pod's current state and verifying that restartCount is present 05/16/23 14:33:53.754
May 16 14:33:53.756: INFO: Initial restart count of pod busybox-c5420cce-4b37-46d1-8b90-4322fea4b2a6 is 0
May 16 14:34:43.859: INFO: Restart count of pod container-probe-7220/busybox-c5420cce-4b37-46d1-8b90-4322fea4b2a6 is now 1 (50.102404898s elapsed)
STEP: deleting the pod 05/16/23 14:34:43.859
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May 16 14:34:43.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7220" for this suite. 05/16/23 14:34:43.874
------------------------------
â€¢ [SLOW TEST] [53.170 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:33:50.709
    May 16 14:33:50.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename container-probe 05/16/23 14:33:50.71
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:33:50.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:33:50.724
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-c5420cce-4b37-46d1-8b90-4322fea4b2a6 in namespace container-probe-7220 05/16/23 14:33:50.727
    May 16 14:33:51.746: INFO: Waiting up to 5m0s for pod "busybox-c5420cce-4b37-46d1-8b90-4322fea4b2a6" in namespace "container-probe-7220" to be "not pending"
    May 16 14:33:51.749: INFO: Pod "busybox-c5420cce-4b37-46d1-8b90-4322fea4b2a6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.017899ms
    May 16 14:33:53.754: INFO: Pod "busybox-c5420cce-4b37-46d1-8b90-4322fea4b2a6": Phase="Running", Reason="", readiness=true. Elapsed: 2.007326056s
    May 16 14:33:53.754: INFO: Pod "busybox-c5420cce-4b37-46d1-8b90-4322fea4b2a6" satisfied condition "not pending"
    May 16 14:33:53.754: INFO: Started pod busybox-c5420cce-4b37-46d1-8b90-4322fea4b2a6 in namespace container-probe-7220
    STEP: checking the pod's current state and verifying that restartCount is present 05/16/23 14:33:53.754
    May 16 14:33:53.756: INFO: Initial restart count of pod busybox-c5420cce-4b37-46d1-8b90-4322fea4b2a6 is 0
    May 16 14:34:43.859: INFO: Restart count of pod container-probe-7220/busybox-c5420cce-4b37-46d1-8b90-4322fea4b2a6 is now 1 (50.102404898s elapsed)
    STEP: deleting the pod 05/16/23 14:34:43.859
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May 16 14:34:43.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7220" for this suite. 05/16/23 14:34:43.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:34:43.879
May 16 14:34:43.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubelet-test 05/16/23 14:34:43.88
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:34:43.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:34:43.904
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
W0516 14:34:43.919478      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "bin-falseafa02f5d-6b6b-48d7-a4e9-76223afb8a5d" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "bin-falseafa02f5d-6b6b-48d7-a4e9-76223afb8a5d" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "bin-falseafa02f5d-6b6b-48d7-a4e9-76223afb8a5d" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "bin-falseafa02f5d-6b6b-48d7-a4e9-76223afb8a5d" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May 16 14:34:43.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2979" for this suite. 05/16/23 14:34:43.946
------------------------------
â€¢ [0.077 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:34:43.879
    May 16 14:34:43.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubelet-test 05/16/23 14:34:43.88
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:34:43.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:34:43.904
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    W0516 14:34:43.919478      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "bin-falseafa02f5d-6b6b-48d7-a4e9-76223afb8a5d" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "bin-falseafa02f5d-6b6b-48d7-a4e9-76223afb8a5d" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "bin-falseafa02f5d-6b6b-48d7-a4e9-76223afb8a5d" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "bin-falseafa02f5d-6b6b-48d7-a4e9-76223afb8a5d" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May 16 14:34:43.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2979" for this suite. 05/16/23 14:34:43.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:34:43.957
May 16 14:34:43.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename container-probe 05/16/23 14:34:43.958
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:34:43.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:34:43.974
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61 in namespace container-probe-3537 05/16/23 14:34:43.975
W0516 14:34:43.988811      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:34:43.988: INFO: Waiting up to 5m0s for pod "liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61" in namespace "container-probe-3537" to be "not pending"
May 16 14:34:43.994: INFO: Pod "liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61": Phase="Pending", Reason="", readiness=false. Elapsed: 5.667163ms
May 16 14:34:45.998: INFO: Pod "liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61": Phase="Running", Reason="", readiness=true. Elapsed: 2.009467075s
May 16 14:34:45.998: INFO: Pod "liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61" satisfied condition "not pending"
May 16 14:34:45.998: INFO: Started pod liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61 in namespace container-probe-3537
STEP: checking the pod's current state and verifying that restartCount is present 05/16/23 14:34:45.998
May 16 14:34:46.001: INFO: Initial restart count of pod liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61 is 0
May 16 14:35:06.042: INFO: Restart count of pod container-probe-3537/liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61 is now 1 (20.041011059s elapsed)
May 16 14:35:26.088: INFO: Restart count of pod container-probe-3537/liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61 is now 2 (40.086468846s elapsed)
May 16 14:35:46.131: INFO: Restart count of pod container-probe-3537/liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61 is now 3 (1m0.129399721s elapsed)
May 16 14:36:06.184: INFO: Restart count of pod container-probe-3537/liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61 is now 4 (1m20.182657594s elapsed)
May 16 14:37:20.324: INFO: Restart count of pod container-probe-3537/liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61 is now 5 (2m34.322862429s elapsed)
STEP: deleting the pod 05/16/23 14:37:20.324
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May 16 14:37:20.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3537" for this suite. 05/16/23 14:37:20.338
------------------------------
â€¢ [SLOW TEST] [156.387 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:34:43.957
    May 16 14:34:43.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename container-probe 05/16/23 14:34:43.958
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:34:43.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:34:43.974
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61 in namespace container-probe-3537 05/16/23 14:34:43.975
    W0516 14:34:43.988811      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:34:43.988: INFO: Waiting up to 5m0s for pod "liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61" in namespace "container-probe-3537" to be "not pending"
    May 16 14:34:43.994: INFO: Pod "liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61": Phase="Pending", Reason="", readiness=false. Elapsed: 5.667163ms
    May 16 14:34:45.998: INFO: Pod "liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61": Phase="Running", Reason="", readiness=true. Elapsed: 2.009467075s
    May 16 14:34:45.998: INFO: Pod "liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61" satisfied condition "not pending"
    May 16 14:34:45.998: INFO: Started pod liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61 in namespace container-probe-3537
    STEP: checking the pod's current state and verifying that restartCount is present 05/16/23 14:34:45.998
    May 16 14:34:46.001: INFO: Initial restart count of pod liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61 is 0
    May 16 14:35:06.042: INFO: Restart count of pod container-probe-3537/liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61 is now 1 (20.041011059s elapsed)
    May 16 14:35:26.088: INFO: Restart count of pod container-probe-3537/liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61 is now 2 (40.086468846s elapsed)
    May 16 14:35:46.131: INFO: Restart count of pod container-probe-3537/liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61 is now 3 (1m0.129399721s elapsed)
    May 16 14:36:06.184: INFO: Restart count of pod container-probe-3537/liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61 is now 4 (1m20.182657594s elapsed)
    May 16 14:37:20.324: INFO: Restart count of pod container-probe-3537/liveness-16e49fef-0cb7-4dfd-81a9-f681c18c0f61 is now 5 (2m34.322862429s elapsed)
    STEP: deleting the pod 05/16/23 14:37:20.324
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May 16 14:37:20.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3537" for this suite. 05/16/23 14:37:20.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:37:20.346
May 16 14:37:20.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename crd-publish-openapi 05/16/23 14:37:20.346
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:37:20.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:37:20.369
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 05/16/23 14:37:20.371
May 16 14:37:20.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 14:37:23.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 14:37:35.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1622" for this suite. 05/16/23 14:37:35.204
------------------------------
â€¢ [SLOW TEST] [14.865 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:37:20.346
    May 16 14:37:20.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename crd-publish-openapi 05/16/23 14:37:20.346
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:37:20.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:37:20.369
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 05/16/23 14:37:20.371
    May 16 14:37:20.371: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 14:37:23.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 14:37:35.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1622" for this suite. 05/16/23 14:37:35.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:37:35.214
May 16 14:37:35.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename downward-api 05/16/23 14:37:35.215
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:37:35.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:37:35.236
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 05/16/23 14:37:35.241
May 16 14:37:35.264: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a81c41e7-4cb0-4eae-ab08-88997bd58acd" in namespace "downward-api-8268" to be "Succeeded or Failed"
May 16 14:37:35.268: INFO: Pod "downwardapi-volume-a81c41e7-4cb0-4eae-ab08-88997bd58acd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.416963ms
May 16 14:37:37.272: INFO: Pod "downwardapi-volume-a81c41e7-4cb0-4eae-ab08-88997bd58acd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007361749s
May 16 14:37:39.272: INFO: Pod "downwardapi-volume-a81c41e7-4cb0-4eae-ab08-88997bd58acd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008179495s
STEP: Saw pod success 05/16/23 14:37:39.272
May 16 14:37:39.273: INFO: Pod "downwardapi-volume-a81c41e7-4cb0-4eae-ab08-88997bd58acd" satisfied condition "Succeeded or Failed"
May 16 14:37:39.288: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-a81c41e7-4cb0-4eae-ab08-88997bd58acd container client-container: <nil>
STEP: delete the pod 05/16/23 14:37:39.299
May 16 14:37:39.309: INFO: Waiting for pod downwardapi-volume-a81c41e7-4cb0-4eae-ab08-88997bd58acd to disappear
May 16 14:37:39.311: INFO: Pod downwardapi-volume-a81c41e7-4cb0-4eae-ab08-88997bd58acd no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 16 14:37:39.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8268" for this suite. 05/16/23 14:37:39.316
------------------------------
â€¢ [4.107 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:37:35.214
    May 16 14:37:35.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename downward-api 05/16/23 14:37:35.215
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:37:35.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:37:35.236
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 05/16/23 14:37:35.241
    May 16 14:37:35.264: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a81c41e7-4cb0-4eae-ab08-88997bd58acd" in namespace "downward-api-8268" to be "Succeeded or Failed"
    May 16 14:37:35.268: INFO: Pod "downwardapi-volume-a81c41e7-4cb0-4eae-ab08-88997bd58acd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.416963ms
    May 16 14:37:37.272: INFO: Pod "downwardapi-volume-a81c41e7-4cb0-4eae-ab08-88997bd58acd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007361749s
    May 16 14:37:39.272: INFO: Pod "downwardapi-volume-a81c41e7-4cb0-4eae-ab08-88997bd58acd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008179495s
    STEP: Saw pod success 05/16/23 14:37:39.272
    May 16 14:37:39.273: INFO: Pod "downwardapi-volume-a81c41e7-4cb0-4eae-ab08-88997bd58acd" satisfied condition "Succeeded or Failed"
    May 16 14:37:39.288: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-a81c41e7-4cb0-4eae-ab08-88997bd58acd container client-container: <nil>
    STEP: delete the pod 05/16/23 14:37:39.299
    May 16 14:37:39.309: INFO: Waiting for pod downwardapi-volume-a81c41e7-4cb0-4eae-ab08-88997bd58acd to disappear
    May 16 14:37:39.311: INFO: Pod downwardapi-volume-a81c41e7-4cb0-4eae-ab08-88997bd58acd no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 16 14:37:39.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8268" for this suite. 05/16/23 14:37:39.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:37:39.322
May 16 14:37:39.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename sched-preemption 05/16/23 14:37:39.323
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:37:39.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:37:39.344
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
May 16 14:37:39.372: INFO: Waiting up to 1m0s for all nodes to be ready
May 16 14:38:39.500: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 05/16/23 14:38:39.505
May 16 14:38:39.545: INFO: Created pod: pod0-0-sched-preemption-low-priority
May 16 14:38:39.557: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May 16 14:38:39.599: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May 16 14:38:39.610: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May 16 14:38:39.658: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May 16 14:38:39.684: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 05/16/23 14:38:39.684
May 16 14:38:39.684: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4844" to be "running"
May 16 14:38:39.690: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.0557ms
May 16 14:38:41.694: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009214012s
May 16 14:38:41.694: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
May 16 14:38:41.694: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4844" to be "running"
May 16 14:38:41.697: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.949161ms
May 16 14:38:41.697: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
May 16 14:38:41.697: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4844" to be "running"
May 16 14:38:41.700: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.102115ms
May 16 14:38:41.700: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
May 16 14:38:41.700: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4844" to be "running"
May 16 14:38:41.702: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.549759ms
May 16 14:38:41.702: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
May 16 14:38:41.702: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4844" to be "running"
May 16 14:38:41.705: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.42945ms
May 16 14:38:41.705: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
May 16 14:38:41.705: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4844" to be "running"
May 16 14:38:41.707: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.58029ms
May 16 14:38:41.707: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 05/16/23 14:38:41.707
May 16 14:38:41.716: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-4844" to be "running"
May 16 14:38:41.719: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.603737ms
May 16 14:38:43.723: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006553611s
May 16 14:38:45.724: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007901016s
May 16 14:38:45.724: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 14:38:45.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-4844" for this suite. 05/16/23 14:38:45.783
------------------------------
â€¢ [SLOW TEST] [66.468 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:37:39.322
    May 16 14:37:39.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename sched-preemption 05/16/23 14:37:39.323
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:37:39.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:37:39.344
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    May 16 14:37:39.372: INFO: Waiting up to 1m0s for all nodes to be ready
    May 16 14:38:39.500: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 05/16/23 14:38:39.505
    May 16 14:38:39.545: INFO: Created pod: pod0-0-sched-preemption-low-priority
    May 16 14:38:39.557: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    May 16 14:38:39.599: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    May 16 14:38:39.610: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    May 16 14:38:39.658: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    May 16 14:38:39.684: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 05/16/23 14:38:39.684
    May 16 14:38:39.684: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4844" to be "running"
    May 16 14:38:39.690: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.0557ms
    May 16 14:38:41.694: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009214012s
    May 16 14:38:41.694: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    May 16 14:38:41.694: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4844" to be "running"
    May 16 14:38:41.697: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.949161ms
    May 16 14:38:41.697: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    May 16 14:38:41.697: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4844" to be "running"
    May 16 14:38:41.700: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.102115ms
    May 16 14:38:41.700: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    May 16 14:38:41.700: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4844" to be "running"
    May 16 14:38:41.702: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.549759ms
    May 16 14:38:41.702: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    May 16 14:38:41.702: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4844" to be "running"
    May 16 14:38:41.705: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.42945ms
    May 16 14:38:41.705: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    May 16 14:38:41.705: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4844" to be "running"
    May 16 14:38:41.707: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.58029ms
    May 16 14:38:41.707: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 05/16/23 14:38:41.707
    May 16 14:38:41.716: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-4844" to be "running"
    May 16 14:38:41.719: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.603737ms
    May 16 14:38:43.723: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006553611s
    May 16 14:38:45.724: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007901016s
    May 16 14:38:45.724: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 14:38:45.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-4844" for this suite. 05/16/23 14:38:45.783
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:38:45.79
May 16 14:38:45.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename endpointslice 05/16/23 14:38:45.791
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:38:45.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:38:45.816
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 05/16/23 14:38:45.82
STEP: getting /apis/discovery.k8s.io 05/16/23 14:38:45.823
STEP: getting /apis/discovery.k8s.iov1 05/16/23 14:38:45.824
STEP: creating 05/16/23 14:38:45.825
STEP: getting 05/16/23 14:38:45.84
STEP: listing 05/16/23 14:38:45.844
STEP: watching 05/16/23 14:38:45.85
May 16 14:38:45.850: INFO: starting watch
STEP: cluster-wide listing 05/16/23 14:38:45.852
STEP: cluster-wide watching 05/16/23 14:38:45.859
May 16 14:38:45.859: INFO: starting watch
STEP: patching 05/16/23 14:38:45.86
STEP: updating 05/16/23 14:38:45.865
May 16 14:38:45.873: INFO: waiting for watch events with expected annotations
May 16 14:38:45.873: INFO: saw patched and updated annotations
STEP: deleting 05/16/23 14:38:45.873
STEP: deleting a collection 05/16/23 14:38:45.886
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
May 16 14:38:45.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-9883" for this suite. 05/16/23 14:38:45.904
------------------------------
â€¢ [0.119 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:38:45.79
    May 16 14:38:45.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename endpointslice 05/16/23 14:38:45.791
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:38:45.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:38:45.816
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 05/16/23 14:38:45.82
    STEP: getting /apis/discovery.k8s.io 05/16/23 14:38:45.823
    STEP: getting /apis/discovery.k8s.iov1 05/16/23 14:38:45.824
    STEP: creating 05/16/23 14:38:45.825
    STEP: getting 05/16/23 14:38:45.84
    STEP: listing 05/16/23 14:38:45.844
    STEP: watching 05/16/23 14:38:45.85
    May 16 14:38:45.850: INFO: starting watch
    STEP: cluster-wide listing 05/16/23 14:38:45.852
    STEP: cluster-wide watching 05/16/23 14:38:45.859
    May 16 14:38:45.859: INFO: starting watch
    STEP: patching 05/16/23 14:38:45.86
    STEP: updating 05/16/23 14:38:45.865
    May 16 14:38:45.873: INFO: waiting for watch events with expected annotations
    May 16 14:38:45.873: INFO: saw patched and updated annotations
    STEP: deleting 05/16/23 14:38:45.873
    STEP: deleting a collection 05/16/23 14:38:45.886
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    May 16 14:38:45.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-9883" for this suite. 05/16/23 14:38:45.904
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:38:45.909
May 16 14:38:45.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename events 05/16/23 14:38:45.909
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:38:45.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:38:45.931
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 05/16/23 14:38:45.935
STEP: listing events in all namespaces 05/16/23 14:38:45.953
STEP: listing events in test namespace 05/16/23 14:38:46.056
STEP: listing events with field selection filtering on source 05/16/23 14:38:46.059
STEP: listing events with field selection filtering on reportingController 05/16/23 14:38:46.062
STEP: getting the test event 05/16/23 14:38:46.064
STEP: patching the test event 05/16/23 14:38:46.066
STEP: getting the test event 05/16/23 14:38:46.075
STEP: updating the test event 05/16/23 14:38:46.077
STEP: getting the test event 05/16/23 14:38:46.084
STEP: deleting the test event 05/16/23 14:38:46.088
STEP: listing events in all namespaces 05/16/23 14:38:46.097
STEP: listing events in test namespace 05/16/23 14:38:46.156
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
May 16 14:38:46.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-5358" for this suite. 05/16/23 14:38:46.163
------------------------------
â€¢ [0.259 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:38:45.909
    May 16 14:38:45.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename events 05/16/23 14:38:45.909
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:38:45.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:38:45.931
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 05/16/23 14:38:45.935
    STEP: listing events in all namespaces 05/16/23 14:38:45.953
    STEP: listing events in test namespace 05/16/23 14:38:46.056
    STEP: listing events with field selection filtering on source 05/16/23 14:38:46.059
    STEP: listing events with field selection filtering on reportingController 05/16/23 14:38:46.062
    STEP: getting the test event 05/16/23 14:38:46.064
    STEP: patching the test event 05/16/23 14:38:46.066
    STEP: getting the test event 05/16/23 14:38:46.075
    STEP: updating the test event 05/16/23 14:38:46.077
    STEP: getting the test event 05/16/23 14:38:46.084
    STEP: deleting the test event 05/16/23 14:38:46.088
    STEP: listing events in all namespaces 05/16/23 14:38:46.097
    STEP: listing events in test namespace 05/16/23 14:38:46.156
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    May 16 14:38:46.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-5358" for this suite. 05/16/23 14:38:46.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:38:46.168
May 16 14:38:46.168: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename services 05/16/23 14:38:46.169
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:38:46.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:38:46.2
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-519 05/16/23 14:38:46.203
STEP: changing the ExternalName service to type=ClusterIP 05/16/23 14:38:46.21
STEP: creating replication controller externalname-service in namespace services-519 05/16/23 14:38:46.242
I0516 14:38:46.248526      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-519, replica count: 2
I0516 14:38:49.299937      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 16 14:38:49.299: INFO: Creating new exec pod
May 16 14:38:49.311: INFO: Waiting up to 5m0s for pod "execpodfxsvb" in namespace "services-519" to be "running"
May 16 14:38:49.314: INFO: Pod "execpodfxsvb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.106602ms
May 16 14:38:51.318: INFO: Pod "execpodfxsvb": Phase="Running", Reason="", readiness=true. Elapsed: 2.006940898s
May 16 14:38:51.318: INFO: Pod "execpodfxsvb" satisfied condition "running"
May 16 14:38:52.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-519 exec execpodfxsvb -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
May 16 14:38:52.416: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 16 14:38:52.416: INFO: stdout: ""
May 16 14:38:52.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-519 exec execpodfxsvb -- /bin/sh -x -c nc -v -z -w 2 172.30.50.219 80'
May 16 14:38:52.512: INFO: stderr: "+ nc -v -z -w 2 172.30.50.219 80\nConnection to 172.30.50.219 80 port [tcp/http] succeeded!\n"
May 16 14:38:52.512: INFO: stdout: ""
May 16 14:38:52.512: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 16 14:38:52.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-519" for this suite. 05/16/23 14:38:52.568
------------------------------
â€¢ [SLOW TEST] [6.408 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:38:46.168
    May 16 14:38:46.168: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename services 05/16/23 14:38:46.169
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:38:46.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:38:46.2
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-519 05/16/23 14:38:46.203
    STEP: changing the ExternalName service to type=ClusterIP 05/16/23 14:38:46.21
    STEP: creating replication controller externalname-service in namespace services-519 05/16/23 14:38:46.242
    I0516 14:38:46.248526      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-519, replica count: 2
    I0516 14:38:49.299937      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 16 14:38:49.299: INFO: Creating new exec pod
    May 16 14:38:49.311: INFO: Waiting up to 5m0s for pod "execpodfxsvb" in namespace "services-519" to be "running"
    May 16 14:38:49.314: INFO: Pod "execpodfxsvb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.106602ms
    May 16 14:38:51.318: INFO: Pod "execpodfxsvb": Phase="Running", Reason="", readiness=true. Elapsed: 2.006940898s
    May 16 14:38:51.318: INFO: Pod "execpodfxsvb" satisfied condition "running"
    May 16 14:38:52.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-519 exec execpodfxsvb -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    May 16 14:38:52.416: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    May 16 14:38:52.416: INFO: stdout: ""
    May 16 14:38:52.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-519 exec execpodfxsvb -- /bin/sh -x -c nc -v -z -w 2 172.30.50.219 80'
    May 16 14:38:52.512: INFO: stderr: "+ nc -v -z -w 2 172.30.50.219 80\nConnection to 172.30.50.219 80 port [tcp/http] succeeded!\n"
    May 16 14:38:52.512: INFO: stdout: ""
    May 16 14:38:52.512: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 16 14:38:52.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-519" for this suite. 05/16/23 14:38:52.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:38:52.577
May 16 14:38:52.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename deployment 05/16/23 14:38:52.577
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:38:52.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:38:52.605
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
May 16 14:38:52.608: INFO: Creating deployment "webserver-deployment"
W0516 14:38:52.649619      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:38:52.649: INFO: Waiting for observed generation 1
May 16 14:38:54.660: INFO: Waiting for all required pods to come up
May 16 14:38:54.663: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 05/16/23 14:38:54.663
May 16 14:38:54.663: INFO: Waiting for deployment "webserver-deployment" to complete
May 16 14:38:54.669: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 16 14:38:54.689: INFO: Updating deployment webserver-deployment
May 16 14:38:54.689: INFO: Waiting for observed generation 2
May 16 14:38:56.696: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 16 14:38:56.698: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 16 14:38:56.701: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 16 14:38:56.709: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 16 14:38:56.709: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 16 14:38:56.711: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 16 14:38:56.717: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 16 14:38:56.717: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 16 14:38:56.724: INFO: Updating deployment webserver-deployment
May 16 14:38:56.725: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 16 14:38:56.731: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 16 14:38:56.734: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 16 14:38:56.746: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2026  3d075497-ca9b-42c5-b8ad-f4fc8c06c9e6 57478 3 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-16 14:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 14:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002bc2598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-05-16 14:38:54 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-16 14:38:56 +0000 UTC,LastTransitionTime:2023-05-16 14:38:56 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 16 14:38:56.752: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-2026  736c9a78-2d1a-42fb-a912-9ce4505bc489 57475 3 2023-05-16 14:38:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 3d075497-ca9b-42c5-b8ad-f4fc8c06c9e6 0xc006284b97 0xc006284b98}] [] [{kube-controller-manager Update apps/v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-16 14:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d075497-ca9b-42c5-b8ad-f4fc8c06c9e6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006284c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 16 14:38:56.752: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 16 14:38:56.752: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-2026  1b29dd15-60b1-4923-9d13-0079296d07db 57473 3 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 3d075497-ca9b-42c5-b8ad-f4fc8c06c9e6 0xc006284aa7 0xc006284aa8}] [] [{kube-controller-manager Update apps/v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-16 14:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d075497-ca9b-42c5-b8ad-f4fc8c06c9e6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006284b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 16 14:38:56.762: INFO: Pod "webserver-deployment-7f5969cbc7-2d2mk" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2d2mk webserver-deployment-7f5969cbc7- deployment-2026  49843e4f-934a-4508-9ae5-667d600235c3 57376 0 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.76/23"],"mac_address":"0a:58:0a:81:02:4c","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.76/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.76"
    ],
    "mac": "0a:58:0a:81:02:4c",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc003f77027 0xc003f77028}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vf68v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vf68v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-212-246.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.212.246,PodIP:10.129.2.76,StartTime:2023-05-16 14:38:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:38:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://8f150a54a31175ec4233a243c4e606556ddaee96664fb09f1612242a5f2dc7ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 16 14:38:56.762: INFO: Pod "webserver-deployment-7f5969cbc7-62xdd" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-62xdd webserver-deployment-7f5969cbc7- deployment-2026  82fd3b79-929b-4855-bb66-169f4c788b7f 57481 0 2023-05-16 14:38:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc003f775c7 0xc003f775c8}] [] [{kube-controller-manager Update v1 2023-05-16 14:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pffvc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pffvc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 16 14:38:56.762: INFO: Pod "webserver-deployment-7f5969cbc7-98lmm" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-98lmm webserver-deployment-7f5969cbc7- deployment-2026  fb9f89d5-67a0-4abb-bfb8-4456442a98a8 57368 0 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.74/23"],"mac_address":"0a:58:0a:81:02:4a","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.74/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.74"
    ],
    "mac": "0a:58:0a:81:02:4a",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc003f77747 0xc003f77748}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cjjz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cjjz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-212-246.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.212.246,PodIP:10.129.2.74,StartTime:2023-05-16 14:38:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:38:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://dee6aa669256cc8d85aaea7a3460d3f214e0ed2913c53192f6294a36e437fabf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 16 14:38:56.762: INFO: Pod "webserver-deployment-7f5969cbc7-9glw7" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9glw7 webserver-deployment-7f5969cbc7- deployment-2026  d27514ac-2fde-4128-aa58-cec4595b216b 57358 0 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.148/23"],"mac_address":"0a:58:0a:83:00:94","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.148/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.148"
    ],
    "mac": "0a:58:0a:83:00:94",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc003f779a7 0xc003f779a8}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ctlns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ctlns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:10.131.0.148,StartTime:2023-05-16 14:38:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:38:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://b45817d1b7c5230cbab33b56005d805c04f1120bdb8b89628ccb1fd15199c533,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 16 14:38:56.762: INFO: Pod "webserver-deployment-7f5969cbc7-h526k" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-h526k webserver-deployment-7f5969cbc7- deployment-2026  20cf7e45-8610-47cf-a379-61b2deea5375 57354 0 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.82/23"],"mac_address":"0a:58:0a:80:02:52","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.82/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.82"
    ],
    "mac": "0a:58:0a:80:02:52",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc003f77c07 0xc003f77c08}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7pg5d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7pg5d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.142,PodIP:10.128.2.82,StartTime:2023-05-16 14:38:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:38:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://e899537c5dbd80feec921a4bc64e0bca607ba51112e969612bc145d570badf9b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 16 14:38:56.762: INFO: Pod "webserver-deployment-7f5969cbc7-hzfv9" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hzfv9 webserver-deployment-7f5969cbc7- deployment-2026  ec9f9ca5-a442-4642-8ad8-5825cec31e3c 57482 0 2023-05-16 14:38:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc0069cc097 0xc0069cc098}] [] [{kube-controller-manager Update v1 2023-05-16 14:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bttmc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bttmc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 16 14:38:56.762: INFO: Pod "webserver-deployment-7f5969cbc7-j5pzq" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-j5pzq webserver-deployment-7f5969cbc7- deployment-2026  52a0635a-6a88-4f6c-8554-51a4b084fef6 57480 0 2023-05-16 14:38:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc0069cc217 0xc0069cc218}] [] [{kube-controller-manager Update v1 2023-05-16 14:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z2jqz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z2jqz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-212-246.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 16 14:38:56.762: INFO: Pod "webserver-deployment-7f5969cbc7-kf5c4" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kf5c4 webserver-deployment-7f5969cbc7- deployment-2026  e910d57d-7dae-4109-ab02-f2d741aa4c48 57363 0 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.149/23"],"mac_address":"0a:58:0a:83:00:95","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.149/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.149"
    ],
    "mac": "0a:58:0a:83:00:95",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc0069cc3b7 0xc0069cc3b8}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jj5vb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jj5vb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:10.131.0.149,StartTime:2023-05-16 14:38:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:38:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://049a9ae2c9b1cca24f0179f92cd40ccc5510477157c5d2f630f2e7fb5ab5e5ee,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 16 14:38:56.763: INFO: Pod "webserver-deployment-7f5969cbc7-kh8xq" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kh8xq webserver-deployment-7f5969cbc7- deployment-2026  1b021e23-003b-41d3-91e8-8831b44464ff 57348 0 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.81/23"],"mac_address":"0a:58:0a:80:02:51","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.81/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.81"
    ],
    "mac": "0a:58:0a:80:02:51",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc0069cc617 0xc0069cc618}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fj4wz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fj4wz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.142,PodIP:10.128.2.81,StartTime:2023-05-16 14:38:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:38:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://a750f8735f4fe2ee7d226167e7effa7771def1be908d3343aade38545e43c428,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 16 14:38:56.763: INFO: Pod "webserver-deployment-7f5969cbc7-shxbc" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-shxbc webserver-deployment-7f5969cbc7- deployment-2026  e03125d9-20a4-4eb5-99d4-7d3dce64450f 57360 0 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.147/23"],"mac_address":"0a:58:0a:83:00:93","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.147/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.147"
    ],
    "mac": "0a:58:0a:83:00:93",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc0069cc877 0xc0069cc878}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rc88c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rc88c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:10.131.0.147,StartTime:2023-05-16 14:38:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:38:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://bb63bd0d27fca54b9420886e8da1b64cc63314f2295a7d99f304f4a08fa3b696,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 16 14:38:56.763: INFO: Pod "webserver-deployment-7f5969cbc7-zvzjh" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zvzjh webserver-deployment-7f5969cbc7- deployment-2026  e210e104-d5aa-49fe-83aa-f162be661cf9 57350 0 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.80/23"],"mac_address":"0a:58:0a:80:02:50","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.80/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.80"
    ],
    "mac": "0a:58:0a:80:02:50",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc0069ccad7 0xc0069ccad8}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wr2fk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wr2fk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.142,PodIP:10.128.2.80,StartTime:2023-05-16 14:38:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:38:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://c5622ac915db37f1c1b6e4d89d6d15cd9c4a00bb2719a9db65b8eb01c07b01ae,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 16 14:38:56.763: INFO: Pod "webserver-deployment-d9f79cb5-26wmb" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-26wmb webserver-deployment-d9f79cb5- deployment-2026  eaef286a-3a10-421f-a069-5018477ae01b 57457 0 2023-05-16 14:38:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.78/23"],"mac_address":"0a:58:0a:81:02:4e","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.78/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.78"
    ],
    "mac": "0a:58:0a:81:02:4e",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 736c9a78-2d1a-42fb-a912-9ce4505bc489 0xc0069ccd37 0xc0069ccd38}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"736c9a78-2d1a-42fb-a912-9ce4505bc489\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b8l7v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b8l7v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-212-246.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.212.246,PodIP:,StartTime:2023-05-16 14:38:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 16 14:38:56.763: INFO: Pod "webserver-deployment-d9f79cb5-2l9fp" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2l9fp webserver-deployment-d9f79cb5- deployment-2026  b8677fdb-86a9-4a5b-82ca-423cf13f330c 57445 0 2023-05-16 14:38:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.150/23"],"mac_address":"0a:58:0a:83:00:96","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.150/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.150"
    ],
    "mac": "0a:58:0a:83:00:96",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 736c9a78-2d1a-42fb-a912-9ce4505bc489 0xc0069ccfa7 0xc0069ccfa8}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"736c9a78-2d1a-42fb-a912-9ce4505bc489\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pgrk5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pgrk5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:,StartTime:2023-05-16 14:38:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 16 14:38:56.763: INFO: Pod "webserver-deployment-d9f79cb5-5k9qw" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5k9qw webserver-deployment-d9f79cb5- deployment-2026  0cad93c4-b00f-43f4-ac75-c8413392d857 57439 0 2023-05-16 14:38:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.77/23"],"mac_address":"0a:58:0a:81:02:4d","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.77/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.129.2.77"
    ],
    "mac": "0a:58:0a:81:02:4d",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 736c9a78-2d1a-42fb-a912-9ce4505bc489 0xc0069cd227 0xc0069cd228}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"736c9a78-2d1a-42fb-a912-9ce4505bc489\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tzfrv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tzfrv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-212-246.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.212.246,PodIP:,StartTime:2023-05-16 14:38:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 16 14:38:56.763: INFO: Pod "webserver-deployment-d9f79cb5-bnmc5" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bnmc5 webserver-deployment-d9f79cb5- deployment-2026  fbe3b5cb-55a6-4357-b86b-34cc35cfd215 57446 0 2023-05-16 14:38:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.83/23"],"mac_address":"0a:58:0a:80:02:53","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.83/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.2.83"
    ],
    "mac": "0a:58:0a:80:02:53",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 736c9a78-2d1a-42fb-a912-9ce4505bc489 0xc0069cd4a7 0xc0069cd4a8}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"736c9a78-2d1a-42fb-a912-9ce4505bc489\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m4j7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m4j7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.142,PodIP:,StartTime:2023-05-16 14:38:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 16 14:38:56.763: INFO: Pod "webserver-deployment-d9f79cb5-wm8vf" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wm8vf webserver-deployment-d9f79cb5- deployment-2026  64a1d49e-e4e8-428e-a941-cf401558d595 57453 0 2023-05-16 14:38:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.151/23"],"mac_address":"0a:58:0a:83:00:97","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.151/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.0.151"
    ],
    "mac": "0a:58:0a:83:00:97",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 736c9a78-2d1a-42fb-a912-9ce4505bc489 0xc0069cd707 0xc0069cd708}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"736c9a78-2d1a-42fb-a912-9ce4505bc489\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wz2qx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wz2qx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:,StartTime:2023-05-16 14:38:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 16 14:38:56.763: INFO: Pod "webserver-deployment-d9f79cb5-zpgsc" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zpgsc webserver-deployment-d9f79cb5- deployment-2026  342a1fc4-897d-4b3c-b2b3-f3716dac0dc2 57484 0 2023-05-16 14:38:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 736c9a78-2d1a-42fb-a912-9ce4505bc489 0xc0069cd967 0xc0069cd968}] [] [{kube-controller-manager Update v1 2023-05-16 14:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"736c9a78-2d1a-42fb-a912-9ce4505bc489\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vshxw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vshxw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May 16 14:38:56.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2026" for this suite. 05/16/23 14:38:56.769
------------------------------
â€¢ [4.235 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:38:52.577
    May 16 14:38:52.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename deployment 05/16/23 14:38:52.577
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:38:52.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:38:52.605
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    May 16 14:38:52.608: INFO: Creating deployment "webserver-deployment"
    W0516 14:38:52.649619      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:38:52.649: INFO: Waiting for observed generation 1
    May 16 14:38:54.660: INFO: Waiting for all required pods to come up
    May 16 14:38:54.663: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 05/16/23 14:38:54.663
    May 16 14:38:54.663: INFO: Waiting for deployment "webserver-deployment" to complete
    May 16 14:38:54.669: INFO: Updating deployment "webserver-deployment" with a non-existent image
    May 16 14:38:54.689: INFO: Updating deployment webserver-deployment
    May 16 14:38:54.689: INFO: Waiting for observed generation 2
    May 16 14:38:56.696: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    May 16 14:38:56.698: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    May 16 14:38:56.701: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    May 16 14:38:56.709: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    May 16 14:38:56.709: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    May 16 14:38:56.711: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    May 16 14:38:56.717: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    May 16 14:38:56.717: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    May 16 14:38:56.724: INFO: Updating deployment webserver-deployment
    May 16 14:38:56.725: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    May 16 14:38:56.731: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    May 16 14:38:56.734: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 16 14:38:56.746: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-2026  3d075497-ca9b-42c5-b8ad-f4fc8c06c9e6 57478 3 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-16 14:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 14:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002bc2598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-05-16 14:38:54 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-16 14:38:56 +0000 UTC,LastTransitionTime:2023-05-16 14:38:56 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    May 16 14:38:56.752: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-2026  736c9a78-2d1a-42fb-a912-9ce4505bc489 57475 3 2023-05-16 14:38:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 3d075497-ca9b-42c5-b8ad-f4fc8c06c9e6 0xc006284b97 0xc006284b98}] [] [{kube-controller-manager Update apps/v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-16 14:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d075497-ca9b-42c5-b8ad-f4fc8c06c9e6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006284c48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May 16 14:38:56.752: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    May 16 14:38:56.752: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-2026  1b29dd15-60b1-4923-9d13-0079296d07db 57473 3 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 3d075497-ca9b-42c5-b8ad-f4fc8c06c9e6 0xc006284aa7 0xc006284aa8}] [] [{kube-controller-manager Update apps/v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-16 14:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d075497-ca9b-42c5-b8ad-f4fc8c06c9e6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006284b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    May 16 14:38:56.762: INFO: Pod "webserver-deployment-7f5969cbc7-2d2mk" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2d2mk webserver-deployment-7f5969cbc7- deployment-2026  49843e4f-934a-4508-9ae5-667d600235c3 57376 0 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.76/23"],"mac_address":"0a:58:0a:81:02:4c","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.76/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.76"
        ],
        "mac": "0a:58:0a:81:02:4c",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc003f77027 0xc003f77028}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vf68v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vf68v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-212-246.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.212.246,PodIP:10.129.2.76,StartTime:2023-05-16 14:38:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:38:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://8f150a54a31175ec4233a243c4e606556ddaee96664fb09f1612242a5f2dc7ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 16 14:38:56.762: INFO: Pod "webserver-deployment-7f5969cbc7-62xdd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-62xdd webserver-deployment-7f5969cbc7- deployment-2026  82fd3b79-929b-4855-bb66-169f4c788b7f 57481 0 2023-05-16 14:38:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc003f775c7 0xc003f775c8}] [] [{kube-controller-manager Update v1 2023-05-16 14:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pffvc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pffvc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 16 14:38:56.762: INFO: Pod "webserver-deployment-7f5969cbc7-98lmm" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-98lmm webserver-deployment-7f5969cbc7- deployment-2026  fb9f89d5-67a0-4abb-bfb8-4456442a98a8 57368 0 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.74/23"],"mac_address":"0a:58:0a:81:02:4a","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.74/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.74"
        ],
        "mac": "0a:58:0a:81:02:4a",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc003f77747 0xc003f77748}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.74\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cjjz6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cjjz6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-212-246.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.212.246,PodIP:10.129.2.74,StartTime:2023-05-16 14:38:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:38:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://dee6aa669256cc8d85aaea7a3460d3f214e0ed2913c53192f6294a36e437fabf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.129.2.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 16 14:38:56.762: INFO: Pod "webserver-deployment-7f5969cbc7-9glw7" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9glw7 webserver-deployment-7f5969cbc7- deployment-2026  d27514ac-2fde-4128-aa58-cec4595b216b 57358 0 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.148/23"],"mac_address":"0a:58:0a:83:00:94","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.148/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.148"
        ],
        "mac": "0a:58:0a:83:00:94",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc003f779a7 0xc003f779a8}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.148\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ctlns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ctlns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:10.131.0.148,StartTime:2023-05-16 14:38:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:38:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://b45817d1b7c5230cbab33b56005d805c04f1120bdb8b89628ccb1fd15199c533,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 16 14:38:56.762: INFO: Pod "webserver-deployment-7f5969cbc7-h526k" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-h526k webserver-deployment-7f5969cbc7- deployment-2026  20cf7e45-8610-47cf-a379-61b2deea5375 57354 0 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.82/23"],"mac_address":"0a:58:0a:80:02:52","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.82/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.82"
        ],
        "mac": "0a:58:0a:80:02:52",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc003f77c07 0xc003f77c08}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.82\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7pg5d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7pg5d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.142,PodIP:10.128.2.82,StartTime:2023-05-16 14:38:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:38:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://e899537c5dbd80feec921a4bc64e0bca607ba51112e969612bc145d570badf9b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.82,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 16 14:38:56.762: INFO: Pod "webserver-deployment-7f5969cbc7-hzfv9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hzfv9 webserver-deployment-7f5969cbc7- deployment-2026  ec9f9ca5-a442-4642-8ad8-5825cec31e3c 57482 0 2023-05-16 14:38:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc0069cc097 0xc0069cc098}] [] [{kube-controller-manager Update v1 2023-05-16 14:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bttmc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bttmc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 16 14:38:56.762: INFO: Pod "webserver-deployment-7f5969cbc7-j5pzq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-j5pzq webserver-deployment-7f5969cbc7- deployment-2026  52a0635a-6a88-4f6c-8554-51a4b084fef6 57480 0 2023-05-16 14:38:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc0069cc217 0xc0069cc218}] [] [{kube-controller-manager Update v1 2023-05-16 14:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z2jqz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z2jqz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-212-246.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 16 14:38:56.762: INFO: Pod "webserver-deployment-7f5969cbc7-kf5c4" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kf5c4 webserver-deployment-7f5969cbc7- deployment-2026  e910d57d-7dae-4109-ab02-f2d741aa4c48 57363 0 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.149/23"],"mac_address":"0a:58:0a:83:00:95","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.149/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.149"
        ],
        "mac": "0a:58:0a:83:00:95",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc0069cc3b7 0xc0069cc3b8}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jj5vb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jj5vb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:10.131.0.149,StartTime:2023-05-16 14:38:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:38:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://049a9ae2c9b1cca24f0179f92cd40ccc5510477157c5d2f630f2e7fb5ab5e5ee,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 16 14:38:56.763: INFO: Pod "webserver-deployment-7f5969cbc7-kh8xq" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kh8xq webserver-deployment-7f5969cbc7- deployment-2026  1b021e23-003b-41d3-91e8-8831b44464ff 57348 0 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.81/23"],"mac_address":"0a:58:0a:80:02:51","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.81/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.81"
        ],
        "mac": "0a:58:0a:80:02:51",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc0069cc617 0xc0069cc618}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fj4wz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fj4wz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.142,PodIP:10.128.2.81,StartTime:2023-05-16 14:38:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:38:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://a750f8735f4fe2ee7d226167e7effa7771def1be908d3343aade38545e43c428,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 16 14:38:56.763: INFO: Pod "webserver-deployment-7f5969cbc7-shxbc" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-shxbc webserver-deployment-7f5969cbc7- deployment-2026  e03125d9-20a4-4eb5-99d4-7d3dce64450f 57360 0 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.147/23"],"mac_address":"0a:58:0a:83:00:93","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.147/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.147"
        ],
        "mac": "0a:58:0a:83:00:93",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc0069cc877 0xc0069cc878}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.0.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rc88c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rc88c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:10.131.0.147,StartTime:2023-05-16 14:38:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:38:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://bb63bd0d27fca54b9420886e8da1b64cc63314f2295a7d99f304f4a08fa3b696,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.0.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 16 14:38:56.763: INFO: Pod "webserver-deployment-7f5969cbc7-zvzjh" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zvzjh webserver-deployment-7f5969cbc7- deployment-2026  e210e104-d5aa-49fe-83aa-f162be661cf9 57350 0 2023-05-16 14:38:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.80/23"],"mac_address":"0a:58:0a:80:02:50","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.80/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.80"
        ],
        "mac": "0a:58:0a:80:02:50",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 1b29dd15-60b1-4923-9d13-0079296d07db 0xc0069ccad7 0xc0069ccad8}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b29dd15-60b1-4923-9d13-0079296d07db\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wr2fk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wr2fk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.142,PodIP:10.128.2.80,StartTime:2023-05-16 14:38:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 14:38:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://c5622ac915db37f1c1b6e4d89d6d15cd9c4a00bb2719a9db65b8eb01c07b01ae,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.2.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 16 14:38:56.763: INFO: Pod "webserver-deployment-d9f79cb5-26wmb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-26wmb webserver-deployment-d9f79cb5- deployment-2026  eaef286a-3a10-421f-a069-5018477ae01b 57457 0 2023-05-16 14:38:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.78/23"],"mac_address":"0a:58:0a:81:02:4e","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.78/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.78"
        ],
        "mac": "0a:58:0a:81:02:4e",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 736c9a78-2d1a-42fb-a912-9ce4505bc489 0xc0069ccd37 0xc0069ccd38}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"736c9a78-2d1a-42fb-a912-9ce4505bc489\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b8l7v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b8l7v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-212-246.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.212.246,PodIP:,StartTime:2023-05-16 14:38:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 16 14:38:56.763: INFO: Pod "webserver-deployment-d9f79cb5-2l9fp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2l9fp webserver-deployment-d9f79cb5- deployment-2026  b8677fdb-86a9-4a5b-82ca-423cf13f330c 57445 0 2023-05-16 14:38:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.150/23"],"mac_address":"0a:58:0a:83:00:96","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.150/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.150"
        ],
        "mac": "0a:58:0a:83:00:96",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 736c9a78-2d1a-42fb-a912-9ce4505bc489 0xc0069ccfa7 0xc0069ccfa8}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"736c9a78-2d1a-42fb-a912-9ce4505bc489\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pgrk5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pgrk5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:,StartTime:2023-05-16 14:38:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 16 14:38:56.763: INFO: Pod "webserver-deployment-d9f79cb5-5k9qw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5k9qw webserver-deployment-d9f79cb5- deployment-2026  0cad93c4-b00f-43f4-ac75-c8413392d857 57439 0 2023-05-16 14:38:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.129.2.77/23"],"mac_address":"0a:58:0a:81:02:4d","gateway_ips":["10.129.2.1"],"ip_address":"10.129.2.77/23","gateway_ip":"10.129.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.129.2.77"
        ],
        "mac": "0a:58:0a:81:02:4d",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 736c9a78-2d1a-42fb-a912-9ce4505bc489 0xc0069cd227 0xc0069cd228}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"736c9a78-2d1a-42fb-a912-9ce4505bc489\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tzfrv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tzfrv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-212-246.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.212.246,PodIP:,StartTime:2023-05-16 14:38:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 16 14:38:56.763: INFO: Pod "webserver-deployment-d9f79cb5-bnmc5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bnmc5 webserver-deployment-d9f79cb5- deployment-2026  fbe3b5cb-55a6-4357-b86b-34cc35cfd215 57446 0 2023-05-16 14:38:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.2.83/23"],"mac_address":"0a:58:0a:80:02:53","gateway_ips":["10.128.2.1"],"ip_address":"10.128.2.83/23","gateway_ip":"10.128.2.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.2.83"
        ],
        "mac": "0a:58:0a:80:02:53",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 736c9a78-2d1a-42fb-a912-9ce4505bc489 0xc0069cd4a7 0xc0069cd4a8}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"736c9a78-2d1a-42fb-a912-9ce4505bc489\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m4j7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m4j7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.132.142,PodIP:,StartTime:2023-05-16 14:38:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 16 14:38:56.763: INFO: Pod "webserver-deployment-d9f79cb5-wm8vf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wm8vf webserver-deployment-d9f79cb5- deployment-2026  64a1d49e-e4e8-428e-a941-cf401558d595 57453 0 2023-05-16 14:38:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.0.151/23"],"mac_address":"0a:58:0a:83:00:97","gateway_ips":["10.131.0.1"],"ip_address":"10.131.0.151/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.0.151"
        ],
        "mac": "0a:58:0a:83:00:97",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 736c9a78-2d1a-42fb-a912-9ce4505bc489 0xc0069cd707 0xc0069cd708}] [] [{ip-10-0-130-191 Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"736c9a78-2d1a-42fb-a912-9ce4505bc489\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 14:38:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-05-16 14:38:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wz2qx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wz2qx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:,StartTime:2023-05-16 14:38:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    May 16 14:38:56.763: INFO: Pod "webserver-deployment-d9f79cb5-zpgsc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-zpgsc webserver-deployment-d9f79cb5- deployment-2026  342a1fc4-897d-4b3c-b2b3-f3716dac0dc2 57484 0 2023-05-16 14:38:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 736c9a78-2d1a-42fb-a912-9ce4505bc489 0xc0069cd967 0xc0069cd968}] [] [{kube-controller-manager Update v1 2023-05-16 14:38:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"736c9a78-2d1a-42fb-a912-9ce4505bc489\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vshxw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vshxw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-132-142.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c45,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-b8snc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 14:38:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May 16 14:38:56.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2026" for this suite. 05/16/23 14:38:56.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:38:56.813
May 16 14:38:56.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename services 05/16/23 14:38:56.813
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:38:56.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:38:56.902
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4282 05/16/23 14:38:56.904
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/16/23 14:38:56.933
STEP: creating service externalsvc in namespace services-4282 05/16/23 14:38:56.933
STEP: creating replication controller externalsvc in namespace services-4282 05/16/23 14:38:56.957
I0516 14:38:56.978956      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4282, replica count: 2
I0516 14:39:00.029996      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 05/16/23 14:39:00.032
May 16 14:39:00.048: INFO: Creating new exec pod
May 16 14:39:00.064: INFO: Waiting up to 5m0s for pod "execpodsc6vd" in namespace "services-4282" to be "running"
May 16 14:39:00.073: INFO: Pod "execpodsc6vd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.879098ms
May 16 14:39:02.077: INFO: Pod "execpodsc6vd": Phase="Running", Reason="", readiness=true. Elapsed: 2.012790053s
May 16 14:39:02.077: INFO: Pod "execpodsc6vd" satisfied condition "running"
May 16 14:39:02.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-4282 exec execpodsc6vd -- /bin/sh -x -c nslookup clusterip-service.services-4282.svc.cluster.local'
May 16 14:39:02.202: INFO: stderr: "+ nslookup clusterip-service.services-4282.svc.cluster.local\n"
May 16 14:39:02.202: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nclusterip-service.services-4282.svc.cluster.local\tcanonical name = externalsvc.services-4282.svc.cluster.local.\nName:\texternalsvc.services-4282.svc.cluster.local\nAddress: 172.30.30.133\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4282, will wait for the garbage collector to delete the pods 05/16/23 14:39:02.202
May 16 14:39:02.262: INFO: Deleting ReplicationController externalsvc took: 5.622471ms
May 16 14:39:02.362: INFO: Terminating ReplicationController externalsvc pods took: 100.568781ms
May 16 14:39:04.285: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 16 14:39:04.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4282" for this suite. 05/16/23 14:39:04.31
------------------------------
â€¢ [SLOW TEST] [7.508 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:38:56.813
    May 16 14:38:56.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename services 05/16/23 14:38:56.813
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:38:56.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:38:56.902
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4282 05/16/23 14:38:56.904
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 05/16/23 14:38:56.933
    STEP: creating service externalsvc in namespace services-4282 05/16/23 14:38:56.933
    STEP: creating replication controller externalsvc in namespace services-4282 05/16/23 14:38:56.957
    I0516 14:38:56.978956      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4282, replica count: 2
    I0516 14:39:00.029996      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 05/16/23 14:39:00.032
    May 16 14:39:00.048: INFO: Creating new exec pod
    May 16 14:39:00.064: INFO: Waiting up to 5m0s for pod "execpodsc6vd" in namespace "services-4282" to be "running"
    May 16 14:39:00.073: INFO: Pod "execpodsc6vd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.879098ms
    May 16 14:39:02.077: INFO: Pod "execpodsc6vd": Phase="Running", Reason="", readiness=true. Elapsed: 2.012790053s
    May 16 14:39:02.077: INFO: Pod "execpodsc6vd" satisfied condition "running"
    May 16 14:39:02.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-4282 exec execpodsc6vd -- /bin/sh -x -c nslookup clusterip-service.services-4282.svc.cluster.local'
    May 16 14:39:02.202: INFO: stderr: "+ nslookup clusterip-service.services-4282.svc.cluster.local\n"
    May 16 14:39:02.202: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nclusterip-service.services-4282.svc.cluster.local\tcanonical name = externalsvc.services-4282.svc.cluster.local.\nName:\texternalsvc.services-4282.svc.cluster.local\nAddress: 172.30.30.133\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-4282, will wait for the garbage collector to delete the pods 05/16/23 14:39:02.202
    May 16 14:39:02.262: INFO: Deleting ReplicationController externalsvc took: 5.622471ms
    May 16 14:39:02.362: INFO: Terminating ReplicationController externalsvc pods took: 100.568781ms
    May 16 14:39:04.285: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 16 14:39:04.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4282" for this suite. 05/16/23 14:39:04.31
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:39:04.321
May 16 14:39:04.321: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename certificates 05/16/23 14:39:04.322
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:04.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:04.356
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 05/16/23 14:39:04.634
STEP: getting /apis/certificates.k8s.io 05/16/23 14:39:04.637
STEP: getting /apis/certificates.k8s.io/v1 05/16/23 14:39:04.638
STEP: creating 05/16/23 14:39:04.639
STEP: getting 05/16/23 14:39:04.656
STEP: listing 05/16/23 14:39:04.659
STEP: watching 05/16/23 14:39:04.663
May 16 14:39:04.663: INFO: starting watch
STEP: patching 05/16/23 14:39:04.664
STEP: updating 05/16/23 14:39:04.669
May 16 14:39:04.678: INFO: waiting for watch events with expected annotations
May 16 14:39:04.678: INFO: saw patched and updated annotations
STEP: getting /approval 05/16/23 14:39:04.678
STEP: patching /approval 05/16/23 14:39:04.682
STEP: updating /approval 05/16/23 14:39:04.689
STEP: getting /status 05/16/23 14:39:04.695
STEP: patching /status 05/16/23 14:39:04.698
STEP: updating /status 05/16/23 14:39:04.706
STEP: deleting 05/16/23 14:39:04.713
STEP: deleting a collection 05/16/23 14:39:04.726
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 14:39:04.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-4964" for this suite. 05/16/23 14:39:04.746
------------------------------
â€¢ [0.431 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:39:04.321
    May 16 14:39:04.321: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename certificates 05/16/23 14:39:04.322
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:04.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:04.356
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 05/16/23 14:39:04.634
    STEP: getting /apis/certificates.k8s.io 05/16/23 14:39:04.637
    STEP: getting /apis/certificates.k8s.io/v1 05/16/23 14:39:04.638
    STEP: creating 05/16/23 14:39:04.639
    STEP: getting 05/16/23 14:39:04.656
    STEP: listing 05/16/23 14:39:04.659
    STEP: watching 05/16/23 14:39:04.663
    May 16 14:39:04.663: INFO: starting watch
    STEP: patching 05/16/23 14:39:04.664
    STEP: updating 05/16/23 14:39:04.669
    May 16 14:39:04.678: INFO: waiting for watch events with expected annotations
    May 16 14:39:04.678: INFO: saw patched and updated annotations
    STEP: getting /approval 05/16/23 14:39:04.678
    STEP: patching /approval 05/16/23 14:39:04.682
    STEP: updating /approval 05/16/23 14:39:04.689
    STEP: getting /status 05/16/23 14:39:04.695
    STEP: patching /status 05/16/23 14:39:04.698
    STEP: updating /status 05/16/23 14:39:04.706
    STEP: deleting 05/16/23 14:39:04.713
    STEP: deleting a collection 05/16/23 14:39:04.726
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 14:39:04.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-4964" for this suite. 05/16/23 14:39:04.746
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:39:04.752
May 16 14:39:04.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename webhook 05/16/23 14:39:04.753
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:04.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:04.776
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/16/23 14:39:04.803
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 14:39:05.352
STEP: Deploying the webhook pod 05/16/23 14:39:05.359
STEP: Wait for the deployment to be ready 05/16/23 14:39:05.37
May 16 14:39:05.380: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/16/23 14:39:07.39
STEP: Verifying the service has paired with the endpoint 05/16/23 14:39:07.4
May 16 14:39:08.400: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
May 16 14:39:08.403: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Registering the custom resource webhook via the AdmissionRegistration API 05/16/23 14:39:08.913
STEP: Creating a custom resource that should be denied by the webhook 05/16/23 14:39:08.926
STEP: Creating a custom resource whose deletion would be denied by the webhook 05/16/23 14:39:10.954
STEP: Updating the custom resource with disallowed data should be denied 05/16/23 14:39:10.959
STEP: Deleting the custom resource should be denied 05/16/23 14:39:10.967
STEP: Remove the offending key and value from the custom resource data 05/16/23 14:39:10.972
STEP: Deleting the updated custom resource should be successful 05/16/23 14:39:10.98
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 14:39:11.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7716" for this suite. 05/16/23 14:39:11.613
STEP: Destroying namespace "webhook-7716-markers" for this suite. 05/16/23 14:39:11.628
------------------------------
â€¢ [SLOW TEST] [6.882 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:39:04.752
    May 16 14:39:04.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename webhook 05/16/23 14:39:04.753
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:04.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:04.776
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/16/23 14:39:04.803
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 14:39:05.352
    STEP: Deploying the webhook pod 05/16/23 14:39:05.359
    STEP: Wait for the deployment to be ready 05/16/23 14:39:05.37
    May 16 14:39:05.380: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/16/23 14:39:07.39
    STEP: Verifying the service has paired with the endpoint 05/16/23 14:39:07.4
    May 16 14:39:08.400: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    May 16 14:39:08.403: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 05/16/23 14:39:08.913
    STEP: Creating a custom resource that should be denied by the webhook 05/16/23 14:39:08.926
    STEP: Creating a custom resource whose deletion would be denied by the webhook 05/16/23 14:39:10.954
    STEP: Updating the custom resource with disallowed data should be denied 05/16/23 14:39:10.959
    STEP: Deleting the custom resource should be denied 05/16/23 14:39:10.967
    STEP: Remove the offending key and value from the custom resource data 05/16/23 14:39:10.972
    STEP: Deleting the updated custom resource should be successful 05/16/23 14:39:10.98
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 14:39:11.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7716" for this suite. 05/16/23 14:39:11.613
    STEP: Destroying namespace "webhook-7716-markers" for this suite. 05/16/23 14:39:11.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:39:11.635
May 16 14:39:11.635: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename svcaccounts 05/16/23 14:39:11.636
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:11.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:11.666
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
W0516 14:39:11.707793      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:39:11.707: INFO: Waiting up to 5m0s for pod "pod-service-account-e0650401-e849-48fa-a506-332893b3946b" in namespace "svcaccounts-2781" to be "running"
May 16 14:39:11.726: INFO: Pod "pod-service-account-e0650401-e849-48fa-a506-332893b3946b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.15029ms
May 16 14:39:13.729: INFO: Pod "pod-service-account-e0650401-e849-48fa-a506-332893b3946b": Phase="Running", Reason="", readiness=true. Elapsed: 2.021737148s
May 16 14:39:13.729: INFO: Pod "pod-service-account-e0650401-e849-48fa-a506-332893b3946b" satisfied condition "running"
STEP: reading a file in the container 05/16/23 14:39:13.729
May 16 14:39:13.729: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2781 pod-service-account-e0650401-e849-48fa-a506-332893b3946b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 05/16/23 14:39:13.837
May 16 14:39:13.837: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2781 pod-service-account-e0650401-e849-48fa-a506-332893b3946b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 05/16/23 14:39:13.938
May 16 14:39:13.938: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2781 pod-service-account-e0650401-e849-48fa-a506-332893b3946b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
May 16 14:39:14.039: INFO: Got root ca configmap in namespace "svcaccounts-2781"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May 16 14:39:14.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2781" for this suite. 05/16/23 14:39:14.045
------------------------------
â€¢ [2.416 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:39:11.635
    May 16 14:39:11.635: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename svcaccounts 05/16/23 14:39:11.636
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:11.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:11.666
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    W0516 14:39:11.707793      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:39:11.707: INFO: Waiting up to 5m0s for pod "pod-service-account-e0650401-e849-48fa-a506-332893b3946b" in namespace "svcaccounts-2781" to be "running"
    May 16 14:39:11.726: INFO: Pod "pod-service-account-e0650401-e849-48fa-a506-332893b3946b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.15029ms
    May 16 14:39:13.729: INFO: Pod "pod-service-account-e0650401-e849-48fa-a506-332893b3946b": Phase="Running", Reason="", readiness=true. Elapsed: 2.021737148s
    May 16 14:39:13.729: INFO: Pod "pod-service-account-e0650401-e849-48fa-a506-332893b3946b" satisfied condition "running"
    STEP: reading a file in the container 05/16/23 14:39:13.729
    May 16 14:39:13.729: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2781 pod-service-account-e0650401-e849-48fa-a506-332893b3946b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 05/16/23 14:39:13.837
    May 16 14:39:13.837: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2781 pod-service-account-e0650401-e849-48fa-a506-332893b3946b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 05/16/23 14:39:13.938
    May 16 14:39:13.938: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2781 pod-service-account-e0650401-e849-48fa-a506-332893b3946b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    May 16 14:39:14.039: INFO: Got root ca configmap in namespace "svcaccounts-2781"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May 16 14:39:14.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2781" for this suite. 05/16/23 14:39:14.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:39:14.051
May 16 14:39:14.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename containers 05/16/23 14:39:14.051
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:14.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:14.085
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 05/16/23 14:39:14.088
May 16 14:39:14.107: INFO: Waiting up to 5m0s for pod "client-containers-4679ec98-4c93-43c9-9193-b6e273616eaa" in namespace "containers-6243" to be "Succeeded or Failed"
May 16 14:39:14.111: INFO: Pod "client-containers-4679ec98-4c93-43c9-9193-b6e273616eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.675069ms
May 16 14:39:16.114: INFO: Pod "client-containers-4679ec98-4c93-43c9-9193-b6e273616eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007394312s
May 16 14:39:18.115: INFO: Pod "client-containers-4679ec98-4c93-43c9-9193-b6e273616eaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00807571s
STEP: Saw pod success 05/16/23 14:39:18.115
May 16 14:39:18.115: INFO: Pod "client-containers-4679ec98-4c93-43c9-9193-b6e273616eaa" satisfied condition "Succeeded or Failed"
May 16 14:39:18.118: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod client-containers-4679ec98-4c93-43c9-9193-b6e273616eaa container agnhost-container: <nil>
STEP: delete the pod 05/16/23 14:39:18.128
May 16 14:39:18.137: INFO: Waiting for pod client-containers-4679ec98-4c93-43c9-9193-b6e273616eaa to disappear
May 16 14:39:18.140: INFO: Pod client-containers-4679ec98-4c93-43c9-9193-b6e273616eaa no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
May 16 14:39:18.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-6243" for this suite. 05/16/23 14:39:18.144
------------------------------
â€¢ [4.098 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:39:14.051
    May 16 14:39:14.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename containers 05/16/23 14:39:14.051
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:14.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:14.085
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 05/16/23 14:39:14.088
    May 16 14:39:14.107: INFO: Waiting up to 5m0s for pod "client-containers-4679ec98-4c93-43c9-9193-b6e273616eaa" in namespace "containers-6243" to be "Succeeded or Failed"
    May 16 14:39:14.111: INFO: Pod "client-containers-4679ec98-4c93-43c9-9193-b6e273616eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.675069ms
    May 16 14:39:16.114: INFO: Pod "client-containers-4679ec98-4c93-43c9-9193-b6e273616eaa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007394312s
    May 16 14:39:18.115: INFO: Pod "client-containers-4679ec98-4c93-43c9-9193-b6e273616eaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00807571s
    STEP: Saw pod success 05/16/23 14:39:18.115
    May 16 14:39:18.115: INFO: Pod "client-containers-4679ec98-4c93-43c9-9193-b6e273616eaa" satisfied condition "Succeeded or Failed"
    May 16 14:39:18.118: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod client-containers-4679ec98-4c93-43c9-9193-b6e273616eaa container agnhost-container: <nil>
    STEP: delete the pod 05/16/23 14:39:18.128
    May 16 14:39:18.137: INFO: Waiting for pod client-containers-4679ec98-4c93-43c9-9193-b6e273616eaa to disappear
    May 16 14:39:18.140: INFO: Pod client-containers-4679ec98-4c93-43c9-9193-b6e273616eaa no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    May 16 14:39:18.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-6243" for this suite. 05/16/23 14:39:18.144
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:39:18.149
May 16 14:39:18.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename configmap 05/16/23 14:39:18.15
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:18.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:18.172
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
May 16 14:39:18.184: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-63f37164-b646-45a5-9a94-c7e524efd1dc 05/16/23 14:39:18.184
STEP: Creating the pod 05/16/23 14:39:18.188
May 16 14:39:18.199: INFO: Waiting up to 5m0s for pod "pod-configmaps-62f01e15-5701-4fb2-b596-da8a0ae4006f" in namespace "configmap-9347" to be "running and ready"
May 16 14:39:18.203: INFO: Pod "pod-configmaps-62f01e15-5701-4fb2-b596-da8a0ae4006f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.767878ms
May 16 14:39:18.203: INFO: The phase of Pod pod-configmaps-62f01e15-5701-4fb2-b596-da8a0ae4006f is Pending, waiting for it to be Running (with Ready = true)
May 16 14:39:20.207: INFO: Pod "pod-configmaps-62f01e15-5701-4fb2-b596-da8a0ae4006f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008038779s
May 16 14:39:20.207: INFO: The phase of Pod pod-configmaps-62f01e15-5701-4fb2-b596-da8a0ae4006f is Running (Ready = true)
May 16 14:39:20.207: INFO: Pod "pod-configmaps-62f01e15-5701-4fb2-b596-da8a0ae4006f" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-63f37164-b646-45a5-9a94-c7e524efd1dc 05/16/23 14:39:20.215
STEP: waiting to observe update in volume 05/16/23 14:39:20.22
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 16 14:39:22.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9347" for this suite. 05/16/23 14:39:22.235
------------------------------
â€¢ [4.093 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:39:18.149
    May 16 14:39:18.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename configmap 05/16/23 14:39:18.15
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:18.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:18.172
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    May 16 14:39:18.184: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name configmap-test-upd-63f37164-b646-45a5-9a94-c7e524efd1dc 05/16/23 14:39:18.184
    STEP: Creating the pod 05/16/23 14:39:18.188
    May 16 14:39:18.199: INFO: Waiting up to 5m0s for pod "pod-configmaps-62f01e15-5701-4fb2-b596-da8a0ae4006f" in namespace "configmap-9347" to be "running and ready"
    May 16 14:39:18.203: INFO: Pod "pod-configmaps-62f01e15-5701-4fb2-b596-da8a0ae4006f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.767878ms
    May 16 14:39:18.203: INFO: The phase of Pod pod-configmaps-62f01e15-5701-4fb2-b596-da8a0ae4006f is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:39:20.207: INFO: Pod "pod-configmaps-62f01e15-5701-4fb2-b596-da8a0ae4006f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008038779s
    May 16 14:39:20.207: INFO: The phase of Pod pod-configmaps-62f01e15-5701-4fb2-b596-da8a0ae4006f is Running (Ready = true)
    May 16 14:39:20.207: INFO: Pod "pod-configmaps-62f01e15-5701-4fb2-b596-da8a0ae4006f" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-63f37164-b646-45a5-9a94-c7e524efd1dc 05/16/23 14:39:20.215
    STEP: waiting to observe update in volume 05/16/23 14:39:20.22
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 16 14:39:22.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9347" for this suite. 05/16/23 14:39:22.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:39:22.243
May 16 14:39:22.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename emptydir 05/16/23 14:39:22.244
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:22.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:22.27
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 05/16/23 14:39:22.273
May 16 14:39:22.285: INFO: Waiting up to 5m0s for pod "pod-ab4708a5-a1d2-4c8e-8367-b0d051a82a9c" in namespace "emptydir-3991" to be "Succeeded or Failed"
May 16 14:39:22.288: INFO: Pod "pod-ab4708a5-a1d2-4c8e-8367-b0d051a82a9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.395395ms
May 16 14:39:24.291: INFO: Pod "pod-ab4708a5-a1d2-4c8e-8367-b0d051a82a9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005747092s
May 16 14:39:26.291: INFO: Pod "pod-ab4708a5-a1d2-4c8e-8367-b0d051a82a9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005863003s
STEP: Saw pod success 05/16/23 14:39:26.291
May 16 14:39:26.291: INFO: Pod "pod-ab4708a5-a1d2-4c8e-8367-b0d051a82a9c" satisfied condition "Succeeded or Failed"
May 16 14:39:26.294: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod pod-ab4708a5-a1d2-4c8e-8367-b0d051a82a9c container test-container: <nil>
STEP: delete the pod 05/16/23 14:39:26.305
May 16 14:39:26.314: INFO: Waiting for pod pod-ab4708a5-a1d2-4c8e-8367-b0d051a82a9c to disappear
May 16 14:39:26.316: INFO: Pod pod-ab4708a5-a1d2-4c8e-8367-b0d051a82a9c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 16 14:39:26.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3991" for this suite. 05/16/23 14:39:26.321
------------------------------
â€¢ [4.085 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:39:22.243
    May 16 14:39:22.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename emptydir 05/16/23 14:39:22.244
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:22.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:22.27
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 05/16/23 14:39:22.273
    May 16 14:39:22.285: INFO: Waiting up to 5m0s for pod "pod-ab4708a5-a1d2-4c8e-8367-b0d051a82a9c" in namespace "emptydir-3991" to be "Succeeded or Failed"
    May 16 14:39:22.288: INFO: Pod "pod-ab4708a5-a1d2-4c8e-8367-b0d051a82a9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.395395ms
    May 16 14:39:24.291: INFO: Pod "pod-ab4708a5-a1d2-4c8e-8367-b0d051a82a9c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005747092s
    May 16 14:39:26.291: INFO: Pod "pod-ab4708a5-a1d2-4c8e-8367-b0d051a82a9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005863003s
    STEP: Saw pod success 05/16/23 14:39:26.291
    May 16 14:39:26.291: INFO: Pod "pod-ab4708a5-a1d2-4c8e-8367-b0d051a82a9c" satisfied condition "Succeeded or Failed"
    May 16 14:39:26.294: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod pod-ab4708a5-a1d2-4c8e-8367-b0d051a82a9c container test-container: <nil>
    STEP: delete the pod 05/16/23 14:39:26.305
    May 16 14:39:26.314: INFO: Waiting for pod pod-ab4708a5-a1d2-4c8e-8367-b0d051a82a9c to disappear
    May 16 14:39:26.316: INFO: Pod pod-ab4708a5-a1d2-4c8e-8367-b0d051a82a9c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 16 14:39:26.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3991" for this suite. 05/16/23 14:39:26.321
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:39:26.329
May 16 14:39:26.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename configmap 05/16/23 14:39:26.329
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:26.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:26.349
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-3771/configmap-test-a84696f6-b729-467d-bb82-1e0ff26432f7 05/16/23 14:39:26.353
STEP: Creating a pod to test consume configMaps 05/16/23 14:39:26.358
May 16 14:39:26.376: INFO: Waiting up to 5m0s for pod "pod-configmaps-347827c1-1664-46b6-a058-b5cecf4a703d" in namespace "configmap-3771" to be "Succeeded or Failed"
May 16 14:39:26.379: INFO: Pod "pod-configmaps-347827c1-1664-46b6-a058-b5cecf4a703d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.144713ms
May 16 14:39:28.383: INFO: Pod "pod-configmaps-347827c1-1664-46b6-a058-b5cecf4a703d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007106311s
May 16 14:39:30.384: INFO: Pod "pod-configmaps-347827c1-1664-46b6-a058-b5cecf4a703d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008633416s
STEP: Saw pod success 05/16/23 14:39:30.384
May 16 14:39:30.385: INFO: Pod "pod-configmaps-347827c1-1664-46b6-a058-b5cecf4a703d" satisfied condition "Succeeded or Failed"
May 16 14:39:30.399: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-configmaps-347827c1-1664-46b6-a058-b5cecf4a703d container env-test: <nil>
STEP: delete the pod 05/16/23 14:39:30.43
May 16 14:39:30.464: INFO: Waiting for pod pod-configmaps-347827c1-1664-46b6-a058-b5cecf4a703d to disappear
May 16 14:39:30.466: INFO: Pod pod-configmaps-347827c1-1664-46b6-a058-b5cecf4a703d no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 16 14:39:30.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3771" for this suite. 05/16/23 14:39:30.47
------------------------------
â€¢ [4.147 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:39:26.329
    May 16 14:39:26.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename configmap 05/16/23 14:39:26.329
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:26.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:26.349
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-3771/configmap-test-a84696f6-b729-467d-bb82-1e0ff26432f7 05/16/23 14:39:26.353
    STEP: Creating a pod to test consume configMaps 05/16/23 14:39:26.358
    May 16 14:39:26.376: INFO: Waiting up to 5m0s for pod "pod-configmaps-347827c1-1664-46b6-a058-b5cecf4a703d" in namespace "configmap-3771" to be "Succeeded or Failed"
    May 16 14:39:26.379: INFO: Pod "pod-configmaps-347827c1-1664-46b6-a058-b5cecf4a703d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.144713ms
    May 16 14:39:28.383: INFO: Pod "pod-configmaps-347827c1-1664-46b6-a058-b5cecf4a703d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007106311s
    May 16 14:39:30.384: INFO: Pod "pod-configmaps-347827c1-1664-46b6-a058-b5cecf4a703d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008633416s
    STEP: Saw pod success 05/16/23 14:39:30.384
    May 16 14:39:30.385: INFO: Pod "pod-configmaps-347827c1-1664-46b6-a058-b5cecf4a703d" satisfied condition "Succeeded or Failed"
    May 16 14:39:30.399: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-configmaps-347827c1-1664-46b6-a058-b5cecf4a703d container env-test: <nil>
    STEP: delete the pod 05/16/23 14:39:30.43
    May 16 14:39:30.464: INFO: Waiting for pod pod-configmaps-347827c1-1664-46b6-a058-b5cecf4a703d to disappear
    May 16 14:39:30.466: INFO: Pod pod-configmaps-347827c1-1664-46b6-a058-b5cecf4a703d no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 16 14:39:30.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3771" for this suite. 05/16/23 14:39:30.47
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:39:30.476
May 16 14:39:30.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename pods 05/16/23 14:39:30.477
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:30.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:30.497
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 05/16/23 14:39:30.5
STEP: setting up watch 05/16/23 14:39:30.5
STEP: submitting the pod to kubernetes 05/16/23 14:39:30.61
STEP: verifying the pod is in kubernetes 05/16/23 14:39:30.621
STEP: verifying pod creation was observed 05/16/23 14:39:30.624
May 16 14:39:30.624: INFO: Waiting up to 5m0s for pod "pod-submit-remove-54ee84f3-11f7-4706-a1c4-cc7a6d5cb365" in namespace "pods-3533" to be "running"
May 16 14:39:30.626: INFO: Pod "pod-submit-remove-54ee84f3-11f7-4706-a1c4-cc7a6d5cb365": Phase="Pending", Reason="", readiness=false. Elapsed: 2.40638ms
May 16 14:39:32.631: INFO: Pod "pod-submit-remove-54ee84f3-11f7-4706-a1c4-cc7a6d5cb365": Phase="Running", Reason="", readiness=true. Elapsed: 2.006669547s
May 16 14:39:32.631: INFO: Pod "pod-submit-remove-54ee84f3-11f7-4706-a1c4-cc7a6d5cb365" satisfied condition "running"
STEP: deleting the pod gracefully 05/16/23 14:39:32.633
STEP: verifying pod deletion was observed 05/16/23 14:39:32.64
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 16 14:39:34.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3533" for this suite. 05/16/23 14:39:34.886
------------------------------
â€¢ [4.416 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:39:30.476
    May 16 14:39:30.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename pods 05/16/23 14:39:30.477
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:30.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:30.497
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 05/16/23 14:39:30.5
    STEP: setting up watch 05/16/23 14:39:30.5
    STEP: submitting the pod to kubernetes 05/16/23 14:39:30.61
    STEP: verifying the pod is in kubernetes 05/16/23 14:39:30.621
    STEP: verifying pod creation was observed 05/16/23 14:39:30.624
    May 16 14:39:30.624: INFO: Waiting up to 5m0s for pod "pod-submit-remove-54ee84f3-11f7-4706-a1c4-cc7a6d5cb365" in namespace "pods-3533" to be "running"
    May 16 14:39:30.626: INFO: Pod "pod-submit-remove-54ee84f3-11f7-4706-a1c4-cc7a6d5cb365": Phase="Pending", Reason="", readiness=false. Elapsed: 2.40638ms
    May 16 14:39:32.631: INFO: Pod "pod-submit-remove-54ee84f3-11f7-4706-a1c4-cc7a6d5cb365": Phase="Running", Reason="", readiness=true. Elapsed: 2.006669547s
    May 16 14:39:32.631: INFO: Pod "pod-submit-remove-54ee84f3-11f7-4706-a1c4-cc7a6d5cb365" satisfied condition "running"
    STEP: deleting the pod gracefully 05/16/23 14:39:32.633
    STEP: verifying pod deletion was observed 05/16/23 14:39:32.64
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 16 14:39:34.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3533" for this suite. 05/16/23 14:39:34.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:39:34.893
May 16 14:39:34.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename sysctl 05/16/23 14:39:34.894
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:34.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:34.915
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 05/16/23 14:39:34.917
W0516 14:39:34.932495      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Watching for error events or started pod 05/16/23 14:39:34.932
STEP: Waiting for pod completion 05/16/23 14:39:36.937
May 16 14:39:36.937: INFO: Waiting up to 3m0s for pod "sysctl-67c87d74-b8e8-4d7f-b17a-c123070ae279" in namespace "sysctl-5971" to be "completed"
May 16 14:39:36.940: INFO: Pod "sysctl-67c87d74-b8e8-4d7f-b17a-c123070ae279": Phase="Pending", Reason="", readiness=false. Elapsed: 2.646539ms
May 16 14:39:38.954: INFO: Pod "sysctl-67c87d74-b8e8-4d7f-b17a-c123070ae279": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017255048s
May 16 14:39:38.954: INFO: Pod "sysctl-67c87d74-b8e8-4d7f-b17a-c123070ae279" satisfied condition "completed"
STEP: Checking that the pod succeeded 05/16/23 14:39:38.958
STEP: Getting logs from the pod 05/16/23 14:39:38.958
STEP: Checking that the sysctl is actually updated 05/16/23 14:39:38.963
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May 16 14:39:38.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-5971" for this suite. 05/16/23 14:39:38.967
------------------------------
â€¢ [4.080 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:39:34.893
    May 16 14:39:34.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename sysctl 05/16/23 14:39:34.894
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:34.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:34.915
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 05/16/23 14:39:34.917
    W0516 14:39:34.932495      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Watching for error events or started pod 05/16/23 14:39:34.932
    STEP: Waiting for pod completion 05/16/23 14:39:36.937
    May 16 14:39:36.937: INFO: Waiting up to 3m0s for pod "sysctl-67c87d74-b8e8-4d7f-b17a-c123070ae279" in namespace "sysctl-5971" to be "completed"
    May 16 14:39:36.940: INFO: Pod "sysctl-67c87d74-b8e8-4d7f-b17a-c123070ae279": Phase="Pending", Reason="", readiness=false. Elapsed: 2.646539ms
    May 16 14:39:38.954: INFO: Pod "sysctl-67c87d74-b8e8-4d7f-b17a-c123070ae279": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017255048s
    May 16 14:39:38.954: INFO: Pod "sysctl-67c87d74-b8e8-4d7f-b17a-c123070ae279" satisfied condition "completed"
    STEP: Checking that the pod succeeded 05/16/23 14:39:38.958
    STEP: Getting logs from the pod 05/16/23 14:39:38.958
    STEP: Checking that the sysctl is actually updated 05/16/23 14:39:38.963
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May 16 14:39:38.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-5971" for this suite. 05/16/23 14:39:38.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:39:38.973
May 16 14:39:38.973: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubectl 05/16/23 14:39:38.974
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:38.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:38.994
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 05/16/23 14:39:38.997
May 16 14:39:38.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 create -f -'
May 16 14:39:40.411: INFO: stderr: ""
May 16 14:39:40.411: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/16/23 14:39:40.411
May 16 14:39:40.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 16 14:39:40.468: INFO: stderr: ""
May 16 14:39:40.468: INFO: stdout: "update-demo-nautilus-g2b8z update-demo-nautilus-sq9qf "
May 16 14:39:40.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-g2b8z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 16 14:39:40.516: INFO: stderr: ""
May 16 14:39:40.516: INFO: stdout: ""
May 16 14:39:40.516: INFO: update-demo-nautilus-g2b8z is created but not running
May 16 14:39:45.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 16 14:39:45.566: INFO: stderr: ""
May 16 14:39:45.566: INFO: stdout: "update-demo-nautilus-g2b8z update-demo-nautilus-sq9qf "
May 16 14:39:45.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-g2b8z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 16 14:39:45.612: INFO: stderr: ""
May 16 14:39:45.612: INFO: stdout: "true"
May 16 14:39:45.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-g2b8z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 16 14:39:45.661: INFO: stderr: ""
May 16 14:39:45.661: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May 16 14:39:45.661: INFO: validating pod update-demo-nautilus-g2b8z
May 16 14:39:45.666: INFO: got data: {
  "image": "nautilus.jpg"
}

May 16 14:39:45.666: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 16 14:39:45.666: INFO: update-demo-nautilus-g2b8z is verified up and running
May 16 14:39:45.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-sq9qf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 16 14:39:45.716: INFO: stderr: ""
May 16 14:39:45.716: INFO: stdout: "true"
May 16 14:39:45.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-sq9qf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 16 14:39:45.763: INFO: stderr: ""
May 16 14:39:45.763: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May 16 14:39:45.763: INFO: validating pod update-demo-nautilus-sq9qf
May 16 14:39:45.770: INFO: got data: {
  "image": "nautilus.jpg"
}

May 16 14:39:45.770: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 16 14:39:45.770: INFO: update-demo-nautilus-sq9qf is verified up and running
STEP: scaling down the replication controller 05/16/23 14:39:45.77
May 16 14:39:45.771: INFO: scanned /root for discovery docs: <nil>
May 16 14:39:45.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May 16 14:39:46.836: INFO: stderr: ""
May 16 14:39:46.836: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/16/23 14:39:46.836
May 16 14:39:46.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 16 14:39:46.884: INFO: stderr: ""
May 16 14:39:46.884: INFO: stdout: "update-demo-nautilus-g2b8z "
May 16 14:39:46.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-g2b8z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 16 14:39:46.935: INFO: stderr: ""
May 16 14:39:46.935: INFO: stdout: "true"
May 16 14:39:46.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-g2b8z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 16 14:39:46.981: INFO: stderr: ""
May 16 14:39:46.981: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May 16 14:39:46.981: INFO: validating pod update-demo-nautilus-g2b8z
May 16 14:39:46.984: INFO: got data: {
  "image": "nautilus.jpg"
}

May 16 14:39:46.984: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 16 14:39:46.984: INFO: update-demo-nautilus-g2b8z is verified up and running
STEP: scaling up the replication controller 05/16/23 14:39:46.984
May 16 14:39:46.986: INFO: scanned /root for discovery docs: <nil>
May 16 14:39:46.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May 16 14:39:48.052: INFO: stderr: ""
May 16 14:39:48.052: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 05/16/23 14:39:48.052
May 16 14:39:48.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 16 14:39:48.104: INFO: stderr: ""
May 16 14:39:48.104: INFO: stdout: "update-demo-nautilus-g2b8z update-demo-nautilus-nlt6q "
May 16 14:39:48.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-g2b8z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 16 14:39:48.148: INFO: stderr: ""
May 16 14:39:48.148: INFO: stdout: "true"
May 16 14:39:48.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-g2b8z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 16 14:39:48.195: INFO: stderr: ""
May 16 14:39:48.195: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May 16 14:39:48.195: INFO: validating pod update-demo-nautilus-g2b8z
May 16 14:39:48.199: INFO: got data: {
  "image": "nautilus.jpg"
}

May 16 14:39:48.199: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 16 14:39:48.199: INFO: update-demo-nautilus-g2b8z is verified up and running
May 16 14:39:48.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-nlt6q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 16 14:39:48.248: INFO: stderr: ""
May 16 14:39:48.248: INFO: stdout: "true"
May 16 14:39:48.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-nlt6q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 16 14:39:48.298: INFO: stderr: ""
May 16 14:39:48.299: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
May 16 14:39:48.299: INFO: validating pod update-demo-nautilus-nlt6q
May 16 14:39:48.305: INFO: got data: {
  "image": "nautilus.jpg"
}

May 16 14:39:48.305: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 16 14:39:48.305: INFO: update-demo-nautilus-nlt6q is verified up and running
STEP: using delete to clean up resources 05/16/23 14:39:48.305
May 16 14:39:48.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 delete --grace-period=0 --force -f -'
May 16 14:39:48.353: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 14:39:48.353: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 16 14:39:48.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get rc,svc -l name=update-demo --no-headers'
May 16 14:39:48.405: INFO: stderr: "No resources found in kubectl-2138 namespace.\n"
May 16 14:39:48.405: INFO: stdout: ""
May 16 14:39:48.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 16 14:39:48.454: INFO: stderr: ""
May 16 14:39:48.454: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 16 14:39:48.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2138" for this suite. 05/16/23 14:39:48.458
------------------------------
â€¢ [SLOW TEST] [9.490 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:39:38.973
    May 16 14:39:38.973: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubectl 05/16/23 14:39:38.974
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:38.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:38.994
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 05/16/23 14:39:38.997
    May 16 14:39:38.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 create -f -'
    May 16 14:39:40.411: INFO: stderr: ""
    May 16 14:39:40.411: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/16/23 14:39:40.411
    May 16 14:39:40.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 16 14:39:40.468: INFO: stderr: ""
    May 16 14:39:40.468: INFO: stdout: "update-demo-nautilus-g2b8z update-demo-nautilus-sq9qf "
    May 16 14:39:40.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-g2b8z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 16 14:39:40.516: INFO: stderr: ""
    May 16 14:39:40.516: INFO: stdout: ""
    May 16 14:39:40.516: INFO: update-demo-nautilus-g2b8z is created but not running
    May 16 14:39:45.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 16 14:39:45.566: INFO: stderr: ""
    May 16 14:39:45.566: INFO: stdout: "update-demo-nautilus-g2b8z update-demo-nautilus-sq9qf "
    May 16 14:39:45.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-g2b8z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 16 14:39:45.612: INFO: stderr: ""
    May 16 14:39:45.612: INFO: stdout: "true"
    May 16 14:39:45.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-g2b8z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 16 14:39:45.661: INFO: stderr: ""
    May 16 14:39:45.661: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May 16 14:39:45.661: INFO: validating pod update-demo-nautilus-g2b8z
    May 16 14:39:45.666: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 16 14:39:45.666: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 16 14:39:45.666: INFO: update-demo-nautilus-g2b8z is verified up and running
    May 16 14:39:45.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-sq9qf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 16 14:39:45.716: INFO: stderr: ""
    May 16 14:39:45.716: INFO: stdout: "true"
    May 16 14:39:45.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-sq9qf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 16 14:39:45.763: INFO: stderr: ""
    May 16 14:39:45.763: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May 16 14:39:45.763: INFO: validating pod update-demo-nautilus-sq9qf
    May 16 14:39:45.770: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 16 14:39:45.770: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 16 14:39:45.770: INFO: update-demo-nautilus-sq9qf is verified up and running
    STEP: scaling down the replication controller 05/16/23 14:39:45.77
    May 16 14:39:45.771: INFO: scanned /root for discovery docs: <nil>
    May 16 14:39:45.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    May 16 14:39:46.836: INFO: stderr: ""
    May 16 14:39:46.836: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/16/23 14:39:46.836
    May 16 14:39:46.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 16 14:39:46.884: INFO: stderr: ""
    May 16 14:39:46.884: INFO: stdout: "update-demo-nautilus-g2b8z "
    May 16 14:39:46.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-g2b8z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 16 14:39:46.935: INFO: stderr: ""
    May 16 14:39:46.935: INFO: stdout: "true"
    May 16 14:39:46.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-g2b8z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 16 14:39:46.981: INFO: stderr: ""
    May 16 14:39:46.981: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May 16 14:39:46.981: INFO: validating pod update-demo-nautilus-g2b8z
    May 16 14:39:46.984: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 16 14:39:46.984: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 16 14:39:46.984: INFO: update-demo-nautilus-g2b8z is verified up and running
    STEP: scaling up the replication controller 05/16/23 14:39:46.984
    May 16 14:39:46.986: INFO: scanned /root for discovery docs: <nil>
    May 16 14:39:46.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    May 16 14:39:48.052: INFO: stderr: ""
    May 16 14:39:48.052: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 05/16/23 14:39:48.052
    May 16 14:39:48.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    May 16 14:39:48.104: INFO: stderr: ""
    May 16 14:39:48.104: INFO: stdout: "update-demo-nautilus-g2b8z update-demo-nautilus-nlt6q "
    May 16 14:39:48.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-g2b8z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 16 14:39:48.148: INFO: stderr: ""
    May 16 14:39:48.148: INFO: stdout: "true"
    May 16 14:39:48.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-g2b8z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 16 14:39:48.195: INFO: stderr: ""
    May 16 14:39:48.195: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May 16 14:39:48.195: INFO: validating pod update-demo-nautilus-g2b8z
    May 16 14:39:48.199: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 16 14:39:48.199: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 16 14:39:48.199: INFO: update-demo-nautilus-g2b8z is verified up and running
    May 16 14:39:48.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-nlt6q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    May 16 14:39:48.248: INFO: stderr: ""
    May 16 14:39:48.248: INFO: stdout: "true"
    May 16 14:39:48.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods update-demo-nautilus-nlt6q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    May 16 14:39:48.298: INFO: stderr: ""
    May 16 14:39:48.299: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    May 16 14:39:48.299: INFO: validating pod update-demo-nautilus-nlt6q
    May 16 14:39:48.305: INFO: got data: {
      "image": "nautilus.jpg"
    }

    May 16 14:39:48.305: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    May 16 14:39:48.305: INFO: update-demo-nautilus-nlt6q is verified up and running
    STEP: using delete to clean up resources 05/16/23 14:39:48.305
    May 16 14:39:48.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 delete --grace-period=0 --force -f -'
    May 16 14:39:48.353: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 16 14:39:48.353: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    May 16 14:39:48.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get rc,svc -l name=update-demo --no-headers'
    May 16 14:39:48.405: INFO: stderr: "No resources found in kubectl-2138 namespace.\n"
    May 16 14:39:48.405: INFO: stdout: ""
    May 16 14:39:48.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2138 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May 16 14:39:48.454: INFO: stderr: ""
    May 16 14:39:48.454: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 16 14:39:48.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2138" for this suite. 05/16/23 14:39:48.458
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:39:48.464
May 16 14:39:48.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename resourcequota 05/16/23 14:39:48.464
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:48.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:48.491
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 05/16/23 14:39:48.494
STEP: Creating a ResourceQuota 05/16/23 14:39:53.516
STEP: Ensuring resource quota status is calculated 05/16/23 14:39:53.521
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 16 14:39:55.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6305" for this suite. 05/16/23 14:39:55.53
------------------------------
â€¢ [SLOW TEST] [7.072 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:39:48.464
    May 16 14:39:48.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename resourcequota 05/16/23 14:39:48.464
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:48.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:48.491
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 05/16/23 14:39:48.494
    STEP: Creating a ResourceQuota 05/16/23 14:39:53.516
    STEP: Ensuring resource quota status is calculated 05/16/23 14:39:53.521
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 16 14:39:55.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6305" for this suite. 05/16/23 14:39:55.53
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:39:55.536
May 16 14:39:55.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 14:39:55.537
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:55.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:55.561
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-9a6eaeb5-7d44-4291-85c3-1db2ecc273f4 05/16/23 14:39:55.563
STEP: Creating a pod to test consume configMaps 05/16/23 14:39:55.569
W0516 14:39:55.582565      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-configmap-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-configmap-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-configmap-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-configmap-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:39:55.582: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c291cfd2-7bcd-46f5-8754-e85f35f2433d" in namespace "projected-6243" to be "Succeeded or Failed"
May 16 14:39:55.588: INFO: Pod "pod-projected-configmaps-c291cfd2-7bcd-46f5-8754-e85f35f2433d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.859596ms
May 16 14:39:57.592: INFO: Pod "pod-projected-configmaps-c291cfd2-7bcd-46f5-8754-e85f35f2433d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010000939s
May 16 14:39:59.592: INFO: Pod "pod-projected-configmaps-c291cfd2-7bcd-46f5-8754-e85f35f2433d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010013632s
STEP: Saw pod success 05/16/23 14:39:59.592
May 16 14:39:59.592: INFO: Pod "pod-projected-configmaps-c291cfd2-7bcd-46f5-8754-e85f35f2433d" satisfied condition "Succeeded or Failed"
May 16 14:39:59.595: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-configmaps-c291cfd2-7bcd-46f5-8754-e85f35f2433d container projected-configmap-volume-test: <nil>
STEP: delete the pod 05/16/23 14:39:59.601
May 16 14:39:59.610: INFO: Waiting for pod pod-projected-configmaps-c291cfd2-7bcd-46f5-8754-e85f35f2433d to disappear
May 16 14:39:59.612: INFO: Pod pod-projected-configmaps-c291cfd2-7bcd-46f5-8754-e85f35f2433d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May 16 14:39:59.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6243" for this suite. 05/16/23 14:39:59.616
------------------------------
â€¢ [4.086 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:39:55.536
    May 16 14:39:55.536: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 14:39:55.537
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:55.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:55.561
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-9a6eaeb5-7d44-4291-85c3-1db2ecc273f4 05/16/23 14:39:55.563
    STEP: Creating a pod to test consume configMaps 05/16/23 14:39:55.569
    W0516 14:39:55.582565      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-configmap-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-configmap-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-configmap-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-configmap-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:39:55.582: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c291cfd2-7bcd-46f5-8754-e85f35f2433d" in namespace "projected-6243" to be "Succeeded or Failed"
    May 16 14:39:55.588: INFO: Pod "pod-projected-configmaps-c291cfd2-7bcd-46f5-8754-e85f35f2433d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.859596ms
    May 16 14:39:57.592: INFO: Pod "pod-projected-configmaps-c291cfd2-7bcd-46f5-8754-e85f35f2433d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010000939s
    May 16 14:39:59.592: INFO: Pod "pod-projected-configmaps-c291cfd2-7bcd-46f5-8754-e85f35f2433d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010013632s
    STEP: Saw pod success 05/16/23 14:39:59.592
    May 16 14:39:59.592: INFO: Pod "pod-projected-configmaps-c291cfd2-7bcd-46f5-8754-e85f35f2433d" satisfied condition "Succeeded or Failed"
    May 16 14:39:59.595: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-configmaps-c291cfd2-7bcd-46f5-8754-e85f35f2433d container projected-configmap-volume-test: <nil>
    STEP: delete the pod 05/16/23 14:39:59.601
    May 16 14:39:59.610: INFO: Waiting for pod pod-projected-configmaps-c291cfd2-7bcd-46f5-8754-e85f35f2433d to disappear
    May 16 14:39:59.612: INFO: Pod pod-projected-configmaps-c291cfd2-7bcd-46f5-8754-e85f35f2433d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May 16 14:39:59.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6243" for this suite. 05/16/23 14:39:59.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:39:59.625
May 16 14:39:59.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename discovery 05/16/23 14:39:59.625
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:59.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:59.645
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 05/16/23 14:39:59.648
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
May 16 14:39:59.828: INFO: Checking APIGroup: apiregistration.k8s.io
May 16 14:39:59.829: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May 16 14:39:59.829: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
May 16 14:39:59.829: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May 16 14:39:59.829: INFO: Checking APIGroup: apps
May 16 14:39:59.830: INFO: PreferredVersion.GroupVersion: apps/v1
May 16 14:39:59.830: INFO: Versions found [{apps/v1 v1}]
May 16 14:39:59.830: INFO: apps/v1 matches apps/v1
May 16 14:39:59.830: INFO: Checking APIGroup: events.k8s.io
May 16 14:39:59.831: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May 16 14:39:59.831: INFO: Versions found [{events.k8s.io/v1 v1}]
May 16 14:39:59.831: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May 16 14:39:59.831: INFO: Checking APIGroup: authentication.k8s.io
May 16 14:39:59.831: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May 16 14:39:59.831: INFO: Versions found [{authentication.k8s.io/v1 v1}]
May 16 14:39:59.831: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May 16 14:39:59.831: INFO: Checking APIGroup: authorization.k8s.io
May 16 14:39:59.832: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May 16 14:39:59.832: INFO: Versions found [{authorization.k8s.io/v1 v1}]
May 16 14:39:59.832: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May 16 14:39:59.832: INFO: Checking APIGroup: autoscaling
May 16 14:39:59.833: INFO: PreferredVersion.GroupVersion: autoscaling/v2
May 16 14:39:59.833: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
May 16 14:39:59.833: INFO: autoscaling/v2 matches autoscaling/v2
May 16 14:39:59.833: INFO: Checking APIGroup: batch
May 16 14:39:59.834: INFO: PreferredVersion.GroupVersion: batch/v1
May 16 14:39:59.834: INFO: Versions found [{batch/v1 v1}]
May 16 14:39:59.834: INFO: batch/v1 matches batch/v1
May 16 14:39:59.834: INFO: Checking APIGroup: certificates.k8s.io
May 16 14:39:59.835: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May 16 14:39:59.835: INFO: Versions found [{certificates.k8s.io/v1 v1}]
May 16 14:39:59.835: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May 16 14:39:59.835: INFO: Checking APIGroup: networking.k8s.io
May 16 14:39:59.836: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May 16 14:39:59.836: INFO: Versions found [{networking.k8s.io/v1 v1}]
May 16 14:39:59.836: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May 16 14:39:59.836: INFO: Checking APIGroup: policy
May 16 14:39:59.836: INFO: PreferredVersion.GroupVersion: policy/v1
May 16 14:39:59.836: INFO: Versions found [{policy/v1 v1}]
May 16 14:39:59.836: INFO: policy/v1 matches policy/v1
May 16 14:39:59.836: INFO: Checking APIGroup: rbac.authorization.k8s.io
May 16 14:39:59.837: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May 16 14:39:59.837: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
May 16 14:39:59.837: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May 16 14:39:59.837: INFO: Checking APIGroup: storage.k8s.io
May 16 14:39:59.838: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May 16 14:39:59.838: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May 16 14:39:59.838: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May 16 14:39:59.838: INFO: Checking APIGroup: admissionregistration.k8s.io
May 16 14:39:59.839: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May 16 14:39:59.839: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
May 16 14:39:59.839: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May 16 14:39:59.839: INFO: Checking APIGroup: apiextensions.k8s.io
May 16 14:39:59.840: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May 16 14:39:59.840: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
May 16 14:39:59.840: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May 16 14:39:59.840: INFO: Checking APIGroup: scheduling.k8s.io
May 16 14:39:59.841: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May 16 14:39:59.841: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
May 16 14:39:59.841: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May 16 14:39:59.841: INFO: Checking APIGroup: coordination.k8s.io
May 16 14:39:59.842: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May 16 14:39:59.842: INFO: Versions found [{coordination.k8s.io/v1 v1}]
May 16 14:39:59.842: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May 16 14:39:59.842: INFO: Checking APIGroup: node.k8s.io
May 16 14:39:59.843: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May 16 14:39:59.843: INFO: Versions found [{node.k8s.io/v1 v1}]
May 16 14:39:59.843: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May 16 14:39:59.843: INFO: Checking APIGroup: discovery.k8s.io
May 16 14:39:59.843: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
May 16 14:39:59.843: INFO: Versions found [{discovery.k8s.io/v1 v1}]
May 16 14:39:59.843: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
May 16 14:39:59.843: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May 16 14:39:59.844: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
May 16 14:39:59.844: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
May 16 14:39:59.844: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
May 16 14:39:59.844: INFO: Checking APIGroup: apps.openshift.io
May 16 14:39:59.845: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
May 16 14:39:59.845: INFO: Versions found [{apps.openshift.io/v1 v1}]
May 16 14:39:59.845: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
May 16 14:39:59.845: INFO: Checking APIGroup: authorization.openshift.io
May 16 14:39:59.846: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
May 16 14:39:59.846: INFO: Versions found [{authorization.openshift.io/v1 v1}]
May 16 14:39:59.846: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
May 16 14:39:59.846: INFO: Checking APIGroup: build.openshift.io
May 16 14:39:59.847: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
May 16 14:39:59.847: INFO: Versions found [{build.openshift.io/v1 v1}]
May 16 14:39:59.847: INFO: build.openshift.io/v1 matches build.openshift.io/v1
May 16 14:39:59.847: INFO: Checking APIGroup: image.openshift.io
May 16 14:39:59.848: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
May 16 14:39:59.848: INFO: Versions found [{image.openshift.io/v1 v1}]
May 16 14:39:59.848: INFO: image.openshift.io/v1 matches image.openshift.io/v1
May 16 14:39:59.848: INFO: Checking APIGroup: oauth.openshift.io
May 16 14:39:59.849: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
May 16 14:39:59.849: INFO: Versions found [{oauth.openshift.io/v1 v1}]
May 16 14:39:59.849: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
May 16 14:39:59.849: INFO: Checking APIGroup: project.openshift.io
May 16 14:39:59.850: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
May 16 14:39:59.850: INFO: Versions found [{project.openshift.io/v1 v1}]
May 16 14:39:59.850: INFO: project.openshift.io/v1 matches project.openshift.io/v1
May 16 14:39:59.850: INFO: Checking APIGroup: quota.openshift.io
May 16 14:39:59.850: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
May 16 14:39:59.850: INFO: Versions found [{quota.openshift.io/v1 v1}]
May 16 14:39:59.850: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
May 16 14:39:59.850: INFO: Checking APIGroup: route.openshift.io
May 16 14:39:59.851: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
May 16 14:39:59.851: INFO: Versions found [{route.openshift.io/v1 v1}]
May 16 14:39:59.851: INFO: route.openshift.io/v1 matches route.openshift.io/v1
May 16 14:39:59.851: INFO: Checking APIGroup: security.openshift.io
May 16 14:39:59.852: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
May 16 14:39:59.852: INFO: Versions found [{security.openshift.io/v1 v1}]
May 16 14:39:59.852: INFO: security.openshift.io/v1 matches security.openshift.io/v1
May 16 14:39:59.852: INFO: Checking APIGroup: template.openshift.io
May 16 14:39:59.853: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
May 16 14:39:59.853: INFO: Versions found [{template.openshift.io/v1 v1}]
May 16 14:39:59.853: INFO: template.openshift.io/v1 matches template.openshift.io/v1
May 16 14:39:59.853: INFO: Checking APIGroup: user.openshift.io
May 16 14:39:59.854: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
May 16 14:39:59.854: INFO: Versions found [{user.openshift.io/v1 v1}]
May 16 14:39:59.854: INFO: user.openshift.io/v1 matches user.openshift.io/v1
May 16 14:39:59.854: INFO: Checking APIGroup: packages.operators.coreos.com
May 16 14:39:59.854: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
May 16 14:39:59.854: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
May 16 14:39:59.854: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
May 16 14:39:59.854: INFO: Checking APIGroup: config.openshift.io
May 16 14:39:59.855: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
May 16 14:39:59.855: INFO: Versions found [{config.openshift.io/v1 v1}]
May 16 14:39:59.855: INFO: config.openshift.io/v1 matches config.openshift.io/v1
May 16 14:39:59.855: INFO: Checking APIGroup: operator.openshift.io
May 16 14:39:59.856: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
May 16 14:39:59.856: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
May 16 14:39:59.856: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
May 16 14:39:59.856: INFO: Checking APIGroup: apiserver.openshift.io
May 16 14:39:59.857: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
May 16 14:39:59.857: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
May 16 14:39:59.857: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
May 16 14:39:59.857: INFO: Checking APIGroup: autoscaling.openshift.io
May 16 14:39:59.857: INFO: PreferredVersion.GroupVersion: autoscaling.openshift.io/v1
May 16 14:39:59.857: INFO: Versions found [{autoscaling.openshift.io/v1 v1} {autoscaling.openshift.io/v1beta1 v1beta1}]
May 16 14:39:59.857: INFO: autoscaling.openshift.io/v1 matches autoscaling.openshift.io/v1
May 16 14:39:59.857: INFO: Checking APIGroup: cloud.network.openshift.io
May 16 14:39:59.858: INFO: PreferredVersion.GroupVersion: cloud.network.openshift.io/v1
May 16 14:39:59.858: INFO: Versions found [{cloud.network.openshift.io/v1 v1}]
May 16 14:39:59.858: INFO: cloud.network.openshift.io/v1 matches cloud.network.openshift.io/v1
May 16 14:39:59.858: INFO: Checking APIGroup: cloudcredential.openshift.io
May 16 14:39:59.859: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
May 16 14:39:59.859: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
May 16 14:39:59.859: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
May 16 14:39:59.859: INFO: Checking APIGroup: console.openshift.io
May 16 14:39:59.860: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
May 16 14:39:59.860: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
May 16 14:39:59.860: INFO: console.openshift.io/v1 matches console.openshift.io/v1
May 16 14:39:59.860: INFO: Checking APIGroup: imageregistry.operator.openshift.io
May 16 14:39:59.861: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
May 16 14:39:59.861: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
May 16 14:39:59.861: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
May 16 14:39:59.861: INFO: Checking APIGroup: ingress.operator.openshift.io
May 16 14:39:59.861: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
May 16 14:39:59.861: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
May 16 14:39:59.861: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
May 16 14:39:59.861: INFO: Checking APIGroup: k8s.cni.cncf.io
May 16 14:39:59.862: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
May 16 14:39:59.862: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
May 16 14:39:59.862: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
May 16 14:39:59.862: INFO: Checking APIGroup: k8s.ovn.org
May 16 14:39:59.863: INFO: PreferredVersion.GroupVersion: k8s.ovn.org/v1
May 16 14:39:59.863: INFO: Versions found [{k8s.ovn.org/v1 v1}]
May 16 14:39:59.863: INFO: k8s.ovn.org/v1 matches k8s.ovn.org/v1
May 16 14:39:59.863: INFO: Checking APIGroup: machine.openshift.io
May 16 14:39:59.864: INFO: PreferredVersion.GroupVersion: machine.openshift.io/v1
May 16 14:39:59.864: INFO: Versions found [{machine.openshift.io/v1 v1} {machine.openshift.io/v1beta1 v1beta1}]
May 16 14:39:59.864: INFO: machine.openshift.io/v1 matches machine.openshift.io/v1
May 16 14:39:59.864: INFO: Checking APIGroup: machineconfiguration.openshift.io
May 16 14:39:59.865: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
May 16 14:39:59.865: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
May 16 14:39:59.865: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
May 16 14:39:59.865: INFO: Checking APIGroup: monitoring.coreos.com
May 16 14:39:59.865: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
May 16 14:39:59.865: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
May 16 14:39:59.865: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
May 16 14:39:59.865: INFO: Checking APIGroup: network.operator.openshift.io
May 16 14:39:59.866: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
May 16 14:39:59.866: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
May 16 14:39:59.866: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
May 16 14:39:59.866: INFO: Checking APIGroup: operators.coreos.com
May 16 14:39:59.867: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
May 16 14:39:59.867: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
May 16 14:39:59.867: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
May 16 14:39:59.867: INFO: Checking APIGroup: performance.openshift.io
May 16 14:39:59.868: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
May 16 14:39:59.868: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
May 16 14:39:59.868: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
May 16 14:39:59.868: INFO: Checking APIGroup: samples.operator.openshift.io
May 16 14:39:59.877: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
May 16 14:39:59.877: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
May 16 14:39:59.877: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
May 16 14:39:59.877: INFO: Checking APIGroup: security.internal.openshift.io
May 16 14:39:59.927: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
May 16 14:39:59.927: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
May 16 14:39:59.927: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
May 16 14:39:59.927: INFO: Checking APIGroup: snapshot.storage.k8s.io
May 16 14:39:59.978: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
May 16 14:39:59.978: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
May 16 14:39:59.978: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
May 16 14:39:59.978: INFO: Checking APIGroup: tuned.openshift.io
May 16 14:40:00.027: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
May 16 14:40:00.027: INFO: Versions found [{tuned.openshift.io/v1 v1}]
May 16 14:40:00.027: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
May 16 14:40:00.027: INFO: Checking APIGroup: controlplane.operator.openshift.io
May 16 14:40:00.077: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
May 16 14:40:00.078: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
May 16 14:40:00.078: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
May 16 14:40:00.078: INFO: Checking APIGroup: metal3.io
May 16 14:40:00.127: INFO: PreferredVersion.GroupVersion: metal3.io/v1alpha1
May 16 14:40:00.127: INFO: Versions found [{metal3.io/v1alpha1 v1alpha1}]
May 16 14:40:00.127: INFO: metal3.io/v1alpha1 matches metal3.io/v1alpha1
May 16 14:40:00.127: INFO: Checking APIGroup: migration.k8s.io
May 16 14:40:00.177: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
May 16 14:40:00.177: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
May 16 14:40:00.177: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
May 16 14:40:00.177: INFO: Checking APIGroup: whereabouts.cni.cncf.io
May 16 14:40:00.228: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
May 16 14:40:00.228: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
May 16 14:40:00.228: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
May 16 14:40:00.228: INFO: Checking APIGroup: infrastructure.cluster.x-k8s.io
May 16 14:40:00.277: INFO: PreferredVersion.GroupVersion: infrastructure.cluster.x-k8s.io/v1beta1
May 16 14:40:00.277: INFO: Versions found [{infrastructure.cluster.x-k8s.io/v1beta1 v1beta1} {infrastructure.cluster.x-k8s.io/v1alpha5 v1alpha5}]
May 16 14:40:00.277: INFO: infrastructure.cluster.x-k8s.io/v1beta1 matches infrastructure.cluster.x-k8s.io/v1beta1
May 16 14:40:00.277: INFO: Checking APIGroup: helm.openshift.io
May 16 14:40:00.327: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
May 16 14:40:00.327: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
May 16 14:40:00.327: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
May 16 14:40:00.327: INFO: Checking APIGroup: metrics.k8s.io
May 16 14:40:00.377: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
May 16 14:40:00.377: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
May 16 14:40:00.377: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
May 16 14:40:00.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-3975" for this suite. 05/16/23 14:40:00.431
------------------------------
â€¢ [0.858 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:39:59.625
    May 16 14:39:59.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename discovery 05/16/23 14:39:59.625
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:39:59.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:39:59.645
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 05/16/23 14:39:59.648
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    May 16 14:39:59.828: INFO: Checking APIGroup: apiregistration.k8s.io
    May 16 14:39:59.829: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    May 16 14:39:59.829: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    May 16 14:39:59.829: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    May 16 14:39:59.829: INFO: Checking APIGroup: apps
    May 16 14:39:59.830: INFO: PreferredVersion.GroupVersion: apps/v1
    May 16 14:39:59.830: INFO: Versions found [{apps/v1 v1}]
    May 16 14:39:59.830: INFO: apps/v1 matches apps/v1
    May 16 14:39:59.830: INFO: Checking APIGroup: events.k8s.io
    May 16 14:39:59.831: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    May 16 14:39:59.831: INFO: Versions found [{events.k8s.io/v1 v1}]
    May 16 14:39:59.831: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    May 16 14:39:59.831: INFO: Checking APIGroup: authentication.k8s.io
    May 16 14:39:59.831: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    May 16 14:39:59.831: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    May 16 14:39:59.831: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    May 16 14:39:59.831: INFO: Checking APIGroup: authorization.k8s.io
    May 16 14:39:59.832: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    May 16 14:39:59.832: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    May 16 14:39:59.832: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    May 16 14:39:59.832: INFO: Checking APIGroup: autoscaling
    May 16 14:39:59.833: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    May 16 14:39:59.833: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    May 16 14:39:59.833: INFO: autoscaling/v2 matches autoscaling/v2
    May 16 14:39:59.833: INFO: Checking APIGroup: batch
    May 16 14:39:59.834: INFO: PreferredVersion.GroupVersion: batch/v1
    May 16 14:39:59.834: INFO: Versions found [{batch/v1 v1}]
    May 16 14:39:59.834: INFO: batch/v1 matches batch/v1
    May 16 14:39:59.834: INFO: Checking APIGroup: certificates.k8s.io
    May 16 14:39:59.835: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    May 16 14:39:59.835: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    May 16 14:39:59.835: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    May 16 14:39:59.835: INFO: Checking APIGroup: networking.k8s.io
    May 16 14:39:59.836: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    May 16 14:39:59.836: INFO: Versions found [{networking.k8s.io/v1 v1}]
    May 16 14:39:59.836: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    May 16 14:39:59.836: INFO: Checking APIGroup: policy
    May 16 14:39:59.836: INFO: PreferredVersion.GroupVersion: policy/v1
    May 16 14:39:59.836: INFO: Versions found [{policy/v1 v1}]
    May 16 14:39:59.836: INFO: policy/v1 matches policy/v1
    May 16 14:39:59.836: INFO: Checking APIGroup: rbac.authorization.k8s.io
    May 16 14:39:59.837: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    May 16 14:39:59.837: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    May 16 14:39:59.837: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    May 16 14:39:59.837: INFO: Checking APIGroup: storage.k8s.io
    May 16 14:39:59.838: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    May 16 14:39:59.838: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    May 16 14:39:59.838: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    May 16 14:39:59.838: INFO: Checking APIGroup: admissionregistration.k8s.io
    May 16 14:39:59.839: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    May 16 14:39:59.839: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    May 16 14:39:59.839: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    May 16 14:39:59.839: INFO: Checking APIGroup: apiextensions.k8s.io
    May 16 14:39:59.840: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    May 16 14:39:59.840: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    May 16 14:39:59.840: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    May 16 14:39:59.840: INFO: Checking APIGroup: scheduling.k8s.io
    May 16 14:39:59.841: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    May 16 14:39:59.841: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    May 16 14:39:59.841: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    May 16 14:39:59.841: INFO: Checking APIGroup: coordination.k8s.io
    May 16 14:39:59.842: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    May 16 14:39:59.842: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    May 16 14:39:59.842: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    May 16 14:39:59.842: INFO: Checking APIGroup: node.k8s.io
    May 16 14:39:59.843: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    May 16 14:39:59.843: INFO: Versions found [{node.k8s.io/v1 v1}]
    May 16 14:39:59.843: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    May 16 14:39:59.843: INFO: Checking APIGroup: discovery.k8s.io
    May 16 14:39:59.843: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    May 16 14:39:59.843: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    May 16 14:39:59.843: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    May 16 14:39:59.843: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    May 16 14:39:59.844: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    May 16 14:39:59.844: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    May 16 14:39:59.844: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    May 16 14:39:59.844: INFO: Checking APIGroup: apps.openshift.io
    May 16 14:39:59.845: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
    May 16 14:39:59.845: INFO: Versions found [{apps.openshift.io/v1 v1}]
    May 16 14:39:59.845: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
    May 16 14:39:59.845: INFO: Checking APIGroup: authorization.openshift.io
    May 16 14:39:59.846: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
    May 16 14:39:59.846: INFO: Versions found [{authorization.openshift.io/v1 v1}]
    May 16 14:39:59.846: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
    May 16 14:39:59.846: INFO: Checking APIGroup: build.openshift.io
    May 16 14:39:59.847: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
    May 16 14:39:59.847: INFO: Versions found [{build.openshift.io/v1 v1}]
    May 16 14:39:59.847: INFO: build.openshift.io/v1 matches build.openshift.io/v1
    May 16 14:39:59.847: INFO: Checking APIGroup: image.openshift.io
    May 16 14:39:59.848: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
    May 16 14:39:59.848: INFO: Versions found [{image.openshift.io/v1 v1}]
    May 16 14:39:59.848: INFO: image.openshift.io/v1 matches image.openshift.io/v1
    May 16 14:39:59.848: INFO: Checking APIGroup: oauth.openshift.io
    May 16 14:39:59.849: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
    May 16 14:39:59.849: INFO: Versions found [{oauth.openshift.io/v1 v1}]
    May 16 14:39:59.849: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
    May 16 14:39:59.849: INFO: Checking APIGroup: project.openshift.io
    May 16 14:39:59.850: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
    May 16 14:39:59.850: INFO: Versions found [{project.openshift.io/v1 v1}]
    May 16 14:39:59.850: INFO: project.openshift.io/v1 matches project.openshift.io/v1
    May 16 14:39:59.850: INFO: Checking APIGroup: quota.openshift.io
    May 16 14:39:59.850: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
    May 16 14:39:59.850: INFO: Versions found [{quota.openshift.io/v1 v1}]
    May 16 14:39:59.850: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
    May 16 14:39:59.850: INFO: Checking APIGroup: route.openshift.io
    May 16 14:39:59.851: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
    May 16 14:39:59.851: INFO: Versions found [{route.openshift.io/v1 v1}]
    May 16 14:39:59.851: INFO: route.openshift.io/v1 matches route.openshift.io/v1
    May 16 14:39:59.851: INFO: Checking APIGroup: security.openshift.io
    May 16 14:39:59.852: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
    May 16 14:39:59.852: INFO: Versions found [{security.openshift.io/v1 v1}]
    May 16 14:39:59.852: INFO: security.openshift.io/v1 matches security.openshift.io/v1
    May 16 14:39:59.852: INFO: Checking APIGroup: template.openshift.io
    May 16 14:39:59.853: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
    May 16 14:39:59.853: INFO: Versions found [{template.openshift.io/v1 v1}]
    May 16 14:39:59.853: INFO: template.openshift.io/v1 matches template.openshift.io/v1
    May 16 14:39:59.853: INFO: Checking APIGroup: user.openshift.io
    May 16 14:39:59.854: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
    May 16 14:39:59.854: INFO: Versions found [{user.openshift.io/v1 v1}]
    May 16 14:39:59.854: INFO: user.openshift.io/v1 matches user.openshift.io/v1
    May 16 14:39:59.854: INFO: Checking APIGroup: packages.operators.coreos.com
    May 16 14:39:59.854: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
    May 16 14:39:59.854: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
    May 16 14:39:59.854: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
    May 16 14:39:59.854: INFO: Checking APIGroup: config.openshift.io
    May 16 14:39:59.855: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
    May 16 14:39:59.855: INFO: Versions found [{config.openshift.io/v1 v1}]
    May 16 14:39:59.855: INFO: config.openshift.io/v1 matches config.openshift.io/v1
    May 16 14:39:59.855: INFO: Checking APIGroup: operator.openshift.io
    May 16 14:39:59.856: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
    May 16 14:39:59.856: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
    May 16 14:39:59.856: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
    May 16 14:39:59.856: INFO: Checking APIGroup: apiserver.openshift.io
    May 16 14:39:59.857: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
    May 16 14:39:59.857: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
    May 16 14:39:59.857: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
    May 16 14:39:59.857: INFO: Checking APIGroup: autoscaling.openshift.io
    May 16 14:39:59.857: INFO: PreferredVersion.GroupVersion: autoscaling.openshift.io/v1
    May 16 14:39:59.857: INFO: Versions found [{autoscaling.openshift.io/v1 v1} {autoscaling.openshift.io/v1beta1 v1beta1}]
    May 16 14:39:59.857: INFO: autoscaling.openshift.io/v1 matches autoscaling.openshift.io/v1
    May 16 14:39:59.857: INFO: Checking APIGroup: cloud.network.openshift.io
    May 16 14:39:59.858: INFO: PreferredVersion.GroupVersion: cloud.network.openshift.io/v1
    May 16 14:39:59.858: INFO: Versions found [{cloud.network.openshift.io/v1 v1}]
    May 16 14:39:59.858: INFO: cloud.network.openshift.io/v1 matches cloud.network.openshift.io/v1
    May 16 14:39:59.858: INFO: Checking APIGroup: cloudcredential.openshift.io
    May 16 14:39:59.859: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
    May 16 14:39:59.859: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
    May 16 14:39:59.859: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
    May 16 14:39:59.859: INFO: Checking APIGroup: console.openshift.io
    May 16 14:39:59.860: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
    May 16 14:39:59.860: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
    May 16 14:39:59.860: INFO: console.openshift.io/v1 matches console.openshift.io/v1
    May 16 14:39:59.860: INFO: Checking APIGroup: imageregistry.operator.openshift.io
    May 16 14:39:59.861: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
    May 16 14:39:59.861: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
    May 16 14:39:59.861: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
    May 16 14:39:59.861: INFO: Checking APIGroup: ingress.operator.openshift.io
    May 16 14:39:59.861: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
    May 16 14:39:59.861: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
    May 16 14:39:59.861: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
    May 16 14:39:59.861: INFO: Checking APIGroup: k8s.cni.cncf.io
    May 16 14:39:59.862: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
    May 16 14:39:59.862: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
    May 16 14:39:59.862: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
    May 16 14:39:59.862: INFO: Checking APIGroup: k8s.ovn.org
    May 16 14:39:59.863: INFO: PreferredVersion.GroupVersion: k8s.ovn.org/v1
    May 16 14:39:59.863: INFO: Versions found [{k8s.ovn.org/v1 v1}]
    May 16 14:39:59.863: INFO: k8s.ovn.org/v1 matches k8s.ovn.org/v1
    May 16 14:39:59.863: INFO: Checking APIGroup: machine.openshift.io
    May 16 14:39:59.864: INFO: PreferredVersion.GroupVersion: machine.openshift.io/v1
    May 16 14:39:59.864: INFO: Versions found [{machine.openshift.io/v1 v1} {machine.openshift.io/v1beta1 v1beta1}]
    May 16 14:39:59.864: INFO: machine.openshift.io/v1 matches machine.openshift.io/v1
    May 16 14:39:59.864: INFO: Checking APIGroup: machineconfiguration.openshift.io
    May 16 14:39:59.865: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
    May 16 14:39:59.865: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
    May 16 14:39:59.865: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
    May 16 14:39:59.865: INFO: Checking APIGroup: monitoring.coreos.com
    May 16 14:39:59.865: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    May 16 14:39:59.865: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    May 16 14:39:59.865: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    May 16 14:39:59.865: INFO: Checking APIGroup: network.operator.openshift.io
    May 16 14:39:59.866: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
    May 16 14:39:59.866: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
    May 16 14:39:59.866: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
    May 16 14:39:59.866: INFO: Checking APIGroup: operators.coreos.com
    May 16 14:39:59.867: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
    May 16 14:39:59.867: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
    May 16 14:39:59.867: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
    May 16 14:39:59.867: INFO: Checking APIGroup: performance.openshift.io
    May 16 14:39:59.868: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
    May 16 14:39:59.868: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
    May 16 14:39:59.868: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
    May 16 14:39:59.868: INFO: Checking APIGroup: samples.operator.openshift.io
    May 16 14:39:59.877: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
    May 16 14:39:59.877: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
    May 16 14:39:59.877: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
    May 16 14:39:59.877: INFO: Checking APIGroup: security.internal.openshift.io
    May 16 14:39:59.927: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
    May 16 14:39:59.927: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
    May 16 14:39:59.927: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
    May 16 14:39:59.927: INFO: Checking APIGroup: snapshot.storage.k8s.io
    May 16 14:39:59.978: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    May 16 14:39:59.978: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    May 16 14:39:59.978: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    May 16 14:39:59.978: INFO: Checking APIGroup: tuned.openshift.io
    May 16 14:40:00.027: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
    May 16 14:40:00.027: INFO: Versions found [{tuned.openshift.io/v1 v1}]
    May 16 14:40:00.027: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
    May 16 14:40:00.027: INFO: Checking APIGroup: controlplane.operator.openshift.io
    May 16 14:40:00.077: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
    May 16 14:40:00.078: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
    May 16 14:40:00.078: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
    May 16 14:40:00.078: INFO: Checking APIGroup: metal3.io
    May 16 14:40:00.127: INFO: PreferredVersion.GroupVersion: metal3.io/v1alpha1
    May 16 14:40:00.127: INFO: Versions found [{metal3.io/v1alpha1 v1alpha1}]
    May 16 14:40:00.127: INFO: metal3.io/v1alpha1 matches metal3.io/v1alpha1
    May 16 14:40:00.127: INFO: Checking APIGroup: migration.k8s.io
    May 16 14:40:00.177: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
    May 16 14:40:00.177: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
    May 16 14:40:00.177: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
    May 16 14:40:00.177: INFO: Checking APIGroup: whereabouts.cni.cncf.io
    May 16 14:40:00.228: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
    May 16 14:40:00.228: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
    May 16 14:40:00.228: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
    May 16 14:40:00.228: INFO: Checking APIGroup: infrastructure.cluster.x-k8s.io
    May 16 14:40:00.277: INFO: PreferredVersion.GroupVersion: infrastructure.cluster.x-k8s.io/v1beta1
    May 16 14:40:00.277: INFO: Versions found [{infrastructure.cluster.x-k8s.io/v1beta1 v1beta1} {infrastructure.cluster.x-k8s.io/v1alpha5 v1alpha5}]
    May 16 14:40:00.277: INFO: infrastructure.cluster.x-k8s.io/v1beta1 matches infrastructure.cluster.x-k8s.io/v1beta1
    May 16 14:40:00.277: INFO: Checking APIGroup: helm.openshift.io
    May 16 14:40:00.327: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
    May 16 14:40:00.327: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
    May 16 14:40:00.327: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
    May 16 14:40:00.327: INFO: Checking APIGroup: metrics.k8s.io
    May 16 14:40:00.377: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    May 16 14:40:00.377: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    May 16 14:40:00.377: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    May 16 14:40:00.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-3975" for this suite. 05/16/23 14:40:00.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:40:00.483
May 16 14:40:00.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename emptydir 05/16/23 14:40:00.484
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:40:00.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:40:00.503
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 05/16/23 14:40:00.506
May 16 14:40:00.521: INFO: Waiting up to 5m0s for pod "pod-c2fdd1e1-c70a-46e4-9086-affd104016e0" in namespace "emptydir-6158" to be "Succeeded or Failed"
May 16 14:40:00.524: INFO: Pod "pod-c2fdd1e1-c70a-46e4-9086-affd104016e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.92373ms
May 16 14:40:02.527: INFO: Pod "pod-c2fdd1e1-c70a-46e4-9086-affd104016e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006711961s
May 16 14:40:04.529: INFO: Pod "pod-c2fdd1e1-c70a-46e4-9086-affd104016e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00815601s
STEP: Saw pod success 05/16/23 14:40:04.529
May 16 14:40:04.529: INFO: Pod "pod-c2fdd1e1-c70a-46e4-9086-affd104016e0" satisfied condition "Succeeded or Failed"
May 16 14:40:04.532: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-c2fdd1e1-c70a-46e4-9086-affd104016e0 container test-container: <nil>
STEP: delete the pod 05/16/23 14:40:04.537
May 16 14:40:04.548: INFO: Waiting for pod pod-c2fdd1e1-c70a-46e4-9086-affd104016e0 to disappear
May 16 14:40:04.550: INFO: Pod pod-c2fdd1e1-c70a-46e4-9086-affd104016e0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 16 14:40:04.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6158" for this suite. 05/16/23 14:40:04.554
------------------------------
â€¢ [4.076 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:40:00.483
    May 16 14:40:00.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename emptydir 05/16/23 14:40:00.484
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:40:00.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:40:00.503
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 05/16/23 14:40:00.506
    May 16 14:40:00.521: INFO: Waiting up to 5m0s for pod "pod-c2fdd1e1-c70a-46e4-9086-affd104016e0" in namespace "emptydir-6158" to be "Succeeded or Failed"
    May 16 14:40:00.524: INFO: Pod "pod-c2fdd1e1-c70a-46e4-9086-affd104016e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.92373ms
    May 16 14:40:02.527: INFO: Pod "pod-c2fdd1e1-c70a-46e4-9086-affd104016e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006711961s
    May 16 14:40:04.529: INFO: Pod "pod-c2fdd1e1-c70a-46e4-9086-affd104016e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00815601s
    STEP: Saw pod success 05/16/23 14:40:04.529
    May 16 14:40:04.529: INFO: Pod "pod-c2fdd1e1-c70a-46e4-9086-affd104016e0" satisfied condition "Succeeded or Failed"
    May 16 14:40:04.532: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-c2fdd1e1-c70a-46e4-9086-affd104016e0 container test-container: <nil>
    STEP: delete the pod 05/16/23 14:40:04.537
    May 16 14:40:04.548: INFO: Waiting for pod pod-c2fdd1e1-c70a-46e4-9086-affd104016e0 to disappear
    May 16 14:40:04.550: INFO: Pod pod-c2fdd1e1-c70a-46e4-9086-affd104016e0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 16 14:40:04.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6158" for this suite. 05/16/23 14:40:04.554
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:40:04.56
May 16 14:40:04.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 14:40:04.561
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:40:04.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:40:04.589
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 05/16/23 14:40:04.618
May 16 14:40:04.640: INFO: Waiting up to 5m0s for pod "downwardapi-volume-963bb403-f27e-422b-9f41-93f271d2c323" in namespace "projected-7350" to be "Succeeded or Failed"
May 16 14:40:04.650: INFO: Pod "downwardapi-volume-963bb403-f27e-422b-9f41-93f271d2c323": Phase="Pending", Reason="", readiness=false. Elapsed: 10.160064ms
May 16 14:40:06.653: INFO: Pod "downwardapi-volume-963bb403-f27e-422b-9f41-93f271d2c323": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013617465s
May 16 14:40:08.655: INFO: Pod "downwardapi-volume-963bb403-f27e-422b-9f41-93f271d2c323": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014910355s
STEP: Saw pod success 05/16/23 14:40:08.655
May 16 14:40:08.655: INFO: Pod "downwardapi-volume-963bb403-f27e-422b-9f41-93f271d2c323" satisfied condition "Succeeded or Failed"
May 16 14:40:08.658: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-963bb403-f27e-422b-9f41-93f271d2c323 container client-container: <nil>
STEP: delete the pod 05/16/23 14:40:08.663
May 16 14:40:08.673: INFO: Waiting for pod downwardapi-volume-963bb403-f27e-422b-9f41-93f271d2c323 to disappear
May 16 14:40:08.676: INFO: Pod downwardapi-volume-963bb403-f27e-422b-9f41-93f271d2c323 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 16 14:40:08.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7350" for this suite. 05/16/23 14:40:08.679
------------------------------
â€¢ [4.124 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:40:04.56
    May 16 14:40:04.560: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 14:40:04.561
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:40:04.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:40:04.589
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 05/16/23 14:40:04.618
    May 16 14:40:04.640: INFO: Waiting up to 5m0s for pod "downwardapi-volume-963bb403-f27e-422b-9f41-93f271d2c323" in namespace "projected-7350" to be "Succeeded or Failed"
    May 16 14:40:04.650: INFO: Pod "downwardapi-volume-963bb403-f27e-422b-9f41-93f271d2c323": Phase="Pending", Reason="", readiness=false. Elapsed: 10.160064ms
    May 16 14:40:06.653: INFO: Pod "downwardapi-volume-963bb403-f27e-422b-9f41-93f271d2c323": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013617465s
    May 16 14:40:08.655: INFO: Pod "downwardapi-volume-963bb403-f27e-422b-9f41-93f271d2c323": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014910355s
    STEP: Saw pod success 05/16/23 14:40:08.655
    May 16 14:40:08.655: INFO: Pod "downwardapi-volume-963bb403-f27e-422b-9f41-93f271d2c323" satisfied condition "Succeeded or Failed"
    May 16 14:40:08.658: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-963bb403-f27e-422b-9f41-93f271d2c323 container client-container: <nil>
    STEP: delete the pod 05/16/23 14:40:08.663
    May 16 14:40:08.673: INFO: Waiting for pod downwardapi-volume-963bb403-f27e-422b-9f41-93f271d2c323 to disappear
    May 16 14:40:08.676: INFO: Pod downwardapi-volume-963bb403-f27e-422b-9f41-93f271d2c323 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 16 14:40:08.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7350" for this suite. 05/16/23 14:40:08.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:40:08.685
May 16 14:40:08.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename dns 05/16/23 14:40:08.686
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:40:08.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:40:08.707
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 05/16/23 14:40:08.709
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4786.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4786.svc.cluster.local; sleep 1; done
 05/16/23 14:40:08.716
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4786.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4786.svc.cluster.local; sleep 1; done
 05/16/23 14:40:08.716
STEP: creating a pod to probe DNS 05/16/23 14:40:08.716
STEP: submitting the pod to kubernetes 05/16/23 14:40:08.716
May 16 14:40:08.735: INFO: Waiting up to 15m0s for pod "dns-test-c22ae0ed-ff08-42d7-8f74-435c2a916bae" in namespace "dns-4786" to be "running"
May 16 14:40:08.740: INFO: Pod "dns-test-c22ae0ed-ff08-42d7-8f74-435c2a916bae": Phase="Pending", Reason="", readiness=false. Elapsed: 5.124874ms
May 16 14:40:10.744: INFO: Pod "dns-test-c22ae0ed-ff08-42d7-8f74-435c2a916bae": Phase="Running", Reason="", readiness=true. Elapsed: 2.00957026s
May 16 14:40:10.744: INFO: Pod "dns-test-c22ae0ed-ff08-42d7-8f74-435c2a916bae" satisfied condition "running"
STEP: retrieving the pod 05/16/23 14:40:10.744
STEP: looking for the results for each expected name from probers 05/16/23 14:40:10.747
May 16 14:40:10.755: INFO: DNS probes using dns-test-c22ae0ed-ff08-42d7-8f74-435c2a916bae succeeded

STEP: deleting the pod 05/16/23 14:40:10.755
STEP: changing the externalName to bar.example.com 05/16/23 14:40:10.765
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4786.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4786.svc.cluster.local; sleep 1; done
 05/16/23 14:40:10.773
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4786.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4786.svc.cluster.local; sleep 1; done
 05/16/23 14:40:10.773
STEP: creating a second pod to probe DNS 05/16/23 14:40:10.773
STEP: submitting the pod to kubernetes 05/16/23 14:40:10.773
May 16 14:40:10.782: INFO: Waiting up to 15m0s for pod "dns-test-8a3a1902-b784-4803-839f-0cf81c76a1ee" in namespace "dns-4786" to be "running"
May 16 14:40:10.789: INFO: Pod "dns-test-8a3a1902-b784-4803-839f-0cf81c76a1ee": Phase="Pending", Reason="", readiness=false. Elapsed: 7.650369ms
May 16 14:40:12.794: INFO: Pod "dns-test-8a3a1902-b784-4803-839f-0cf81c76a1ee": Phase="Running", Reason="", readiness=true. Elapsed: 2.012043163s
May 16 14:40:12.794: INFO: Pod "dns-test-8a3a1902-b784-4803-839f-0cf81c76a1ee" satisfied condition "running"
STEP: retrieving the pod 05/16/23 14:40:12.794
STEP: looking for the results for each expected name from probers 05/16/23 14:40:12.797
May 16 14:40:12.802: INFO: File wheezy_udp@dns-test-service-3.dns-4786.svc.cluster.local from pod  dns-4786/dns-test-8a3a1902-b784-4803-839f-0cf81c76a1ee contains 'foo.example.com.
' instead of 'bar.example.com.'
May 16 14:40:12.805: INFO: File jessie_udp@dns-test-service-3.dns-4786.svc.cluster.local from pod  dns-4786/dns-test-8a3a1902-b784-4803-839f-0cf81c76a1ee contains 'foo.example.com.
' instead of 'bar.example.com.'
May 16 14:40:12.805: INFO: Lookups using dns-4786/dns-test-8a3a1902-b784-4803-839f-0cf81c76a1ee failed for: [wheezy_udp@dns-test-service-3.dns-4786.svc.cluster.local jessie_udp@dns-test-service-3.dns-4786.svc.cluster.local]

May 16 14:40:17.814: INFO: DNS probes using dns-test-8a3a1902-b784-4803-839f-0cf81c76a1ee succeeded

STEP: deleting the pod 05/16/23 14:40:17.814
STEP: changing the service to type=ClusterIP 05/16/23 14:40:17.826
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4786.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4786.svc.cluster.local; sleep 1; done
 05/16/23 14:40:17.84
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4786.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4786.svc.cluster.local; sleep 1; done
 05/16/23 14:40:17.84
STEP: creating a third pod to probe DNS 05/16/23 14:40:17.84
STEP: submitting the pod to kubernetes 05/16/23 14:40:17.843
May 16 14:40:17.853: INFO: Waiting up to 15m0s for pod "dns-test-82fe8440-43fc-4409-969d-fa78a9b9cf22" in namespace "dns-4786" to be "running"
May 16 14:40:17.860: INFO: Pod "dns-test-82fe8440-43fc-4409-969d-fa78a9b9cf22": Phase="Pending", Reason="", readiness=false. Elapsed: 6.841706ms
May 16 14:40:19.864: INFO: Pod "dns-test-82fe8440-43fc-4409-969d-fa78a9b9cf22": Phase="Running", Reason="", readiness=true. Elapsed: 2.010253551s
May 16 14:40:19.864: INFO: Pod "dns-test-82fe8440-43fc-4409-969d-fa78a9b9cf22" satisfied condition "running"
STEP: retrieving the pod 05/16/23 14:40:19.864
STEP: looking for the results for each expected name from probers 05/16/23 14:40:19.867
May 16 14:40:19.874: INFO: DNS probes using dns-test-82fe8440-43fc-4409-969d-fa78a9b9cf22 succeeded

STEP: deleting the pod 05/16/23 14:40:19.874
STEP: deleting the test externalName service 05/16/23 14:40:19.884
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May 16 14:40:19.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4786" for this suite. 05/16/23 14:40:19.912
------------------------------
â€¢ [SLOW TEST] [11.239 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:40:08.685
    May 16 14:40:08.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename dns 05/16/23 14:40:08.686
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:40:08.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:40:08.707
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 05/16/23 14:40:08.709
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4786.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4786.svc.cluster.local; sleep 1; done
     05/16/23 14:40:08.716
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4786.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4786.svc.cluster.local; sleep 1; done
     05/16/23 14:40:08.716
    STEP: creating a pod to probe DNS 05/16/23 14:40:08.716
    STEP: submitting the pod to kubernetes 05/16/23 14:40:08.716
    May 16 14:40:08.735: INFO: Waiting up to 15m0s for pod "dns-test-c22ae0ed-ff08-42d7-8f74-435c2a916bae" in namespace "dns-4786" to be "running"
    May 16 14:40:08.740: INFO: Pod "dns-test-c22ae0ed-ff08-42d7-8f74-435c2a916bae": Phase="Pending", Reason="", readiness=false. Elapsed: 5.124874ms
    May 16 14:40:10.744: INFO: Pod "dns-test-c22ae0ed-ff08-42d7-8f74-435c2a916bae": Phase="Running", Reason="", readiness=true. Elapsed: 2.00957026s
    May 16 14:40:10.744: INFO: Pod "dns-test-c22ae0ed-ff08-42d7-8f74-435c2a916bae" satisfied condition "running"
    STEP: retrieving the pod 05/16/23 14:40:10.744
    STEP: looking for the results for each expected name from probers 05/16/23 14:40:10.747
    May 16 14:40:10.755: INFO: DNS probes using dns-test-c22ae0ed-ff08-42d7-8f74-435c2a916bae succeeded

    STEP: deleting the pod 05/16/23 14:40:10.755
    STEP: changing the externalName to bar.example.com 05/16/23 14:40:10.765
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4786.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4786.svc.cluster.local; sleep 1; done
     05/16/23 14:40:10.773
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4786.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4786.svc.cluster.local; sleep 1; done
     05/16/23 14:40:10.773
    STEP: creating a second pod to probe DNS 05/16/23 14:40:10.773
    STEP: submitting the pod to kubernetes 05/16/23 14:40:10.773
    May 16 14:40:10.782: INFO: Waiting up to 15m0s for pod "dns-test-8a3a1902-b784-4803-839f-0cf81c76a1ee" in namespace "dns-4786" to be "running"
    May 16 14:40:10.789: INFO: Pod "dns-test-8a3a1902-b784-4803-839f-0cf81c76a1ee": Phase="Pending", Reason="", readiness=false. Elapsed: 7.650369ms
    May 16 14:40:12.794: INFO: Pod "dns-test-8a3a1902-b784-4803-839f-0cf81c76a1ee": Phase="Running", Reason="", readiness=true. Elapsed: 2.012043163s
    May 16 14:40:12.794: INFO: Pod "dns-test-8a3a1902-b784-4803-839f-0cf81c76a1ee" satisfied condition "running"
    STEP: retrieving the pod 05/16/23 14:40:12.794
    STEP: looking for the results for each expected name from probers 05/16/23 14:40:12.797
    May 16 14:40:12.802: INFO: File wheezy_udp@dns-test-service-3.dns-4786.svc.cluster.local from pod  dns-4786/dns-test-8a3a1902-b784-4803-839f-0cf81c76a1ee contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May 16 14:40:12.805: INFO: File jessie_udp@dns-test-service-3.dns-4786.svc.cluster.local from pod  dns-4786/dns-test-8a3a1902-b784-4803-839f-0cf81c76a1ee contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    May 16 14:40:12.805: INFO: Lookups using dns-4786/dns-test-8a3a1902-b784-4803-839f-0cf81c76a1ee failed for: [wheezy_udp@dns-test-service-3.dns-4786.svc.cluster.local jessie_udp@dns-test-service-3.dns-4786.svc.cluster.local]

    May 16 14:40:17.814: INFO: DNS probes using dns-test-8a3a1902-b784-4803-839f-0cf81c76a1ee succeeded

    STEP: deleting the pod 05/16/23 14:40:17.814
    STEP: changing the service to type=ClusterIP 05/16/23 14:40:17.826
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4786.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4786.svc.cluster.local; sleep 1; done
     05/16/23 14:40:17.84
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4786.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4786.svc.cluster.local; sleep 1; done
     05/16/23 14:40:17.84
    STEP: creating a third pod to probe DNS 05/16/23 14:40:17.84
    STEP: submitting the pod to kubernetes 05/16/23 14:40:17.843
    May 16 14:40:17.853: INFO: Waiting up to 15m0s for pod "dns-test-82fe8440-43fc-4409-969d-fa78a9b9cf22" in namespace "dns-4786" to be "running"
    May 16 14:40:17.860: INFO: Pod "dns-test-82fe8440-43fc-4409-969d-fa78a9b9cf22": Phase="Pending", Reason="", readiness=false. Elapsed: 6.841706ms
    May 16 14:40:19.864: INFO: Pod "dns-test-82fe8440-43fc-4409-969d-fa78a9b9cf22": Phase="Running", Reason="", readiness=true. Elapsed: 2.010253551s
    May 16 14:40:19.864: INFO: Pod "dns-test-82fe8440-43fc-4409-969d-fa78a9b9cf22" satisfied condition "running"
    STEP: retrieving the pod 05/16/23 14:40:19.864
    STEP: looking for the results for each expected name from probers 05/16/23 14:40:19.867
    May 16 14:40:19.874: INFO: DNS probes using dns-test-82fe8440-43fc-4409-969d-fa78a9b9cf22 succeeded

    STEP: deleting the pod 05/16/23 14:40:19.874
    STEP: deleting the test externalName service 05/16/23 14:40:19.884
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May 16 14:40:19.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4786" for this suite. 05/16/23 14:40:19.912
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:40:19.925
May 16 14:40:19.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename replication-controller 05/16/23 14:40:19.926
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:40:19.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:40:19.961
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 05/16/23 14:40:19.963
W0516 14:40:19.975458      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-adoption" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:40:19.975: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6344" to be "running and ready"
May 16 14:40:19.982: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 7.168872ms
May 16 14:40:19.982: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
May 16 14:40:21.987: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.011858735s
May 16 14:40:21.987: INFO: The phase of Pod pod-adoption is Running (Ready = true)
May 16 14:40:21.987: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 05/16/23 14:40:21.99
STEP: Then the orphan pod is adopted 05/16/23 14:40:21.995
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May 16 14:40:23.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6344" for this suite. 05/16/23 14:40:23.006
------------------------------
â€¢ [3.088 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:40:19.925
    May 16 14:40:19.925: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename replication-controller 05/16/23 14:40:19.926
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:40:19.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:40:19.961
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 05/16/23 14:40:19.963
    W0516 14:40:19.975458      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-adoption" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:40:19.975: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6344" to be "running and ready"
    May 16 14:40:19.982: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 7.168872ms
    May 16 14:40:19.982: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:40:21.987: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.011858735s
    May 16 14:40:21.987: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    May 16 14:40:21.987: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 05/16/23 14:40:21.99
    STEP: Then the orphan pod is adopted 05/16/23 14:40:21.995
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May 16 14:40:23.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6344" for this suite. 05/16/23 14:40:23.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:40:23.014
May 16 14:40:23.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 14:40:23.015
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:40:23.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:40:23.039
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-3bcf5ad7-7803-4ae7-9976-242c35ec1b96 05/16/23 14:40:23.041
STEP: Creating a pod to test consume configMaps 05/16/23 14:40:23.05
May 16 14:40:23.061: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b86d511b-b4bd-4e4f-8149-f012b83371a5" in namespace "projected-6986" to be "Succeeded or Failed"
May 16 14:40:23.064: INFO: Pod "pod-projected-configmaps-b86d511b-b4bd-4e4f-8149-f012b83371a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.435538ms
May 16 14:40:25.068: INFO: Pod "pod-projected-configmaps-b86d511b-b4bd-4e4f-8149-f012b83371a5": Phase="Running", Reason="", readiness=false. Elapsed: 2.007279431s
May 16 14:40:27.068: INFO: Pod "pod-projected-configmaps-b86d511b-b4bd-4e4f-8149-f012b83371a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007402191s
STEP: Saw pod success 05/16/23 14:40:27.068
May 16 14:40:27.068: INFO: Pod "pod-projected-configmaps-b86d511b-b4bd-4e4f-8149-f012b83371a5" satisfied condition "Succeeded or Failed"
May 16 14:40:27.071: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod pod-projected-configmaps-b86d511b-b4bd-4e4f-8149-f012b83371a5 container agnhost-container: <nil>
STEP: delete the pod 05/16/23 14:40:27.076
May 16 14:40:27.085: INFO: Waiting for pod pod-projected-configmaps-b86d511b-b4bd-4e4f-8149-f012b83371a5 to disappear
May 16 14:40:27.088: INFO: Pod pod-projected-configmaps-b86d511b-b4bd-4e4f-8149-f012b83371a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May 16 14:40:27.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6986" for this suite. 05/16/23 14:40:27.092
------------------------------
â€¢ [4.084 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:40:23.014
    May 16 14:40:23.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 14:40:23.015
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:40:23.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:40:23.039
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-3bcf5ad7-7803-4ae7-9976-242c35ec1b96 05/16/23 14:40:23.041
    STEP: Creating a pod to test consume configMaps 05/16/23 14:40:23.05
    May 16 14:40:23.061: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b86d511b-b4bd-4e4f-8149-f012b83371a5" in namespace "projected-6986" to be "Succeeded or Failed"
    May 16 14:40:23.064: INFO: Pod "pod-projected-configmaps-b86d511b-b4bd-4e4f-8149-f012b83371a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.435538ms
    May 16 14:40:25.068: INFO: Pod "pod-projected-configmaps-b86d511b-b4bd-4e4f-8149-f012b83371a5": Phase="Running", Reason="", readiness=false. Elapsed: 2.007279431s
    May 16 14:40:27.068: INFO: Pod "pod-projected-configmaps-b86d511b-b4bd-4e4f-8149-f012b83371a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007402191s
    STEP: Saw pod success 05/16/23 14:40:27.068
    May 16 14:40:27.068: INFO: Pod "pod-projected-configmaps-b86d511b-b4bd-4e4f-8149-f012b83371a5" satisfied condition "Succeeded or Failed"
    May 16 14:40:27.071: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod pod-projected-configmaps-b86d511b-b4bd-4e4f-8149-f012b83371a5 container agnhost-container: <nil>
    STEP: delete the pod 05/16/23 14:40:27.076
    May 16 14:40:27.085: INFO: Waiting for pod pod-projected-configmaps-b86d511b-b4bd-4e4f-8149-f012b83371a5 to disappear
    May 16 14:40:27.088: INFO: Pod pod-projected-configmaps-b86d511b-b4bd-4e4f-8149-f012b83371a5 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May 16 14:40:27.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6986" for this suite. 05/16/23 14:40:27.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:40:27.098
May 16 14:40:27.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename endpointslice 05/16/23 14:40:27.099
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:40:27.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:40:27.119
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
May 16 14:40:27.137: INFO: Endpoints addresses: [10.0.130.191 10.0.175.210 10.0.203.52] , ports: [6443]
May 16 14:40:27.137: INFO: EndpointSlices addresses: [10.0.130.191 10.0.175.210 10.0.203.52] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
May 16 14:40:27.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4660" for this suite. 05/16/23 14:40:27.141
------------------------------
â€¢ [0.048 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:40:27.098
    May 16 14:40:27.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename endpointslice 05/16/23 14:40:27.099
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:40:27.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:40:27.119
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    May 16 14:40:27.137: INFO: Endpoints addresses: [10.0.130.191 10.0.175.210 10.0.203.52] , ports: [6443]
    May 16 14:40:27.137: INFO: EndpointSlices addresses: [10.0.130.191 10.0.175.210 10.0.203.52] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    May 16 14:40:27.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4660" for this suite. 05/16/23 14:40:27.141
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:40:27.147
May 16 14:40:27.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename daemonsets 05/16/23 14:40:27.147
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:40:27.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:40:27.216
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 05/16/23 14:40:27.237
STEP: Check that daemon pods launch on every node of the cluster. 05/16/23 14:40:27.242
May 16 14:40:27.247: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:27.247: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:27.247: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:27.251: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 14:40:27.251: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 14:40:28.256: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:28.256: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:28.256: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:28.259: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 16 14:40:28.259: INFO: Node ip-10-0-161-164.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 14:40:29.257: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:29.257: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:29.257: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:29.261: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 16 14:40:29.261: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 05/16/23 14:40:29.265
May 16 14:40:29.278: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:29.278: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:29.278: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:29.282: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 16 14:40:29.282: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 14:40:30.288: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:30.288: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:30.288: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:30.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 16 14:40:30.291: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 14:40:31.287: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:31.287: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:31.287: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:31.290: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 16 14:40:31.290: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 14:40:32.287: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:32.287: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:32.287: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 14:40:32.290: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 16 14:40:32.290: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/16/23 14:40:32.294
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6707, will wait for the garbage collector to delete the pods 05/16/23 14:40:32.294
May 16 14:40:32.352: INFO: Deleting DaemonSet.extensions daemon-set took: 4.998486ms
May 16 14:40:32.453: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.822464ms
May 16 14:40:34.957: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 14:40:34.957: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 16 14:40:34.959: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"60172"},"items":null}

May 16 14:40:34.962: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"60172"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 14:40:34.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6707" for this suite. 05/16/23 14:40:34.985
------------------------------
â€¢ [SLOW TEST] [7.845 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:40:27.147
    May 16 14:40:27.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename daemonsets 05/16/23 14:40:27.147
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:40:27.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:40:27.216
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 05/16/23 14:40:27.237
    STEP: Check that daemon pods launch on every node of the cluster. 05/16/23 14:40:27.242
    May 16 14:40:27.247: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:27.247: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:27.247: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:27.251: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 14:40:27.251: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 14:40:28.256: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:28.256: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:28.256: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:28.259: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 16 14:40:28.259: INFO: Node ip-10-0-161-164.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 14:40:29.257: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:29.257: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:29.257: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:29.261: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May 16 14:40:29.261: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 05/16/23 14:40:29.265
    May 16 14:40:29.278: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:29.278: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:29.278: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:29.282: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 16 14:40:29.282: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 14:40:30.288: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:30.288: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:30.288: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:30.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 16 14:40:30.291: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 14:40:31.287: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:31.287: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:31.287: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:31.290: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 16 14:40:31.290: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 14:40:32.287: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:32.287: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:32.287: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 14:40:32.290: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May 16 14:40:32.290: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/16/23 14:40:32.294
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6707, will wait for the garbage collector to delete the pods 05/16/23 14:40:32.294
    May 16 14:40:32.352: INFO: Deleting DaemonSet.extensions daemon-set took: 4.998486ms
    May 16 14:40:32.453: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.822464ms
    May 16 14:40:34.957: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 14:40:34.957: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May 16 14:40:34.959: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"60172"},"items":null}

    May 16 14:40:34.962: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"60172"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 14:40:34.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6707" for this suite. 05/16/23 14:40:34.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:40:34.992
May 16 14:40:34.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename var-expansion 05/16/23 14:40:34.992
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:40:35.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:40:35.011
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 05/16/23 14:40:35.014
W0516 14:40:35.028194      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:40:35.028: INFO: Waiting up to 2m0s for pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a" in namespace "var-expansion-6798" to be "running"
May 16 14:40:35.031: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.291175ms
May 16 14:40:37.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00705324s
May 16 14:40:39.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007679312s
May 16 14:40:41.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007057166s
May 16 14:40:43.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008698079s
May 16 14:40:45.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007406718s
May 16 14:40:47.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007585537s
May 16 14:40:49.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007585915s
May 16 14:40:51.034: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006555446s
May 16 14:40:53.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006925309s
May 16 14:40:55.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008687629s
May 16 14:40:57.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007626172s
May 16 14:40:59.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006984985s
May 16 14:41:01.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007099745s
May 16 14:41:03.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008496176s
May 16 14:41:05.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008531426s
May 16 14:41:07.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 32.006885913s
May 16 14:41:09.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007993136s
May 16 14:41:11.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00759433s
May 16 14:41:13.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007533275s
May 16 14:41:15.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007317232s
May 16 14:41:17.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008097354s
May 16 14:41:19.037: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009000707s
May 16 14:41:21.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007291541s
May 16 14:41:23.039: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 48.010801165s
May 16 14:41:25.038: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 50.010700577s
May 16 14:41:27.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006764676s
May 16 14:41:29.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008050379s
May 16 14:41:31.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007572184s
May 16 14:41:33.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008339363s
May 16 14:41:35.037: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.00907307s
May 16 14:41:37.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.007544667s
May 16 14:41:39.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.008700483s
May 16 14:41:41.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007083619s
May 16 14:41:43.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008233162s
May 16 14:41:45.039: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010768297s
May 16 14:41:47.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007126227s
May 16 14:41:49.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007940872s
May 16 14:41:51.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.006962244s
May 16 14:41:53.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008567565s
May 16 14:41:55.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.00723658s
May 16 14:41:57.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.00800007s
May 16 14:41:59.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006856346s
May 16 14:42:01.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007476085s
May 16 14:42:03.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007013709s
May 16 14:42:05.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008008519s
May 16 14:42:07.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006904371s
May 16 14:42:09.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008004341s
May 16 14:42:11.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.007444861s
May 16 14:42:13.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008673356s
May 16 14:42:15.040: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.012261242s
May 16 14:42:17.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008152237s
May 16 14:42:19.037: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008980627s
May 16 14:42:21.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.007663611s
May 16 14:42:23.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008137309s
May 16 14:42:25.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.00815171s
May 16 14:42:27.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.007704719s
May 16 14:42:29.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.007315886s
May 16 14:42:31.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006813873s
May 16 14:42:33.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007729908s
May 16 14:42:35.037: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008788007s
May 16 14:42:35.040: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011799488s
STEP: updating the pod 05/16/23 14:42:35.04
May 16 14:42:35.555: INFO: Successfully updated pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a"
STEP: waiting for pod running 05/16/23 14:42:35.555
May 16 14:42:35.555: INFO: Waiting up to 2m0s for pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a" in namespace "var-expansion-6798" to be "running"
May 16 14:42:35.557: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.659779ms
May 16 14:42:37.561: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Running", Reason="", readiness=true. Elapsed: 2.006059418s
May 16 14:42:37.561: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a" satisfied condition "running"
STEP: deleting the pod gracefully 05/16/23 14:42:37.561
May 16 14:42:37.561: INFO: Deleting pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a" in namespace "var-expansion-6798"
May 16 14:42:37.569: INFO: Wait up to 5m0s for pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May 16 14:43:09.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6798" for this suite. 05/16/23 14:43:09.581
------------------------------
â€¢ [SLOW TEST] [154.594 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:40:34.992
    May 16 14:40:34.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename var-expansion 05/16/23 14:40:34.992
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:40:35.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:40:35.011
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 05/16/23 14:40:35.014
    W0516 14:40:35.028194      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:40:35.028: INFO: Waiting up to 2m0s for pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a" in namespace "var-expansion-6798" to be "running"
    May 16 14:40:35.031: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.291175ms
    May 16 14:40:37.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00705324s
    May 16 14:40:39.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007679312s
    May 16 14:40:41.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007057166s
    May 16 14:40:43.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008698079s
    May 16 14:40:45.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007406718s
    May 16 14:40:47.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007585537s
    May 16 14:40:49.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007585915s
    May 16 14:40:51.034: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006555446s
    May 16 14:40:53.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006925309s
    May 16 14:40:55.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008687629s
    May 16 14:40:57.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007626172s
    May 16 14:40:59.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006984985s
    May 16 14:41:01.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007099745s
    May 16 14:41:03.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008496176s
    May 16 14:41:05.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008531426s
    May 16 14:41:07.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 32.006885913s
    May 16 14:41:09.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007993136s
    May 16 14:41:11.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00759433s
    May 16 14:41:13.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007533275s
    May 16 14:41:15.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007317232s
    May 16 14:41:17.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008097354s
    May 16 14:41:19.037: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 44.009000707s
    May 16 14:41:21.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007291541s
    May 16 14:41:23.039: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 48.010801165s
    May 16 14:41:25.038: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 50.010700577s
    May 16 14:41:27.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006764676s
    May 16 14:41:29.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008050379s
    May 16 14:41:31.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007572184s
    May 16 14:41:33.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008339363s
    May 16 14:41:35.037: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.00907307s
    May 16 14:41:37.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.007544667s
    May 16 14:41:39.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.008700483s
    May 16 14:41:41.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007083619s
    May 16 14:41:43.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.008233162s
    May 16 14:41:45.039: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010768297s
    May 16 14:41:47.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007126227s
    May 16 14:41:49.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007940872s
    May 16 14:41:51.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.006962244s
    May 16 14:41:53.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008567565s
    May 16 14:41:55.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.00723658s
    May 16 14:41:57.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.00800007s
    May 16 14:41:59.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006856346s
    May 16 14:42:01.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007476085s
    May 16 14:42:03.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007013709s
    May 16 14:42:05.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008008519s
    May 16 14:42:07.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006904371s
    May 16 14:42:09.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008004341s
    May 16 14:42:11.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.007444861s
    May 16 14:42:13.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008673356s
    May 16 14:42:15.040: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.012261242s
    May 16 14:42:17.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008152237s
    May 16 14:42:19.037: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.008980627s
    May 16 14:42:21.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.007663611s
    May 16 14:42:23.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008137309s
    May 16 14:42:25.036: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.00815171s
    May 16 14:42:27.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.007704719s
    May 16 14:42:29.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.007315886s
    May 16 14:42:31.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006813873s
    May 16 14:42:33.035: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007729908s
    May 16 14:42:35.037: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008788007s
    May 16 14:42:35.040: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.011799488s
    STEP: updating the pod 05/16/23 14:42:35.04
    May 16 14:42:35.555: INFO: Successfully updated pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a"
    STEP: waiting for pod running 05/16/23 14:42:35.555
    May 16 14:42:35.555: INFO: Waiting up to 2m0s for pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a" in namespace "var-expansion-6798" to be "running"
    May 16 14:42:35.557: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.659779ms
    May 16 14:42:37.561: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a": Phase="Running", Reason="", readiness=true. Elapsed: 2.006059418s
    May 16 14:42:37.561: INFO: Pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a" satisfied condition "running"
    STEP: deleting the pod gracefully 05/16/23 14:42:37.561
    May 16 14:42:37.561: INFO: Deleting pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a" in namespace "var-expansion-6798"
    May 16 14:42:37.569: INFO: Wait up to 5m0s for pod "var-expansion-65da0f50-726d-47ca-b966-9ab28f46599a" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May 16 14:43:09.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6798" for this suite. 05/16/23 14:43:09.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:43:09.586
May 16 14:43:09.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename container-probe 05/16/23 14:43:09.587
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:43:09.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:43:09.607
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-be25a43b-7846-4bc1-a730-e2f5ba8654c5 in namespace container-probe-9565 05/16/23 14:43:09.61
May 16 14:43:09.631: INFO: Waiting up to 5m0s for pod "busybox-be25a43b-7846-4bc1-a730-e2f5ba8654c5" in namespace "container-probe-9565" to be "not pending"
May 16 14:43:09.635: INFO: Pod "busybox-be25a43b-7846-4bc1-a730-e2f5ba8654c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.136749ms
May 16 14:43:11.639: INFO: Pod "busybox-be25a43b-7846-4bc1-a730-e2f5ba8654c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007722449s
May 16 14:43:11.639: INFO: Pod "busybox-be25a43b-7846-4bc1-a730-e2f5ba8654c5" satisfied condition "not pending"
May 16 14:43:11.639: INFO: Started pod busybox-be25a43b-7846-4bc1-a730-e2f5ba8654c5 in namespace container-probe-9565
STEP: checking the pod's current state and verifying that restartCount is present 05/16/23 14:43:11.639
May 16 14:43:11.642: INFO: Initial restart count of pod busybox-be25a43b-7846-4bc1-a730-e2f5ba8654c5 is 0
STEP: deleting the pod 05/16/23 14:47:12.171
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May 16 14:47:12.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9565" for this suite. 05/16/23 14:47:12.187
------------------------------
â€¢ [SLOW TEST] [242.606 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:43:09.586
    May 16 14:43:09.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename container-probe 05/16/23 14:43:09.587
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:43:09.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:43:09.607
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-be25a43b-7846-4bc1-a730-e2f5ba8654c5 in namespace container-probe-9565 05/16/23 14:43:09.61
    May 16 14:43:09.631: INFO: Waiting up to 5m0s for pod "busybox-be25a43b-7846-4bc1-a730-e2f5ba8654c5" in namespace "container-probe-9565" to be "not pending"
    May 16 14:43:09.635: INFO: Pod "busybox-be25a43b-7846-4bc1-a730-e2f5ba8654c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.136749ms
    May 16 14:43:11.639: INFO: Pod "busybox-be25a43b-7846-4bc1-a730-e2f5ba8654c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007722449s
    May 16 14:43:11.639: INFO: Pod "busybox-be25a43b-7846-4bc1-a730-e2f5ba8654c5" satisfied condition "not pending"
    May 16 14:43:11.639: INFO: Started pod busybox-be25a43b-7846-4bc1-a730-e2f5ba8654c5 in namespace container-probe-9565
    STEP: checking the pod's current state and verifying that restartCount is present 05/16/23 14:43:11.639
    May 16 14:43:11.642: INFO: Initial restart count of pod busybox-be25a43b-7846-4bc1-a730-e2f5ba8654c5 is 0
    STEP: deleting the pod 05/16/23 14:47:12.171
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May 16 14:47:12.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9565" for this suite. 05/16/23 14:47:12.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:47:12.193
May 16 14:47:12.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename container-lifecycle-hook 05/16/23 14:47:12.194
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:47:12.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:47:12.219
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 05/16/23 14:47:12.229
May 16 14:47:13.245: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4304" to be "running and ready"
May 16 14:47:13.248: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.203075ms
May 16 14:47:13.248: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 16 14:47:15.252: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007267473s
May 16 14:47:15.252: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May 16 14:47:15.252: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 05/16/23 14:47:15.255
May 16 14:47:15.262: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4304" to be "running and ready"
May 16 14:47:15.265: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.618859ms
May 16 14:47:15.265: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
May 16 14:47:17.268: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006233891s
May 16 14:47:17.268: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
May 16 14:47:17.268: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 05/16/23 14:47:17.272
STEP: delete the pod with lifecycle hook 05/16/23 14:47:17.283
May 16 14:47:17.291: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 16 14:47:17.295: INFO: Pod pod-with-poststart-http-hook still exists
May 16 14:47:19.297: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 16 14:47:19.300: INFO: Pod pod-with-poststart-http-hook still exists
May 16 14:47:21.296: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 16 14:47:21.299: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
May 16 14:47:21.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4304" for this suite. 05/16/23 14:47:21.303
------------------------------
â€¢ [SLOW TEST] [9.115 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:47:12.193
    May 16 14:47:12.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/16/23 14:47:12.194
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:47:12.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:47:12.219
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 05/16/23 14:47:12.229
    May 16 14:47:13.245: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4304" to be "running and ready"
    May 16 14:47:13.248: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.203075ms
    May 16 14:47:13.248: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:47:15.252: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007267473s
    May 16 14:47:15.252: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May 16 14:47:15.252: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 05/16/23 14:47:15.255
    May 16 14:47:15.262: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4304" to be "running and ready"
    May 16 14:47:15.265: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.618859ms
    May 16 14:47:15.265: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:47:17.268: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006233891s
    May 16 14:47:17.268: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    May 16 14:47:17.268: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 05/16/23 14:47:17.272
    STEP: delete the pod with lifecycle hook 05/16/23 14:47:17.283
    May 16 14:47:17.291: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May 16 14:47:17.295: INFO: Pod pod-with-poststart-http-hook still exists
    May 16 14:47:19.297: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May 16 14:47:19.300: INFO: Pod pod-with-poststart-http-hook still exists
    May 16 14:47:21.296: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    May 16 14:47:21.299: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    May 16 14:47:21.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4304" for this suite. 05/16/23 14:47:21.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:47:21.311
May 16 14:47:21.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename taint-single-pod 05/16/23 14:47:21.312
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:47:21.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:47:21.335
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
May 16 14:47:21.340: INFO: Waiting up to 1m0s for all nodes to be ready
May 16 14:48:21.454: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
May 16 14:48:21.459: INFO: Starting informer...
STEP: Starting pod... 05/16/23 14:48:21.459
May 16 14:48:21.673: INFO: Pod is running on ip-10-0-161-164.eu-central-1.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node 05/16/23 14:48:21.673
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/16/23 14:48:21.683
STEP: Waiting short time to make sure Pod is queued for deletion 05/16/23 14:48:21.691
May 16 14:48:21.691: INFO: Pod wasn't evicted. Proceeding
May 16 14:48:21.691: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/16/23 14:48:21.722
STEP: Waiting some time to make sure that toleration time passed. 05/16/23 14:48:21.735
May 16 14:49:36.735: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 14:49:36.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-7207" for this suite. 05/16/23 14:49:36.74
------------------------------
â€¢ [SLOW TEST] [135.436 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:47:21.311
    May 16 14:47:21.311: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename taint-single-pod 05/16/23 14:47:21.312
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:47:21.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:47:21.335
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    May 16 14:47:21.340: INFO: Waiting up to 1m0s for all nodes to be ready
    May 16 14:48:21.454: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    May 16 14:48:21.459: INFO: Starting informer...
    STEP: Starting pod... 05/16/23 14:48:21.459
    May 16 14:48:21.673: INFO: Pod is running on ip-10-0-161-164.eu-central-1.compute.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 05/16/23 14:48:21.673
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/16/23 14:48:21.683
    STEP: Waiting short time to make sure Pod is queued for deletion 05/16/23 14:48:21.691
    May 16 14:48:21.691: INFO: Pod wasn't evicted. Proceeding
    May 16 14:48:21.691: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/16/23 14:48:21.722
    STEP: Waiting some time to make sure that toleration time passed. 05/16/23 14:48:21.735
    May 16 14:49:36.735: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 14:49:36.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-7207" for this suite. 05/16/23 14:49:36.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:49:36.751
May 16 14:49:36.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename crd-webhook 05/16/23 14:49:36.751
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:49:36.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:49:36.771
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 05/16/23 14:49:36.776
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/16/23 14:49:37.181
STEP: Deploying the custom resource conversion webhook pod 05/16/23 14:49:37.191
STEP: Wait for the deployment to be ready 05/16/23 14:49:37.203
May 16 14:49:37.211: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/16/23 14:49:39.221
STEP: Verifying the service has paired with the endpoint 05/16/23 14:49:39.233
May 16 14:49:40.233: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
May 16 14:49:40.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Creating a v1 custom resource 05/16/23 14:49:42.82
STEP: v2 custom resource should be converted 05/16/23 14:49:42.824
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 14:49:43.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-5140" for this suite. 05/16/23 14:49:43.423
------------------------------
â€¢ [SLOW TEST] [6.688 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:49:36.751
    May 16 14:49:36.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename crd-webhook 05/16/23 14:49:36.751
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:49:36.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:49:36.771
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 05/16/23 14:49:36.776
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 05/16/23 14:49:37.181
    STEP: Deploying the custom resource conversion webhook pod 05/16/23 14:49:37.191
    STEP: Wait for the deployment to be ready 05/16/23 14:49:37.203
    May 16 14:49:37.211: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/16/23 14:49:39.221
    STEP: Verifying the service has paired with the endpoint 05/16/23 14:49:39.233
    May 16 14:49:40.233: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    May 16 14:49:40.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Creating a v1 custom resource 05/16/23 14:49:42.82
    STEP: v2 custom resource should be converted 05/16/23 14:49:42.824
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 14:49:43.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-5140" for this suite. 05/16/23 14:49:43.423
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:49:43.439
May 16 14:49:43.439: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename emptydir 05/16/23 14:49:43.44
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:49:43.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:49:43.493
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 05/16/23 14:49:43.504
May 16 14:49:43.524: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-dfebd790-157e-4675-9bcc-d6ee5ffee97c" in namespace "emptydir-6766" to be "running"
May 16 14:49:43.538: INFO: Pod "pod-sharedvolume-dfebd790-157e-4675-9bcc-d6ee5ffee97c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.050461ms
May 16 14:49:45.541: INFO: Pod "pod-sharedvolume-dfebd790-157e-4675-9bcc-d6ee5ffee97c": Phase="Running", Reason="", readiness=false. Elapsed: 2.017701634s
May 16 14:49:45.541: INFO: Pod "pod-sharedvolume-dfebd790-157e-4675-9bcc-d6ee5ffee97c" satisfied condition "running"
STEP: Reading file content from the nginx-container 05/16/23 14:49:45.541
May 16 14:49:45.541: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6766 PodName:pod-sharedvolume-dfebd790-157e-4675-9bcc-d6ee5ffee97c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 14:49:45.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 14:49:45.542: INFO: ExecWithOptions: Clientset creation
May 16 14:49:45.542: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/emptydir-6766/pods/pod-sharedvolume-dfebd790-157e-4675-9bcc-d6ee5ffee97c/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
May 16 14:49:45.632: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 16 14:49:45.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6766" for this suite. 05/16/23 14:49:45.637
------------------------------
â€¢ [2.204 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:49:43.439
    May 16 14:49:43.439: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename emptydir 05/16/23 14:49:43.44
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:49:43.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:49:43.493
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 05/16/23 14:49:43.504
    May 16 14:49:43.524: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-dfebd790-157e-4675-9bcc-d6ee5ffee97c" in namespace "emptydir-6766" to be "running"
    May 16 14:49:43.538: INFO: Pod "pod-sharedvolume-dfebd790-157e-4675-9bcc-d6ee5ffee97c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.050461ms
    May 16 14:49:45.541: INFO: Pod "pod-sharedvolume-dfebd790-157e-4675-9bcc-d6ee5ffee97c": Phase="Running", Reason="", readiness=false. Elapsed: 2.017701634s
    May 16 14:49:45.541: INFO: Pod "pod-sharedvolume-dfebd790-157e-4675-9bcc-d6ee5ffee97c" satisfied condition "running"
    STEP: Reading file content from the nginx-container 05/16/23 14:49:45.541
    May 16 14:49:45.541: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6766 PodName:pod-sharedvolume-dfebd790-157e-4675-9bcc-d6ee5ffee97c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 14:49:45.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 14:49:45.542: INFO: ExecWithOptions: Clientset creation
    May 16 14:49:45.542: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/emptydir-6766/pods/pod-sharedvolume-dfebd790-157e-4675-9bcc-d6ee5ffee97c/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    May 16 14:49:45.632: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 16 14:49:45.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6766" for this suite. 05/16/23 14:49:45.637
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:49:45.643
May 16 14:49:45.643: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename secrets 05/16/23 14:49:45.644
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:49:45.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:49:45.665
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 05/16/23 14:49:45.668
STEP: listing secrets in all namespaces to ensure that there are more than zero 05/16/23 14:49:45.676
STEP: patching the secret 05/16/23 14:49:45.773
STEP: deleting the secret using a LabelSelector 05/16/23 14:49:45.793
STEP: listing secrets in all namespaces, searching for label name and value in patch 05/16/23 14:49:45.815
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
May 16 14:49:45.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2986" for this suite. 05/16/23 14:49:45.92
------------------------------
â€¢ [0.293 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:49:45.643
    May 16 14:49:45.643: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename secrets 05/16/23 14:49:45.644
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:49:45.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:49:45.665
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 05/16/23 14:49:45.668
    STEP: listing secrets in all namespaces to ensure that there are more than zero 05/16/23 14:49:45.676
    STEP: patching the secret 05/16/23 14:49:45.773
    STEP: deleting the secret using a LabelSelector 05/16/23 14:49:45.793
    STEP: listing secrets in all namespaces, searching for label name and value in patch 05/16/23 14:49:45.815
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    May 16 14:49:45.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2986" for this suite. 05/16/23 14:49:45.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:49:45.937
May 16 14:49:45.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename job 05/16/23 14:49:45.937
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:49:45.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:49:45.966
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 05/16/23 14:49:45.972
W0516 14:49:45.991020      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensure pods equal to parallelism count is attached to the job 05/16/23 14:49:45.991
STEP: patching /status 05/16/23 14:49:47.995
STEP: updating /status 05/16/23 14:49:48.002
STEP: get /status 05/16/23 14:49:48.056
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May 16 14:49:48.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7761" for this suite. 05/16/23 14:49:48.063
------------------------------
â€¢ [2.132 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:49:45.937
    May 16 14:49:45.937: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename job 05/16/23 14:49:45.937
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:49:45.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:49:45.966
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 05/16/23 14:49:45.972
    W0516 14:49:45.991020      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensure pods equal to parallelism count is attached to the job 05/16/23 14:49:45.991
    STEP: patching /status 05/16/23 14:49:47.995
    STEP: updating /status 05/16/23 14:49:48.002
    STEP: get /status 05/16/23 14:49:48.056
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May 16 14:49:48.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7761" for this suite. 05/16/23 14:49:48.063
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:49:48.068
May 16 14:49:48.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename replicaset 05/16/23 14:49:48.069
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:49:48.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:49:48.105
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 05/16/23 14:49:48.115
STEP: Verify that the required pods have come up. 05/16/23 14:49:48.12
May 16 14:49:48.123: INFO: Pod name sample-pod: Found 0 pods out of 1
May 16 14:49:53.129: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/16/23 14:49:53.129
STEP: Getting /status 05/16/23 14:49:53.129
May 16 14:49:53.132: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 05/16/23 14:49:53.132
May 16 14:49:53.141: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 05/16/23 14:49:53.141
May 16 14:49:53.142: INFO: Observed &ReplicaSet event: ADDED
May 16 14:49:53.142: INFO: Observed &ReplicaSet event: MODIFIED
May 16 14:49:53.142: INFO: Observed &ReplicaSet event: MODIFIED
May 16 14:49:53.142: INFO: Observed &ReplicaSet event: MODIFIED
May 16 14:49:53.142: INFO: Found replicaset test-rs in namespace replicaset-4907 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 16 14:49:53.142: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 05/16/23 14:49:53.142
May 16 14:49:53.142: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May 16 14:49:53.148: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 05/16/23 14:49:53.148
May 16 14:49:53.149: INFO: Observed &ReplicaSet event: ADDED
May 16 14:49:53.149: INFO: Observed &ReplicaSet event: MODIFIED
May 16 14:49:53.149: INFO: Observed &ReplicaSet event: MODIFIED
May 16 14:49:53.149: INFO: Observed &ReplicaSet event: MODIFIED
May 16 14:49:53.149: INFO: Observed replicaset test-rs in namespace replicaset-4907 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 16 14:49:53.149: INFO: Observed &ReplicaSet event: MODIFIED
May 16 14:49:53.149: INFO: Found replicaset test-rs in namespace replicaset-4907 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
May 16 14:49:53.149: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May 16 14:49:53.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4907" for this suite. 05/16/23 14:49:53.154
------------------------------
â€¢ [SLOW TEST] [5.091 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:49:48.068
    May 16 14:49:48.068: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename replicaset 05/16/23 14:49:48.069
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:49:48.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:49:48.105
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 05/16/23 14:49:48.115
    STEP: Verify that the required pods have come up. 05/16/23 14:49:48.12
    May 16 14:49:48.123: INFO: Pod name sample-pod: Found 0 pods out of 1
    May 16 14:49:53.129: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/16/23 14:49:53.129
    STEP: Getting /status 05/16/23 14:49:53.129
    May 16 14:49:53.132: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 05/16/23 14:49:53.132
    May 16 14:49:53.141: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 05/16/23 14:49:53.141
    May 16 14:49:53.142: INFO: Observed &ReplicaSet event: ADDED
    May 16 14:49:53.142: INFO: Observed &ReplicaSet event: MODIFIED
    May 16 14:49:53.142: INFO: Observed &ReplicaSet event: MODIFIED
    May 16 14:49:53.142: INFO: Observed &ReplicaSet event: MODIFIED
    May 16 14:49:53.142: INFO: Found replicaset test-rs in namespace replicaset-4907 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May 16 14:49:53.142: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 05/16/23 14:49:53.142
    May 16 14:49:53.142: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May 16 14:49:53.148: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 05/16/23 14:49:53.148
    May 16 14:49:53.149: INFO: Observed &ReplicaSet event: ADDED
    May 16 14:49:53.149: INFO: Observed &ReplicaSet event: MODIFIED
    May 16 14:49:53.149: INFO: Observed &ReplicaSet event: MODIFIED
    May 16 14:49:53.149: INFO: Observed &ReplicaSet event: MODIFIED
    May 16 14:49:53.149: INFO: Observed replicaset test-rs in namespace replicaset-4907 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May 16 14:49:53.149: INFO: Observed &ReplicaSet event: MODIFIED
    May 16 14:49:53.149: INFO: Found replicaset test-rs in namespace replicaset-4907 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    May 16 14:49:53.149: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May 16 14:49:53.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4907" for this suite. 05/16/23 14:49:53.154
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:49:53.16
May 16 14:49:53.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename downward-api 05/16/23 14:49:53.161
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:49:53.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:49:53.184
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 05/16/23 14:49:53.187
May 16 14:49:53.201: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1de25d6a-8cfe-497e-867c-795cbe1971fb" in namespace "downward-api-7144" to be "Succeeded or Failed"
May 16 14:49:53.204: INFO: Pod "downwardapi-volume-1de25d6a-8cfe-497e-867c-795cbe1971fb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.107876ms
May 16 14:49:55.208: INFO: Pod "downwardapi-volume-1de25d6a-8cfe-497e-867c-795cbe1971fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007687763s
May 16 14:49:57.208: INFO: Pod "downwardapi-volume-1de25d6a-8cfe-497e-867c-795cbe1971fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007086647s
STEP: Saw pod success 05/16/23 14:49:57.208
May 16 14:49:57.208: INFO: Pod "downwardapi-volume-1de25d6a-8cfe-497e-867c-795cbe1971fb" satisfied condition "Succeeded or Failed"
May 16 14:49:57.211: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-1de25d6a-8cfe-497e-867c-795cbe1971fb container client-container: <nil>
STEP: delete the pod 05/16/23 14:49:57.22
May 16 14:49:57.229: INFO: Waiting for pod downwardapi-volume-1de25d6a-8cfe-497e-867c-795cbe1971fb to disappear
May 16 14:49:57.231: INFO: Pod downwardapi-volume-1de25d6a-8cfe-497e-867c-795cbe1971fb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 16 14:49:57.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7144" for this suite. 05/16/23 14:49:57.235
------------------------------
â€¢ [4.081 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:49:53.16
    May 16 14:49:53.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename downward-api 05/16/23 14:49:53.161
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:49:53.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:49:53.184
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 05/16/23 14:49:53.187
    May 16 14:49:53.201: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1de25d6a-8cfe-497e-867c-795cbe1971fb" in namespace "downward-api-7144" to be "Succeeded or Failed"
    May 16 14:49:53.204: INFO: Pod "downwardapi-volume-1de25d6a-8cfe-497e-867c-795cbe1971fb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.107876ms
    May 16 14:49:55.208: INFO: Pod "downwardapi-volume-1de25d6a-8cfe-497e-867c-795cbe1971fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007687763s
    May 16 14:49:57.208: INFO: Pod "downwardapi-volume-1de25d6a-8cfe-497e-867c-795cbe1971fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007086647s
    STEP: Saw pod success 05/16/23 14:49:57.208
    May 16 14:49:57.208: INFO: Pod "downwardapi-volume-1de25d6a-8cfe-497e-867c-795cbe1971fb" satisfied condition "Succeeded or Failed"
    May 16 14:49:57.211: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-1de25d6a-8cfe-497e-867c-795cbe1971fb container client-container: <nil>
    STEP: delete the pod 05/16/23 14:49:57.22
    May 16 14:49:57.229: INFO: Waiting for pod downwardapi-volume-1de25d6a-8cfe-497e-867c-795cbe1971fb to disappear
    May 16 14:49:57.231: INFO: Pod downwardapi-volume-1de25d6a-8cfe-497e-867c-795cbe1971fb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 16 14:49:57.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7144" for this suite. 05/16/23 14:49:57.235
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:49:57.241
May 16 14:49:57.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename statefulset 05/16/23 14:49:57.242
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:49:57.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:49:57.265
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5697 05/16/23 14:49:57.267
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-5697 05/16/23 14:49:57.273
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5697 05/16/23 14:49:57.283
May 16 14:49:57.287: INFO: Found 0 stateful pods, waiting for 1
May 16 14:50:07.291: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 05/16/23 14:50:07.291
May 16 14:50:07.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-5697 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 16 14:50:07.404: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 16 14:50:07.404: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 16 14:50:07.404: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 16 14:50:07.407: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 16 14:50:17.411: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 16 14:50:17.411: INFO: Waiting for statefulset status.replicas updated to 0
May 16 14:50:17.426: INFO: POD   NODE                                           PHASE    GRACE  CONDITIONS
May 16 14:50:17.426: INFO: ss-0  ip-10-0-161-164.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:49:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:49:57 +0000 UTC  }]
May 16 14:50:17.426: INFO: 
May 16 14:50:17.426: INFO: StatefulSet ss has not reached scale 3, at 1
May 16 14:50:18.430: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996295775s
May 16 14:50:19.434: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992304594s
May 16 14:50:20.439: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987296041s
May 16 14:50:21.443: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983593801s
May 16 14:50:22.447: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.979853588s
May 16 14:50:23.451: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975288212s
May 16 14:50:24.455: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971664605s
May 16 14:50:25.460: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96730109s
May 16 14:50:26.464: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.208376ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5697 05/16/23 14:50:27.464
May 16 14:50:27.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-5697 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 16 14:50:27.585: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 16 14:50:27.585: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 16 14:50:27.585: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 16 14:50:27.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-5697 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 16 14:50:27.719: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 16 14:50:27.719: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 16 14:50:27.719: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 16 14:50:27.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-5697 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 16 14:50:27.850: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 16 14:50:27.850: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 16 14:50:27.850: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 16 14:50:27.853: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 16 14:50:27.853: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 16 14:50:27.853: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 05/16/23 14:50:27.853
May 16 14:50:27.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-5697 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 16 14:50:27.964: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 16 14:50:27.964: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 16 14:50:27.964: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 16 14:50:27.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-5697 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 16 14:50:28.073: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 16 14:50:28.073: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 16 14:50:28.073: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 16 14:50:28.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-5697 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 16 14:50:28.170: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 16 14:50:28.170: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 16 14:50:28.170: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 16 14:50:28.170: INFO: Waiting for statefulset status.replicas updated to 0
May 16 14:50:28.173: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 16 14:50:38.181: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 16 14:50:38.181: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 16 14:50:38.181: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 16 14:50:38.191: INFO: POD   NODE                                           PHASE    GRACE  CONDITIONS
May 16 14:50:38.191: INFO: ss-0  ip-10-0-161-164.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:49:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:49:57 +0000 UTC  }]
May 16 14:50:38.191: INFO: ss-1  ip-10-0-212-246.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:17 +0000 UTC  }]
May 16 14:50:38.191: INFO: ss-2  ip-10-0-132-142.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:17 +0000 UTC  }]
May 16 14:50:38.191: INFO: 
May 16 14:50:38.191: INFO: StatefulSet ss has not reached scale 0, at 3
May 16 14:50:39.195: INFO: POD   NODE                                           PHASE    GRACE  CONDITIONS
May 16 14:50:39.195: INFO: ss-0  ip-10-0-161-164.eu-central-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:49:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:49:57 +0000 UTC  }]
May 16 14:50:39.195: INFO: 
May 16 14:50:39.195: INFO: StatefulSet ss has not reached scale 0, at 1
May 16 14:50:40.199: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992783968s
May 16 14:50:41.203: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.988534082s
May 16 14:50:42.207: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.98477719s
May 16 14:50:43.211: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.98078108s
May 16 14:50:44.214: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.977147259s
May 16 14:50:45.218: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.973777346s
May 16 14:50:46.222: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.970636955s
May 16 14:50:47.226: INFO: Verifying statefulset ss doesn't scale past 0 for another 965.721018ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5697 05/16/23 14:50:48.226
May 16 14:50:48.230: INFO: Scaling statefulset ss to 0
May 16 14:50:48.239: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May 16 14:50:48.241: INFO: Deleting all statefulset in ns statefulset-5697
May 16 14:50:48.244: INFO: Scaling statefulset ss to 0
May 16 14:50:48.253: INFO: Waiting for statefulset status.replicas updated to 0
May 16 14:50:48.255: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May 16 14:50:48.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5697" for this suite. 05/16/23 14:50:48.271
------------------------------
â€¢ [SLOW TEST] [51.038 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:49:57.241
    May 16 14:49:57.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename statefulset 05/16/23 14:49:57.242
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:49:57.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:49:57.265
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5697 05/16/23 14:49:57.267
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-5697 05/16/23 14:49:57.273
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5697 05/16/23 14:49:57.283
    May 16 14:49:57.287: INFO: Found 0 stateful pods, waiting for 1
    May 16 14:50:07.291: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 05/16/23 14:50:07.291
    May 16 14:50:07.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-5697 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 16 14:50:07.404: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 16 14:50:07.404: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 16 14:50:07.404: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 16 14:50:07.407: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    May 16 14:50:17.411: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May 16 14:50:17.411: INFO: Waiting for statefulset status.replicas updated to 0
    May 16 14:50:17.426: INFO: POD   NODE                                           PHASE    GRACE  CONDITIONS
    May 16 14:50:17.426: INFO: ss-0  ip-10-0-161-164.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:49:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:49:57 +0000 UTC  }]
    May 16 14:50:17.426: INFO: 
    May 16 14:50:17.426: INFO: StatefulSet ss has not reached scale 3, at 1
    May 16 14:50:18.430: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996295775s
    May 16 14:50:19.434: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992304594s
    May 16 14:50:20.439: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987296041s
    May 16 14:50:21.443: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983593801s
    May 16 14:50:22.447: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.979853588s
    May 16 14:50:23.451: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975288212s
    May 16 14:50:24.455: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971664605s
    May 16 14:50:25.460: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96730109s
    May 16 14:50:26.464: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.208376ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5697 05/16/23 14:50:27.464
    May 16 14:50:27.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-5697 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 16 14:50:27.585: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 16 14:50:27.585: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 16 14:50:27.585: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 16 14:50:27.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-5697 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 16 14:50:27.719: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    May 16 14:50:27.719: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 16 14:50:27.719: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 16 14:50:27.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-5697 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 16 14:50:27.850: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    May 16 14:50:27.850: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 16 14:50:27.850: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 16 14:50:27.853: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May 16 14:50:27.853: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    May 16 14:50:27.853: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 05/16/23 14:50:27.853
    May 16 14:50:27.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-5697 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 16 14:50:27.964: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 16 14:50:27.964: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 16 14:50:27.964: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 16 14:50:27.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-5697 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 16 14:50:28.073: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 16 14:50:28.073: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 16 14:50:28.073: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 16 14:50:28.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-5697 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 16 14:50:28.170: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 16 14:50:28.170: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 16 14:50:28.170: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 16 14:50:28.170: INFO: Waiting for statefulset status.replicas updated to 0
    May 16 14:50:28.173: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    May 16 14:50:38.181: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May 16 14:50:38.181: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    May 16 14:50:38.181: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    May 16 14:50:38.191: INFO: POD   NODE                                           PHASE    GRACE  CONDITIONS
    May 16 14:50:38.191: INFO: ss-0  ip-10-0-161-164.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:49:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:49:57 +0000 UTC  }]
    May 16 14:50:38.191: INFO: ss-1  ip-10-0-212-246.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:17 +0000 UTC  }]
    May 16 14:50:38.191: INFO: ss-2  ip-10-0-132-142.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:17 +0000 UTC  }]
    May 16 14:50:38.191: INFO: 
    May 16 14:50:38.191: INFO: StatefulSet ss has not reached scale 0, at 3
    May 16 14:50:39.195: INFO: POD   NODE                                           PHASE    GRACE  CONDITIONS
    May 16 14:50:39.195: INFO: ss-0  ip-10-0-161-164.eu-central-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:49:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:50:28 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 14:49:57 +0000 UTC  }]
    May 16 14:50:39.195: INFO: 
    May 16 14:50:39.195: INFO: StatefulSet ss has not reached scale 0, at 1
    May 16 14:50:40.199: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.992783968s
    May 16 14:50:41.203: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.988534082s
    May 16 14:50:42.207: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.98477719s
    May 16 14:50:43.211: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.98078108s
    May 16 14:50:44.214: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.977147259s
    May 16 14:50:45.218: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.973777346s
    May 16 14:50:46.222: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.970636955s
    May 16 14:50:47.226: INFO: Verifying statefulset ss doesn't scale past 0 for another 965.721018ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5697 05/16/23 14:50:48.226
    May 16 14:50:48.230: INFO: Scaling statefulset ss to 0
    May 16 14:50:48.239: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May 16 14:50:48.241: INFO: Deleting all statefulset in ns statefulset-5697
    May 16 14:50:48.244: INFO: Scaling statefulset ss to 0
    May 16 14:50:48.253: INFO: Waiting for statefulset status.replicas updated to 0
    May 16 14:50:48.255: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May 16 14:50:48.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5697" for this suite. 05/16/23 14:50:48.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:50:48.279
May 16 14:50:48.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename cronjob 05/16/23 14:50:48.28
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:50:48.304
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:50:48.307
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 05/16/23 14:50:48.31
W0516 14:50:48.316945      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a job is scheduled 05/16/23 14:50:48.316
STEP: Ensuring exactly one is scheduled 05/16/23 14:51:00.32
STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/16/23 14:51:00.323
STEP: Ensuring no more jobs are scheduled 05/16/23 14:51:00.326
STEP: Removing cronjob 05/16/23 14:56:00.332
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May 16 14:56:00.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2115" for this suite. 05/16/23 14:56:00.342
------------------------------
â€¢ [SLOW TEST] [312.070 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:50:48.279
    May 16 14:50:48.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename cronjob 05/16/23 14:50:48.28
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:50:48.304
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:50:48.307
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 05/16/23 14:50:48.31
    W0516 14:50:48.316945      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring a job is scheduled 05/16/23 14:50:48.316
    STEP: Ensuring exactly one is scheduled 05/16/23 14:51:00.32
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 05/16/23 14:51:00.323
    STEP: Ensuring no more jobs are scheduled 05/16/23 14:51:00.326
    STEP: Removing cronjob 05/16/23 14:56:00.332
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May 16 14:56:00.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2115" for this suite. 05/16/23 14:56:00.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:56:00.35
May 16 14:56:00.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename configmap 05/16/23 14:56:00.351
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:56:00.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:56:00.376
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-2c5ab7c3-ea28-49c6-8b4c-079188134a80 05/16/23 14:56:00.379
STEP: Creating a pod to test consume configMaps 05/16/23 14:56:00.388
May 16 14:56:00.406: INFO: Waiting up to 5m0s for pod "pod-configmaps-bb8f1d55-8625-4cd0-b7af-40fc5dab3d43" in namespace "configmap-956" to be "Succeeded or Failed"
May 16 14:56:00.409: INFO: Pod "pod-configmaps-bb8f1d55-8625-4cd0-b7af-40fc5dab3d43": Phase="Pending", Reason="", readiness=false. Elapsed: 3.018206ms
May 16 14:56:02.414: INFO: Pod "pod-configmaps-bb8f1d55-8625-4cd0-b7af-40fc5dab3d43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007884704s
May 16 14:56:04.414: INFO: Pod "pod-configmaps-bb8f1d55-8625-4cd0-b7af-40fc5dab3d43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00825262s
STEP: Saw pod success 05/16/23 14:56:04.414
May 16 14:56:04.414: INFO: Pod "pod-configmaps-bb8f1d55-8625-4cd0-b7af-40fc5dab3d43" satisfied condition "Succeeded or Failed"
May 16 14:56:04.417: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-configmaps-bb8f1d55-8625-4cd0-b7af-40fc5dab3d43 container configmap-volume-test: <nil>
STEP: delete the pod 05/16/23 14:56:04.426
May 16 14:56:04.435: INFO: Waiting for pod pod-configmaps-bb8f1d55-8625-4cd0-b7af-40fc5dab3d43 to disappear
May 16 14:56:04.438: INFO: Pod pod-configmaps-bb8f1d55-8625-4cd0-b7af-40fc5dab3d43 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 16 14:56:04.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-956" for this suite. 05/16/23 14:56:04.443
------------------------------
â€¢ [4.098 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:56:00.35
    May 16 14:56:00.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename configmap 05/16/23 14:56:00.351
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:56:00.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:56:00.376
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-2c5ab7c3-ea28-49c6-8b4c-079188134a80 05/16/23 14:56:00.379
    STEP: Creating a pod to test consume configMaps 05/16/23 14:56:00.388
    May 16 14:56:00.406: INFO: Waiting up to 5m0s for pod "pod-configmaps-bb8f1d55-8625-4cd0-b7af-40fc5dab3d43" in namespace "configmap-956" to be "Succeeded or Failed"
    May 16 14:56:00.409: INFO: Pod "pod-configmaps-bb8f1d55-8625-4cd0-b7af-40fc5dab3d43": Phase="Pending", Reason="", readiness=false. Elapsed: 3.018206ms
    May 16 14:56:02.414: INFO: Pod "pod-configmaps-bb8f1d55-8625-4cd0-b7af-40fc5dab3d43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007884704s
    May 16 14:56:04.414: INFO: Pod "pod-configmaps-bb8f1d55-8625-4cd0-b7af-40fc5dab3d43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00825262s
    STEP: Saw pod success 05/16/23 14:56:04.414
    May 16 14:56:04.414: INFO: Pod "pod-configmaps-bb8f1d55-8625-4cd0-b7af-40fc5dab3d43" satisfied condition "Succeeded or Failed"
    May 16 14:56:04.417: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-configmaps-bb8f1d55-8625-4cd0-b7af-40fc5dab3d43 container configmap-volume-test: <nil>
    STEP: delete the pod 05/16/23 14:56:04.426
    May 16 14:56:04.435: INFO: Waiting for pod pod-configmaps-bb8f1d55-8625-4cd0-b7af-40fc5dab3d43 to disappear
    May 16 14:56:04.438: INFO: Pod pod-configmaps-bb8f1d55-8625-4cd0-b7af-40fc5dab3d43 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 16 14:56:04.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-956" for this suite. 05/16/23 14:56:04.443
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:56:04.449
May 16 14:56:04.449: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename downward-api 05/16/23 14:56:04.449
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:56:04.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:56:04.471
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 05/16/23 14:56:04.475
May 16 14:56:04.491: INFO: Waiting up to 5m0s for pod "labelsupdate31277993-ac11-4a36-a1c7-8508678507fc" in namespace "downward-api-5546" to be "running and ready"
May 16 14:56:04.494: INFO: Pod "labelsupdate31277993-ac11-4a36-a1c7-8508678507fc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.086669ms
May 16 14:56:04.494: INFO: The phase of Pod labelsupdate31277993-ac11-4a36-a1c7-8508678507fc is Pending, waiting for it to be Running (with Ready = true)
May 16 14:56:06.497: INFO: Pod "labelsupdate31277993-ac11-4a36-a1c7-8508678507fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.006722277s
May 16 14:56:06.497: INFO: The phase of Pod labelsupdate31277993-ac11-4a36-a1c7-8508678507fc is Running (Ready = true)
May 16 14:56:06.497: INFO: Pod "labelsupdate31277993-ac11-4a36-a1c7-8508678507fc" satisfied condition "running and ready"
May 16 14:56:07.018: INFO: Successfully updated pod "labelsupdate31277993-ac11-4a36-a1c7-8508678507fc"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 16 14:56:11.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5546" for this suite. 05/16/23 14:56:11.044
------------------------------
â€¢ [SLOW TEST] [6.602 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:56:04.449
    May 16 14:56:04.449: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename downward-api 05/16/23 14:56:04.449
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:56:04.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:56:04.471
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 05/16/23 14:56:04.475
    May 16 14:56:04.491: INFO: Waiting up to 5m0s for pod "labelsupdate31277993-ac11-4a36-a1c7-8508678507fc" in namespace "downward-api-5546" to be "running and ready"
    May 16 14:56:04.494: INFO: Pod "labelsupdate31277993-ac11-4a36-a1c7-8508678507fc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.086669ms
    May 16 14:56:04.494: INFO: The phase of Pod labelsupdate31277993-ac11-4a36-a1c7-8508678507fc is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:56:06.497: INFO: Pod "labelsupdate31277993-ac11-4a36-a1c7-8508678507fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.006722277s
    May 16 14:56:06.497: INFO: The phase of Pod labelsupdate31277993-ac11-4a36-a1c7-8508678507fc is Running (Ready = true)
    May 16 14:56:06.497: INFO: Pod "labelsupdate31277993-ac11-4a36-a1c7-8508678507fc" satisfied condition "running and ready"
    May 16 14:56:07.018: INFO: Successfully updated pod "labelsupdate31277993-ac11-4a36-a1c7-8508678507fc"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 16 14:56:11.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5546" for this suite. 05/16/23 14:56:11.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:56:11.051
May 16 14:56:11.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename downward-api 05/16/23 14:56:11.052
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:56:11.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:56:11.075
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 05/16/23 14:56:11.078
May 16 14:56:11.090: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1469084e-2ef7-4791-9dba-8f18b41669a2" in namespace "downward-api-5324" to be "Succeeded or Failed"
May 16 14:56:11.093: INFO: Pod "downwardapi-volume-1469084e-2ef7-4791-9dba-8f18b41669a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.943054ms
May 16 14:56:13.097: INFO: Pod "downwardapi-volume-1469084e-2ef7-4791-9dba-8f18b41669a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007204499s
May 16 14:56:15.097: INFO: Pod "downwardapi-volume-1469084e-2ef7-4791-9dba-8f18b41669a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006854021s
STEP: Saw pod success 05/16/23 14:56:15.097
May 16 14:56:15.097: INFO: Pod "downwardapi-volume-1469084e-2ef7-4791-9dba-8f18b41669a2" satisfied condition "Succeeded or Failed"
May 16 14:56:15.100: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-1469084e-2ef7-4791-9dba-8f18b41669a2 container client-container: <nil>
STEP: delete the pod 05/16/23 14:56:15.105
May 16 14:56:15.114: INFO: Waiting for pod downwardapi-volume-1469084e-2ef7-4791-9dba-8f18b41669a2 to disappear
May 16 14:56:15.117: INFO: Pod downwardapi-volume-1469084e-2ef7-4791-9dba-8f18b41669a2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 16 14:56:15.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5324" for this suite. 05/16/23 14:56:15.121
------------------------------
â€¢ [4.076 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:56:11.051
    May 16 14:56:11.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename downward-api 05/16/23 14:56:11.052
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:56:11.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:56:11.075
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 05/16/23 14:56:11.078
    May 16 14:56:11.090: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1469084e-2ef7-4791-9dba-8f18b41669a2" in namespace "downward-api-5324" to be "Succeeded or Failed"
    May 16 14:56:11.093: INFO: Pod "downwardapi-volume-1469084e-2ef7-4791-9dba-8f18b41669a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.943054ms
    May 16 14:56:13.097: INFO: Pod "downwardapi-volume-1469084e-2ef7-4791-9dba-8f18b41669a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007204499s
    May 16 14:56:15.097: INFO: Pod "downwardapi-volume-1469084e-2ef7-4791-9dba-8f18b41669a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006854021s
    STEP: Saw pod success 05/16/23 14:56:15.097
    May 16 14:56:15.097: INFO: Pod "downwardapi-volume-1469084e-2ef7-4791-9dba-8f18b41669a2" satisfied condition "Succeeded or Failed"
    May 16 14:56:15.100: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-1469084e-2ef7-4791-9dba-8f18b41669a2 container client-container: <nil>
    STEP: delete the pod 05/16/23 14:56:15.105
    May 16 14:56:15.114: INFO: Waiting for pod downwardapi-volume-1469084e-2ef7-4791-9dba-8f18b41669a2 to disappear
    May 16 14:56:15.117: INFO: Pod downwardapi-volume-1469084e-2ef7-4791-9dba-8f18b41669a2 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 16 14:56:15.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5324" for this suite. 05/16/23 14:56:15.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:56:15.128
May 16 14:56:15.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename sched-preemption 05/16/23 14:56:15.129
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:56:15.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:56:15.148
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
May 16 14:56:15.167: INFO: Waiting up to 1m0s for all nodes to be ready
May 16 14:57:15.286: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:57:15.291
May 16 14:57:15.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename sched-preemption-path 05/16/23 14:57:15.291
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:57:15.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:57:15.315
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 05/16/23 14:57:15.319
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/16/23 14:57:15.319
May 16 14:57:15.333: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7156" to be "running"
May 16 14:57:15.338: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.784695ms
May 16 14:57:17.343: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009242666s
May 16 14:57:17.343: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/16/23 14:57:17.345
May 16 14:57:17.354: INFO: found a healthy node: ip-10-0-161-164.eu-central-1.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
May 16 14:57:23.446: INFO: pods created so far: [1 1 1]
May 16 14:57:23.446: INFO: length of pods created so far: 3
May 16 14:57:25.458: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
May 16 14:57:32.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 14:57:32.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-7156" for this suite. 05/16/23 14:57:32.554
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-4342" for this suite. 05/16/23 14:57:32.564
------------------------------
â€¢ [SLOW TEST] [77.451 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:56:15.128
    May 16 14:56:15.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename sched-preemption 05/16/23 14:56:15.129
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:56:15.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:56:15.148
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    May 16 14:56:15.167: INFO: Waiting up to 1m0s for all nodes to be ready
    May 16 14:57:15.286: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:57:15.291
    May 16 14:57:15.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename sched-preemption-path 05/16/23 14:57:15.291
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:57:15.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:57:15.315
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 05/16/23 14:57:15.319
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/16/23 14:57:15.319
    May 16 14:57:15.333: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7156" to be "running"
    May 16 14:57:15.338: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.784695ms
    May 16 14:57:17.343: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009242666s
    May 16 14:57:17.343: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/16/23 14:57:17.345
    May 16 14:57:17.354: INFO: found a healthy node: ip-10-0-161-164.eu-central-1.compute.internal
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    May 16 14:57:23.446: INFO: pods created so far: [1 1 1]
    May 16 14:57:23.446: INFO: length of pods created so far: 3
    May 16 14:57:25.458: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    May 16 14:57:32.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 14:57:32.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-7156" for this suite. 05/16/23 14:57:32.554
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-4342" for this suite. 05/16/23 14:57:32.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:57:32.58
May 16 14:57:32.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename containers 05/16/23 14:57:32.58
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:57:32.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:57:32.617
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 05/16/23 14:57:32.621
May 16 14:57:32.638: INFO: Waiting up to 5m0s for pod "client-containers-a6e4d06e-b342-42a1-a2d2-8ea2edb10a33" in namespace "containers-336" to be "Succeeded or Failed"
May 16 14:57:32.641: INFO: Pod "client-containers-a6e4d06e-b342-42a1-a2d2-8ea2edb10a33": Phase="Pending", Reason="", readiness=false. Elapsed: 3.305458ms
May 16 14:57:34.645: INFO: Pod "client-containers-a6e4d06e-b342-42a1-a2d2-8ea2edb10a33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006946983s
May 16 14:57:36.645: INFO: Pod "client-containers-a6e4d06e-b342-42a1-a2d2-8ea2edb10a33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007424648s
STEP: Saw pod success 05/16/23 14:57:36.645
May 16 14:57:36.645: INFO: Pod "client-containers-a6e4d06e-b342-42a1-a2d2-8ea2edb10a33" satisfied condition "Succeeded or Failed"
May 16 14:57:36.648: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod client-containers-a6e4d06e-b342-42a1-a2d2-8ea2edb10a33 container agnhost-container: <nil>
STEP: delete the pod 05/16/23 14:57:36.659
May 16 14:57:36.669: INFO: Waiting for pod client-containers-a6e4d06e-b342-42a1-a2d2-8ea2edb10a33 to disappear
May 16 14:57:36.671: INFO: Pod client-containers-a6e4d06e-b342-42a1-a2d2-8ea2edb10a33 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
May 16 14:57:36.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-336" for this suite. 05/16/23 14:57:36.675
------------------------------
â€¢ [4.101 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:57:32.58
    May 16 14:57:32.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename containers 05/16/23 14:57:32.58
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:57:32.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:57:32.617
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 05/16/23 14:57:32.621
    May 16 14:57:32.638: INFO: Waiting up to 5m0s for pod "client-containers-a6e4d06e-b342-42a1-a2d2-8ea2edb10a33" in namespace "containers-336" to be "Succeeded or Failed"
    May 16 14:57:32.641: INFO: Pod "client-containers-a6e4d06e-b342-42a1-a2d2-8ea2edb10a33": Phase="Pending", Reason="", readiness=false. Elapsed: 3.305458ms
    May 16 14:57:34.645: INFO: Pod "client-containers-a6e4d06e-b342-42a1-a2d2-8ea2edb10a33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006946983s
    May 16 14:57:36.645: INFO: Pod "client-containers-a6e4d06e-b342-42a1-a2d2-8ea2edb10a33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007424648s
    STEP: Saw pod success 05/16/23 14:57:36.645
    May 16 14:57:36.645: INFO: Pod "client-containers-a6e4d06e-b342-42a1-a2d2-8ea2edb10a33" satisfied condition "Succeeded or Failed"
    May 16 14:57:36.648: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod client-containers-a6e4d06e-b342-42a1-a2d2-8ea2edb10a33 container agnhost-container: <nil>
    STEP: delete the pod 05/16/23 14:57:36.659
    May 16 14:57:36.669: INFO: Waiting for pod client-containers-a6e4d06e-b342-42a1-a2d2-8ea2edb10a33 to disappear
    May 16 14:57:36.671: INFO: Pod client-containers-a6e4d06e-b342-42a1-a2d2-8ea2edb10a33 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    May 16 14:57:36.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-336" for this suite. 05/16/23 14:57:36.675
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:57:36.681
May 16 14:57:36.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename pods 05/16/23 14:57:36.681
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:57:36.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:57:36.702
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 05/16/23 14:57:36.707
STEP: submitting the pod to kubernetes 05/16/23 14:57:36.707
May 16 14:57:36.720: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5" in namespace "pods-4533" to be "running and ready"
May 16 14:57:36.723: INFO: Pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.260719ms
May 16 14:57:36.723: INFO: The phase of Pod pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5 is Pending, waiting for it to be Running (with Ready = true)
May 16 14:57:38.728: INFO: Pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007503756s
May 16 14:57:38.728: INFO: The phase of Pod pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5 is Running (Ready = true)
May 16 14:57:38.728: INFO: Pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 05/16/23 14:57:38.731
STEP: updating the pod 05/16/23 14:57:38.734
May 16 14:57:39.246: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5"
May 16 14:57:39.246: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5" in namespace "pods-4533" to be "terminated with reason DeadlineExceeded"
May 16 14:57:39.249: INFO: Pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5": Phase="Running", Reason="", readiness=true. Elapsed: 2.926438ms
May 16 14:57:41.253: INFO: Pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007192139s
May 16 14:57:43.253: INFO: Pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5": Phase="Running", Reason="", readiness=false. Elapsed: 4.007120468s
May 16 14:57:45.253: INFO: Pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.007033663s
May 16 14:57:45.253: INFO: Pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 16 14:57:45.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4533" for this suite. 05/16/23 14:57:45.257
------------------------------
â€¢ [SLOW TEST] [8.583 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:57:36.681
    May 16 14:57:36.681: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename pods 05/16/23 14:57:36.681
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:57:36.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:57:36.702
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 05/16/23 14:57:36.707
    STEP: submitting the pod to kubernetes 05/16/23 14:57:36.707
    May 16 14:57:36.720: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5" in namespace "pods-4533" to be "running and ready"
    May 16 14:57:36.723: INFO: Pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.260719ms
    May 16 14:57:36.723: INFO: The phase of Pod pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5 is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:57:38.728: INFO: Pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007503756s
    May 16 14:57:38.728: INFO: The phase of Pod pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5 is Running (Ready = true)
    May 16 14:57:38.728: INFO: Pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 05/16/23 14:57:38.731
    STEP: updating the pod 05/16/23 14:57:38.734
    May 16 14:57:39.246: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5"
    May 16 14:57:39.246: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5" in namespace "pods-4533" to be "terminated with reason DeadlineExceeded"
    May 16 14:57:39.249: INFO: Pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5": Phase="Running", Reason="", readiness=true. Elapsed: 2.926438ms
    May 16 14:57:41.253: INFO: Pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007192139s
    May 16 14:57:43.253: INFO: Pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5": Phase="Running", Reason="", readiness=false. Elapsed: 4.007120468s
    May 16 14:57:45.253: INFO: Pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.007033663s
    May 16 14:57:45.253: INFO: Pod "pod-update-activedeadlineseconds-1b8c03cf-89e1-4954-86ee-d83cde3e0de5" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 16 14:57:45.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4533" for this suite. 05/16/23 14:57:45.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:57:45.264
May 16 14:57:45.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename webhook 05/16/23 14:57:45.265
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:57:45.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:57:45.285
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/16/23 14:57:45.311
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 14:57:45.616
STEP: Deploying the webhook pod 05/16/23 14:57:45.625
STEP: Wait for the deployment to be ready 05/16/23 14:57:45.635
May 16 14:57:45.643: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/16/23 14:57:47.651
STEP: Verifying the service has paired with the endpoint 05/16/23 14:57:47.66
May 16 14:57:48.662: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 05/16/23 14:57:48.665
STEP: Registering slow webhook via the AdmissionRegistration API 05/16/23 14:57:48.665
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 05/16/23 14:57:48.68
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 05/16/23 14:57:49.688
STEP: Registering slow webhook via the AdmissionRegistration API 05/16/23 14:57:49.688
STEP: Having no error when timeout is longer than webhook latency 05/16/23 14:57:50.715
STEP: Registering slow webhook via the AdmissionRegistration API 05/16/23 14:57:50.715
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 05/16/23 14:57:55.745
STEP: Registering slow webhook via the AdmissionRegistration API 05/16/23 14:57:55.745
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 14:58:00.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3021" for this suite. 05/16/23 14:58:00.846
STEP: Destroying namespace "webhook-3021-markers" for this suite. 05/16/23 14:58:00.857
------------------------------
â€¢ [SLOW TEST] [15.600 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:57:45.264
    May 16 14:57:45.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename webhook 05/16/23 14:57:45.265
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:57:45.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:57:45.285
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/16/23 14:57:45.311
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 14:57:45.616
    STEP: Deploying the webhook pod 05/16/23 14:57:45.625
    STEP: Wait for the deployment to be ready 05/16/23 14:57:45.635
    May 16 14:57:45.643: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/16/23 14:57:47.651
    STEP: Verifying the service has paired with the endpoint 05/16/23 14:57:47.66
    May 16 14:57:48.662: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 05/16/23 14:57:48.665
    STEP: Registering slow webhook via the AdmissionRegistration API 05/16/23 14:57:48.665
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 05/16/23 14:57:48.68
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 05/16/23 14:57:49.688
    STEP: Registering slow webhook via the AdmissionRegistration API 05/16/23 14:57:49.688
    STEP: Having no error when timeout is longer than webhook latency 05/16/23 14:57:50.715
    STEP: Registering slow webhook via the AdmissionRegistration API 05/16/23 14:57:50.715
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 05/16/23 14:57:55.745
    STEP: Registering slow webhook via the AdmissionRegistration API 05/16/23 14:57:55.745
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 14:58:00.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3021" for this suite. 05/16/23 14:58:00.846
    STEP: Destroying namespace "webhook-3021-markers" for this suite. 05/16/23 14:58:00.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:58:00.865
May 16 14:58:00.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename replicaset 05/16/23 14:58:00.866
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:58:00.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:58:00.904
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
May 16 14:58:00.907: INFO: Creating ReplicaSet my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a
W0516 14:58:00.915679      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 14:58:00.920: INFO: Pod name my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a: Found 0 pods out of 1
May 16 14:58:05.923: INFO: Pod name my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a: Found 1 pods out of 1
May 16 14:58:05.923: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a" is running
May 16 14:58:05.923: INFO: Waiting up to 5m0s for pod "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a-mpm4d" in namespace "replicaset-5621" to be "running"
May 16 14:58:05.927: INFO: Pod "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a-mpm4d": Phase="Running", Reason="", readiness=true. Elapsed: 3.652843ms
May 16 14:58:05.927: INFO: Pod "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a-mpm4d" satisfied condition "running"
May 16 14:58:05.927: INFO: Pod "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a-mpm4d" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-16 14:58:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-16 14:58:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-16 14:58:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-16 14:58:00 +0000 UTC Reason: Message:}])
May 16 14:58:05.927: INFO: Trying to dial the pod
May 16 14:58:10.937: INFO: Controller my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a: Got expected result from replica 1 [my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a-mpm4d]: "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a-mpm4d", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May 16 14:58:10.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5621" for this suite. 05/16/23 14:58:10.941
------------------------------
â€¢ [SLOW TEST] [10.082 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:58:00.865
    May 16 14:58:00.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename replicaset 05/16/23 14:58:00.866
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:58:00.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:58:00.904
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    May 16 14:58:00.907: INFO: Creating ReplicaSet my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a
    W0516 14:58:00.915679      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 14:58:00.920: INFO: Pod name my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a: Found 0 pods out of 1
    May 16 14:58:05.923: INFO: Pod name my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a: Found 1 pods out of 1
    May 16 14:58:05.923: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a" is running
    May 16 14:58:05.923: INFO: Waiting up to 5m0s for pod "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a-mpm4d" in namespace "replicaset-5621" to be "running"
    May 16 14:58:05.927: INFO: Pod "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a-mpm4d": Phase="Running", Reason="", readiness=true. Elapsed: 3.652843ms
    May 16 14:58:05.927: INFO: Pod "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a-mpm4d" satisfied condition "running"
    May 16 14:58:05.927: INFO: Pod "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a-mpm4d" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-16 14:58:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-16 14:58:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-16 14:58:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-16 14:58:00 +0000 UTC Reason: Message:}])
    May 16 14:58:05.927: INFO: Trying to dial the pod
    May 16 14:58:10.937: INFO: Controller my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a: Got expected result from replica 1 [my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a-mpm4d]: "my-hostname-basic-8811aa7c-eea2-4908-9f5f-ba047eac5d9a-mpm4d", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May 16 14:58:10.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5621" for this suite. 05/16/23 14:58:10.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:58:10.948
May 16 14:58:10.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename container-probe 05/16/23 14:58:10.949
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:58:10.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:58:10.969
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May 16 14:59:10.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8397" for this suite. 05/16/23 14:59:10.992
------------------------------
â€¢ [SLOW TEST] [60.050 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:58:10.948
    May 16 14:58:10.948: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename container-probe 05/16/23 14:58:10.949
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:58:10.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:58:10.969
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May 16 14:59:10.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8397" for this suite. 05/16/23 14:59:10.992
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:59:10.998
May 16 14:59:10.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename webhook 05/16/23 14:59:10.999
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:59:11.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:59:11.042
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/16/23 14:59:11.06
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 14:59:11.376
STEP: Deploying the webhook pod 05/16/23 14:59:11.384
STEP: Wait for the deployment to be ready 05/16/23 14:59:11.395
May 16 14:59:11.403: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/16/23 14:59:13.412
STEP: Verifying the service has paired with the endpoint 05/16/23 14:59:13.423
May 16 14:59:14.423: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 05/16/23 14:59:14.426
STEP: create a configmap that should be updated by the webhook 05/16/23 14:59:14.438
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 14:59:14.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8341" for this suite. 05/16/23 14:59:14.53
STEP: Destroying namespace "webhook-8341-markers" for this suite. 05/16/23 14:59:14.544
------------------------------
â€¢ [3.555 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:59:10.998
    May 16 14:59:10.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename webhook 05/16/23 14:59:10.999
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:59:11.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:59:11.042
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/16/23 14:59:11.06
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 14:59:11.376
    STEP: Deploying the webhook pod 05/16/23 14:59:11.384
    STEP: Wait for the deployment to be ready 05/16/23 14:59:11.395
    May 16 14:59:11.403: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/16/23 14:59:13.412
    STEP: Verifying the service has paired with the endpoint 05/16/23 14:59:13.423
    May 16 14:59:14.423: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 05/16/23 14:59:14.426
    STEP: create a configmap that should be updated by the webhook 05/16/23 14:59:14.438
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 14:59:14.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8341" for this suite. 05/16/23 14:59:14.53
    STEP: Destroying namespace "webhook-8341-markers" for this suite. 05/16/23 14:59:14.544
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:59:14.553
May 16 14:59:14.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename emptydir 05/16/23 14:59:14.554
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:59:14.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:59:14.586
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 05/16/23 14:59:14.589
May 16 14:59:14.606: INFO: Waiting up to 5m0s for pod "pod-ab09418e-cd43-47a0-b331-74471924c4e3" in namespace "emptydir-5208" to be "Succeeded or Failed"
May 16 14:59:14.609: INFO: Pod "pod-ab09418e-cd43-47a0-b331-74471924c4e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.6267ms
May 16 14:59:16.612: INFO: Pod "pod-ab09418e-cd43-47a0-b331-74471924c4e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006062592s
May 16 14:59:18.613: INFO: Pod "pod-ab09418e-cd43-47a0-b331-74471924c4e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006797724s
STEP: Saw pod success 05/16/23 14:59:18.613
May 16 14:59:18.613: INFO: Pod "pod-ab09418e-cd43-47a0-b331-74471924c4e3" satisfied condition "Succeeded or Failed"
May 16 14:59:18.616: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-ab09418e-cd43-47a0-b331-74471924c4e3 container test-container: <nil>
STEP: delete the pod 05/16/23 14:59:18.624
May 16 14:59:18.633: INFO: Waiting for pod pod-ab09418e-cd43-47a0-b331-74471924c4e3 to disappear
May 16 14:59:18.636: INFO: Pod pod-ab09418e-cd43-47a0-b331-74471924c4e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 16 14:59:18.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5208" for this suite. 05/16/23 14:59:18.641
------------------------------
â€¢ [4.093 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:59:14.553
    May 16 14:59:14.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename emptydir 05/16/23 14:59:14.554
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:59:14.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:59:14.586
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 05/16/23 14:59:14.589
    May 16 14:59:14.606: INFO: Waiting up to 5m0s for pod "pod-ab09418e-cd43-47a0-b331-74471924c4e3" in namespace "emptydir-5208" to be "Succeeded or Failed"
    May 16 14:59:14.609: INFO: Pod "pod-ab09418e-cd43-47a0-b331-74471924c4e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.6267ms
    May 16 14:59:16.612: INFO: Pod "pod-ab09418e-cd43-47a0-b331-74471924c4e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006062592s
    May 16 14:59:18.613: INFO: Pod "pod-ab09418e-cd43-47a0-b331-74471924c4e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006797724s
    STEP: Saw pod success 05/16/23 14:59:18.613
    May 16 14:59:18.613: INFO: Pod "pod-ab09418e-cd43-47a0-b331-74471924c4e3" satisfied condition "Succeeded or Failed"
    May 16 14:59:18.616: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-ab09418e-cd43-47a0-b331-74471924c4e3 container test-container: <nil>
    STEP: delete the pod 05/16/23 14:59:18.624
    May 16 14:59:18.633: INFO: Waiting for pod pod-ab09418e-cd43-47a0-b331-74471924c4e3 to disappear
    May 16 14:59:18.636: INFO: Pod pod-ab09418e-cd43-47a0-b331-74471924c4e3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 16 14:59:18.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5208" for this suite. 05/16/23 14:59:18.641
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:59:18.646
May 16 14:59:18.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename disruption 05/16/23 14:59:18.647
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:59:18.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:59:18.679
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 05/16/23 14:59:18.694
STEP: Updating PodDisruptionBudget status 05/16/23 14:59:20.702
STEP: Waiting for all pods to be running 05/16/23 14:59:20.711
May 16 14:59:20.713: INFO: running pods: 0 < 1
STEP: locating a running pod 05/16/23 14:59:22.717
STEP: Waiting for the pdb to be processed 05/16/23 14:59:22.728
STEP: Patching PodDisruptionBudget status 05/16/23 14:59:22.733
STEP: Waiting for the pdb to be processed 05/16/23 14:59:22.741
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May 16 14:59:22.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4138" for this suite. 05/16/23 14:59:22.748
------------------------------
â€¢ [4.108 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:59:18.646
    May 16 14:59:18.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename disruption 05/16/23 14:59:18.647
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:59:18.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:59:18.679
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 05/16/23 14:59:18.694
    STEP: Updating PodDisruptionBudget status 05/16/23 14:59:20.702
    STEP: Waiting for all pods to be running 05/16/23 14:59:20.711
    May 16 14:59:20.713: INFO: running pods: 0 < 1
    STEP: locating a running pod 05/16/23 14:59:22.717
    STEP: Waiting for the pdb to be processed 05/16/23 14:59:22.728
    STEP: Patching PodDisruptionBudget status 05/16/23 14:59:22.733
    STEP: Waiting for the pdb to be processed 05/16/23 14:59:22.741
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May 16 14:59:22.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4138" for this suite. 05/16/23 14:59:22.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:59:22.756
May 16 14:59:22.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 14:59:22.757
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:59:22.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:59:22.775
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 05/16/23 14:59:22.778
May 16 14:59:22.791: INFO: Waiting up to 5m0s for pod "annotationupdateee354e2b-cec7-4217-988f-137033d77118" in namespace "projected-8286" to be "running and ready"
May 16 14:59:22.793: INFO: Pod "annotationupdateee354e2b-cec7-4217-988f-137033d77118": Phase="Pending", Reason="", readiness=false. Elapsed: 2.68858ms
May 16 14:59:22.793: INFO: The phase of Pod annotationupdateee354e2b-cec7-4217-988f-137033d77118 is Pending, waiting for it to be Running (with Ready = true)
May 16 14:59:24.797: INFO: Pod "annotationupdateee354e2b-cec7-4217-988f-137033d77118": Phase="Running", Reason="", readiness=true. Elapsed: 2.00647004s
May 16 14:59:24.797: INFO: The phase of Pod annotationupdateee354e2b-cec7-4217-988f-137033d77118 is Running (Ready = true)
May 16 14:59:24.797: INFO: Pod "annotationupdateee354e2b-cec7-4217-988f-137033d77118" satisfied condition "running and ready"
May 16 14:59:25.317: INFO: Successfully updated pod "annotationupdateee354e2b-cec7-4217-988f-137033d77118"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 16 14:59:29.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8286" for this suite. 05/16/23 14:59:29.344
------------------------------
â€¢ [SLOW TEST] [6.593 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:59:22.756
    May 16 14:59:22.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 14:59:22.757
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:59:22.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:59:22.775
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 05/16/23 14:59:22.778
    May 16 14:59:22.791: INFO: Waiting up to 5m0s for pod "annotationupdateee354e2b-cec7-4217-988f-137033d77118" in namespace "projected-8286" to be "running and ready"
    May 16 14:59:22.793: INFO: Pod "annotationupdateee354e2b-cec7-4217-988f-137033d77118": Phase="Pending", Reason="", readiness=false. Elapsed: 2.68858ms
    May 16 14:59:22.793: INFO: The phase of Pod annotationupdateee354e2b-cec7-4217-988f-137033d77118 is Pending, waiting for it to be Running (with Ready = true)
    May 16 14:59:24.797: INFO: Pod "annotationupdateee354e2b-cec7-4217-988f-137033d77118": Phase="Running", Reason="", readiness=true. Elapsed: 2.00647004s
    May 16 14:59:24.797: INFO: The phase of Pod annotationupdateee354e2b-cec7-4217-988f-137033d77118 is Running (Ready = true)
    May 16 14:59:24.797: INFO: Pod "annotationupdateee354e2b-cec7-4217-988f-137033d77118" satisfied condition "running and ready"
    May 16 14:59:25.317: INFO: Successfully updated pod "annotationupdateee354e2b-cec7-4217-988f-137033d77118"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 16 14:59:29.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8286" for this suite. 05/16/23 14:59:29.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:59:29.35
May 16 14:59:29.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename aggregator 05/16/23 14:59:29.35
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:59:29.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:59:29.371
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
May 16 14:59:29.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 05/16/23 14:59:29.374
May 16 14:59:29.650: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 16 14:59:31.698: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 14:59:33.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 14:59:35.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 14:59:37.704: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 14:59:39.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 14:59:41.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 14:59:43.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 14:59:45.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 14:59:47.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 14:59:49.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 14:59:51.705: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 14:59:53.847: INFO: Waited 139.197041ms for the sample-apiserver to be ready to handle requests.
I0516 14:59:54.925084      22 request.go:690] Waited for 1.044747098s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/infrastructure.cluster.x-k8s.io/v1alpha5
STEP: Read Status for v1alpha1.wardle.example.com 05/16/23 14:59:55.586
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 05/16/23 14:59:55.627
STEP: List APIServices 05/16/23 14:59:55.679
May 16 14:59:55.733: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
May 16 14:59:56.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-9731" for this suite. 05/16/23 14:59:56.578
------------------------------
â€¢ [SLOW TEST] [27.280 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:59:29.35
    May 16 14:59:29.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename aggregator 05/16/23 14:59:29.35
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:59:29.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:59:29.371
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    May 16 14:59:29.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 05/16/23 14:59:29.374
    May 16 14:59:29.650: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    May 16 14:59:31.698: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 16 14:59:33.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 16 14:59:35.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 16 14:59:37.704: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 16 14:59:39.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 16 14:59:41.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 16 14:59:43.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 16 14:59:45.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 16 14:59:47.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 16 14:59:49.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 16 14:59:51.705: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 14, 59, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 16 14:59:53.847: INFO: Waited 139.197041ms for the sample-apiserver to be ready to handle requests.
    I0516 14:59:54.925084      22 request.go:690] Waited for 1.044747098s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/infrastructure.cluster.x-k8s.io/v1alpha5
    STEP: Read Status for v1alpha1.wardle.example.com 05/16/23 14:59:55.586
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 05/16/23 14:59:55.627
    STEP: List APIServices 05/16/23 14:59:55.679
    May 16 14:59:55.733: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    May 16 14:59:56.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-9731" for this suite. 05/16/23 14:59:56.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 14:59:56.629
May 16 14:59:56.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename dns 05/16/23 14:59:56.63
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:59:56.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:59:56.653
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 05/16/23 14:59:56.658
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-437.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-437.svc.cluster.local;sleep 1; done
 05/16/23 14:59:56.664
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-437.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-437.svc.cluster.local;sleep 1; done
 05/16/23 14:59:56.664
STEP: creating a pod to probe DNS 05/16/23 14:59:56.664
STEP: submitting the pod to kubernetes 05/16/23 14:59:56.664
May 16 14:59:56.680: INFO: Waiting up to 15m0s for pod "dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b" in namespace "dns-437" to be "running"
May 16 14:59:56.686: INFO: Pod "dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.581775ms
May 16 14:59:58.690: INFO: Pod "dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009792518s
May 16 14:59:58.690: INFO: Pod "dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b" satisfied condition "running"
STEP: retrieving the pod 05/16/23 14:59:58.69
STEP: looking for the results for each expected name from probers 05/16/23 14:59:58.693
May 16 14:59:58.697: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local from pod dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b: the server could not find the requested resource (get pods dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b)
May 16 14:59:58.701: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local from pod dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b: the server could not find the requested resource (get pods dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b)
May 16 14:59:58.703: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-437.svc.cluster.local from pod dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b: the server could not find the requested resource (get pods dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b)
May 16 14:59:58.706: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-437.svc.cluster.local from pod dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b: the server could not find the requested resource (get pods dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b)
May 16 14:59:58.709: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local from pod dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b: the server could not find the requested resource (get pods dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b)
May 16 14:59:58.713: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local from pod dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b: the server could not find the requested resource (get pods dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b)
May 16 14:59:58.716: INFO: Unable to read jessie_udp@dns-test-service-2.dns-437.svc.cluster.local from pod dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b: the server could not find the requested resource (get pods dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b)
May 16 14:59:58.718: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-437.svc.cluster.local from pod dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b: the server could not find the requested resource (get pods dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b)
May 16 14:59:58.718: INFO: Lookups using dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local wheezy_udp@dns-test-service-2.dns-437.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-437.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local jessie_udp@dns-test-service-2.dns-437.svc.cluster.local jessie_tcp@dns-test-service-2.dns-437.svc.cluster.local]

May 16 15:00:03.747: INFO: DNS probes using dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b succeeded

STEP: deleting the pod 05/16/23 15:00:03.747
STEP: deleting the test headless service 05/16/23 15:00:03.757
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May 16 15:00:03.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-437" for this suite. 05/16/23 15:00:03.78
------------------------------
â€¢ [SLOW TEST] [7.158 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 14:59:56.629
    May 16 14:59:56.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename dns 05/16/23 14:59:56.63
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 14:59:56.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 14:59:56.653
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 05/16/23 14:59:56.658
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-437.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-437.svc.cluster.local;sleep 1; done
     05/16/23 14:59:56.664
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-437.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-437.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-437.svc.cluster.local;sleep 1; done
     05/16/23 14:59:56.664
    STEP: creating a pod to probe DNS 05/16/23 14:59:56.664
    STEP: submitting the pod to kubernetes 05/16/23 14:59:56.664
    May 16 14:59:56.680: INFO: Waiting up to 15m0s for pod "dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b" in namespace "dns-437" to be "running"
    May 16 14:59:56.686: INFO: Pod "dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.581775ms
    May 16 14:59:58.690: INFO: Pod "dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009792518s
    May 16 14:59:58.690: INFO: Pod "dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b" satisfied condition "running"
    STEP: retrieving the pod 05/16/23 14:59:58.69
    STEP: looking for the results for each expected name from probers 05/16/23 14:59:58.693
    May 16 14:59:58.697: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local from pod dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b: the server could not find the requested resource (get pods dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b)
    May 16 14:59:58.701: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local from pod dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b: the server could not find the requested resource (get pods dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b)
    May 16 14:59:58.703: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-437.svc.cluster.local from pod dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b: the server could not find the requested resource (get pods dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b)
    May 16 14:59:58.706: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-437.svc.cluster.local from pod dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b: the server could not find the requested resource (get pods dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b)
    May 16 14:59:58.709: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local from pod dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b: the server could not find the requested resource (get pods dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b)
    May 16 14:59:58.713: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local from pod dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b: the server could not find the requested resource (get pods dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b)
    May 16 14:59:58.716: INFO: Unable to read jessie_udp@dns-test-service-2.dns-437.svc.cluster.local from pod dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b: the server could not find the requested resource (get pods dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b)
    May 16 14:59:58.718: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-437.svc.cluster.local from pod dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b: the server could not find the requested resource (get pods dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b)
    May 16 14:59:58.718: INFO: Lookups using dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local wheezy_udp@dns-test-service-2.dns-437.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-437.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-437.svc.cluster.local jessie_udp@dns-test-service-2.dns-437.svc.cluster.local jessie_tcp@dns-test-service-2.dns-437.svc.cluster.local]

    May 16 15:00:03.747: INFO: DNS probes using dns-437/dns-test-4b69b411-6f9f-47f2-ac9f-f544fc6a9f4b succeeded

    STEP: deleting the pod 05/16/23 15:00:03.747
    STEP: deleting the test headless service 05/16/23 15:00:03.757
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May 16 15:00:03.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-437" for this suite. 05/16/23 15:00:03.78
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:00:03.787
May 16 15:00:03.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename gc 05/16/23 15:00:03.788
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:00:03.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:00:03.815
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 05/16/23 15:00:03.817
W0516 15:00:03.825714      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the rc 05/16/23 15:00:08.832
STEP: wait for all pods to be garbage collected 05/16/23 15:00:08.838
STEP: Gathering metrics 05/16/23 15:00:13.844
W0516 15:00:13.847514      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0516 15:00:13.847529      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May 16 15:00:13.847: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May 16 15:00:13.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6060" for this suite. 05/16/23 15:00:13.852
------------------------------
â€¢ [SLOW TEST] [10.070 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:00:03.787
    May 16 15:00:03.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename gc 05/16/23 15:00:03.788
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:00:03.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:00:03.815
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 05/16/23 15:00:03.817
    W0516 15:00:03.825714      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: delete the rc 05/16/23 15:00:08.832
    STEP: wait for all pods to be garbage collected 05/16/23 15:00:08.838
    STEP: Gathering metrics 05/16/23 15:00:13.844
    W0516 15:00:13.847514      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
    W0516 15:00:13.847529      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May 16 15:00:13.847: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May 16 15:00:13.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6060" for this suite. 05/16/23 15:00:13.852
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:00:13.858
May 16 15:00:13.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename secrets 05/16/23 15:00:13.858
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:00:13.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:00:13.877
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-551dfdd9-e62e-40b8-af04-9e02d3d4af57 05/16/23 15:00:13.882
STEP: Creating a pod to test consume secrets 05/16/23 15:00:13.891
May 16 15:00:13.902: INFO: Waiting up to 5m0s for pod "pod-secrets-70496988-e8e3-4281-8ebe-7e3d0772738c" in namespace "secrets-9252" to be "Succeeded or Failed"
May 16 15:00:13.905: INFO: Pod "pod-secrets-70496988-e8e3-4281-8ebe-7e3d0772738c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.936186ms
May 16 15:00:15.911: INFO: Pod "pod-secrets-70496988-e8e3-4281-8ebe-7e3d0772738c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008804996s
May 16 15:00:17.910: INFO: Pod "pod-secrets-70496988-e8e3-4281-8ebe-7e3d0772738c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008199923s
STEP: Saw pod success 05/16/23 15:00:17.91
May 16 15:00:17.910: INFO: Pod "pod-secrets-70496988-e8e3-4281-8ebe-7e3d0772738c" satisfied condition "Succeeded or Failed"
May 16 15:00:17.913: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-secrets-70496988-e8e3-4281-8ebe-7e3d0772738c container secret-env-test: <nil>
STEP: delete the pod 05/16/23 15:00:17.919
May 16 15:00:17.929: INFO: Waiting for pod pod-secrets-70496988-e8e3-4281-8ebe-7e3d0772738c to disappear
May 16 15:00:17.932: INFO: Pod pod-secrets-70496988-e8e3-4281-8ebe-7e3d0772738c no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
May 16 15:00:17.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9252" for this suite. 05/16/23 15:00:17.936
------------------------------
â€¢ [4.085 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:00:13.858
    May 16 15:00:13.858: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename secrets 05/16/23 15:00:13.858
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:00:13.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:00:13.877
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-551dfdd9-e62e-40b8-af04-9e02d3d4af57 05/16/23 15:00:13.882
    STEP: Creating a pod to test consume secrets 05/16/23 15:00:13.891
    May 16 15:00:13.902: INFO: Waiting up to 5m0s for pod "pod-secrets-70496988-e8e3-4281-8ebe-7e3d0772738c" in namespace "secrets-9252" to be "Succeeded or Failed"
    May 16 15:00:13.905: INFO: Pod "pod-secrets-70496988-e8e3-4281-8ebe-7e3d0772738c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.936186ms
    May 16 15:00:15.911: INFO: Pod "pod-secrets-70496988-e8e3-4281-8ebe-7e3d0772738c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008804996s
    May 16 15:00:17.910: INFO: Pod "pod-secrets-70496988-e8e3-4281-8ebe-7e3d0772738c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008199923s
    STEP: Saw pod success 05/16/23 15:00:17.91
    May 16 15:00:17.910: INFO: Pod "pod-secrets-70496988-e8e3-4281-8ebe-7e3d0772738c" satisfied condition "Succeeded or Failed"
    May 16 15:00:17.913: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-secrets-70496988-e8e3-4281-8ebe-7e3d0772738c container secret-env-test: <nil>
    STEP: delete the pod 05/16/23 15:00:17.919
    May 16 15:00:17.929: INFO: Waiting for pod pod-secrets-70496988-e8e3-4281-8ebe-7e3d0772738c to disappear
    May 16 15:00:17.932: INFO: Pod pod-secrets-70496988-e8e3-4281-8ebe-7e3d0772738c no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    May 16 15:00:17.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9252" for this suite. 05/16/23 15:00:17.936
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:00:17.943
May 16 15:00:17.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename gc 05/16/23 15:00:17.943
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:00:17.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:00:17.97
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 05/16/23 15:00:17.972
STEP: Wait for the Deployment to create new ReplicaSet 05/16/23 15:00:17.978
STEP: delete the deployment 05/16/23 15:00:18.487
STEP: wait for all rs to be garbage collected 05/16/23 15:00:18.495
STEP: expected 0 rs, got 1 rs 05/16/23 15:00:18.502
STEP: expected 0 pods, got 2 pods 05/16/23 15:00:18.506
STEP: Gathering metrics 05/16/23 15:00:19.028
W0516 15:00:19.032225      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0516 15:00:19.032236      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May 16 15:00:19.032: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May 16 15:00:19.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9785" for this suite. 05/16/23 15:00:19.036
------------------------------
â€¢ [1.100 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:00:17.943
    May 16 15:00:17.943: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename gc 05/16/23 15:00:17.943
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:00:17.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:00:17.97
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 05/16/23 15:00:17.972
    STEP: Wait for the Deployment to create new ReplicaSet 05/16/23 15:00:17.978
    STEP: delete the deployment 05/16/23 15:00:18.487
    STEP: wait for all rs to be garbage collected 05/16/23 15:00:18.495
    STEP: expected 0 rs, got 1 rs 05/16/23 15:00:18.502
    STEP: expected 0 pods, got 2 pods 05/16/23 15:00:18.506
    STEP: Gathering metrics 05/16/23 15:00:19.028
    W0516 15:00:19.032225      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
    W0516 15:00:19.032236      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May 16 15:00:19.032: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May 16 15:00:19.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9785" for this suite. 05/16/23 15:00:19.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:00:19.042
May 16 15:00:19.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename services 05/16/23 15:00:19.043
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:00:19.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:00:19.074
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-6070 05/16/23 15:00:19.077
STEP: creating service affinity-nodeport in namespace services-6070 05/16/23 15:00:19.077
STEP: creating replication controller affinity-nodeport in namespace services-6070 05/16/23 15:00:19.102
I0516 15:00:19.109549      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-6070, replica count: 3
I0516 15:00:22.160988      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 16 15:00:22.172: INFO: Creating new exec pod
May 16 15:00:22.183: INFO: Waiting up to 5m0s for pod "execpod-affinityslpkh" in namespace "services-6070" to be "running"
May 16 15:00:22.186: INFO: Pod "execpod-affinityslpkh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.552941ms
May 16 15:00:24.190: INFO: Pod "execpod-affinityslpkh": Phase="Running", Reason="", readiness=true. Elapsed: 2.007022602s
May 16 15:00:24.190: INFO: Pod "execpod-affinityslpkh" satisfied condition "running"
May 16 15:00:25.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6070 exec execpod-affinityslpkh -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
May 16 15:00:26.311: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May 16 15:00:26.311: INFO: stdout: ""
May 16 15:00:26.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6070 exec execpod-affinityslpkh -- /bin/sh -x -c nc -v -z -w 2 172.30.41.121 80'
May 16 15:00:26.406: INFO: stderr: "+ nc -v -z -w 2 172.30.41.121 80\nConnection to 172.30.41.121 80 port [tcp/http] succeeded!\n"
May 16 15:00:26.406: INFO: stdout: ""
May 16 15:00:26.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6070 exec execpod-affinityslpkh -- /bin/sh -x -c nc -v -z -w 2 10.0.132.142 30324'
May 16 15:00:27.529: INFO: stderr: "+ nc -v -z -w 2 10.0.132.142 30324\nConnection to 10.0.132.142 30324 port [tcp/*] succeeded!\n"
May 16 15:00:27.529: INFO: stdout: ""
May 16 15:00:27.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6070 exec execpod-affinityslpkh -- /bin/sh -x -c nc -v -z -w 2 10.0.212.246 30324'
May 16 15:00:28.744: INFO: stderr: "+ nc -v -z -w 2 10.0.212.246 30324\nConnection to 10.0.212.246 30324 port [tcp/*] succeeded!\n"
May 16 15:00:28.744: INFO: stdout: ""
May 16 15:00:28.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6070 exec execpod-affinityslpkh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.132.142:30324/ ; done'
May 16 15:00:28.936: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n"
May 16 15:00:28.936: INFO: stdout: "\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn"
May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
May 16 15:00:28.936: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-6070, will wait for the garbage collector to delete the pods 05/16/23 15:00:28.946
May 16 15:00:29.006: INFO: Deleting ReplicationController affinity-nodeport took: 6.432402ms
May 16 15:00:29.106: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.099408ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 16 15:00:31.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6070" for this suite. 05/16/23 15:00:31.351
------------------------------
â€¢ [SLOW TEST] [12.317 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:00:19.042
    May 16 15:00:19.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename services 05/16/23 15:00:19.043
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:00:19.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:00:19.074
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-6070 05/16/23 15:00:19.077
    STEP: creating service affinity-nodeport in namespace services-6070 05/16/23 15:00:19.077
    STEP: creating replication controller affinity-nodeport in namespace services-6070 05/16/23 15:00:19.102
    I0516 15:00:19.109549      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-6070, replica count: 3
    I0516 15:00:22.160988      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 16 15:00:22.172: INFO: Creating new exec pod
    May 16 15:00:22.183: INFO: Waiting up to 5m0s for pod "execpod-affinityslpkh" in namespace "services-6070" to be "running"
    May 16 15:00:22.186: INFO: Pod "execpod-affinityslpkh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.552941ms
    May 16 15:00:24.190: INFO: Pod "execpod-affinityslpkh": Phase="Running", Reason="", readiness=true. Elapsed: 2.007022602s
    May 16 15:00:24.190: INFO: Pod "execpod-affinityslpkh" satisfied condition "running"
    May 16 15:00:25.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6070 exec execpod-affinityslpkh -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    May 16 15:00:26.311: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    May 16 15:00:26.311: INFO: stdout: ""
    May 16 15:00:26.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6070 exec execpod-affinityslpkh -- /bin/sh -x -c nc -v -z -w 2 172.30.41.121 80'
    May 16 15:00:26.406: INFO: stderr: "+ nc -v -z -w 2 172.30.41.121 80\nConnection to 172.30.41.121 80 port [tcp/http] succeeded!\n"
    May 16 15:00:26.406: INFO: stdout: ""
    May 16 15:00:26.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6070 exec execpod-affinityslpkh -- /bin/sh -x -c nc -v -z -w 2 10.0.132.142 30324'
    May 16 15:00:27.529: INFO: stderr: "+ nc -v -z -w 2 10.0.132.142 30324\nConnection to 10.0.132.142 30324 port [tcp/*] succeeded!\n"
    May 16 15:00:27.529: INFO: stdout: ""
    May 16 15:00:27.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6070 exec execpod-affinityslpkh -- /bin/sh -x -c nc -v -z -w 2 10.0.212.246 30324'
    May 16 15:00:28.744: INFO: stderr: "+ nc -v -z -w 2 10.0.212.246 30324\nConnection to 10.0.212.246 30324 port [tcp/*] succeeded!\n"
    May 16 15:00:28.744: INFO: stdout: ""
    May 16 15:00:28.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6070 exec execpod-affinityslpkh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.132.142:30324/ ; done'
    May 16 15:00:28.936: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30324/\n"
    May 16 15:00:28.936: INFO: stdout: "\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn\naffinity-nodeport-lw9pn"
    May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
    May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
    May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
    May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
    May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
    May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
    May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
    May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
    May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
    May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
    May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
    May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
    May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
    May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
    May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
    May 16 15:00:28.936: INFO: Received response from host: affinity-nodeport-lw9pn
    May 16 15:00:28.936: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-6070, will wait for the garbage collector to delete the pods 05/16/23 15:00:28.946
    May 16 15:00:29.006: INFO: Deleting ReplicationController affinity-nodeport took: 6.432402ms
    May 16 15:00:29.106: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.099408ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 16 15:00:31.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6070" for this suite. 05/16/23 15:00:31.351
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:00:31.359
May 16 15:00:31.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename namespaces 05/16/23 15:00:31.36
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:00:31.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:00:31.391
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-f46m6" 05/16/23 15:00:31.393
May 16 15:00:31.432: INFO: Namespace "e2e-ns-f46m6-7531" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-f46m6-7531" 05/16/23 15:00:31.432
May 16 15:00:31.481: INFO: Namespace "e2e-ns-f46m6-7531" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-f46m6-7531" 05/16/23 15:00:31.481
May 16 15:00:31.491: INFO: Namespace "e2e-ns-f46m6-7531" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 15:00:31.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-415" for this suite. 05/16/23 15:00:31.497
STEP: Destroying namespace "e2e-ns-f46m6-7531" for this suite. 05/16/23 15:00:31.504
------------------------------
â€¢ [0.155 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:00:31.359
    May 16 15:00:31.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename namespaces 05/16/23 15:00:31.36
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:00:31.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:00:31.391
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-f46m6" 05/16/23 15:00:31.393
    May 16 15:00:31.432: INFO: Namespace "e2e-ns-f46m6-7531" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-f46m6-7531" 05/16/23 15:00:31.432
    May 16 15:00:31.481: INFO: Namespace "e2e-ns-f46m6-7531" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-f46m6-7531" 05/16/23 15:00:31.481
    May 16 15:00:31.491: INFO: Namespace "e2e-ns-f46m6-7531" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 15:00:31.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-415" for this suite. 05/16/23 15:00:31.497
    STEP: Destroying namespace "e2e-ns-f46m6-7531" for this suite. 05/16/23 15:00:31.504
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:00:31.515
May 16 15:00:31.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename statefulset 05/16/23 15:00:31.516
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:00:31.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:00:31.543
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3065 05/16/23 15:00:31.546
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 05/16/23 15:00:31.551
STEP: Creating stateful set ss in namespace statefulset-3065 05/16/23 15:00:31.555
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3065 05/16/23 15:00:31.564
May 16 15:00:31.567: INFO: Found 0 stateful pods, waiting for 1
May 16 15:00:41.571: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 05/16/23 15:00:41.571
May 16 15:00:41.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-3065 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 16 15:00:41.697: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 16 15:00:41.697: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 16 15:00:41.697: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 16 15:00:41.700: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 16 15:00:51.704: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 16 15:00:51.704: INFO: Waiting for statefulset status.replicas updated to 0
May 16 15:00:51.718: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999874s
May 16 15:00:52.721: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997223365s
May 16 15:00:53.726: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993516514s
May 16 15:00:54.730: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988549012s
May 16 15:00:55.734: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.983516441s
May 16 15:00:56.738: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.98052302s
May 16 15:00:57.743: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976061539s
May 16 15:00:58.746: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.972385237s
May 16 15:00:59.750: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.968485207s
May 16 15:01:00.755: INFO: Verifying statefulset ss doesn't scale past 1 for another 964.394584ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3065 05/16/23 15:01:01.755
May 16 15:01:01.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-3065 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 16 15:01:01.866: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 16 15:01:01.866: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 16 15:01:01.866: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 16 15:01:01.869: INFO: Found 1 stateful pods, waiting for 3
May 16 15:01:11.874: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 16 15:01:11.874: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 16 15:01:11.874: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 05/16/23 15:01:11.874
STEP: Scale down will halt with unhealthy stateful pod 05/16/23 15:01:11.874
May 16 15:01:11.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-3065 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 16 15:01:11.997: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 16 15:01:11.997: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 16 15:01:11.997: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 16 15:01:11.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-3065 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 16 15:01:12.127: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 16 15:01:12.127: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 16 15:01:12.127: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 16 15:01:12.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-3065 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 16 15:01:12.257: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 16 15:01:12.257: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 16 15:01:12.257: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 16 15:01:12.257: INFO: Waiting for statefulset status.replicas updated to 0
May 16 15:01:12.260: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 16 15:01:22.270: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 16 15:01:22.270: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 16 15:01:22.270: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 16 15:01:22.281: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999875s
May 16 15:01:23.285: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996284919s
May 16 15:01:24.291: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992289297s
May 16 15:01:25.296: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985498178s
May 16 15:01:26.300: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981295749s
May 16 15:01:27.304: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977788036s
May 16 15:01:28.309: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972675488s
May 16 15:01:29.312: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.968973014s
May 16 15:01:30.316: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.965696437s
May 16 15:01:31.319: INFO: Verifying statefulset ss doesn't scale past 3 for another 961.933001ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3065 05/16/23 15:01:32.32
May 16 15:01:32.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-3065 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 16 15:01:32.455: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 16 15:01:32.455: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 16 15:01:32.455: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 16 15:01:32.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-3065 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 16 15:01:32.559: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 16 15:01:32.559: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 16 15:01:32.559: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 16 15:01:32.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-3065 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 16 15:01:32.685: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 16 15:01:32.685: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 16 15:01:32.685: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 16 15:01:32.685: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 05/16/23 15:01:42.699
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May 16 15:01:42.699: INFO: Deleting all statefulset in ns statefulset-3065
May 16 15:01:42.702: INFO: Scaling statefulset ss to 0
May 16 15:01:42.711: INFO: Waiting for statefulset status.replicas updated to 0
May 16 15:01:42.714: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May 16 15:01:42.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3065" for this suite. 05/16/23 15:01:42.731
------------------------------
â€¢ [SLOW TEST] [71.223 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:00:31.515
    May 16 15:00:31.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename statefulset 05/16/23 15:00:31.516
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:00:31.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:00:31.543
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3065 05/16/23 15:00:31.546
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 05/16/23 15:00:31.551
    STEP: Creating stateful set ss in namespace statefulset-3065 05/16/23 15:00:31.555
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3065 05/16/23 15:00:31.564
    May 16 15:00:31.567: INFO: Found 0 stateful pods, waiting for 1
    May 16 15:00:41.571: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 05/16/23 15:00:41.571
    May 16 15:00:41.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-3065 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 16 15:00:41.697: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 16 15:00:41.697: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 16 15:00:41.697: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 16 15:00:41.700: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    May 16 15:00:51.704: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May 16 15:00:51.704: INFO: Waiting for statefulset status.replicas updated to 0
    May 16 15:00:51.718: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999874s
    May 16 15:00:52.721: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997223365s
    May 16 15:00:53.726: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993516514s
    May 16 15:00:54.730: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988549012s
    May 16 15:00:55.734: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.983516441s
    May 16 15:00:56.738: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.98052302s
    May 16 15:00:57.743: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.976061539s
    May 16 15:00:58.746: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.972385237s
    May 16 15:00:59.750: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.968485207s
    May 16 15:01:00.755: INFO: Verifying statefulset ss doesn't scale past 1 for another 964.394584ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3065 05/16/23 15:01:01.755
    May 16 15:01:01.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-3065 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 16 15:01:01.866: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 16 15:01:01.866: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 16 15:01:01.866: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 16 15:01:01.869: INFO: Found 1 stateful pods, waiting for 3
    May 16 15:01:11.874: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May 16 15:01:11.874: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    May 16 15:01:11.874: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 05/16/23 15:01:11.874
    STEP: Scale down will halt with unhealthy stateful pod 05/16/23 15:01:11.874
    May 16 15:01:11.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-3065 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 16 15:01:11.997: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 16 15:01:11.997: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 16 15:01:11.997: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 16 15:01:11.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-3065 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 16 15:01:12.127: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 16 15:01:12.127: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 16 15:01:12.127: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 16 15:01:12.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-3065 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 16 15:01:12.257: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 16 15:01:12.257: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 16 15:01:12.257: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 16 15:01:12.257: INFO: Waiting for statefulset status.replicas updated to 0
    May 16 15:01:12.260: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    May 16 15:01:22.270: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    May 16 15:01:22.270: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    May 16 15:01:22.270: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    May 16 15:01:22.281: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999875s
    May 16 15:01:23.285: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996284919s
    May 16 15:01:24.291: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992289297s
    May 16 15:01:25.296: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985498178s
    May 16 15:01:26.300: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981295749s
    May 16 15:01:27.304: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977788036s
    May 16 15:01:28.309: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972675488s
    May 16 15:01:29.312: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.968973014s
    May 16 15:01:30.316: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.965696437s
    May 16 15:01:31.319: INFO: Verifying statefulset ss doesn't scale past 3 for another 961.933001ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3065 05/16/23 15:01:32.32
    May 16 15:01:32.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-3065 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 16 15:01:32.455: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 16 15:01:32.455: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 16 15:01:32.455: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 16 15:01:32.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-3065 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 16 15:01:32.559: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 16 15:01:32.559: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 16 15:01:32.559: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 16 15:01:32.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-3065 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 16 15:01:32.685: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 16 15:01:32.685: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 16 15:01:32.685: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    May 16 15:01:32.685: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 05/16/23 15:01:42.699
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May 16 15:01:42.699: INFO: Deleting all statefulset in ns statefulset-3065
    May 16 15:01:42.702: INFO: Scaling statefulset ss to 0
    May 16 15:01:42.711: INFO: Waiting for statefulset status.replicas updated to 0
    May 16 15:01:42.714: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May 16 15:01:42.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3065" for this suite. 05/16/23 15:01:42.731
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:01:42.738
May 16 15:01:42.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubelet-test 05/16/23 15:01:42.739
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:01:42.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:01:42.763
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
W0516 15:01:42.778095      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:01:42.778: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871" in namespace "kubelet-test-6503" to be "running and ready"
May 16 15:01:42.782: INFO: Pod "busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871": Phase="Pending", Reason="", readiness=false. Elapsed: 4.47787ms
May 16 15:01:42.782: INFO: The phase of Pod busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871 is Pending, waiting for it to be Running (with Ready = true)
May 16 15:01:44.787: INFO: Pod "busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871": Phase="Running", Reason="", readiness=true. Elapsed: 2.009038986s
May 16 15:01:44.787: INFO: The phase of Pod busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871 is Running (Ready = true)
May 16 15:01:44.787: INFO: Pod "busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May 16 15:01:44.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6503" for this suite. 05/16/23 15:01:44.8
------------------------------
â€¢ [2.068 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:01:42.738
    May 16 15:01:42.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubelet-test 05/16/23 15:01:42.739
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:01:42.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:01:42.763
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    W0516 15:01:42.778095      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:01:42.778: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871" in namespace "kubelet-test-6503" to be "running and ready"
    May 16 15:01:42.782: INFO: Pod "busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871": Phase="Pending", Reason="", readiness=false. Elapsed: 4.47787ms
    May 16 15:01:42.782: INFO: The phase of Pod busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871 is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:01:44.787: INFO: Pod "busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871": Phase="Running", Reason="", readiness=true. Elapsed: 2.009038986s
    May 16 15:01:44.787: INFO: The phase of Pod busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871 is Running (Ready = true)
    May 16 15:01:44.787: INFO: Pod "busybox-readonly-fs613a1eb8-4689-435b-b9e6-239d2cf7a871" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May 16 15:01:44.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6503" for this suite. 05/16/23 15:01:44.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:01:44.806
May 16 15:01:44.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename ephemeral-containers-test 05/16/23 15:01:44.807
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:01:44.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:01:44.827
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 05/16/23 15:01:44.83
May 16 15:01:44.846: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2896" to be "running and ready"
May 16 15:01:44.850: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.392266ms
May 16 15:01:44.850: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
May 16 15:01:46.854: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007763902s
May 16 15:01:46.854: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
May 16 15:01:46.854: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 05/16/23 15:01:46.857
May 16 15:01:46.867: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2896" to be "container debugger running"
May 16 15:01:46.870: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.833134ms
May 16 15:01:48.874: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006807065s
May 16 15:01:50.873: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005878909s
May 16 15:01:50.873: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 05/16/23 15:01:50.873
May 16 15:01:50.873: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2896 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:01:50.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:01:50.873: INFO: ExecWithOptions: Clientset creation
May 16 15:01:50.873: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/ephemeral-containers-test-2896/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
May 16 15:01:50.945: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May 16 15:01:50.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-2896" for this suite. 05/16/23 15:01:50.955
------------------------------
â€¢ [SLOW TEST] [6.155 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:01:44.806
    May 16 15:01:44.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename ephemeral-containers-test 05/16/23 15:01:44.807
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:01:44.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:01:44.827
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 05/16/23 15:01:44.83
    May 16 15:01:44.846: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2896" to be "running and ready"
    May 16 15:01:44.850: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.392266ms
    May 16 15:01:44.850: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:01:46.854: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007763902s
    May 16 15:01:46.854: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    May 16 15:01:46.854: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 05/16/23 15:01:46.857
    May 16 15:01:46.867: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2896" to be "container debugger running"
    May 16 15:01:46.870: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.833134ms
    May 16 15:01:48.874: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006807065s
    May 16 15:01:50.873: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005878909s
    May 16 15:01:50.873: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 05/16/23 15:01:50.873
    May 16 15:01:50.873: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2896 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:01:50.873: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:01:50.873: INFO: ExecWithOptions: Clientset creation
    May 16 15:01:50.873: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/ephemeral-containers-test-2896/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    May 16 15:01:50.945: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May 16 15:01:50.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-2896" for this suite. 05/16/23 15:01:50.955
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:01:50.961
May 16 15:01:50.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename watch 05/16/23 15:01:50.962
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:01:50.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:01:50.983
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 05/16/23 15:01:50.986
STEP: creating a new configmap 05/16/23 15:01:50.989
STEP: modifying the configmap once 05/16/23 15:01:50.998
STEP: closing the watch once it receives two notifications 05/16/23 15:01:51.006
May 16 15:01:51.006: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1554  5ed57b86-71e3-438c-8531-f097102e4ce3 71463 0 2023-05-16 15:01:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-16 15:01:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 16 15:01:51.006: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1554  5ed57b86-71e3-438c-8531-f097102e4ce3 71467 0 2023-05-16 15:01:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-16 15:01:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 05/16/23 15:01:51.006
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 05/16/23 15:01:51.014
STEP: deleting the configmap 05/16/23 15:01:51.015
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 05/16/23 15:01:51.023
May 16 15:01:51.024: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1554  5ed57b86-71e3-438c-8531-f097102e4ce3 71468 0 2023-05-16 15:01:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-16 15:01:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 16 15:01:51.024: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1554  5ed57b86-71e3-438c-8531-f097102e4ce3 71473 0 2023-05-16 15:01:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-16 15:01:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May 16 15:01:51.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1554" for this suite. 05/16/23 15:01:51.031
------------------------------
â€¢ [0.079 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:01:50.961
    May 16 15:01:50.961: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename watch 05/16/23 15:01:50.962
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:01:50.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:01:50.983
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 05/16/23 15:01:50.986
    STEP: creating a new configmap 05/16/23 15:01:50.989
    STEP: modifying the configmap once 05/16/23 15:01:50.998
    STEP: closing the watch once it receives two notifications 05/16/23 15:01:51.006
    May 16 15:01:51.006: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1554  5ed57b86-71e3-438c-8531-f097102e4ce3 71463 0 2023-05-16 15:01:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-16 15:01:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May 16 15:01:51.006: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1554  5ed57b86-71e3-438c-8531-f097102e4ce3 71467 0 2023-05-16 15:01:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-16 15:01:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 05/16/23 15:01:51.006
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 05/16/23 15:01:51.014
    STEP: deleting the configmap 05/16/23 15:01:51.015
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 05/16/23 15:01:51.023
    May 16 15:01:51.024: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1554  5ed57b86-71e3-438c-8531-f097102e4ce3 71468 0 2023-05-16 15:01:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-16 15:01:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 16 15:01:51.024: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1554  5ed57b86-71e3-438c-8531-f097102e4ce3 71473 0 2023-05-16 15:01:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-16 15:01:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May 16 15:01:51.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1554" for this suite. 05/16/23 15:01:51.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:01:51.04
May 16 15:01:51.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename podtemplate 05/16/23 15:01:51.041
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:01:51.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:01:51.063
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 05/16/23 15:01:51.066
W0516 15:01:51.072390      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Replace a pod template 05/16/23 15:01:51.072
May 16 15:01:51.082: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
May 16 15:01:51.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-6232" for this suite. 05/16/23 15:01:51.092
------------------------------
â€¢ [0.061 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:01:51.04
    May 16 15:01:51.040: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename podtemplate 05/16/23 15:01:51.041
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:01:51.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:01:51.063
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 05/16/23 15:01:51.066
    W0516 15:01:51.072390      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Replace a pod template 05/16/23 15:01:51.072
    May 16 15:01:51.082: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    May 16 15:01:51.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-6232" for this suite. 05/16/23 15:01:51.092
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:01:51.101
May 16 15:01:51.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubectl 05/16/23 15:01:51.102
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:01:51.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:01:51.123
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
May 16 15:01:51.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2184 version'
May 16 15:01:51.163: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
May 16 15:01:51.163: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:40:17Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3+b404935\", GitCommit:\"9ded806a7d5deed41a20f680cf89dae58bbb5697\", GitTreeState:\"clean\", BuildDate:\"2023-04-19T02:20:48Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 16 15:01:51.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2184" for this suite. 05/16/23 15:01:51.17
------------------------------
â€¢ [0.074 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:01:51.101
    May 16 15:01:51.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubectl 05/16/23 15:01:51.102
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:01:51.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:01:51.123
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    May 16 15:01:51.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2184 version'
    May 16 15:01:51.163: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    May 16 15:01:51.163: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:40:17Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3+b404935\", GitCommit:\"9ded806a7d5deed41a20f680cf89dae58bbb5697\", GitTreeState:\"clean\", BuildDate:\"2023-04-19T02:20:48Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 16 15:01:51.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2184" for this suite. 05/16/23 15:01:51.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:01:51.176
May 16 15:01:51.176: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 05/16/23 15:01:51.177
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:01:51.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:01:51.215
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 05/16/23 15:01:51.22
STEP: Creating hostNetwork=false pod 05/16/23 15:01:51.22
May 16 15:01:51.257: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-8115" to be "running and ready"
May 16 15:01:51.263: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.513254ms
May 16 15:01:51.263: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May 16 15:01:53.267: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010704441s
May 16 15:01:53.267: INFO: The phase of Pod test-pod is Running (Ready = true)
May 16 15:01:53.268: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 05/16/23 15:01:53.271
May 16 15:01:53.278: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-8115" to be "running and ready"
May 16 15:01:53.281: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.612429ms
May 16 15:01:53.281: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
May 16 15:01:55.284: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005874262s
May 16 15:01:55.284: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
May 16 15:01:55.284: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 05/16/23 15:01:55.287
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 05/16/23 15:01:55.287
May 16 15:01:55.287: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:01:55.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:01:55.287: INFO: ExecWithOptions: Clientset creation
May 16 15:01:55.287: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May 16 15:01:55.364: INFO: Exec stderr: ""
May 16 15:01:55.364: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:01:55.364: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:01:55.364: INFO: ExecWithOptions: Clientset creation
May 16 15:01:55.364: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May 16 15:01:55.423: INFO: Exec stderr: ""
May 16 15:01:55.423: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:01:55.423: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:01:55.423: INFO: ExecWithOptions: Clientset creation
May 16 15:01:55.423: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May 16 15:01:55.463: INFO: Exec stderr: ""
May 16 15:01:55.463: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:01:55.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:01:55.464: INFO: ExecWithOptions: Clientset creation
May 16 15:01:55.464: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May 16 15:01:55.529: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 05/16/23 15:01:55.529
May 16 15:01:55.530: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:01:55.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:01:55.530: INFO: ExecWithOptions: Clientset creation
May 16 15:01:55.530: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
May 16 15:01:55.587: INFO: Exec stderr: ""
May 16 15:01:55.587: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:01:55.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:01:55.587: INFO: ExecWithOptions: Clientset creation
May 16 15:01:55.587: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
May 16 15:01:55.646: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 05/16/23 15:01:55.646
May 16 15:01:55.646: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:01:55.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:01:55.647: INFO: ExecWithOptions: Clientset creation
May 16 15:01:55.647: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May 16 15:01:55.718: INFO: Exec stderr: ""
May 16 15:01:55.718: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:01:55.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:01:55.718: INFO: ExecWithOptions: Clientset creation
May 16 15:01:55.718: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
May 16 15:01:55.777: INFO: Exec stderr: ""
May 16 15:01:55.777: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:01:55.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:01:55.778: INFO: ExecWithOptions: Clientset creation
May 16 15:01:55.778: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May 16 15:01:55.823: INFO: Exec stderr: ""
May 16 15:01:55.823: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:01:55.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:01:55.823: INFO: ExecWithOptions: Clientset creation
May 16 15:01:55.824: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
May 16 15:01:55.869: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
May 16 15:01:55.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8115" for this suite. 05/16/23 15:01:55.873
------------------------------
â€¢ [4.702 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:01:51.176
    May 16 15:01:51.176: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 05/16/23 15:01:51.177
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:01:51.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:01:51.215
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 05/16/23 15:01:51.22
    STEP: Creating hostNetwork=false pod 05/16/23 15:01:51.22
    May 16 15:01:51.257: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-8115" to be "running and ready"
    May 16 15:01:51.263: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.513254ms
    May 16 15:01:51.263: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:01:53.267: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010704441s
    May 16 15:01:53.267: INFO: The phase of Pod test-pod is Running (Ready = true)
    May 16 15:01:53.268: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 05/16/23 15:01:53.271
    May 16 15:01:53.278: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-8115" to be "running and ready"
    May 16 15:01:53.281: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.612429ms
    May 16 15:01:53.281: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:01:55.284: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005874262s
    May 16 15:01:55.284: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    May 16 15:01:55.284: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 05/16/23 15:01:55.287
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 05/16/23 15:01:55.287
    May 16 15:01:55.287: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:01:55.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:01:55.287: INFO: ExecWithOptions: Clientset creation
    May 16 15:01:55.287: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May 16 15:01:55.364: INFO: Exec stderr: ""
    May 16 15:01:55.364: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:01:55.364: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:01:55.364: INFO: ExecWithOptions: Clientset creation
    May 16 15:01:55.364: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May 16 15:01:55.423: INFO: Exec stderr: ""
    May 16 15:01:55.423: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:01:55.423: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:01:55.423: INFO: ExecWithOptions: Clientset creation
    May 16 15:01:55.423: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May 16 15:01:55.463: INFO: Exec stderr: ""
    May 16 15:01:55.463: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:01:55.463: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:01:55.464: INFO: ExecWithOptions: Clientset creation
    May 16 15:01:55.464: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May 16 15:01:55.529: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 05/16/23 15:01:55.529
    May 16 15:01:55.530: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:01:55.530: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:01:55.530: INFO: ExecWithOptions: Clientset creation
    May 16 15:01:55.530: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    May 16 15:01:55.587: INFO: Exec stderr: ""
    May 16 15:01:55.587: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:01:55.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:01:55.587: INFO: ExecWithOptions: Clientset creation
    May 16 15:01:55.587: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    May 16 15:01:55.646: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 05/16/23 15:01:55.646
    May 16 15:01:55.646: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:01:55.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:01:55.647: INFO: ExecWithOptions: Clientset creation
    May 16 15:01:55.647: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May 16 15:01:55.718: INFO: Exec stderr: ""
    May 16 15:01:55.718: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:01:55.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:01:55.718: INFO: ExecWithOptions: Clientset creation
    May 16 15:01:55.718: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    May 16 15:01:55.777: INFO: Exec stderr: ""
    May 16 15:01:55.777: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:01:55.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:01:55.778: INFO: ExecWithOptions: Clientset creation
    May 16 15:01:55.778: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May 16 15:01:55.823: INFO: Exec stderr: ""
    May 16 15:01:55.823: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8115 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:01:55.823: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:01:55.823: INFO: ExecWithOptions: Clientset creation
    May 16 15:01:55.824: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8115/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    May 16 15:01:55.869: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    May 16 15:01:55.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-8115" for this suite. 05/16/23 15:01:55.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:01:55.88
May 16 15:01:55.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 15:01:55.88
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:01:55.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:01:55.899
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-e7be7638-67a2-4230-946c-57c2c3bf7cc7 05/16/23 15:01:55.902
STEP: Creating secret with name secret-projected-all-test-volume-a2670baf-c5f5-45cc-8c03-530995c82267 05/16/23 15:01:55.908
STEP: Creating a pod to test Check all projections for projected volume plugin 05/16/23 15:01:55.914
May 16 15:01:55.927: INFO: Waiting up to 5m0s for pod "projected-volume-84f4e4a2-0b4d-4083-885b-e884174e022a" in namespace "projected-4603" to be "Succeeded or Failed"
May 16 15:01:55.932: INFO: Pod "projected-volume-84f4e4a2-0b4d-4083-885b-e884174e022a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.130973ms
May 16 15:01:57.936: INFO: Pod "projected-volume-84f4e4a2-0b4d-4083-885b-e884174e022a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008758849s
May 16 15:01:59.937: INFO: Pod "projected-volume-84f4e4a2-0b4d-4083-885b-e884174e022a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009841165s
STEP: Saw pod success 05/16/23 15:01:59.937
May 16 15:01:59.937: INFO: Pod "projected-volume-84f4e4a2-0b4d-4083-885b-e884174e022a" satisfied condition "Succeeded or Failed"
May 16 15:01:59.940: INFO: Trying to get logs from node ip-10-0-212-246.eu-central-1.compute.internal pod projected-volume-84f4e4a2-0b4d-4083-885b-e884174e022a container projected-all-volume-test: <nil>
STEP: delete the pod 05/16/23 15:01:59.952
May 16 15:01:59.963: INFO: Waiting for pod projected-volume-84f4e4a2-0b4d-4083-885b-e884174e022a to disappear
May 16 15:01:59.966: INFO: Pod projected-volume-84f4e4a2-0b4d-4083-885b-e884174e022a no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
May 16 15:01:59.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4603" for this suite. 05/16/23 15:01:59.97
------------------------------
â€¢ [4.096 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:01:55.88
    May 16 15:01:55.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 15:01:55.88
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:01:55.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:01:55.899
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-e7be7638-67a2-4230-946c-57c2c3bf7cc7 05/16/23 15:01:55.902
    STEP: Creating secret with name secret-projected-all-test-volume-a2670baf-c5f5-45cc-8c03-530995c82267 05/16/23 15:01:55.908
    STEP: Creating a pod to test Check all projections for projected volume plugin 05/16/23 15:01:55.914
    May 16 15:01:55.927: INFO: Waiting up to 5m0s for pod "projected-volume-84f4e4a2-0b4d-4083-885b-e884174e022a" in namespace "projected-4603" to be "Succeeded or Failed"
    May 16 15:01:55.932: INFO: Pod "projected-volume-84f4e4a2-0b4d-4083-885b-e884174e022a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.130973ms
    May 16 15:01:57.936: INFO: Pod "projected-volume-84f4e4a2-0b4d-4083-885b-e884174e022a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008758849s
    May 16 15:01:59.937: INFO: Pod "projected-volume-84f4e4a2-0b4d-4083-885b-e884174e022a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009841165s
    STEP: Saw pod success 05/16/23 15:01:59.937
    May 16 15:01:59.937: INFO: Pod "projected-volume-84f4e4a2-0b4d-4083-885b-e884174e022a" satisfied condition "Succeeded or Failed"
    May 16 15:01:59.940: INFO: Trying to get logs from node ip-10-0-212-246.eu-central-1.compute.internal pod projected-volume-84f4e4a2-0b4d-4083-885b-e884174e022a container projected-all-volume-test: <nil>
    STEP: delete the pod 05/16/23 15:01:59.952
    May 16 15:01:59.963: INFO: Waiting for pod projected-volume-84f4e4a2-0b4d-4083-885b-e884174e022a to disappear
    May 16 15:01:59.966: INFO: Pod projected-volume-84f4e4a2-0b4d-4083-885b-e884174e022a no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    May 16 15:01:59.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4603" for this suite. 05/16/23 15:01:59.97
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:01:59.975
May 16 15:01:59.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename container-lifecycle-hook 05/16/23 15:01:59.976
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:01:59.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:01:59.998
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 05/16/23 15:02:00.009
May 16 15:02:00.020: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9759" to be "running and ready"
May 16 15:02:00.024: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.623971ms
May 16 15:02:00.024: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 16 15:02:02.028: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008032718s
May 16 15:02:02.028: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May 16 15:02:02.028: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 05/16/23 15:02:02.033
May 16 15:02:02.040: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9759" to be "running and ready"
May 16 15:02:02.044: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.414618ms
May 16 15:02:02.044: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 16 15:02:04.049: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00835715s
May 16 15:02:04.049: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
May 16 15:02:04.049: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 05/16/23 15:02:04.052
May 16 15:02:04.058: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 15:02:04.061: INFO: Pod pod-with-prestop-exec-hook still exists
May 16 15:02:06.061: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 15:02:06.065: INFO: Pod pod-with-prestop-exec-hook still exists
May 16 15:02:08.063: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 16 15:02:08.066: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 05/16/23 15:02:08.066
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
May 16 15:02:08.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9759" for this suite. 05/16/23 15:02:08.08
------------------------------
â€¢ [SLOW TEST] [8.110 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:01:59.975
    May 16 15:01:59.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/16/23 15:01:59.976
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:01:59.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:01:59.998
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 05/16/23 15:02:00.009
    May 16 15:02:00.020: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9759" to be "running and ready"
    May 16 15:02:00.024: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.623971ms
    May 16 15:02:00.024: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:02:02.028: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008032718s
    May 16 15:02:02.028: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May 16 15:02:02.028: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 05/16/23 15:02:02.033
    May 16 15:02:02.040: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9759" to be "running and ready"
    May 16 15:02:02.044: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.414618ms
    May 16 15:02:02.044: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:02:04.049: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00835715s
    May 16 15:02:04.049: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    May 16 15:02:04.049: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 05/16/23 15:02:04.052
    May 16 15:02:04.058: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May 16 15:02:04.061: INFO: Pod pod-with-prestop-exec-hook still exists
    May 16 15:02:06.061: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May 16 15:02:06.065: INFO: Pod pod-with-prestop-exec-hook still exists
    May 16 15:02:08.063: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    May 16 15:02:08.066: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 05/16/23 15:02:08.066
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    May 16 15:02:08.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9759" for this suite. 05/16/23 15:02:08.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:02:08.086
May 16 15:02:08.086: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename daemonsets 05/16/23 15:02:08.086
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:02:08.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:02:08.106
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 05/16/23 15:02:08.152
STEP: Check that daemon pods launch on every node of the cluster. 05/16/23 15:02:08.158
May 16 15:02:08.166: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:02:08.166: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:02:08.166: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:02:08.169: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 15:02:08.169: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 15:02:09.173: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:02:09.173: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:02:09.173: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:02:09.176: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 15:02:09.176: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 15:02:10.174: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:02:10.174: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:02:10.174: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:02:10.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 16 15:02:10.177: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 05/16/23 15:02:10.18
May 16 15:02:10.195: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:02:10.195: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:02:10.195: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:02:10.202: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 16 15:02:10.202: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 15:02:11.207: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:02:11.207: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:02:11.207: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:02:11.210: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 16 15:02:11.210: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 15:02:12.207: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:02:12.207: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:02:12.207: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:02:12.210: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 16 15:02:12.210: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 05/16/23 15:02:12.21
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/16/23 15:02:12.215
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5016, will wait for the garbage collector to delete the pods 05/16/23 15:02:12.215
May 16 15:02:12.274: INFO: Deleting DaemonSet.extensions daemon-set took: 6.149803ms
May 16 15:02:12.374: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.441908ms
May 16 15:02:14.578: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 15:02:14.578: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 16 15:02:14.581: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"72093"},"items":null}

May 16 15:02:14.583: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"72093"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 15:02:14.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5016" for this suite. 05/16/23 15:02:14.599
------------------------------
â€¢ [SLOW TEST] [6.518 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:02:08.086
    May 16 15:02:08.086: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename daemonsets 05/16/23 15:02:08.086
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:02:08.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:02:08.106
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 05/16/23 15:02:08.152
    STEP: Check that daemon pods launch on every node of the cluster. 05/16/23 15:02:08.158
    May 16 15:02:08.166: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:02:08.166: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:02:08.166: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:02:08.169: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 15:02:08.169: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 15:02:09.173: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:02:09.173: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:02:09.173: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:02:09.176: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 15:02:09.176: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 15:02:10.174: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:02:10.174: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:02:10.174: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:02:10.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May 16 15:02:10.177: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 05/16/23 15:02:10.18
    May 16 15:02:10.195: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:02:10.195: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:02:10.195: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:02:10.202: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 16 15:02:10.202: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 15:02:11.207: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:02:11.207: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:02:11.207: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:02:11.210: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 16 15:02:11.210: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 15:02:12.207: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:02:12.207: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:02:12.207: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:02:12.210: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May 16 15:02:12.210: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 05/16/23 15:02:12.21
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/16/23 15:02:12.215
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5016, will wait for the garbage collector to delete the pods 05/16/23 15:02:12.215
    May 16 15:02:12.274: INFO: Deleting DaemonSet.extensions daemon-set took: 6.149803ms
    May 16 15:02:12.374: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.441908ms
    May 16 15:02:14.578: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 15:02:14.578: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May 16 15:02:14.581: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"72093"},"items":null}

    May 16 15:02:14.583: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"72093"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 15:02:14.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5016" for this suite. 05/16/23 15:02:14.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:02:14.604
May 16 15:02:14.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename subpath 05/16/23 15:02:14.604
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:02:14.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:02:14.628
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/16/23 15:02:14.631
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-znrt 05/16/23 15:02:14.739
STEP: Creating a pod to test atomic-volume-subpath 05/16/23 15:02:14.739
May 16 15:02:14.754: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-znrt" in namespace "subpath-3787" to be "Succeeded or Failed"
May 16 15:02:14.761: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.48078ms
May 16 15:02:16.764: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 2.010127096s
May 16 15:02:18.764: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 4.010004587s
May 16 15:02:20.765: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 6.010388023s
May 16 15:02:22.765: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 8.010608096s
May 16 15:02:24.766: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 10.011222013s
May 16 15:02:26.764: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 12.00992701s
May 16 15:02:28.764: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 14.009903822s
May 16 15:02:30.765: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 16.010291464s
May 16 15:02:32.764: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 18.009785897s
May 16 15:02:34.764: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 20.009722439s
May 16 15:02:36.765: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=false. Elapsed: 22.010301885s
May 16 15:02:38.764: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00967198s
STEP: Saw pod success 05/16/23 15:02:38.764
May 16 15:02:38.764: INFO: Pod "pod-subpath-test-secret-znrt" satisfied condition "Succeeded or Failed"
May 16 15:02:38.767: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-subpath-test-secret-znrt container test-container-subpath-secret-znrt: <nil>
STEP: delete the pod 05/16/23 15:02:38.773
May 16 15:02:38.782: INFO: Waiting for pod pod-subpath-test-secret-znrt to disappear
May 16 15:02:38.784: INFO: Pod pod-subpath-test-secret-znrt no longer exists
STEP: Deleting pod pod-subpath-test-secret-znrt 05/16/23 15:02:38.784
May 16 15:02:38.784: INFO: Deleting pod "pod-subpath-test-secret-znrt" in namespace "subpath-3787"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May 16 15:02:38.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3787" for this suite. 05/16/23 15:02:38.791
------------------------------
â€¢ [SLOW TEST] [24.194 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:02:14.604
    May 16 15:02:14.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename subpath 05/16/23 15:02:14.604
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:02:14.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:02:14.628
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/16/23 15:02:14.631
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-znrt 05/16/23 15:02:14.739
    STEP: Creating a pod to test atomic-volume-subpath 05/16/23 15:02:14.739
    May 16 15:02:14.754: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-znrt" in namespace "subpath-3787" to be "Succeeded or Failed"
    May 16 15:02:14.761: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Pending", Reason="", readiness=false. Elapsed: 6.48078ms
    May 16 15:02:16.764: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 2.010127096s
    May 16 15:02:18.764: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 4.010004587s
    May 16 15:02:20.765: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 6.010388023s
    May 16 15:02:22.765: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 8.010608096s
    May 16 15:02:24.766: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 10.011222013s
    May 16 15:02:26.764: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 12.00992701s
    May 16 15:02:28.764: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 14.009903822s
    May 16 15:02:30.765: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 16.010291464s
    May 16 15:02:32.764: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 18.009785897s
    May 16 15:02:34.764: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=true. Elapsed: 20.009722439s
    May 16 15:02:36.765: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Running", Reason="", readiness=false. Elapsed: 22.010301885s
    May 16 15:02:38.764: INFO: Pod "pod-subpath-test-secret-znrt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00967198s
    STEP: Saw pod success 05/16/23 15:02:38.764
    May 16 15:02:38.764: INFO: Pod "pod-subpath-test-secret-znrt" satisfied condition "Succeeded or Failed"
    May 16 15:02:38.767: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-subpath-test-secret-znrt container test-container-subpath-secret-znrt: <nil>
    STEP: delete the pod 05/16/23 15:02:38.773
    May 16 15:02:38.782: INFO: Waiting for pod pod-subpath-test-secret-znrt to disappear
    May 16 15:02:38.784: INFO: Pod pod-subpath-test-secret-znrt no longer exists
    STEP: Deleting pod pod-subpath-test-secret-znrt 05/16/23 15:02:38.784
    May 16 15:02:38.784: INFO: Deleting pod "pod-subpath-test-secret-znrt" in namespace "subpath-3787"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May 16 15:02:38.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3787" for this suite. 05/16/23 15:02:38.791
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:02:38.798
May 16 15:02:38.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 15:02:38.799
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:02:38.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:02:38.818
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
May 16 15:02:38.829: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-43631aa1-d44e-4a48-8c22-95d8410dd7e9 05/16/23 15:02:38.829
STEP: Creating secret with name s-test-opt-upd-c908648c-2905-4a1c-b5f7-c19538af11bb 05/16/23 15:02:38.834
STEP: Creating the pod 05/16/23 15:02:38.839
May 16 15:02:38.854: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-10934210-3413-419d-a1c9-f8ae0b032815" in namespace "projected-2183" to be "running and ready"
May 16 15:02:38.865: INFO: Pod "pod-projected-secrets-10934210-3413-419d-a1c9-f8ae0b032815": Phase="Pending", Reason="", readiness=false. Elapsed: 10.577798ms
May 16 15:02:38.865: INFO: The phase of Pod pod-projected-secrets-10934210-3413-419d-a1c9-f8ae0b032815 is Pending, waiting for it to be Running (with Ready = true)
May 16 15:02:40.868: INFO: Pod "pod-projected-secrets-10934210-3413-419d-a1c9-f8ae0b032815": Phase="Running", Reason="", readiness=true. Elapsed: 2.013823058s
May 16 15:02:40.868: INFO: The phase of Pod pod-projected-secrets-10934210-3413-419d-a1c9-f8ae0b032815 is Running (Ready = true)
May 16 15:02:40.868: INFO: Pod "pod-projected-secrets-10934210-3413-419d-a1c9-f8ae0b032815" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-43631aa1-d44e-4a48-8c22-95d8410dd7e9 05/16/23 15:02:40.887
STEP: Updating secret s-test-opt-upd-c908648c-2905-4a1c-b5f7-c19538af11bb 05/16/23 15:02:40.892
STEP: Creating secret with name s-test-opt-create-2fae616e-9c18-4737-b338-127d89a0b3c9 05/16/23 15:02:40.896
STEP: waiting to observe update in volume 05/16/23 15:02:40.9
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May 16 15:02:44.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2183" for this suite. 05/16/23 15:02:44.937
------------------------------
â€¢ [SLOW TEST] [6.144 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:02:38.798
    May 16 15:02:38.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 15:02:38.799
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:02:38.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:02:38.818
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    May 16 15:02:38.829: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating secret with name s-test-opt-del-43631aa1-d44e-4a48-8c22-95d8410dd7e9 05/16/23 15:02:38.829
    STEP: Creating secret with name s-test-opt-upd-c908648c-2905-4a1c-b5f7-c19538af11bb 05/16/23 15:02:38.834
    STEP: Creating the pod 05/16/23 15:02:38.839
    May 16 15:02:38.854: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-10934210-3413-419d-a1c9-f8ae0b032815" in namespace "projected-2183" to be "running and ready"
    May 16 15:02:38.865: INFO: Pod "pod-projected-secrets-10934210-3413-419d-a1c9-f8ae0b032815": Phase="Pending", Reason="", readiness=false. Elapsed: 10.577798ms
    May 16 15:02:38.865: INFO: The phase of Pod pod-projected-secrets-10934210-3413-419d-a1c9-f8ae0b032815 is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:02:40.868: INFO: Pod "pod-projected-secrets-10934210-3413-419d-a1c9-f8ae0b032815": Phase="Running", Reason="", readiness=true. Elapsed: 2.013823058s
    May 16 15:02:40.868: INFO: The phase of Pod pod-projected-secrets-10934210-3413-419d-a1c9-f8ae0b032815 is Running (Ready = true)
    May 16 15:02:40.868: INFO: Pod "pod-projected-secrets-10934210-3413-419d-a1c9-f8ae0b032815" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-43631aa1-d44e-4a48-8c22-95d8410dd7e9 05/16/23 15:02:40.887
    STEP: Updating secret s-test-opt-upd-c908648c-2905-4a1c-b5f7-c19538af11bb 05/16/23 15:02:40.892
    STEP: Creating secret with name s-test-opt-create-2fae616e-9c18-4737-b338-127d89a0b3c9 05/16/23 15:02:40.896
    STEP: waiting to observe update in volume 05/16/23 15:02:40.9
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May 16 15:02:44.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2183" for this suite. 05/16/23 15:02:44.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:02:44.942
May 16 15:02:44.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename var-expansion 05/16/23 15:02:44.943
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:02:44.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:02:44.963
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
W0516 15:02:44.978556      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:02:44.978: INFO: Waiting up to 2m0s for pod "var-expansion-39afff0c-9576-4592-898d-96277c06f461" in namespace "var-expansion-9970" to be "container 0 failed with reason CreateContainerConfigError"
May 16 15:02:44.981: INFO: Pod "var-expansion-39afff0c-9576-4592-898d-96277c06f461": Phase="Pending", Reason="", readiness=false. Elapsed: 2.44571ms
May 16 15:02:46.992: INFO: Pod "var-expansion-39afff0c-9576-4592-898d-96277c06f461": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013759918s
May 16 15:02:46.992: INFO: Pod "var-expansion-39afff0c-9576-4592-898d-96277c06f461" satisfied condition "container 0 failed with reason CreateContainerConfigError"
May 16 15:02:46.992: INFO: Deleting pod "var-expansion-39afff0c-9576-4592-898d-96277c06f461" in namespace "var-expansion-9970"
May 16 15:02:47.008: INFO: Wait up to 5m0s for pod "var-expansion-39afff0c-9576-4592-898d-96277c06f461" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May 16 15:02:49.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9970" for this suite. 05/16/23 15:02:49.029
------------------------------
â€¢ [4.094 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:02:44.942
    May 16 15:02:44.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename var-expansion 05/16/23 15:02:44.943
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:02:44.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:02:44.963
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    W0516 15:02:44.978556      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:02:44.978: INFO: Waiting up to 2m0s for pod "var-expansion-39afff0c-9576-4592-898d-96277c06f461" in namespace "var-expansion-9970" to be "container 0 failed with reason CreateContainerConfigError"
    May 16 15:02:44.981: INFO: Pod "var-expansion-39afff0c-9576-4592-898d-96277c06f461": Phase="Pending", Reason="", readiness=false. Elapsed: 2.44571ms
    May 16 15:02:46.992: INFO: Pod "var-expansion-39afff0c-9576-4592-898d-96277c06f461": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013759918s
    May 16 15:02:46.992: INFO: Pod "var-expansion-39afff0c-9576-4592-898d-96277c06f461" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    May 16 15:02:46.992: INFO: Deleting pod "var-expansion-39afff0c-9576-4592-898d-96277c06f461" in namespace "var-expansion-9970"
    May 16 15:02:47.008: INFO: Wait up to 5m0s for pod "var-expansion-39afff0c-9576-4592-898d-96277c06f461" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May 16 15:02:49.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9970" for this suite. 05/16/23 15:02:49.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:02:49.037
May 16 15:02:49.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename csiinlinevolumes 05/16/23 15:02:49.038
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:02:49.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:02:49.065
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 05/16/23 15:02:49.069
STEP: getting 05/16/23 15:02:49.097
STEP: listing 05/16/23 15:02:49.108
STEP: deleting 05/16/23 15:02:49.111
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
May 16 15:02:49.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-1904" for this suite. 05/16/23 15:02:49.136
------------------------------
â€¢ [0.107 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:02:49.037
    May 16 15:02:49.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename csiinlinevolumes 05/16/23 15:02:49.038
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:02:49.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:02:49.065
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 05/16/23 15:02:49.069
    STEP: getting 05/16/23 15:02:49.097
    STEP: listing 05/16/23 15:02:49.108
    STEP: deleting 05/16/23 15:02:49.111
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    May 16 15:02:49.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-1904" for this suite. 05/16/23 15:02:49.136
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:02:49.144
May 16 15:02:49.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename pods 05/16/23 15:02:49.145
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:02:49.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:02:49.17
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 05/16/23 15:02:49.177
May 16 15:02:49.198: INFO: Waiting up to 5m0s for pod "pod-hostip-55d5de99-f5ca-4c46-9c14-198e75fd7ef0" in namespace "pods-203" to be "running and ready"
May 16 15:02:49.203: INFO: Pod "pod-hostip-55d5de99-f5ca-4c46-9c14-198e75fd7ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.642832ms
May 16 15:02:49.203: INFO: The phase of Pod pod-hostip-55d5de99-f5ca-4c46-9c14-198e75fd7ef0 is Pending, waiting for it to be Running (with Ready = true)
May 16 15:02:51.207: INFO: Pod "pod-hostip-55d5de99-f5ca-4c46-9c14-198e75fd7ef0": Phase="Running", Reason="", readiness=true. Elapsed: 2.008868201s
May 16 15:02:51.207: INFO: The phase of Pod pod-hostip-55d5de99-f5ca-4c46-9c14-198e75fd7ef0 is Running (Ready = true)
May 16 15:02:51.207: INFO: Pod "pod-hostip-55d5de99-f5ca-4c46-9c14-198e75fd7ef0" satisfied condition "running and ready"
May 16 15:02:51.212: INFO: Pod pod-hostip-55d5de99-f5ca-4c46-9c14-198e75fd7ef0 has hostIP: 10.0.132.142
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 16 15:02:51.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-203" for this suite. 05/16/23 15:02:51.216
------------------------------
â€¢ [2.077 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:02:49.144
    May 16 15:02:49.144: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename pods 05/16/23 15:02:49.145
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:02:49.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:02:49.17
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 05/16/23 15:02:49.177
    May 16 15:02:49.198: INFO: Waiting up to 5m0s for pod "pod-hostip-55d5de99-f5ca-4c46-9c14-198e75fd7ef0" in namespace "pods-203" to be "running and ready"
    May 16 15:02:49.203: INFO: Pod "pod-hostip-55d5de99-f5ca-4c46-9c14-198e75fd7ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.642832ms
    May 16 15:02:49.203: INFO: The phase of Pod pod-hostip-55d5de99-f5ca-4c46-9c14-198e75fd7ef0 is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:02:51.207: INFO: Pod "pod-hostip-55d5de99-f5ca-4c46-9c14-198e75fd7ef0": Phase="Running", Reason="", readiness=true. Elapsed: 2.008868201s
    May 16 15:02:51.207: INFO: The phase of Pod pod-hostip-55d5de99-f5ca-4c46-9c14-198e75fd7ef0 is Running (Ready = true)
    May 16 15:02:51.207: INFO: Pod "pod-hostip-55d5de99-f5ca-4c46-9c14-198e75fd7ef0" satisfied condition "running and ready"
    May 16 15:02:51.212: INFO: Pod pod-hostip-55d5de99-f5ca-4c46-9c14-198e75fd7ef0 has hostIP: 10.0.132.142
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 16 15:02:51.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-203" for this suite. 05/16/23 15:02:51.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:02:51.221
May 16 15:02:51.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename cronjob 05/16/23 15:02:51.222
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:02:51.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:02:51.24
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 05/16/23 15:02:51.243
W0516 15:02:51.257471      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring more than one job is running at a time 05/16/23 15:02:51.257
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 05/16/23 15:04:01.26
STEP: Removing cronjob 05/16/23 15:04:01.263
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
May 16 15:04:01.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2667" for this suite. 05/16/23 15:04:01.274
------------------------------
â€¢ [SLOW TEST] [70.061 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:02:51.221
    May 16 15:02:51.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename cronjob 05/16/23 15:02:51.222
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:02:51.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:02:51.24
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 05/16/23 15:02:51.243
    W0516 15:02:51.257471      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring more than one job is running at a time 05/16/23 15:02:51.257
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 05/16/23 15:04:01.26
    STEP: Removing cronjob 05/16/23 15:04:01.263
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    May 16 15:04:01.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2667" for this suite. 05/16/23 15:04:01.274
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:04:01.283
May 16 15:04:01.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename subpath 05/16/23 15:04:01.284
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:04:01.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:04:01.309
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/16/23 15:04:01.312
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-r82c 05/16/23 15:04:01.33
STEP: Creating a pod to test atomic-volume-subpath 05/16/23 15:04:01.33
May 16 15:04:01.351: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-r82c" in namespace "subpath-7332" to be "Succeeded or Failed"
May 16 15:04:01.359: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.970072ms
May 16 15:04:03.364: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 2.012993671s
May 16 15:04:05.364: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 4.013446225s
May 16 15:04:07.364: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 6.013225085s
May 16 15:04:09.363: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 8.012289721s
May 16 15:04:11.362: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 10.011208355s
May 16 15:04:13.363: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 12.012307583s
May 16 15:04:15.364: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 14.013105178s
May 16 15:04:17.363: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 16.012492199s
May 16 15:04:19.362: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 18.011617969s
May 16 15:04:21.363: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 20.012315532s
May 16 15:04:23.363: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=false. Elapsed: 22.012573634s
May 16 15:04:25.363: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011911977s
STEP: Saw pod success 05/16/23 15:04:25.363
May 16 15:04:25.363: INFO: Pod "pod-subpath-test-projected-r82c" satisfied condition "Succeeded or Failed"
May 16 15:04:25.367: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-subpath-test-projected-r82c container test-container-subpath-projected-r82c: <nil>
STEP: delete the pod 05/16/23 15:04:25.375
May 16 15:04:25.385: INFO: Waiting for pod pod-subpath-test-projected-r82c to disappear
May 16 15:04:25.388: INFO: Pod pod-subpath-test-projected-r82c no longer exists
STEP: Deleting pod pod-subpath-test-projected-r82c 05/16/23 15:04:25.388
May 16 15:04:25.388: INFO: Deleting pod "pod-subpath-test-projected-r82c" in namespace "subpath-7332"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May 16 15:04:25.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7332" for this suite. 05/16/23 15:04:25.395
------------------------------
â€¢ [SLOW TEST] [24.117 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:04:01.283
    May 16 15:04:01.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename subpath 05/16/23 15:04:01.284
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:04:01.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:04:01.309
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/16/23 15:04:01.312
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-r82c 05/16/23 15:04:01.33
    STEP: Creating a pod to test atomic-volume-subpath 05/16/23 15:04:01.33
    May 16 15:04:01.351: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-r82c" in namespace "subpath-7332" to be "Succeeded or Failed"
    May 16 15:04:01.359: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.970072ms
    May 16 15:04:03.364: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 2.012993671s
    May 16 15:04:05.364: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 4.013446225s
    May 16 15:04:07.364: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 6.013225085s
    May 16 15:04:09.363: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 8.012289721s
    May 16 15:04:11.362: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 10.011208355s
    May 16 15:04:13.363: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 12.012307583s
    May 16 15:04:15.364: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 14.013105178s
    May 16 15:04:17.363: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 16.012492199s
    May 16 15:04:19.362: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 18.011617969s
    May 16 15:04:21.363: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=true. Elapsed: 20.012315532s
    May 16 15:04:23.363: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Running", Reason="", readiness=false. Elapsed: 22.012573634s
    May 16 15:04:25.363: INFO: Pod "pod-subpath-test-projected-r82c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011911977s
    STEP: Saw pod success 05/16/23 15:04:25.363
    May 16 15:04:25.363: INFO: Pod "pod-subpath-test-projected-r82c" satisfied condition "Succeeded or Failed"
    May 16 15:04:25.367: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-subpath-test-projected-r82c container test-container-subpath-projected-r82c: <nil>
    STEP: delete the pod 05/16/23 15:04:25.375
    May 16 15:04:25.385: INFO: Waiting for pod pod-subpath-test-projected-r82c to disappear
    May 16 15:04:25.388: INFO: Pod pod-subpath-test-projected-r82c no longer exists
    STEP: Deleting pod pod-subpath-test-projected-r82c 05/16/23 15:04:25.388
    May 16 15:04:25.388: INFO: Deleting pod "pod-subpath-test-projected-r82c" in namespace "subpath-7332"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May 16 15:04:25.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7332" for this suite. 05/16/23 15:04:25.395
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:04:25.401
May 16 15:04:25.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename configmap 05/16/23 15:04:25.402
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:04:25.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:04:25.421
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-9fea1fa7-4c1f-464d-b458-52d306c29552 05/16/23 15:04:25.424
STEP: Creating a pod to test consume configMaps 05/16/23 15:04:25.432
May 16 15:04:25.444: INFO: Waiting up to 5m0s for pod "pod-configmaps-1176f8c7-378e-4553-b8d4-293e446b8834" in namespace "configmap-5479" to be "Succeeded or Failed"
May 16 15:04:25.450: INFO: Pod "pod-configmaps-1176f8c7-378e-4553-b8d4-293e446b8834": Phase="Pending", Reason="", readiness=false. Elapsed: 6.249136ms
May 16 15:04:27.454: INFO: Pod "pod-configmaps-1176f8c7-378e-4553-b8d4-293e446b8834": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010398754s
May 16 15:04:29.454: INFO: Pod "pod-configmaps-1176f8c7-378e-4553-b8d4-293e446b8834": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010545699s
STEP: Saw pod success 05/16/23 15:04:29.454
May 16 15:04:29.454: INFO: Pod "pod-configmaps-1176f8c7-378e-4553-b8d4-293e446b8834" satisfied condition "Succeeded or Failed"
May 16 15:04:29.458: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod pod-configmaps-1176f8c7-378e-4553-b8d4-293e446b8834 container agnhost-container: <nil>
STEP: delete the pod 05/16/23 15:04:29.469
May 16 15:04:29.478: INFO: Waiting for pod pod-configmaps-1176f8c7-378e-4553-b8d4-293e446b8834 to disappear
May 16 15:04:29.481: INFO: Pod pod-configmaps-1176f8c7-378e-4553-b8d4-293e446b8834 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 16 15:04:29.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5479" for this suite. 05/16/23 15:04:29.485
------------------------------
â€¢ [4.089 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:04:25.401
    May 16 15:04:25.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename configmap 05/16/23 15:04:25.402
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:04:25.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:04:25.421
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-9fea1fa7-4c1f-464d-b458-52d306c29552 05/16/23 15:04:25.424
    STEP: Creating a pod to test consume configMaps 05/16/23 15:04:25.432
    May 16 15:04:25.444: INFO: Waiting up to 5m0s for pod "pod-configmaps-1176f8c7-378e-4553-b8d4-293e446b8834" in namespace "configmap-5479" to be "Succeeded or Failed"
    May 16 15:04:25.450: INFO: Pod "pod-configmaps-1176f8c7-378e-4553-b8d4-293e446b8834": Phase="Pending", Reason="", readiness=false. Elapsed: 6.249136ms
    May 16 15:04:27.454: INFO: Pod "pod-configmaps-1176f8c7-378e-4553-b8d4-293e446b8834": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010398754s
    May 16 15:04:29.454: INFO: Pod "pod-configmaps-1176f8c7-378e-4553-b8d4-293e446b8834": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010545699s
    STEP: Saw pod success 05/16/23 15:04:29.454
    May 16 15:04:29.454: INFO: Pod "pod-configmaps-1176f8c7-378e-4553-b8d4-293e446b8834" satisfied condition "Succeeded or Failed"
    May 16 15:04:29.458: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod pod-configmaps-1176f8c7-378e-4553-b8d4-293e446b8834 container agnhost-container: <nil>
    STEP: delete the pod 05/16/23 15:04:29.469
    May 16 15:04:29.478: INFO: Waiting for pod pod-configmaps-1176f8c7-378e-4553-b8d4-293e446b8834 to disappear
    May 16 15:04:29.481: INFO: Pod pod-configmaps-1176f8c7-378e-4553-b8d4-293e446b8834 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 16 15:04:29.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5479" for this suite. 05/16/23 15:04:29.485
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:04:29.49
May 16 15:04:29.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 15:04:29.49
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:04:29.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:04:29.511
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 05/16/23 15:04:29.515
May 16 15:04:29.531: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e80b6477-be40-4dc9-9cb4-5c402496fd5e" in namespace "projected-8435" to be "Succeeded or Failed"
May 16 15:04:29.534: INFO: Pod "downwardapi-volume-e80b6477-be40-4dc9-9cb4-5c402496fd5e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.191952ms
May 16 15:04:31.538: INFO: Pod "downwardapi-volume-e80b6477-be40-4dc9-9cb4-5c402496fd5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007209837s
May 16 15:04:33.540: INFO: Pod "downwardapi-volume-e80b6477-be40-4dc9-9cb4-5c402496fd5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008879972s
STEP: Saw pod success 05/16/23 15:04:33.54
May 16 15:04:33.540: INFO: Pod "downwardapi-volume-e80b6477-be40-4dc9-9cb4-5c402496fd5e" satisfied condition "Succeeded or Failed"
May 16 15:04:33.543: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-e80b6477-be40-4dc9-9cb4-5c402496fd5e container client-container: <nil>
STEP: delete the pod 05/16/23 15:04:33.548
May 16 15:04:33.557: INFO: Waiting for pod downwardapi-volume-e80b6477-be40-4dc9-9cb4-5c402496fd5e to disappear
May 16 15:04:33.560: INFO: Pod downwardapi-volume-e80b6477-be40-4dc9-9cb4-5c402496fd5e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 16 15:04:33.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8435" for this suite. 05/16/23 15:04:33.564
------------------------------
â€¢ [4.079 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:04:29.49
    May 16 15:04:29.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 15:04:29.49
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:04:29.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:04:29.511
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 05/16/23 15:04:29.515
    May 16 15:04:29.531: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e80b6477-be40-4dc9-9cb4-5c402496fd5e" in namespace "projected-8435" to be "Succeeded or Failed"
    May 16 15:04:29.534: INFO: Pod "downwardapi-volume-e80b6477-be40-4dc9-9cb4-5c402496fd5e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.191952ms
    May 16 15:04:31.538: INFO: Pod "downwardapi-volume-e80b6477-be40-4dc9-9cb4-5c402496fd5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007209837s
    May 16 15:04:33.540: INFO: Pod "downwardapi-volume-e80b6477-be40-4dc9-9cb4-5c402496fd5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008879972s
    STEP: Saw pod success 05/16/23 15:04:33.54
    May 16 15:04:33.540: INFO: Pod "downwardapi-volume-e80b6477-be40-4dc9-9cb4-5c402496fd5e" satisfied condition "Succeeded or Failed"
    May 16 15:04:33.543: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-e80b6477-be40-4dc9-9cb4-5c402496fd5e container client-container: <nil>
    STEP: delete the pod 05/16/23 15:04:33.548
    May 16 15:04:33.557: INFO: Waiting for pod downwardapi-volume-e80b6477-be40-4dc9-9cb4-5c402496fd5e to disappear
    May 16 15:04:33.560: INFO: Pod downwardapi-volume-e80b6477-be40-4dc9-9cb4-5c402496fd5e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 16 15:04:33.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8435" for this suite. 05/16/23 15:04:33.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:04:33.57
May 16 15:04:33.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename gc 05/16/23 15:04:33.57
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:04:33.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:04:33.59
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 05/16/23 15:04:33.601
STEP: delete the rc 05/16/23 15:04:38.614
STEP: wait for the rc to be deleted 05/16/23 15:04:38.626
STEP: Gathering metrics 05/16/23 15:04:39.634
W0516 15:04:39.637395      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0516 15:04:39.637409      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May 16 15:04:39.637: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May 16 15:04:39.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1778" for this suite. 05/16/23 15:04:39.641
------------------------------
â€¢ [SLOW TEST] [6.080 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:04:33.57
    May 16 15:04:33.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename gc 05/16/23 15:04:33.57
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:04:33.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:04:33.59
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 05/16/23 15:04:33.601
    STEP: delete the rc 05/16/23 15:04:38.614
    STEP: wait for the rc to be deleted 05/16/23 15:04:38.626
    STEP: Gathering metrics 05/16/23 15:04:39.634
    W0516 15:04:39.637395      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
    W0516 15:04:39.637409      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May 16 15:04:39.637: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May 16 15:04:39.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1778" for this suite. 05/16/23 15:04:39.641
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:04:39.65
May 16 15:04:39.650: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename services 05/16/23 15:04:39.651
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:04:39.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:04:39.675
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-2414 05/16/23 15:04:39.677
STEP: creating service affinity-clusterip in namespace services-2414 05/16/23 15:04:39.677
STEP: creating replication controller affinity-clusterip in namespace services-2414 05/16/23 15:04:39.698
I0516 15:04:39.713263      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-2414, replica count: 3
I0516 15:04:42.800991      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0516 15:04:45.801216      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 16 15:04:45.807: INFO: Creating new exec pod
May 16 15:04:45.816: INFO: Waiting up to 5m0s for pod "execpod-affinityvgnmq" in namespace "services-2414" to be "running"
May 16 15:04:45.819: INFO: Pod "execpod-affinityvgnmq": Phase="Pending", Reason="", readiness=false. Elapsed: 3.108859ms
May 16 15:04:47.823: INFO: Pod "execpod-affinityvgnmq": Phase="Running", Reason="", readiness=true. Elapsed: 2.00758939s
May 16 15:04:47.824: INFO: Pod "execpod-affinityvgnmq" satisfied condition "running"
May 16 15:04:48.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-2414 exec execpod-affinityvgnmq -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
May 16 15:04:49.992: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May 16 15:04:49.992: INFO: stdout: ""
May 16 15:04:49.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-2414 exec execpod-affinityvgnmq -- /bin/sh -x -c nc -v -z -w 2 172.30.96.88 80'
May 16 15:04:50.088: INFO: stderr: "+ nc -v -z -w 2 172.30.96.88 80\nConnection to 172.30.96.88 80 port [tcp/http] succeeded!\n"
May 16 15:04:50.088: INFO: stdout: ""
May 16 15:04:50.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-2414 exec execpod-affinityvgnmq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.96.88:80/ ; done'
May 16 15:04:50.235: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n"
May 16 15:04:50.235: INFO: stdout: "\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz"
May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
May 16 15:04:50.235: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-2414, will wait for the garbage collector to delete the pods 05/16/23 15:04:50.245
May 16 15:04:50.304: INFO: Deleting ReplicationController affinity-clusterip took: 5.861504ms
May 16 15:04:50.404: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.55118ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 16 15:04:52.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2414" for this suite. 05/16/23 15:04:52.75
------------------------------
â€¢ [SLOW TEST] [13.107 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:04:39.65
    May 16 15:04:39.650: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename services 05/16/23 15:04:39.651
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:04:39.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:04:39.675
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-2414 05/16/23 15:04:39.677
    STEP: creating service affinity-clusterip in namespace services-2414 05/16/23 15:04:39.677
    STEP: creating replication controller affinity-clusterip in namespace services-2414 05/16/23 15:04:39.698
    I0516 15:04:39.713263      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-2414, replica count: 3
    I0516 15:04:42.800991      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0516 15:04:45.801216      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 16 15:04:45.807: INFO: Creating new exec pod
    May 16 15:04:45.816: INFO: Waiting up to 5m0s for pod "execpod-affinityvgnmq" in namespace "services-2414" to be "running"
    May 16 15:04:45.819: INFO: Pod "execpod-affinityvgnmq": Phase="Pending", Reason="", readiness=false. Elapsed: 3.108859ms
    May 16 15:04:47.823: INFO: Pod "execpod-affinityvgnmq": Phase="Running", Reason="", readiness=true. Elapsed: 2.00758939s
    May 16 15:04:47.824: INFO: Pod "execpod-affinityvgnmq" satisfied condition "running"
    May 16 15:04:48.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-2414 exec execpod-affinityvgnmq -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    May 16 15:04:49.992: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    May 16 15:04:49.992: INFO: stdout: ""
    May 16 15:04:49.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-2414 exec execpod-affinityvgnmq -- /bin/sh -x -c nc -v -z -w 2 172.30.96.88 80'
    May 16 15:04:50.088: INFO: stderr: "+ nc -v -z -w 2 172.30.96.88 80\nConnection to 172.30.96.88 80 port [tcp/http] succeeded!\n"
    May 16 15:04:50.088: INFO: stdout: ""
    May 16 15:04:50.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-2414 exec execpod-affinityvgnmq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.96.88:80/ ; done'
    May 16 15:04:50.235: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.96.88:80/\n"
    May 16 15:04:50.235: INFO: stdout: "\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz\naffinity-clusterip-lrsvz"
    May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
    May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
    May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
    May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
    May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
    May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
    May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
    May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
    May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
    May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
    May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
    May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
    May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
    May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
    May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
    May 16 15:04:50.235: INFO: Received response from host: affinity-clusterip-lrsvz
    May 16 15:04:50.235: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-2414, will wait for the garbage collector to delete the pods 05/16/23 15:04:50.245
    May 16 15:04:50.304: INFO: Deleting ReplicationController affinity-clusterip took: 5.861504ms
    May 16 15:04:50.404: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.55118ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 16 15:04:52.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2414" for this suite. 05/16/23 15:04:52.75
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:04:52.757
May 16 15:04:52.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename gc 05/16/23 15:04:52.758
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:04:52.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:04:52.783
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 05/16/23 15:04:52.789
W0516 15:04:52.796251      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the Deployment to create new ReplicaSet 05/16/23 15:04:52.796
STEP: delete the deployment 05/16/23 15:04:53.313
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 05/16/23 15:04:53.331
STEP: Gathering metrics 05/16/23 15:04:53.869
W0516 15:04:53.873174      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0516 15:04:53.873190      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May 16 15:04:53.873: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May 16 15:04:53.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6107" for this suite. 05/16/23 15:04:53.878
------------------------------
â€¢ [1.142 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:04:52.757
    May 16 15:04:52.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename gc 05/16/23 15:04:52.758
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:04:52.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:04:52.783
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 05/16/23 15:04:52.789
    W0516 15:04:52.796251      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the Deployment to create new ReplicaSet 05/16/23 15:04:52.796
    STEP: delete the deployment 05/16/23 15:04:53.313
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 05/16/23 15:04:53.331
    STEP: Gathering metrics 05/16/23 15:04:53.869
    W0516 15:04:53.873174      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
    W0516 15:04:53.873190      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May 16 15:04:53.873: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May 16 15:04:53.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6107" for this suite. 05/16/23 15:04:53.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:04:53.9
May 16 15:04:53.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename replicaset 05/16/23 15:04:53.901
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:04:53.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:04:53.94
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 05/16/23 15:04:53.945
W0516 15:04:53.955506      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Verify that the required pods have come up 05/16/23 15:04:53.955
May 16 15:04:53.966: INFO: Pod name sample-pod: Found 0 pods out of 3
May 16 15:04:58.971: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 05/16/23 15:04:58.971
May 16 15:04:58.975: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 05/16/23 15:04:58.975
STEP: DeleteCollection of the ReplicaSets 05/16/23 15:04:58.982
STEP: After DeleteCollection verify that ReplicaSets have been deleted 05/16/23 15:04:58.989
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May 16 15:04:58.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7074" for this suite. 05/16/23 15:04:59.006
------------------------------
â€¢ [SLOW TEST] [5.124 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:04:53.9
    May 16 15:04:53.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename replicaset 05/16/23 15:04:53.901
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:04:53.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:04:53.94
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 05/16/23 15:04:53.945
    W0516 15:04:53.955506      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Verify that the required pods have come up 05/16/23 15:04:53.955
    May 16 15:04:53.966: INFO: Pod name sample-pod: Found 0 pods out of 3
    May 16 15:04:58.971: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 05/16/23 15:04:58.971
    May 16 15:04:58.975: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 05/16/23 15:04:58.975
    STEP: DeleteCollection of the ReplicaSets 05/16/23 15:04:58.982
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 05/16/23 15:04:58.989
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May 16 15:04:58.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7074" for this suite. 05/16/23 15:04:59.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:04:59.024
May 16 15:04:59.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 15:04:59.025
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:04:59.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:04:59.061
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
May 16 15:04:59.080: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-9249b95c-63a6-49f4-85f6-91a5eee70439 05/16/23 15:04:59.08
STEP: Creating the pod 05/16/23 15:04:59.086
May 16 15:04:59.095: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-89ccc6a9-d352-4824-9403-41c2e7673ac0" in namespace "projected-472" to be "running and ready"
May 16 15:04:59.104: INFO: Pod "pod-projected-configmaps-89ccc6a9-d352-4824-9403-41c2e7673ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.826188ms
May 16 15:04:59.104: INFO: The phase of Pod pod-projected-configmaps-89ccc6a9-d352-4824-9403-41c2e7673ac0 is Pending, waiting for it to be Running (with Ready = true)
May 16 15:05:01.108: INFO: Pod "pod-projected-configmaps-89ccc6a9-d352-4824-9403-41c2e7673ac0": Phase="Running", Reason="", readiness=true. Elapsed: 2.012231257s
May 16 15:05:01.108: INFO: The phase of Pod pod-projected-configmaps-89ccc6a9-d352-4824-9403-41c2e7673ac0 is Running (Ready = true)
May 16 15:05:01.108: INFO: Pod "pod-projected-configmaps-89ccc6a9-d352-4824-9403-41c2e7673ac0" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-9249b95c-63a6-49f4-85f6-91a5eee70439 05/16/23 15:05:01.116
STEP: waiting to observe update in volume 05/16/23 15:05:01.12
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May 16 15:05:03.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-472" for this suite. 05/16/23 15:05:03.136
------------------------------
â€¢ [4.128 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:04:59.024
    May 16 15:04:59.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 15:04:59.025
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:04:59.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:04:59.061
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    May 16 15:04:59.080: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-9249b95c-63a6-49f4-85f6-91a5eee70439 05/16/23 15:04:59.08
    STEP: Creating the pod 05/16/23 15:04:59.086
    May 16 15:04:59.095: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-89ccc6a9-d352-4824-9403-41c2e7673ac0" in namespace "projected-472" to be "running and ready"
    May 16 15:04:59.104: INFO: Pod "pod-projected-configmaps-89ccc6a9-d352-4824-9403-41c2e7673ac0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.826188ms
    May 16 15:04:59.104: INFO: The phase of Pod pod-projected-configmaps-89ccc6a9-d352-4824-9403-41c2e7673ac0 is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:05:01.108: INFO: Pod "pod-projected-configmaps-89ccc6a9-d352-4824-9403-41c2e7673ac0": Phase="Running", Reason="", readiness=true. Elapsed: 2.012231257s
    May 16 15:05:01.108: INFO: The phase of Pod pod-projected-configmaps-89ccc6a9-d352-4824-9403-41c2e7673ac0 is Running (Ready = true)
    May 16 15:05:01.108: INFO: Pod "pod-projected-configmaps-89ccc6a9-d352-4824-9403-41c2e7673ac0" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-9249b95c-63a6-49f4-85f6-91a5eee70439 05/16/23 15:05:01.116
    STEP: waiting to observe update in volume 05/16/23 15:05:01.12
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May 16 15:05:03.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-472" for this suite. 05/16/23 15:05:03.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:05:03.153
May 16 15:05:03.153: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 15:05:03.153
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:03.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:03.176
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
May 16 15:05:03.187: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-832706cf-0e12-4194-a310-8bfccb82fd3b 05/16/23 15:05:03.187
STEP: Creating configMap with name cm-test-opt-upd-f425f72a-8834-4877-84fe-7260a307b7f8 05/16/23 15:05:03.193
STEP: Creating the pod 05/16/23 15:05:03.197
May 16 15:05:03.212: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-391c4c11-5a87-4fb8-bbee-ea248a285fc9" in namespace "projected-5371" to be "running and ready"
May 16 15:05:03.220: INFO: Pod "pod-projected-configmaps-391c4c11-5a87-4fb8-bbee-ea248a285fc9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.293059ms
May 16 15:05:03.220: INFO: The phase of Pod pod-projected-configmaps-391c4c11-5a87-4fb8-bbee-ea248a285fc9 is Pending, waiting for it to be Running (with Ready = true)
May 16 15:05:05.223: INFO: Pod "pod-projected-configmaps-391c4c11-5a87-4fb8-bbee-ea248a285fc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.011646565s
May 16 15:05:05.223: INFO: The phase of Pod pod-projected-configmaps-391c4c11-5a87-4fb8-bbee-ea248a285fc9 is Running (Ready = true)
May 16 15:05:05.223: INFO: Pod "pod-projected-configmaps-391c4c11-5a87-4fb8-bbee-ea248a285fc9" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-832706cf-0e12-4194-a310-8bfccb82fd3b 05/16/23 15:05:05.242
STEP: Updating configmap cm-test-opt-upd-f425f72a-8834-4877-84fe-7260a307b7f8 05/16/23 15:05:05.247
STEP: Creating configMap with name cm-test-opt-create-f353e9fa-bd5f-4544-b6f7-196c9a047dde 05/16/23 15:05:05.251
STEP: waiting to observe update in volume 05/16/23 15:05:05.256
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May 16 15:05:07.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5371" for this suite. 05/16/23 15:05:07.281
------------------------------
â€¢ [4.134 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:05:03.153
    May 16 15:05:03.153: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 15:05:03.153
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:03.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:03.176
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    May 16 15:05:03.187: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name cm-test-opt-del-832706cf-0e12-4194-a310-8bfccb82fd3b 05/16/23 15:05:03.187
    STEP: Creating configMap with name cm-test-opt-upd-f425f72a-8834-4877-84fe-7260a307b7f8 05/16/23 15:05:03.193
    STEP: Creating the pod 05/16/23 15:05:03.197
    May 16 15:05:03.212: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-391c4c11-5a87-4fb8-bbee-ea248a285fc9" in namespace "projected-5371" to be "running and ready"
    May 16 15:05:03.220: INFO: Pod "pod-projected-configmaps-391c4c11-5a87-4fb8-bbee-ea248a285fc9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.293059ms
    May 16 15:05:03.220: INFO: The phase of Pod pod-projected-configmaps-391c4c11-5a87-4fb8-bbee-ea248a285fc9 is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:05:05.223: INFO: Pod "pod-projected-configmaps-391c4c11-5a87-4fb8-bbee-ea248a285fc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.011646565s
    May 16 15:05:05.223: INFO: The phase of Pod pod-projected-configmaps-391c4c11-5a87-4fb8-bbee-ea248a285fc9 is Running (Ready = true)
    May 16 15:05:05.223: INFO: Pod "pod-projected-configmaps-391c4c11-5a87-4fb8-bbee-ea248a285fc9" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-832706cf-0e12-4194-a310-8bfccb82fd3b 05/16/23 15:05:05.242
    STEP: Updating configmap cm-test-opt-upd-f425f72a-8834-4877-84fe-7260a307b7f8 05/16/23 15:05:05.247
    STEP: Creating configMap with name cm-test-opt-create-f353e9fa-bd5f-4544-b6f7-196c9a047dde 05/16/23 15:05:05.251
    STEP: waiting to observe update in volume 05/16/23 15:05:05.256
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May 16 15:05:07.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5371" for this suite. 05/16/23 15:05:07.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:05:07.287
May 16 15:05:07.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename deployment 05/16/23 15:05:07.288
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:07.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:07.309
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
May 16 15:05:07.312: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
W0516 15:05:07.320280      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:05:07.323: INFO: Pod name sample-pod: Found 0 pods out of 1
May 16 15:05:12.327: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/16/23 15:05:12.327
May 16 15:05:12.327: INFO: Creating deployment "test-rolling-update-deployment"
May 16 15:05:12.334: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 16 15:05:12.340: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 16 15:05:14.347: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 16 15:05:14.349: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 16 15:05:14.357: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1256  6d00833e-8db1-422e-a747-a912aa81b3dc 75960 1 2023-05-16 15:05:12 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-16 15:05:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:05:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039f3358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-16 15:05:12 +0000 UTC,LastTransitionTime:2023-05-16 15:05:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-05-16 15:05:13 +0000 UTC,LastTransitionTime:2023-05-16 15:05:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 16 15:05:14.359: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1256  ca3aa12e-f96f-4dd1-a71a-c4284c3a3a95 75950 1 2023-05-16 15:05:12 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 6d00833e-8db1-422e-a747-a912aa81b3dc 0xc0039f3b57 0xc0039f3b58}] [] [{kube-controller-manager Update apps/v1 2023-05-16 15:05:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d00833e-8db1-422e-a747-a912aa81b3dc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:05:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039f3c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 16 15:05:14.359: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 16 15:05:14.359: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1256  4066ce39-e214-481e-984e-b2e76c877b12 75959 2 2023-05-16 15:05:07 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 6d00833e-8db1-422e-a747-a912aa81b3dc 0xc0039f3a27 0xc0039f3a28}] [] [{e2e.test Update apps/v1 2023-05-16 15:05:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:05:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d00833e-8db1-422e-a747-a912aa81b3dc\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:05:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0039f3ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 16 15:05:14.362: INFO: Pod "test-rolling-update-deployment-7549d9f46d-hlchw" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-hlchw test-rolling-update-deployment-7549d9f46d- deployment-1256  2f10f9b2-eb94-4b0c-9119-0f774e78f62d 75949 0 2023-05-16 15:05:12 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.1.5/23"],"mac_address":"0a:58:0a:83:01:05","gateway_ips":["10.131.0.1"],"ip_address":"10.131.1.5/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.1.5"
    ],
    "mac": "0a:58:0a:83:01:05",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d ca3aa12e-f96f-4dd1-a71a-c4284c3a3a95 0xc007682077 0xc007682078}] [] [{ip-10-0-130-191 Update v1 2023-05-16 15:05:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 15:05:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca3aa12e-f96f-4dd1-a71a-c4284c3a3a95\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 15:05:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-05-16 15:05:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gtvh6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gtvh6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c53,c32,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-8wgkr,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:05:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:05:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:05:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:05:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:10.131.1.5,StartTime:2023-05-16 15:05:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 15:05:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://ca40f0793b1381b2417edf61699fa39613726a78d73d8c3c9eb1da4c10af8d45,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.1.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May 16 15:05:14.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1256" for this suite. 05/16/23 15:05:14.365
------------------------------
â€¢ [SLOW TEST] [7.085 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:05:07.287
    May 16 15:05:07.287: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename deployment 05/16/23 15:05:07.288
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:07.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:07.309
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    May 16 15:05:07.312: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    W0516 15:05:07.320280      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:05:07.323: INFO: Pod name sample-pod: Found 0 pods out of 1
    May 16 15:05:12.327: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/16/23 15:05:12.327
    May 16 15:05:12.327: INFO: Creating deployment "test-rolling-update-deployment"
    May 16 15:05:12.334: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    May 16 15:05:12.340: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    May 16 15:05:14.347: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    May 16 15:05:14.349: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 16 15:05:14.357: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1256  6d00833e-8db1-422e-a747-a912aa81b3dc 75960 1 2023-05-16 15:05:12 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-16 15:05:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:05:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039f3358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-16 15:05:12 +0000 UTC,LastTransitionTime:2023-05-16 15:05:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-05-16 15:05:13 +0000 UTC,LastTransitionTime:2023-05-16 15:05:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May 16 15:05:14.359: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1256  ca3aa12e-f96f-4dd1-a71a-c4284c3a3a95 75950 1 2023-05-16 15:05:12 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 6d00833e-8db1-422e-a747-a912aa81b3dc 0xc0039f3b57 0xc0039f3b58}] [] [{kube-controller-manager Update apps/v1 2023-05-16 15:05:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d00833e-8db1-422e-a747-a912aa81b3dc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:05:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039f3c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May 16 15:05:14.359: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    May 16 15:05:14.359: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1256  4066ce39-e214-481e-984e-b2e76c877b12 75959 2 2023-05-16 15:05:07 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 6d00833e-8db1-422e-a747-a912aa81b3dc 0xc0039f3a27 0xc0039f3a28}] [] [{e2e.test Update apps/v1 2023-05-16 15:05:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:05:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d00833e-8db1-422e-a747-a912aa81b3dc\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:05:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0039f3ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May 16 15:05:14.362: INFO: Pod "test-rolling-update-deployment-7549d9f46d-hlchw" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-hlchw test-rolling-update-deployment-7549d9f46d- deployment-1256  2f10f9b2-eb94-4b0c-9119-0f774e78f62d 75949 0 2023-05-16 15:05:12 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.1.5/23"],"mac_address":"0a:58:0a:83:01:05","gateway_ips":["10.131.0.1"],"ip_address":"10.131.1.5/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.1.5"
        ],
        "mac": "0a:58:0a:83:01:05",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d ca3aa12e-f96f-4dd1-a71a-c4284c3a3a95 0xc007682077 0xc007682078}] [] [{ip-10-0-130-191 Update v1 2023-05-16 15:05:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 15:05:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca3aa12e-f96f-4dd1-a71a-c4284c3a3a95\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 15:05:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-05-16 15:05:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gtvh6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gtvh6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c53,c32,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-8wgkr,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:05:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:05:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:05:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:05:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:10.131.1.5,StartTime:2023-05-16 15:05:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 15:05:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://ca40f0793b1381b2417edf61699fa39613726a78d73d8c3c9eb1da4c10af8d45,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.1.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May 16 15:05:14.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1256" for this suite. 05/16/23 15:05:14.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:05:14.372
May 16 15:05:14.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename sched-pred 05/16/23 15:05:14.373
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:14.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:14.396
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
May 16 15:05:14.399: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 16 15:05:14.410: INFO: Waiting for terminating namespaces to be deleted...
May 16 15:05:14.415: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-132-142.eu-central-1.compute.internal before test
May 16 15:05:14.443: INFO: aws-ebs-csi-driver-node-27d5z from openshift-cluster-csi-drivers started at 2023-05-16 13:52:17 +0000 UTC (3 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container csi-driver ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 16 15:05:14.443: INFO: tuned-j9bjl from openshift-cluster-node-tuning-operator started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container tuned ready: true, restart count 0
May 16 15:05:14.443: INFO: dns-default-9lcb7 from openshift-dns started at 2023-05-16 13:53:17 +0000 UTC (2 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container dns ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.443: INFO: node-resolver-b47gh from openshift-dns started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 16 15:05:14.443: INFO: image-registry-b549d4f89-4ck4v from openshift-image-registry started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container registry ready: true, restart count 0
May 16 15:05:14.443: INFO: node-ca-px455 from openshift-image-registry started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container node-ca ready: true, restart count 0
May 16 15:05:14.443: INFO: ingress-canary-4hv85 from openshift-ingress-canary started at 2023-05-16 13:53:17 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May 16 15:05:14.443: INFO: router-default-8f95c5-srcbl from openshift-ingress started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container router ready: true, restart count 0
May 16 15:05:14.443: INFO: machine-config-daemon-l6bpj from openshift-machine-config-operator started at 2023-05-16 13:52:17 +0000 UTC (2 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 15:05:14.443: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (6 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container alertmanager ready: true, restart count 1
May 16 15:05:14.443: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container config-reloader ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 16 15:05:14.443: INFO: node-exporter-pltn9 from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container node-exporter ready: true, restart count 0
May 16 15:05:14.443: INFO: prometheus-adapter-7767c6bd47-bfq4d from openshift-monitoring started at 2023-05-16 13:53:50 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 16 15:05:14.443: INFO: prometheus-operator-admission-webhook-5d679565bb-8lkws from openshift-monitoring started at 2023-05-16 14:48:21 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
May 16 15:05:14.443: INFO: telemeter-client-859dbffbb4-mt92v from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (3 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container reload ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container telemeter-client ready: true, restart count 0
May 16 15:05:14.443: INFO: thanos-querier-5c9d876b5c-xdt6m from openshift-monitoring started at 2023-05-16 13:53:54 +0000 UTC (6 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container thanos-query ready: true, restart count 0
May 16 15:05:14.443: INFO: multus-2wjqr from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container kube-multus ready: true, restart count 0
May 16 15:05:14.443: INFO: multus-additional-cni-plugins-z2h4l from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May 16 15:05:14.443: INFO: network-metrics-daemon-rqdtf from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (2 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 16 15:05:14.443: INFO: network-check-target-28wts from openshift-network-diagnostics started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container network-check-target-container ready: true, restart count 0
May 16 15:05:14.443: INFO: ovnkube-node-npwgq from openshift-ovn-kubernetes started at 2023-05-16 13:52:17 +0000 UTC (5 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container ovn-acl-logging ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container ovn-controller ready: true, restart count 1
May 16 15:05:14.443: INFO: 	Container ovnkube-node ready: true, restart count 0
May 16 15:05:14.443: INFO: sonobuoy from sonobuoy started at 2023-05-16 14:11:07 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 16 15:05:14.443: INFO: sonobuoy-e2e-job-840dd79630d44353 from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container e2e ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 15:05:14.443: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-7zsf6 from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
May 16 15:05:14.443: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 15:05:14.443: INFO: 	Container systemd-logs ready: true, restart count 0
May 16 15:05:14.443: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-161-164.eu-central-1.compute.internal before test
May 16 15:05:14.480: INFO: test-rolling-update-deployment-7549d9f46d-hlchw from deployment-1256 started at 2023-05-16 15:05:12 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.480: INFO: 	Container agnhost ready: true, restart count 0
May 16 15:05:14.480: INFO: aws-ebs-csi-driver-node-9m6vn from openshift-cluster-csi-drivers started at 2023-05-16 13:44:25 +0000 UTC (3 container statuses recorded)
May 16 15:05:14.480: INFO: 	Container csi-driver ready: true, restart count 0
May 16 15:05:14.480: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 16 15:05:14.480: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 16 15:05:14.480: INFO: tuned-6n5b7 from openshift-cluster-node-tuning-operator started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.480: INFO: 	Container tuned ready: true, restart count 0
May 16 15:05:14.480: INFO: dns-default-pwtdq from openshift-dns started at 2023-05-16 14:48:42 +0000 UTC (2 container statuses recorded)
May 16 15:05:14.480: INFO: 	Container dns ready: true, restart count 0
May 16 15:05:14.480: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.480: INFO: node-resolver-jbmqq from openshift-dns started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.480: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 16 15:05:14.480: INFO: node-ca-gw7rz from openshift-image-registry started at 2023-05-16 13:50:21 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.480: INFO: 	Container node-ca ready: true, restart count 0
May 16 15:05:14.480: INFO: ingress-canary-mvqst from openshift-ingress-canary started at 2023-05-16 14:48:23 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.480: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May 16 15:05:14.480: INFO: machine-config-daemon-qhdsv from openshift-machine-config-operator started at 2023-05-16 13:44:25 +0000 UTC (2 container statuses recorded)
May 16 15:05:14.480: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 16 15:05:14.480: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 15:05:14.480: INFO: node-exporter-l2zvs from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
May 16 15:05:14.480: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.480: INFO: 	Container node-exporter ready: true, restart count 0
May 16 15:05:14.480: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-05-16 14:48:24 +0000 UTC (6 container statuses recorded)
May 16 15:05:14.480: INFO: 	Container config-reloader ready: true, restart count 0
May 16 15:05:14.480: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.480: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May 16 15:05:14.480: INFO: 	Container prometheus ready: true, restart count 0
May 16 15:05:14.480: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 16 15:05:14.480: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 16 15:05:14.480: INFO: multus-additional-cni-plugins-jkn7l from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.480: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May 16 15:05:14.480: INFO: multus-mc6qb from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.480: INFO: 	Container kube-multus ready: true, restart count 0
May 16 15:05:14.480: INFO: network-metrics-daemon-n4p75 from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (2 container statuses recorded)
May 16 15:05:14.480: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.480: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 16 15:05:14.480: INFO: network-check-source-7f6b75fdb6-lxqsg from openshift-network-diagnostics started at 2023-05-16 14:48:21 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.480: INFO: 	Container check-endpoints ready: true, restart count 0
May 16 15:05:14.480: INFO: network-check-target-bprb5 from openshift-network-diagnostics started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.480: INFO: 	Container network-check-target-container ready: true, restart count 0
May 16 15:05:14.480: INFO: collect-profiles-28070820-8vnf5 from openshift-operator-lifecycle-manager started at 2023-05-16 15:00:00 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.480: INFO: 	Container collect-profiles ready: false, restart count 0
May 16 15:05:14.480: INFO: ovnkube-node-w8qn8 from openshift-ovn-kubernetes started at 2023-05-16 13:44:25 +0000 UTC (5 container statuses recorded)
May 16 15:05:14.480: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.480: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
May 16 15:05:14.481: INFO: 	Container ovn-acl-logging ready: true, restart count 0
May 16 15:05:14.481: INFO: 	Container ovn-controller ready: true, restart count 1
May 16 15:05:14.481: INFO: 	Container ovnkube-node ready: true, restart count 0
May 16 15:05:14.481: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-5r4tm from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
May 16 15:05:14.481: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 15:05:14.481: INFO: 	Container systemd-logs ready: true, restart count 0
May 16 15:05:14.481: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-212-246.eu-central-1.compute.internal before test
May 16 15:05:14.533: INFO: aws-ebs-csi-driver-node-b4s28 from openshift-cluster-csi-drivers started at 2023-05-16 13:52:19 +0000 UTC (3 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container csi-driver ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 16 15:05:14.533: INFO: tuned-8x9rx from openshift-cluster-node-tuning-operator started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container tuned ready: true, restart count 0
May 16 15:05:14.533: INFO: dns-default-dbkcs from openshift-dns started at 2023-05-16 13:53:17 +0000 UTC (2 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container dns ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.533: INFO: node-resolver-gdzz2 from openshift-dns started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 16 15:05:14.533: INFO: image-registry-b549d4f89-hs4qn from openshift-image-registry started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container registry ready: true, restart count 0
May 16 15:05:14.533: INFO: node-ca-nsxtf from openshift-image-registry started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container node-ca ready: true, restart count 0
May 16 15:05:14.533: INFO: ingress-canary-wphz5 from openshift-ingress-canary started at 2023-05-16 13:53:17 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May 16 15:05:14.533: INFO: router-default-8f95c5-bbmm5 from openshift-ingress started at 2023-05-16 14:48:21 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container router ready: true, restart count 0
May 16 15:05:14.533: INFO: machine-config-daemon-z7sw6 from openshift-machine-config-operator started at 2023-05-16 13:52:19 +0000 UTC (2 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 15:05:14.533: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (6 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container alertmanager ready: true, restart count 1
May 16 15:05:14.533: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container config-reloader ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 16 15:05:14.533: INFO: kube-state-metrics-6ccfb58dc4-wgwts from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (3 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 16 15:05:14.533: INFO: node-exporter-jrknj from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container node-exporter ready: true, restart count 0
May 16 15:05:14.533: INFO: openshift-state-metrics-7d7f8b4cf8-9lbg2 from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (3 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container openshift-state-metrics ready: true, restart count 0
May 16 15:05:14.533: INFO: prometheus-adapter-7767c6bd47-bqlnh from openshift-monitoring started at 2023-05-16 13:53:50 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 16 15:05:14.533: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-05-16 13:53:57 +0000 UTC (6 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container config-reloader ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container prometheus ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 16 15:05:14.533: INFO: prometheus-operator-admission-webhook-5d679565bb-96qsx from openshift-monitoring started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
May 16 15:05:14.533: INFO: thanos-querier-5c9d876b5c-wwd7q from openshift-monitoring started at 2023-05-16 13:53:54 +0000 UTC (6 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container thanos-query ready: true, restart count 0
May 16 15:05:14.533: INFO: multus-additional-cni-plugins-wxcnn from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May 16 15:05:14.533: INFO: multus-rpn9n from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container kube-multus ready: true, restart count 0
May 16 15:05:14.533: INFO: network-metrics-daemon-7sqls from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (2 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 16 15:05:14.533: INFO: network-check-target-8pcqz from openshift-network-diagnostics started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container network-check-target-container ready: true, restart count 0
May 16 15:05:14.533: INFO: ovnkube-node-kqvdm from openshift-ovn-kubernetes started at 2023-05-16 13:52:19 +0000 UTC (5 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container ovn-acl-logging ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container ovn-controller ready: true, restart count 1
May 16 15:05:14.533: INFO: 	Container ovnkube-node ready: true, restart count 0
May 16 15:05:14.533: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-gsqlr from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
May 16 15:05:14.533: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 15:05:14.533: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/16/23 15:05:14.533
May 16 15:05:14.547: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4892" to be "running"
May 16 15:05:14.565: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 18.291326ms
May 16 15:05:16.569: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.022314765s
May 16 15:05:16.569: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/16/23 15:05:16.572
STEP: Trying to apply a random label on the found node. 05/16/23 15:05:16.583
STEP: verifying the node has the label kubernetes.io/e2e-30ec892c-1cf9-4121-a84f-eec0dc612f31 42 05/16/23 15:05:16.596
STEP: Trying to relaunch the pod, now with labels. 05/16/23 15:05:16.604
May 16 15:05:16.616: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-4892" to be "not pending"
May 16 15:05:16.627: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 11.321974ms
May 16 15:05:18.631: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.01498698s
May 16 15:05:18.631: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-30ec892c-1cf9-4121-a84f-eec0dc612f31 off the node ip-10-0-161-164.eu-central-1.compute.internal 05/16/23 15:05:18.634
STEP: verifying the node doesn't have the label kubernetes.io/e2e-30ec892c-1cf9-4121-a84f-eec0dc612f31 05/16/23 15:05:18.647
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 15:05:18.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4892" for this suite. 05/16/23 15:05:18.667
------------------------------
â€¢ [4.302 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:05:14.372
    May 16 15:05:14.372: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename sched-pred 05/16/23 15:05:14.373
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:14.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:14.396
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    May 16 15:05:14.399: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May 16 15:05:14.410: INFO: Waiting for terminating namespaces to be deleted...
    May 16 15:05:14.415: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-132-142.eu-central-1.compute.internal before test
    May 16 15:05:14.443: INFO: aws-ebs-csi-driver-node-27d5z from openshift-cluster-csi-drivers started at 2023-05-16 13:52:17 +0000 UTC (3 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container csi-driver ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    May 16 15:05:14.443: INFO: tuned-j9bjl from openshift-cluster-node-tuning-operator started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container tuned ready: true, restart count 0
    May 16 15:05:14.443: INFO: dns-default-9lcb7 from openshift-dns started at 2023-05-16 13:53:17 +0000 UTC (2 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container dns ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.443: INFO: node-resolver-b47gh from openshift-dns started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May 16 15:05:14.443: INFO: image-registry-b549d4f89-4ck4v from openshift-image-registry started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container registry ready: true, restart count 0
    May 16 15:05:14.443: INFO: node-ca-px455 from openshift-image-registry started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container node-ca ready: true, restart count 0
    May 16 15:05:14.443: INFO: ingress-canary-4hv85 from openshift-ingress-canary started at 2023-05-16 13:53:17 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May 16 15:05:14.443: INFO: router-default-8f95c5-srcbl from openshift-ingress started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container router ready: true, restart count 0
    May 16 15:05:14.443: INFO: machine-config-daemon-l6bpj from openshift-machine-config-operator started at 2023-05-16 13:52:17 +0000 UTC (2 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container machine-config-daemon ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 15:05:14.443: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (6 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container alertmanager ready: true, restart count 1
    May 16 15:05:14.443: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container config-reloader ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May 16 15:05:14.443: INFO: node-exporter-pltn9 from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container node-exporter ready: true, restart count 0
    May 16 15:05:14.443: INFO: prometheus-adapter-7767c6bd47-bfq4d from openshift-monitoring started at 2023-05-16 13:53:50 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container prometheus-adapter ready: true, restart count 0
    May 16 15:05:14.443: INFO: prometheus-operator-admission-webhook-5d679565bb-8lkws from openshift-monitoring started at 2023-05-16 14:48:21 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    May 16 15:05:14.443: INFO: telemeter-client-859dbffbb4-mt92v from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (3 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container reload ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container telemeter-client ready: true, restart count 0
    May 16 15:05:14.443: INFO: thanos-querier-5c9d876b5c-xdt6m from openshift-monitoring started at 2023-05-16 13:53:54 +0000 UTC (6 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container thanos-query ready: true, restart count 0
    May 16 15:05:14.443: INFO: multus-2wjqr from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container kube-multus ready: true, restart count 0
    May 16 15:05:14.443: INFO: multus-additional-cni-plugins-z2h4l from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May 16 15:05:14.443: INFO: network-metrics-daemon-rqdtf from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (2 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May 16 15:05:14.443: INFO: network-check-target-28wts from openshift-network-diagnostics started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container network-check-target-container ready: true, restart count 0
    May 16 15:05:14.443: INFO: ovnkube-node-npwgq from openshift-ovn-kubernetes started at 2023-05-16 13:52:17 +0000 UTC (5 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container ovn-controller ready: true, restart count 1
    May 16 15:05:14.443: INFO: 	Container ovnkube-node ready: true, restart count 0
    May 16 15:05:14.443: INFO: sonobuoy from sonobuoy started at 2023-05-16 14:11:07 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May 16 15:05:14.443: INFO: sonobuoy-e2e-job-840dd79630d44353 from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container e2e ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 16 15:05:14.443: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-7zsf6 from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
    May 16 15:05:14.443: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 16 15:05:14.443: INFO: 	Container systemd-logs ready: true, restart count 0
    May 16 15:05:14.443: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-161-164.eu-central-1.compute.internal before test
    May 16 15:05:14.480: INFO: test-rolling-update-deployment-7549d9f46d-hlchw from deployment-1256 started at 2023-05-16 15:05:12 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.480: INFO: 	Container agnhost ready: true, restart count 0
    May 16 15:05:14.480: INFO: aws-ebs-csi-driver-node-9m6vn from openshift-cluster-csi-drivers started at 2023-05-16 13:44:25 +0000 UTC (3 container statuses recorded)
    May 16 15:05:14.480: INFO: 	Container csi-driver ready: true, restart count 0
    May 16 15:05:14.480: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    May 16 15:05:14.480: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    May 16 15:05:14.480: INFO: tuned-6n5b7 from openshift-cluster-node-tuning-operator started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.480: INFO: 	Container tuned ready: true, restart count 0
    May 16 15:05:14.480: INFO: dns-default-pwtdq from openshift-dns started at 2023-05-16 14:48:42 +0000 UTC (2 container statuses recorded)
    May 16 15:05:14.480: INFO: 	Container dns ready: true, restart count 0
    May 16 15:05:14.480: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.480: INFO: node-resolver-jbmqq from openshift-dns started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.480: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May 16 15:05:14.480: INFO: node-ca-gw7rz from openshift-image-registry started at 2023-05-16 13:50:21 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.480: INFO: 	Container node-ca ready: true, restart count 0
    May 16 15:05:14.480: INFO: ingress-canary-mvqst from openshift-ingress-canary started at 2023-05-16 14:48:23 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.480: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May 16 15:05:14.480: INFO: machine-config-daemon-qhdsv from openshift-machine-config-operator started at 2023-05-16 13:44:25 +0000 UTC (2 container statuses recorded)
    May 16 15:05:14.480: INFO: 	Container machine-config-daemon ready: true, restart count 0
    May 16 15:05:14.480: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 15:05:14.480: INFO: node-exporter-l2zvs from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
    May 16 15:05:14.480: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.480: INFO: 	Container node-exporter ready: true, restart count 0
    May 16 15:05:14.480: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-05-16 14:48:24 +0000 UTC (6 container statuses recorded)
    May 16 15:05:14.480: INFO: 	Container config-reloader ready: true, restart count 0
    May 16 15:05:14.480: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.480: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    May 16 15:05:14.480: INFO: 	Container prometheus ready: true, restart count 0
    May 16 15:05:14.480: INFO: 	Container prometheus-proxy ready: true, restart count 0
    May 16 15:05:14.480: INFO: 	Container thanos-sidecar ready: true, restart count 0
    May 16 15:05:14.480: INFO: multus-additional-cni-plugins-jkn7l from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.480: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May 16 15:05:14.480: INFO: multus-mc6qb from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.480: INFO: 	Container kube-multus ready: true, restart count 0
    May 16 15:05:14.480: INFO: network-metrics-daemon-n4p75 from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (2 container statuses recorded)
    May 16 15:05:14.480: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.480: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May 16 15:05:14.480: INFO: network-check-source-7f6b75fdb6-lxqsg from openshift-network-diagnostics started at 2023-05-16 14:48:21 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.480: INFO: 	Container check-endpoints ready: true, restart count 0
    May 16 15:05:14.480: INFO: network-check-target-bprb5 from openshift-network-diagnostics started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.480: INFO: 	Container network-check-target-container ready: true, restart count 0
    May 16 15:05:14.480: INFO: collect-profiles-28070820-8vnf5 from openshift-operator-lifecycle-manager started at 2023-05-16 15:00:00 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.480: INFO: 	Container collect-profiles ready: false, restart count 0
    May 16 15:05:14.480: INFO: ovnkube-node-w8qn8 from openshift-ovn-kubernetes started at 2023-05-16 13:44:25 +0000 UTC (5 container statuses recorded)
    May 16 15:05:14.480: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.480: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    May 16 15:05:14.481: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    May 16 15:05:14.481: INFO: 	Container ovn-controller ready: true, restart count 1
    May 16 15:05:14.481: INFO: 	Container ovnkube-node ready: true, restart count 0
    May 16 15:05:14.481: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-5r4tm from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
    May 16 15:05:14.481: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 16 15:05:14.481: INFO: 	Container systemd-logs ready: true, restart count 0
    May 16 15:05:14.481: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-212-246.eu-central-1.compute.internal before test
    May 16 15:05:14.533: INFO: aws-ebs-csi-driver-node-b4s28 from openshift-cluster-csi-drivers started at 2023-05-16 13:52:19 +0000 UTC (3 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container csi-driver ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    May 16 15:05:14.533: INFO: tuned-8x9rx from openshift-cluster-node-tuning-operator started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container tuned ready: true, restart count 0
    May 16 15:05:14.533: INFO: dns-default-dbkcs from openshift-dns started at 2023-05-16 13:53:17 +0000 UTC (2 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container dns ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.533: INFO: node-resolver-gdzz2 from openshift-dns started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May 16 15:05:14.533: INFO: image-registry-b549d4f89-hs4qn from openshift-image-registry started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container registry ready: true, restart count 0
    May 16 15:05:14.533: INFO: node-ca-nsxtf from openshift-image-registry started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container node-ca ready: true, restart count 0
    May 16 15:05:14.533: INFO: ingress-canary-wphz5 from openshift-ingress-canary started at 2023-05-16 13:53:17 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May 16 15:05:14.533: INFO: router-default-8f95c5-bbmm5 from openshift-ingress started at 2023-05-16 14:48:21 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container router ready: true, restart count 0
    May 16 15:05:14.533: INFO: machine-config-daemon-z7sw6 from openshift-machine-config-operator started at 2023-05-16 13:52:19 +0000 UTC (2 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container machine-config-daemon ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 15:05:14.533: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (6 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container alertmanager ready: true, restart count 1
    May 16 15:05:14.533: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container config-reloader ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May 16 15:05:14.533: INFO: kube-state-metrics-6ccfb58dc4-wgwts from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (3 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container kube-state-metrics ready: true, restart count 0
    May 16 15:05:14.533: INFO: node-exporter-jrknj from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container node-exporter ready: true, restart count 0
    May 16 15:05:14.533: INFO: openshift-state-metrics-7d7f8b4cf8-9lbg2 from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (3 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    May 16 15:05:14.533: INFO: prometheus-adapter-7767c6bd47-bqlnh from openshift-monitoring started at 2023-05-16 13:53:50 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container prometheus-adapter ready: true, restart count 0
    May 16 15:05:14.533: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-05-16 13:53:57 +0000 UTC (6 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container config-reloader ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container prometheus ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container prometheus-proxy ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container thanos-sidecar ready: true, restart count 0
    May 16 15:05:14.533: INFO: prometheus-operator-admission-webhook-5d679565bb-96qsx from openshift-monitoring started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    May 16 15:05:14.533: INFO: thanos-querier-5c9d876b5c-wwd7q from openshift-monitoring started at 2023-05-16 13:53:54 +0000 UTC (6 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container thanos-query ready: true, restart count 0
    May 16 15:05:14.533: INFO: multus-additional-cni-plugins-wxcnn from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May 16 15:05:14.533: INFO: multus-rpn9n from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container kube-multus ready: true, restart count 0
    May 16 15:05:14.533: INFO: network-metrics-daemon-7sqls from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (2 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May 16 15:05:14.533: INFO: network-check-target-8pcqz from openshift-network-diagnostics started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container network-check-target-container ready: true, restart count 0
    May 16 15:05:14.533: INFO: ovnkube-node-kqvdm from openshift-ovn-kubernetes started at 2023-05-16 13:52:19 +0000 UTC (5 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container ovn-controller ready: true, restart count 1
    May 16 15:05:14.533: INFO: 	Container ovnkube-node ready: true, restart count 0
    May 16 15:05:14.533: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-gsqlr from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
    May 16 15:05:14.533: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 16 15:05:14.533: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/16/23 15:05:14.533
    May 16 15:05:14.547: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4892" to be "running"
    May 16 15:05:14.565: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 18.291326ms
    May 16 15:05:16.569: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.022314765s
    May 16 15:05:16.569: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/16/23 15:05:16.572
    STEP: Trying to apply a random label on the found node. 05/16/23 15:05:16.583
    STEP: verifying the node has the label kubernetes.io/e2e-30ec892c-1cf9-4121-a84f-eec0dc612f31 42 05/16/23 15:05:16.596
    STEP: Trying to relaunch the pod, now with labels. 05/16/23 15:05:16.604
    May 16 15:05:16.616: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-4892" to be "not pending"
    May 16 15:05:16.627: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 11.321974ms
    May 16 15:05:18.631: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.01498698s
    May 16 15:05:18.631: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-30ec892c-1cf9-4121-a84f-eec0dc612f31 off the node ip-10-0-161-164.eu-central-1.compute.internal 05/16/23 15:05:18.634
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-30ec892c-1cf9-4121-a84f-eec0dc612f31 05/16/23 15:05:18.647
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 15:05:18.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4892" for this suite. 05/16/23 15:05:18.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:05:18.674
May 16 15:05:18.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename init-container 05/16/23 15:05:18.675
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:18.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:18.753
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 05/16/23 15:05:18.755
May 16 15:05:18.755: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May 16 15:05:22.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1816" for this suite. 05/16/23 15:05:22.687
------------------------------
â€¢ [4.018 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:05:18.674
    May 16 15:05:18.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename init-container 05/16/23 15:05:18.675
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:18.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:18.753
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 05/16/23 15:05:18.755
    May 16 15:05:18.755: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May 16 15:05:22.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1816" for this suite. 05/16/23 15:05:22.687
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:05:22.693
May 16 15:05:22.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename svcaccounts 05/16/23 15:05:22.693
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:22.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:22.724
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
May 16 15:05:22.731: INFO: Got root ca configmap in namespace "svcaccounts-8896"
May 16 15:05:22.743: INFO: Deleted root ca configmap in namespace "svcaccounts-8896"
STEP: waiting for a new root ca configmap created 05/16/23 15:05:23.244
May 16 15:05:23.247: INFO: Recreated root ca configmap in namespace "svcaccounts-8896"
May 16 15:05:23.251: INFO: Updated root ca configmap in namespace "svcaccounts-8896"
STEP: waiting for the root ca configmap reconciled 05/16/23 15:05:23.752
May 16 15:05:23.754: INFO: Reconciled root ca configmap in namespace "svcaccounts-8896"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May 16 15:05:23.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8896" for this suite. 05/16/23 15:05:23.758
------------------------------
â€¢ [1.071 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:05:22.693
    May 16 15:05:22.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename svcaccounts 05/16/23 15:05:22.693
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:22.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:22.724
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    May 16 15:05:22.731: INFO: Got root ca configmap in namespace "svcaccounts-8896"
    May 16 15:05:22.743: INFO: Deleted root ca configmap in namespace "svcaccounts-8896"
    STEP: waiting for a new root ca configmap created 05/16/23 15:05:23.244
    May 16 15:05:23.247: INFO: Recreated root ca configmap in namespace "svcaccounts-8896"
    May 16 15:05:23.251: INFO: Updated root ca configmap in namespace "svcaccounts-8896"
    STEP: waiting for the root ca configmap reconciled 05/16/23 15:05:23.752
    May 16 15:05:23.754: INFO: Reconciled root ca configmap in namespace "svcaccounts-8896"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May 16 15:05:23.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8896" for this suite. 05/16/23 15:05:23.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:05:23.765
May 16 15:05:23.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename csiinlinevolumes 05/16/23 15:05:23.765
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:23.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:23.785
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 05/16/23 15:05:23.788
STEP: getting 05/16/23 15:05:23.811
STEP: listing in namespace 05/16/23 15:05:23.816
STEP: patching 05/16/23 15:05:23.823
STEP: deleting 05/16/23 15:05:23.842
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
May 16 15:05:23.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-3137" for this suite. 05/16/23 15:05:23.858
------------------------------
â€¢ [0.099 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:05:23.765
    May 16 15:05:23.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename csiinlinevolumes 05/16/23 15:05:23.765
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:23.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:23.785
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 05/16/23 15:05:23.788
    STEP: getting 05/16/23 15:05:23.811
    STEP: listing in namespace 05/16/23 15:05:23.816
    STEP: patching 05/16/23 15:05:23.823
    STEP: deleting 05/16/23 15:05:23.842
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    May 16 15:05:23.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-3137" for this suite. 05/16/23 15:05:23.858
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:05:23.865
May 16 15:05:23.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename daemonsets 05/16/23 15:05:23.865
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:23.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:23.888
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 05/16/23 15:05:23.948
STEP: Check that daemon pods launch on every node of the cluster. 05/16/23 15:05:23.953
May 16 15:05:23.958: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:05:23.958: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:05:23.958: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:05:23.962: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 15:05:23.962: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 15:05:24.967: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:05:24.967: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:05:24.967: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:05:24.970: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 15:05:24.970: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 15:05:25.973: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:05:25.973: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:05:25.973: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:05:25.978: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 16 15:05:25.978: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 05/16/23 15:05:25.98
May 16 15:05:25.984: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 05/16/23 15:05:25.984
May 16 15:05:25.992: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 05/16/23 15:05:25.992
May 16 15:05:25.993: INFO: Observed &DaemonSet event: ADDED
May 16 15:05:25.993: INFO: Observed &DaemonSet event: MODIFIED
May 16 15:05:25.993: INFO: Observed &DaemonSet event: MODIFIED
May 16 15:05:25.994: INFO: Observed &DaemonSet event: MODIFIED
May 16 15:05:25.994: INFO: Observed &DaemonSet event: MODIFIED
May 16 15:05:25.994: INFO: Found daemon set daemon-set in namespace daemonsets-1132 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 16 15:05:25.994: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 05/16/23 15:05:25.994
STEP: watching for the daemon set status to be patched 05/16/23 15:05:25.999
May 16 15:05:26.000: INFO: Observed &DaemonSet event: ADDED
May 16 15:05:26.000: INFO: Observed &DaemonSet event: MODIFIED
May 16 15:05:26.001: INFO: Observed &DaemonSet event: MODIFIED
May 16 15:05:26.001: INFO: Observed &DaemonSet event: MODIFIED
May 16 15:05:26.001: INFO: Observed &DaemonSet event: MODIFIED
May 16 15:05:26.001: INFO: Observed daemon set daemon-set in namespace daemonsets-1132 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 16 15:05:26.001: INFO: Observed &DaemonSet event: MODIFIED
May 16 15:05:26.001: INFO: Found daemon set daemon-set in namespace daemonsets-1132 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
May 16 15:05:26.001: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/16/23 15:05:26.006
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1132, will wait for the garbage collector to delete the pods 05/16/23 15:05:26.006
May 16 15:05:26.064: INFO: Deleting DaemonSet.extensions daemon-set took: 5.659323ms
May 16 15:05:26.165: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.522504ms
May 16 15:05:28.769: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 15:05:28.769: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 16 15:05:28.773: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"76432"},"items":null}

May 16 15:05:28.777: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"76432"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 15:05:28.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1132" for this suite. 05/16/23 15:05:28.795
------------------------------
â€¢ [4.938 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:05:23.865
    May 16 15:05:23.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename daemonsets 05/16/23 15:05:23.865
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:23.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:23.888
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 05/16/23 15:05:23.948
    STEP: Check that daemon pods launch on every node of the cluster. 05/16/23 15:05:23.953
    May 16 15:05:23.958: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:05:23.958: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:05:23.958: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:05:23.962: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 15:05:23.962: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 15:05:24.967: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:05:24.967: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:05:24.967: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:05:24.970: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 15:05:24.970: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 15:05:25.973: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:05:25.973: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:05:25.973: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:05:25.978: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May 16 15:05:25.978: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 05/16/23 15:05:25.98
    May 16 15:05:25.984: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 05/16/23 15:05:25.984
    May 16 15:05:25.992: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 05/16/23 15:05:25.992
    May 16 15:05:25.993: INFO: Observed &DaemonSet event: ADDED
    May 16 15:05:25.993: INFO: Observed &DaemonSet event: MODIFIED
    May 16 15:05:25.993: INFO: Observed &DaemonSet event: MODIFIED
    May 16 15:05:25.994: INFO: Observed &DaemonSet event: MODIFIED
    May 16 15:05:25.994: INFO: Observed &DaemonSet event: MODIFIED
    May 16 15:05:25.994: INFO: Found daemon set daemon-set in namespace daemonsets-1132 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May 16 15:05:25.994: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 05/16/23 15:05:25.994
    STEP: watching for the daemon set status to be patched 05/16/23 15:05:25.999
    May 16 15:05:26.000: INFO: Observed &DaemonSet event: ADDED
    May 16 15:05:26.000: INFO: Observed &DaemonSet event: MODIFIED
    May 16 15:05:26.001: INFO: Observed &DaemonSet event: MODIFIED
    May 16 15:05:26.001: INFO: Observed &DaemonSet event: MODIFIED
    May 16 15:05:26.001: INFO: Observed &DaemonSet event: MODIFIED
    May 16 15:05:26.001: INFO: Observed daemon set daemon-set in namespace daemonsets-1132 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May 16 15:05:26.001: INFO: Observed &DaemonSet event: MODIFIED
    May 16 15:05:26.001: INFO: Found daemon set daemon-set in namespace daemonsets-1132 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    May 16 15:05:26.001: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/16/23 15:05:26.006
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1132, will wait for the garbage collector to delete the pods 05/16/23 15:05:26.006
    May 16 15:05:26.064: INFO: Deleting DaemonSet.extensions daemon-set took: 5.659323ms
    May 16 15:05:26.165: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.522504ms
    May 16 15:05:28.769: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 15:05:28.769: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May 16 15:05:28.773: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"76432"},"items":null}

    May 16 15:05:28.777: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"76432"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 15:05:28.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1132" for this suite. 05/16/23 15:05:28.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:05:28.803
May 16 15:05:28.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename emptydir-wrapper 05/16/23 15:05:28.804
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:28.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:28.824
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 05/16/23 15:05:28.827
STEP: Creating RC which spawns configmap-volume pods 05/16/23 15:05:29.068
May 16 15:05:29.161: INFO: Pod name wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6: Found 2 pods out of 5
May 16 15:05:34.166: INFO: Pod name wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/16/23 15:05:34.166
May 16 15:05:34.166: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-7h749" in namespace "emptydir-wrapper-7504" to be "running"
May 16 15:05:34.169: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-7h749": Phase="Running", Reason="", readiness=true. Elapsed: 2.770376ms
May 16 15:05:34.169: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-7h749" satisfied condition "running"
May 16 15:05:34.169: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-dq4rb" in namespace "emptydir-wrapper-7504" to be "running"
May 16 15:05:34.173: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-dq4rb": Phase="Running", Reason="", readiness=true. Elapsed: 3.913587ms
May 16 15:05:34.173: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-dq4rb" satisfied condition "running"
May 16 15:05:34.173: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-h7v75" in namespace "emptydir-wrapper-7504" to be "running"
May 16 15:05:34.176: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-h7v75": Phase="Running", Reason="", readiness=true. Elapsed: 3.195288ms
May 16 15:05:34.176: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-h7v75" satisfied condition "running"
May 16 15:05:34.176: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-ntlq2" in namespace "emptydir-wrapper-7504" to be "running"
May 16 15:05:34.179: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-ntlq2": Phase="Running", Reason="", readiness=true. Elapsed: 2.676017ms
May 16 15:05:34.179: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-ntlq2" satisfied condition "running"
May 16 15:05:34.179: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-nzflf" in namespace "emptydir-wrapper-7504" to be "running"
May 16 15:05:34.182: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-nzflf": Phase="Running", Reason="", readiness=true. Elapsed: 2.477251ms
May 16 15:05:34.182: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-nzflf" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6 in namespace emptydir-wrapper-7504, will wait for the garbage collector to delete the pods 05/16/23 15:05:34.182
May 16 15:05:34.241: INFO: Deleting ReplicationController wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6 took: 6.100378ms
May 16 15:05:34.342: INFO: Terminating ReplicationController wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6 pods took: 100.34186ms
STEP: Creating RC which spawns configmap-volume pods 05/16/23 15:05:35.947
May 16 15:05:35.957: INFO: Pod name wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3: Found 0 pods out of 5
May 16 15:05:40.963: INFO: Pod name wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/16/23 15:05:40.963
May 16 15:05:40.964: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-68rct" in namespace "emptydir-wrapper-7504" to be "running"
May 16 15:05:40.967: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-68rct": Phase="Running", Reason="", readiness=true. Elapsed: 3.001162ms
May 16 15:05:40.967: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-68rct" satisfied condition "running"
May 16 15:05:40.967: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-cl8kw" in namespace "emptydir-wrapper-7504" to be "running"
May 16 15:05:40.969: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-cl8kw": Phase="Running", Reason="", readiness=true. Elapsed: 2.608986ms
May 16 15:05:40.969: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-cl8kw" satisfied condition "running"
May 16 15:05:40.969: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-ddg46" in namespace "emptydir-wrapper-7504" to be "running"
May 16 15:05:40.975: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-ddg46": Phase="Running", Reason="", readiness=true. Elapsed: 5.55283ms
May 16 15:05:40.975: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-ddg46" satisfied condition "running"
May 16 15:05:40.975: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-jzdnd" in namespace "emptydir-wrapper-7504" to be "running"
May 16 15:05:40.978: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-jzdnd": Phase="Running", Reason="", readiness=true. Elapsed: 3.647481ms
May 16 15:05:40.978: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-jzdnd" satisfied condition "running"
May 16 15:05:40.978: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-r9q8b" in namespace "emptydir-wrapper-7504" to be "running"
May 16 15:05:40.982: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-r9q8b": Phase="Running", Reason="", readiness=true. Elapsed: 3.347783ms
May 16 15:05:40.982: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-r9q8b" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3 in namespace emptydir-wrapper-7504, will wait for the garbage collector to delete the pods 05/16/23 15:05:40.982
May 16 15:05:41.041: INFO: Deleting ReplicationController wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3 took: 6.198289ms
May 16 15:05:41.142: INFO: Terminating ReplicationController wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3 pods took: 100.32713ms
STEP: Creating RC which spawns configmap-volume pods 05/16/23 15:05:42.746
May 16 15:05:42.757: INFO: Pod name wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3: Found 0 pods out of 5
May 16 15:05:47.763: INFO: Pod name wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3: Found 5 pods out of 5
STEP: Ensuring each pod is running 05/16/23 15:05:47.763
May 16 15:05:47.763: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-jnxsv" in namespace "emptydir-wrapper-7504" to be "running"
May 16 15:05:47.767: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-jnxsv": Phase="Running", Reason="", readiness=true. Elapsed: 3.215107ms
May 16 15:05:47.767: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-jnxsv" satisfied condition "running"
May 16 15:05:47.767: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-tjwxx" in namespace "emptydir-wrapper-7504" to be "running"
May 16 15:05:47.770: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-tjwxx": Phase="Running", Reason="", readiness=true. Elapsed: 3.321695ms
May 16 15:05:47.770: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-tjwxx" satisfied condition "running"
May 16 15:05:47.770: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-v85j4" in namespace "emptydir-wrapper-7504" to be "running"
May 16 15:05:47.773: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-v85j4": Phase="Running", Reason="", readiness=true. Elapsed: 2.826476ms
May 16 15:05:47.773: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-v85j4" satisfied condition "running"
May 16 15:05:47.773: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-vcwnr" in namespace "emptydir-wrapper-7504" to be "running"
May 16 15:05:47.776: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-vcwnr": Phase="Running", Reason="", readiness=true. Elapsed: 2.950105ms
May 16 15:05:47.776: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-vcwnr" satisfied condition "running"
May 16 15:05:47.776: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-wggwh" in namespace "emptydir-wrapper-7504" to be "running"
May 16 15:05:47.778: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-wggwh": Phase="Running", Reason="", readiness=true. Elapsed: 2.792735ms
May 16 15:05:47.778: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-wggwh" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3 in namespace emptydir-wrapper-7504, will wait for the garbage collector to delete the pods 05/16/23 15:05:47.779
May 16 15:05:47.838: INFO: Deleting ReplicationController wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3 took: 6.157272ms
May 16 15:05:47.939: INFO: Terminating ReplicationController wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3 pods took: 100.482292ms
STEP: Cleaning up the configMaps 05/16/23 15:05:48.939
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
May 16 15:05:49.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-7504" for this suite. 05/16/23 15:05:49.255
------------------------------
â€¢ [SLOW TEST] [20.457 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:05:28.803
    May 16 15:05:28.804: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename emptydir-wrapper 05/16/23 15:05:28.804
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:28.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:28.824
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 05/16/23 15:05:28.827
    STEP: Creating RC which spawns configmap-volume pods 05/16/23 15:05:29.068
    May 16 15:05:29.161: INFO: Pod name wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6: Found 2 pods out of 5
    May 16 15:05:34.166: INFO: Pod name wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/16/23 15:05:34.166
    May 16 15:05:34.166: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-7h749" in namespace "emptydir-wrapper-7504" to be "running"
    May 16 15:05:34.169: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-7h749": Phase="Running", Reason="", readiness=true. Elapsed: 2.770376ms
    May 16 15:05:34.169: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-7h749" satisfied condition "running"
    May 16 15:05:34.169: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-dq4rb" in namespace "emptydir-wrapper-7504" to be "running"
    May 16 15:05:34.173: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-dq4rb": Phase="Running", Reason="", readiness=true. Elapsed: 3.913587ms
    May 16 15:05:34.173: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-dq4rb" satisfied condition "running"
    May 16 15:05:34.173: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-h7v75" in namespace "emptydir-wrapper-7504" to be "running"
    May 16 15:05:34.176: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-h7v75": Phase="Running", Reason="", readiness=true. Elapsed: 3.195288ms
    May 16 15:05:34.176: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-h7v75" satisfied condition "running"
    May 16 15:05:34.176: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-ntlq2" in namespace "emptydir-wrapper-7504" to be "running"
    May 16 15:05:34.179: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-ntlq2": Phase="Running", Reason="", readiness=true. Elapsed: 2.676017ms
    May 16 15:05:34.179: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-ntlq2" satisfied condition "running"
    May 16 15:05:34.179: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-nzflf" in namespace "emptydir-wrapper-7504" to be "running"
    May 16 15:05:34.182: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-nzflf": Phase="Running", Reason="", readiness=true. Elapsed: 2.477251ms
    May 16 15:05:34.182: INFO: Pod "wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6-nzflf" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6 in namespace emptydir-wrapper-7504, will wait for the garbage collector to delete the pods 05/16/23 15:05:34.182
    May 16 15:05:34.241: INFO: Deleting ReplicationController wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6 took: 6.100378ms
    May 16 15:05:34.342: INFO: Terminating ReplicationController wrapped-volume-race-7a871463-9ae0-439b-a69b-bf54ebed14c6 pods took: 100.34186ms
    STEP: Creating RC which spawns configmap-volume pods 05/16/23 15:05:35.947
    May 16 15:05:35.957: INFO: Pod name wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3: Found 0 pods out of 5
    May 16 15:05:40.963: INFO: Pod name wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/16/23 15:05:40.963
    May 16 15:05:40.964: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-68rct" in namespace "emptydir-wrapper-7504" to be "running"
    May 16 15:05:40.967: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-68rct": Phase="Running", Reason="", readiness=true. Elapsed: 3.001162ms
    May 16 15:05:40.967: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-68rct" satisfied condition "running"
    May 16 15:05:40.967: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-cl8kw" in namespace "emptydir-wrapper-7504" to be "running"
    May 16 15:05:40.969: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-cl8kw": Phase="Running", Reason="", readiness=true. Elapsed: 2.608986ms
    May 16 15:05:40.969: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-cl8kw" satisfied condition "running"
    May 16 15:05:40.969: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-ddg46" in namespace "emptydir-wrapper-7504" to be "running"
    May 16 15:05:40.975: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-ddg46": Phase="Running", Reason="", readiness=true. Elapsed: 5.55283ms
    May 16 15:05:40.975: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-ddg46" satisfied condition "running"
    May 16 15:05:40.975: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-jzdnd" in namespace "emptydir-wrapper-7504" to be "running"
    May 16 15:05:40.978: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-jzdnd": Phase="Running", Reason="", readiness=true. Elapsed: 3.647481ms
    May 16 15:05:40.978: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-jzdnd" satisfied condition "running"
    May 16 15:05:40.978: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-r9q8b" in namespace "emptydir-wrapper-7504" to be "running"
    May 16 15:05:40.982: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-r9q8b": Phase="Running", Reason="", readiness=true. Elapsed: 3.347783ms
    May 16 15:05:40.982: INFO: Pod "wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3-r9q8b" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3 in namespace emptydir-wrapper-7504, will wait for the garbage collector to delete the pods 05/16/23 15:05:40.982
    May 16 15:05:41.041: INFO: Deleting ReplicationController wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3 took: 6.198289ms
    May 16 15:05:41.142: INFO: Terminating ReplicationController wrapped-volume-race-c0b67c32-0ae8-4764-8222-dd308f8a48d3 pods took: 100.32713ms
    STEP: Creating RC which spawns configmap-volume pods 05/16/23 15:05:42.746
    May 16 15:05:42.757: INFO: Pod name wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3: Found 0 pods out of 5
    May 16 15:05:47.763: INFO: Pod name wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3: Found 5 pods out of 5
    STEP: Ensuring each pod is running 05/16/23 15:05:47.763
    May 16 15:05:47.763: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-jnxsv" in namespace "emptydir-wrapper-7504" to be "running"
    May 16 15:05:47.767: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-jnxsv": Phase="Running", Reason="", readiness=true. Elapsed: 3.215107ms
    May 16 15:05:47.767: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-jnxsv" satisfied condition "running"
    May 16 15:05:47.767: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-tjwxx" in namespace "emptydir-wrapper-7504" to be "running"
    May 16 15:05:47.770: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-tjwxx": Phase="Running", Reason="", readiness=true. Elapsed: 3.321695ms
    May 16 15:05:47.770: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-tjwxx" satisfied condition "running"
    May 16 15:05:47.770: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-v85j4" in namespace "emptydir-wrapper-7504" to be "running"
    May 16 15:05:47.773: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-v85j4": Phase="Running", Reason="", readiness=true. Elapsed: 2.826476ms
    May 16 15:05:47.773: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-v85j4" satisfied condition "running"
    May 16 15:05:47.773: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-vcwnr" in namespace "emptydir-wrapper-7504" to be "running"
    May 16 15:05:47.776: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-vcwnr": Phase="Running", Reason="", readiness=true. Elapsed: 2.950105ms
    May 16 15:05:47.776: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-vcwnr" satisfied condition "running"
    May 16 15:05:47.776: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-wggwh" in namespace "emptydir-wrapper-7504" to be "running"
    May 16 15:05:47.778: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-wggwh": Phase="Running", Reason="", readiness=true. Elapsed: 2.792735ms
    May 16 15:05:47.778: INFO: Pod "wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3-wggwh" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3 in namespace emptydir-wrapper-7504, will wait for the garbage collector to delete the pods 05/16/23 15:05:47.779
    May 16 15:05:47.838: INFO: Deleting ReplicationController wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3 took: 6.157272ms
    May 16 15:05:47.939: INFO: Terminating ReplicationController wrapped-volume-race-2327772b-6ce7-442b-b7a1-3ed4ab04a1b3 pods took: 100.482292ms
    STEP: Cleaning up the configMaps 05/16/23 15:05:48.939
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    May 16 15:05:49.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-7504" for this suite. 05/16/23 15:05:49.255
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:05:49.26
May 16 15:05:49.260: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename webhook 05/16/23 15:05:49.261
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:49.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:49.283
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/16/23 15:05:49.306
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 15:05:49.566
STEP: Deploying the webhook pod 05/16/23 15:05:49.572
STEP: Wait for the deployment to be ready 05/16/23 15:05:49.583
May 16 15:05:49.590: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/16/23 15:05:51.599
STEP: Verifying the service has paired with the endpoint 05/16/23 15:05:51.608
May 16 15:05:52.608: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 05/16/23 15:05:52.671
STEP: Creating a configMap that does not comply to the validation webhook rules 05/16/23 15:05:52.703
STEP: Deleting the collection of validation webhooks 05/16/23 15:05:52.727
STEP: Creating a configMap that does not comply to the validation webhook rules 05/16/23 15:05:52.781
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 15:05:52.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6333" for this suite. 05/16/23 15:05:52.847
STEP: Destroying namespace "webhook-6333-markers" for this suite. 05/16/23 15:05:52.867
------------------------------
â€¢ [3.616 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:05:49.26
    May 16 15:05:49.260: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename webhook 05/16/23 15:05:49.261
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:49.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:49.283
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/16/23 15:05:49.306
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 15:05:49.566
    STEP: Deploying the webhook pod 05/16/23 15:05:49.572
    STEP: Wait for the deployment to be ready 05/16/23 15:05:49.583
    May 16 15:05:49.590: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/16/23 15:05:51.599
    STEP: Verifying the service has paired with the endpoint 05/16/23 15:05:51.608
    May 16 15:05:52.608: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 05/16/23 15:05:52.671
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/16/23 15:05:52.703
    STEP: Deleting the collection of validation webhooks 05/16/23 15:05:52.727
    STEP: Creating a configMap that does not comply to the validation webhook rules 05/16/23 15:05:52.781
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 15:05:52.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6333" for this suite. 05/16/23 15:05:52.847
    STEP: Destroying namespace "webhook-6333-markers" for this suite. 05/16/23 15:05:52.867
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:05:52.877
May 16 15:05:52.877: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename var-expansion 05/16/23 15:05:52.878
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:52.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:52.906
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
May 16 15:05:52.932: INFO: Waiting up to 2m0s for pod "var-expansion-ea0c495c-d228-4101-94f1-0837c2f88755" in namespace "var-expansion-9883" to be "container 0 failed with reason CreateContainerConfigError"
May 16 15:05:52.937: INFO: Pod "var-expansion-ea0c495c-d228-4101-94f1-0837c2f88755": Phase="Pending", Reason="", readiness=false. Elapsed: 4.231599ms
May 16 15:05:54.942: INFO: Pod "var-expansion-ea0c495c-d228-4101-94f1-0837c2f88755": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010017071s
May 16 15:05:54.942: INFO: Pod "var-expansion-ea0c495c-d228-4101-94f1-0837c2f88755" satisfied condition "container 0 failed with reason CreateContainerConfigError"
May 16 15:05:54.942: INFO: Deleting pod "var-expansion-ea0c495c-d228-4101-94f1-0837c2f88755" in namespace "var-expansion-9883"
May 16 15:05:54.948: INFO: Wait up to 5m0s for pod "var-expansion-ea0c495c-d228-4101-94f1-0837c2f88755" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May 16 15:05:56.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9883" for this suite. 05/16/23 15:05:56.961
------------------------------
â€¢ [4.089 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:05:52.877
    May 16 15:05:52.877: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename var-expansion 05/16/23 15:05:52.878
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:52.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:52.906
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    May 16 15:05:52.932: INFO: Waiting up to 2m0s for pod "var-expansion-ea0c495c-d228-4101-94f1-0837c2f88755" in namespace "var-expansion-9883" to be "container 0 failed with reason CreateContainerConfigError"
    May 16 15:05:52.937: INFO: Pod "var-expansion-ea0c495c-d228-4101-94f1-0837c2f88755": Phase="Pending", Reason="", readiness=false. Elapsed: 4.231599ms
    May 16 15:05:54.942: INFO: Pod "var-expansion-ea0c495c-d228-4101-94f1-0837c2f88755": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010017071s
    May 16 15:05:54.942: INFO: Pod "var-expansion-ea0c495c-d228-4101-94f1-0837c2f88755" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    May 16 15:05:54.942: INFO: Deleting pod "var-expansion-ea0c495c-d228-4101-94f1-0837c2f88755" in namespace "var-expansion-9883"
    May 16 15:05:54.948: INFO: Wait up to 5m0s for pod "var-expansion-ea0c495c-d228-4101-94f1-0837c2f88755" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May 16 15:05:56.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9883" for this suite. 05/16/23 15:05:56.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:05:56.97
May 16 15:05:56.970: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename configmap 05/16/23 15:05:56.97
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:56.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:56.993
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-677c659b-885e-46f3-b54d-ca737ef315e2 05/16/23 15:05:56.996
STEP: Creating a pod to test consume configMaps 05/16/23 15:05:57.002
May 16 15:05:57.014: INFO: Waiting up to 5m0s for pod "pod-configmaps-3236da67-ea6c-47b2-aa98-30c750a83e7e" in namespace "configmap-7332" to be "Succeeded or Failed"
May 16 15:05:57.017: INFO: Pod "pod-configmaps-3236da67-ea6c-47b2-aa98-30c750a83e7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.479763ms
May 16 15:05:59.021: INFO: Pod "pod-configmaps-3236da67-ea6c-47b2-aa98-30c750a83e7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006826777s
May 16 15:06:01.021: INFO: Pod "pod-configmaps-3236da67-ea6c-47b2-aa98-30c750a83e7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006643111s
STEP: Saw pod success 05/16/23 15:06:01.021
May 16 15:06:01.021: INFO: Pod "pod-configmaps-3236da67-ea6c-47b2-aa98-30c750a83e7e" satisfied condition "Succeeded or Failed"
May 16 15:06:01.024: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-configmaps-3236da67-ea6c-47b2-aa98-30c750a83e7e container agnhost-container: <nil>
STEP: delete the pod 05/16/23 15:06:01.029
May 16 15:06:01.039: INFO: Waiting for pod pod-configmaps-3236da67-ea6c-47b2-aa98-30c750a83e7e to disappear
May 16 15:06:01.042: INFO: Pod pod-configmaps-3236da67-ea6c-47b2-aa98-30c750a83e7e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 16 15:06:01.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7332" for this suite. 05/16/23 15:06:01.046
------------------------------
â€¢ [4.082 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:05:56.97
    May 16 15:05:56.970: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename configmap 05/16/23 15:05:56.97
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:05:56.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:05:56.993
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-677c659b-885e-46f3-b54d-ca737ef315e2 05/16/23 15:05:56.996
    STEP: Creating a pod to test consume configMaps 05/16/23 15:05:57.002
    May 16 15:05:57.014: INFO: Waiting up to 5m0s for pod "pod-configmaps-3236da67-ea6c-47b2-aa98-30c750a83e7e" in namespace "configmap-7332" to be "Succeeded or Failed"
    May 16 15:05:57.017: INFO: Pod "pod-configmaps-3236da67-ea6c-47b2-aa98-30c750a83e7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.479763ms
    May 16 15:05:59.021: INFO: Pod "pod-configmaps-3236da67-ea6c-47b2-aa98-30c750a83e7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006826777s
    May 16 15:06:01.021: INFO: Pod "pod-configmaps-3236da67-ea6c-47b2-aa98-30c750a83e7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006643111s
    STEP: Saw pod success 05/16/23 15:06:01.021
    May 16 15:06:01.021: INFO: Pod "pod-configmaps-3236da67-ea6c-47b2-aa98-30c750a83e7e" satisfied condition "Succeeded or Failed"
    May 16 15:06:01.024: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-configmaps-3236da67-ea6c-47b2-aa98-30c750a83e7e container agnhost-container: <nil>
    STEP: delete the pod 05/16/23 15:06:01.029
    May 16 15:06:01.039: INFO: Waiting for pod pod-configmaps-3236da67-ea6c-47b2-aa98-30c750a83e7e to disappear
    May 16 15:06:01.042: INFO: Pod pod-configmaps-3236da67-ea6c-47b2-aa98-30c750a83e7e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 16 15:06:01.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7332" for this suite. 05/16/23 15:06:01.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:06:01.052
May 16 15:06:01.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename secrets 05/16/23 15:06:01.053
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:01.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:01.087
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-22c581fb-be6d-4402-aa0c-cc01bfb6e315 05/16/23 15:06:01.09
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
May 16 15:06:01.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8635" for this suite. 05/16/23 15:06:01.098
------------------------------
â€¢ [0.052 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:06:01.052
    May 16 15:06:01.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename secrets 05/16/23 15:06:01.053
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:01.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:01.087
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-22c581fb-be6d-4402-aa0c-cc01bfb6e315 05/16/23 15:06:01.09
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    May 16 15:06:01.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8635" for this suite. 05/16/23 15:06:01.098
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:06:01.104
May 16 15:06:01.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename custom-resource-definition 05/16/23 15:06:01.105
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:01.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:01.128
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 05/16/23 15:06:01.133
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 05/16/23 15:06:01.134
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 05/16/23 15:06:01.135
STEP: fetching the /apis/apiextensions.k8s.io discovery document 05/16/23 15:06:01.135
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 05/16/23 15:06:01.135
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 05/16/23 15:06:01.136
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 05/16/23 15:06:01.137
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 15:06:01.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8551" for this suite. 05/16/23 15:06:01.142
------------------------------
â€¢ [0.045 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:06:01.104
    May 16 15:06:01.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename custom-resource-definition 05/16/23 15:06:01.105
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:01.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:01.128
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 05/16/23 15:06:01.133
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 05/16/23 15:06:01.134
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 05/16/23 15:06:01.135
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 05/16/23 15:06:01.135
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 05/16/23 15:06:01.135
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 05/16/23 15:06:01.136
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 05/16/23 15:06:01.137
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 15:06:01.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8551" for this suite. 05/16/23 15:06:01.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:06:01.149
May 16 15:06:01.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename webhook 05/16/23 15:06:01.15
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:01.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:01.183
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/16/23 15:06:01.203
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 15:06:01.38
STEP: Deploying the webhook pod 05/16/23 15:06:01.388
STEP: Wait for the deployment to be ready 05/16/23 15:06:01.399
May 16 15:06:01.408: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/16/23 15:06:03.417
STEP: Verifying the service has paired with the endpoint 05/16/23 15:06:03.427
May 16 15:06:04.427: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
May 16 15:06:04.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1024-crds.webhook.example.com via the AdmissionRegistration API 05/16/23 15:06:04.939
STEP: Creating a custom resource that should be mutated by the webhook 05/16/23 15:06:04.953
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 15:06:07.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8481" for this suite. 05/16/23 15:06:07.573
STEP: Destroying namespace "webhook-8481-markers" for this suite. 05/16/23 15:06:07.581
------------------------------
â€¢ [SLOW TEST] [6.440 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:06:01.149
    May 16 15:06:01.150: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename webhook 05/16/23 15:06:01.15
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:01.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:01.183
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/16/23 15:06:01.203
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 15:06:01.38
    STEP: Deploying the webhook pod 05/16/23 15:06:01.388
    STEP: Wait for the deployment to be ready 05/16/23 15:06:01.399
    May 16 15:06:01.408: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/16/23 15:06:03.417
    STEP: Verifying the service has paired with the endpoint 05/16/23 15:06:03.427
    May 16 15:06:04.427: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    May 16 15:06:04.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1024-crds.webhook.example.com via the AdmissionRegistration API 05/16/23 15:06:04.939
    STEP: Creating a custom resource that should be mutated by the webhook 05/16/23 15:06:04.953
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 15:06:07.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8481" for this suite. 05/16/23 15:06:07.573
    STEP: Destroying namespace "webhook-8481-markers" for this suite. 05/16/23 15:06:07.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:06:07.59
May 16 15:06:07.590: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename var-expansion 05/16/23 15:06:07.59
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:07.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:07.619
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 05/16/23 15:06:07.622
May 16 15:06:07.646: INFO: Waiting up to 5m0s for pod "var-expansion-2a82a00c-2303-45ea-afe4-92a1c175225b" in namespace "var-expansion-2660" to be "Succeeded or Failed"
May 16 15:06:07.649: INFO: Pod "var-expansion-2a82a00c-2303-45ea-afe4-92a1c175225b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.166955ms
May 16 15:06:09.653: INFO: Pod "var-expansion-2a82a00c-2303-45ea-afe4-92a1c175225b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007187904s
May 16 15:06:11.653: INFO: Pod "var-expansion-2a82a00c-2303-45ea-afe4-92a1c175225b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006606084s
STEP: Saw pod success 05/16/23 15:06:11.653
May 16 15:06:11.653: INFO: Pod "var-expansion-2a82a00c-2303-45ea-afe4-92a1c175225b" satisfied condition "Succeeded or Failed"
May 16 15:06:11.656: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod var-expansion-2a82a00c-2303-45ea-afe4-92a1c175225b container dapi-container: <nil>
STEP: delete the pod 05/16/23 15:06:11.661
May 16 15:06:11.671: INFO: Waiting for pod var-expansion-2a82a00c-2303-45ea-afe4-92a1c175225b to disappear
May 16 15:06:11.674: INFO: Pod var-expansion-2a82a00c-2303-45ea-afe4-92a1c175225b no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May 16 15:06:11.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2660" for this suite. 05/16/23 15:06:11.678
------------------------------
â€¢ [4.094 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:06:07.59
    May 16 15:06:07.590: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename var-expansion 05/16/23 15:06:07.59
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:07.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:07.619
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 05/16/23 15:06:07.622
    May 16 15:06:07.646: INFO: Waiting up to 5m0s for pod "var-expansion-2a82a00c-2303-45ea-afe4-92a1c175225b" in namespace "var-expansion-2660" to be "Succeeded or Failed"
    May 16 15:06:07.649: INFO: Pod "var-expansion-2a82a00c-2303-45ea-afe4-92a1c175225b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.166955ms
    May 16 15:06:09.653: INFO: Pod "var-expansion-2a82a00c-2303-45ea-afe4-92a1c175225b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007187904s
    May 16 15:06:11.653: INFO: Pod "var-expansion-2a82a00c-2303-45ea-afe4-92a1c175225b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006606084s
    STEP: Saw pod success 05/16/23 15:06:11.653
    May 16 15:06:11.653: INFO: Pod "var-expansion-2a82a00c-2303-45ea-afe4-92a1c175225b" satisfied condition "Succeeded or Failed"
    May 16 15:06:11.656: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod var-expansion-2a82a00c-2303-45ea-afe4-92a1c175225b container dapi-container: <nil>
    STEP: delete the pod 05/16/23 15:06:11.661
    May 16 15:06:11.671: INFO: Waiting for pod var-expansion-2a82a00c-2303-45ea-afe4-92a1c175225b to disappear
    May 16 15:06:11.674: INFO: Pod var-expansion-2a82a00c-2303-45ea-afe4-92a1c175225b no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May 16 15:06:11.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2660" for this suite. 05/16/23 15:06:11.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:06:11.685
May 16 15:06:11.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubectl 05/16/23 15:06:11.685
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:11.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:11.705
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 05/16/23 15:06:11.707
May 16 15:06:11.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-8116 create -f -'
May 16 15:06:13.552: INFO: stderr: ""
May 16 15:06:13.552: INFO: stdout: "pod/pause created\n"
May 16 15:06:13.552: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 16 15:06:13.552: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8116" to be "running and ready"
May 16 15:06:13.555: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.476964ms
May 16 15:06:13.555: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-10-0-161-164.eu-central-1.compute.internal' to be 'Running' but was 'Pending'
May 16 15:06:15.560: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.008402232s
May 16 15:06:15.560: INFO: Pod "pause" satisfied condition "running and ready"
May 16 15:06:15.560: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 05/16/23 15:06:15.56
May 16 15:06:15.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-8116 label pods pause testing-label=testing-label-value'
May 16 15:06:15.620: INFO: stderr: ""
May 16 15:06:15.620: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 05/16/23 15:06:15.62
May 16 15:06:15.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-8116 get pod pause -L testing-label'
May 16 15:06:15.666: INFO: stderr: ""
May 16 15:06:15.666: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 05/16/23 15:06:15.666
May 16 15:06:15.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-8116 label pods pause testing-label-'
May 16 15:06:15.724: INFO: stderr: ""
May 16 15:06:15.724: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 05/16/23 15:06:15.724
May 16 15:06:15.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-8116 get pod pause -L testing-label'
May 16 15:06:15.773: INFO: stderr: ""
May 16 15:06:15.773: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 05/16/23 15:06:15.773
May 16 15:06:15.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-8116 delete --grace-period=0 --force -f -'
May 16 15:06:15.825: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 16 15:06:15.825: INFO: stdout: "pod \"pause\" force deleted\n"
May 16 15:06:15.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-8116 get rc,svc -l name=pause --no-headers'
May 16 15:06:15.881: INFO: stderr: "No resources found in kubectl-8116 namespace.\n"
May 16 15:06:15.881: INFO: stdout: ""
May 16 15:06:15.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-8116 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 16 15:06:15.929: INFO: stderr: ""
May 16 15:06:15.929: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 16 15:06:15.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8116" for this suite. 05/16/23 15:06:15.934
------------------------------
â€¢ [4.259 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:06:11.685
    May 16 15:06:11.685: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubectl 05/16/23 15:06:11.685
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:11.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:11.705
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 05/16/23 15:06:11.707
    May 16 15:06:11.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-8116 create -f -'
    May 16 15:06:13.552: INFO: stderr: ""
    May 16 15:06:13.552: INFO: stdout: "pod/pause created\n"
    May 16 15:06:13.552: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    May 16 15:06:13.552: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8116" to be "running and ready"
    May 16 15:06:13.555: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.476964ms
    May 16 15:06:13.555: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-10-0-161-164.eu-central-1.compute.internal' to be 'Running' but was 'Pending'
    May 16 15:06:15.560: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.008402232s
    May 16 15:06:15.560: INFO: Pod "pause" satisfied condition "running and ready"
    May 16 15:06:15.560: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 05/16/23 15:06:15.56
    May 16 15:06:15.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-8116 label pods pause testing-label=testing-label-value'
    May 16 15:06:15.620: INFO: stderr: ""
    May 16 15:06:15.620: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 05/16/23 15:06:15.62
    May 16 15:06:15.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-8116 get pod pause -L testing-label'
    May 16 15:06:15.666: INFO: stderr: ""
    May 16 15:06:15.666: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 05/16/23 15:06:15.666
    May 16 15:06:15.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-8116 label pods pause testing-label-'
    May 16 15:06:15.724: INFO: stderr: ""
    May 16 15:06:15.724: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 05/16/23 15:06:15.724
    May 16 15:06:15.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-8116 get pod pause -L testing-label'
    May 16 15:06:15.773: INFO: stderr: ""
    May 16 15:06:15.773: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 05/16/23 15:06:15.773
    May 16 15:06:15.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-8116 delete --grace-period=0 --force -f -'
    May 16 15:06:15.825: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    May 16 15:06:15.825: INFO: stdout: "pod \"pause\" force deleted\n"
    May 16 15:06:15.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-8116 get rc,svc -l name=pause --no-headers'
    May 16 15:06:15.881: INFO: stderr: "No resources found in kubectl-8116 namespace.\n"
    May 16 15:06:15.881: INFO: stdout: ""
    May 16 15:06:15.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-8116 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    May 16 15:06:15.929: INFO: stderr: ""
    May 16 15:06:15.929: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 16 15:06:15.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8116" for this suite. 05/16/23 15:06:15.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:06:15.945
May 16 15:06:15.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename crd-publish-openapi 05/16/23 15:06:15.945
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:15.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:15.967
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 05/16/23 15:06:15.969
May 16 15:06:15.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: mark a version not serverd 05/16/23 15:06:21.481
STEP: check the unserved version gets removed 05/16/23 15:06:21.494
STEP: check the other version is not changed 05/16/23 15:06:24.959
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 15:06:30.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9283" for this suite. 05/16/23 15:06:30.275
------------------------------
â€¢ [SLOW TEST] [14.338 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:06:15.945
    May 16 15:06:15.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename crd-publish-openapi 05/16/23 15:06:15.945
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:15.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:15.967
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 05/16/23 15:06:15.969
    May 16 15:06:15.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: mark a version not serverd 05/16/23 15:06:21.481
    STEP: check the unserved version gets removed 05/16/23 15:06:21.494
    STEP: check the other version is not changed 05/16/23 15:06:24.959
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 15:06:30.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9283" for this suite. 05/16/23 15:06:30.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:06:30.283
May 16 15:06:30.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename watch 05/16/23 15:06:30.284
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:30.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:30.308
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 05/16/23 15:06:30.311
STEP: creating a new configmap 05/16/23 15:06:30.313
STEP: modifying the configmap once 05/16/23 15:06:30.32
STEP: changing the label value of the configmap 05/16/23 15:06:30.329
STEP: Expecting to observe a delete notification for the watched object 05/16/23 15:06:30.342
May 16 15:06:30.342: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6113  5349c983-3d05-460f-8047-29bc6ae3456c 78191 0 2023-05-16 15:06:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-16 15:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 16 15:06:30.342: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6113  5349c983-3d05-460f-8047-29bc6ae3456c 78193 0 2023-05-16 15:06:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-16 15:06:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 16 15:06:30.342: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6113  5349c983-3d05-460f-8047-29bc6ae3456c 78198 0 2023-05-16 15:06:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-16 15:06:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 05/16/23 15:06:30.342
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 05/16/23 15:06:30.351
STEP: changing the label value of the configmap back 05/16/23 15:06:40.352
STEP: modifying the configmap a third time 05/16/23 15:06:40.359
STEP: deleting the configmap 05/16/23 15:06:40.366
STEP: Expecting to observe an add notification for the watched object when the label value was restored 05/16/23 15:06:40.372
May 16 15:06:40.372: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6113  5349c983-3d05-460f-8047-29bc6ae3456c 78333 0 2023-05-16 15:06:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-16 15:06:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 16 15:06:40.372: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6113  5349c983-3d05-460f-8047-29bc6ae3456c 78334 0 2023-05-16 15:06:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-16 15:06:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May 16 15:06:40.372: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6113  5349c983-3d05-460f-8047-29bc6ae3456c 78335 0 2023-05-16 15:06:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-16 15:06:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May 16 15:06:40.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6113" for this suite. 05/16/23 15:06:40.376
------------------------------
â€¢ [SLOW TEST] [10.098 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:06:30.283
    May 16 15:06:30.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename watch 05/16/23 15:06:30.284
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:30.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:30.308
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 05/16/23 15:06:30.311
    STEP: creating a new configmap 05/16/23 15:06:30.313
    STEP: modifying the configmap once 05/16/23 15:06:30.32
    STEP: changing the label value of the configmap 05/16/23 15:06:30.329
    STEP: Expecting to observe a delete notification for the watched object 05/16/23 15:06:30.342
    May 16 15:06:30.342: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6113  5349c983-3d05-460f-8047-29bc6ae3456c 78191 0 2023-05-16 15:06:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-16 15:06:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    May 16 15:06:30.342: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6113  5349c983-3d05-460f-8047-29bc6ae3456c 78193 0 2023-05-16 15:06:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-16 15:06:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 16 15:06:30.342: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6113  5349c983-3d05-460f-8047-29bc6ae3456c 78198 0 2023-05-16 15:06:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-16 15:06:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 05/16/23 15:06:30.342
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 05/16/23 15:06:30.351
    STEP: changing the label value of the configmap back 05/16/23 15:06:40.352
    STEP: modifying the configmap a third time 05/16/23 15:06:40.359
    STEP: deleting the configmap 05/16/23 15:06:40.366
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 05/16/23 15:06:40.372
    May 16 15:06:40.372: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6113  5349c983-3d05-460f-8047-29bc6ae3456c 78333 0 2023-05-16 15:06:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-16 15:06:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 16 15:06:40.372: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6113  5349c983-3d05-460f-8047-29bc6ae3456c 78334 0 2023-05-16 15:06:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-16 15:06:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    May 16 15:06:40.372: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6113  5349c983-3d05-460f-8047-29bc6ae3456c 78335 0 2023-05-16 15:06:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-16 15:06:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May 16 15:06:40.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6113" for this suite. 05/16/23 15:06:40.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:06:40.382
May 16 15:06:40.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubelet-test 05/16/23 15:06:40.383
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:40.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:40.403
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
May 16 15:06:40.432: INFO: Waiting up to 5m0s for pod "busybox-scheduling-c44ffd62-2a8c-4498-bcc0-5d18bbd59290" in namespace "kubelet-test-2790" to be "running and ready"
May 16 15:06:40.440: INFO: Pod "busybox-scheduling-c44ffd62-2a8c-4498-bcc0-5d18bbd59290": Phase="Pending", Reason="", readiness=false. Elapsed: 8.260329ms
May 16 15:06:40.440: INFO: The phase of Pod busybox-scheduling-c44ffd62-2a8c-4498-bcc0-5d18bbd59290 is Pending, waiting for it to be Running (with Ready = true)
May 16 15:06:42.445: INFO: Pod "busybox-scheduling-c44ffd62-2a8c-4498-bcc0-5d18bbd59290": Phase="Running", Reason="", readiness=true. Elapsed: 2.013598623s
May 16 15:06:42.445: INFO: The phase of Pod busybox-scheduling-c44ffd62-2a8c-4498-bcc0-5d18bbd59290 is Running (Ready = true)
May 16 15:06:42.445: INFO: Pod "busybox-scheduling-c44ffd62-2a8c-4498-bcc0-5d18bbd59290" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May 16 15:06:42.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2790" for this suite. 05/16/23 15:06:42.457
------------------------------
â€¢ [2.081 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:06:40.382
    May 16 15:06:40.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubelet-test 05/16/23 15:06:40.383
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:40.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:40.403
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    May 16 15:06:40.432: INFO: Waiting up to 5m0s for pod "busybox-scheduling-c44ffd62-2a8c-4498-bcc0-5d18bbd59290" in namespace "kubelet-test-2790" to be "running and ready"
    May 16 15:06:40.440: INFO: Pod "busybox-scheduling-c44ffd62-2a8c-4498-bcc0-5d18bbd59290": Phase="Pending", Reason="", readiness=false. Elapsed: 8.260329ms
    May 16 15:06:40.440: INFO: The phase of Pod busybox-scheduling-c44ffd62-2a8c-4498-bcc0-5d18bbd59290 is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:06:42.445: INFO: Pod "busybox-scheduling-c44ffd62-2a8c-4498-bcc0-5d18bbd59290": Phase="Running", Reason="", readiness=true. Elapsed: 2.013598623s
    May 16 15:06:42.445: INFO: The phase of Pod busybox-scheduling-c44ffd62-2a8c-4498-bcc0-5d18bbd59290 is Running (Ready = true)
    May 16 15:06:42.445: INFO: Pod "busybox-scheduling-c44ffd62-2a8c-4498-bcc0-5d18bbd59290" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May 16 15:06:42.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2790" for this suite. 05/16/23 15:06:42.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:06:42.463
May 16 15:06:42.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubectl 05/16/23 15:06:42.464
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:42.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:42.483
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 05/16/23 15:06:42.492
May 16 15:06:42.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-9927 api-versions'
May 16 15:06:42.562: INFO: stderr: ""
May 16 15:06:42.562: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloud.network.openshift.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.openshift.io/v1beta1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ninfrastructure.cluster.x-k8s.io/v1alpha5\ninfrastructure.cluster.x-k8s.io/v1beta1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nk8s.ovn.org/v1\nmachine.openshift.io/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 16 15:06:42.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9927" for this suite. 05/16/23 15:06:42.575
------------------------------
â€¢ [0.116 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:06:42.463
    May 16 15:06:42.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubectl 05/16/23 15:06:42.464
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:42.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:42.483
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 05/16/23 15:06:42.492
    May 16 15:06:42.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-9927 api-versions'
    May 16 15:06:42.562: INFO: stderr: ""
    May 16 15:06:42.562: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloud.network.openshift.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.openshift.io/v1beta1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ninfrastructure.cluster.x-k8s.io/v1alpha5\ninfrastructure.cluster.x-k8s.io/v1beta1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nk8s.ovn.org/v1\nmachine.openshift.io/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 16 15:06:42.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9927" for this suite. 05/16/23 15:06:42.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:06:42.581
May 16 15:06:42.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename subpath 05/16/23 15:06:42.581
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:42.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:42.603
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/16/23 15:06:42.606
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-9wpn 05/16/23 15:06:42.619
STEP: Creating a pod to test atomic-volume-subpath 05/16/23 15:06:42.619
May 16 15:06:42.630: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9wpn" in namespace "subpath-2072" to be "Succeeded or Failed"
May 16 15:06:42.644: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Pending", Reason="", readiness=false. Elapsed: 13.518736ms
May 16 15:06:44.648: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 2.017255697s
May 16 15:06:46.648: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 4.017541027s
May 16 15:06:48.648: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 6.01773069s
May 16 15:06:50.648: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 8.017970581s
May 16 15:06:52.648: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 10.017252606s
May 16 15:06:54.649: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 12.018547433s
May 16 15:06:56.647: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 14.016829852s
May 16 15:06:58.648: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 16.017463525s
May 16 15:07:00.650: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 18.019850456s
May 16 15:07:02.647: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 20.016896965s
May 16 15:07:04.648: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=false. Elapsed: 22.017246438s
May 16 15:07:06.649: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.018097633s
STEP: Saw pod success 05/16/23 15:07:06.649
May 16 15:07:06.649: INFO: Pod "pod-subpath-test-configmap-9wpn" satisfied condition "Succeeded or Failed"
May 16 15:07:06.652: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-subpath-test-configmap-9wpn container test-container-subpath-configmap-9wpn: <nil>
STEP: delete the pod 05/16/23 15:07:06.66
May 16 15:07:06.669: INFO: Waiting for pod pod-subpath-test-configmap-9wpn to disappear
May 16 15:07:06.672: INFO: Pod pod-subpath-test-configmap-9wpn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9wpn 05/16/23 15:07:06.672
May 16 15:07:06.672: INFO: Deleting pod "pod-subpath-test-configmap-9wpn" in namespace "subpath-2072"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May 16 15:07:06.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2072" for this suite. 05/16/23 15:07:06.68
------------------------------
â€¢ [SLOW TEST] [24.105 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:06:42.581
    May 16 15:06:42.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename subpath 05/16/23 15:06:42.581
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:06:42.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:06:42.603
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/16/23 15:06:42.606
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-9wpn 05/16/23 15:06:42.619
    STEP: Creating a pod to test atomic-volume-subpath 05/16/23 15:06:42.619
    May 16 15:06:42.630: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9wpn" in namespace "subpath-2072" to be "Succeeded or Failed"
    May 16 15:06:42.644: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Pending", Reason="", readiness=false. Elapsed: 13.518736ms
    May 16 15:06:44.648: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 2.017255697s
    May 16 15:06:46.648: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 4.017541027s
    May 16 15:06:48.648: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 6.01773069s
    May 16 15:06:50.648: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 8.017970581s
    May 16 15:06:52.648: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 10.017252606s
    May 16 15:06:54.649: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 12.018547433s
    May 16 15:06:56.647: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 14.016829852s
    May 16 15:06:58.648: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 16.017463525s
    May 16 15:07:00.650: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 18.019850456s
    May 16 15:07:02.647: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=true. Elapsed: 20.016896965s
    May 16 15:07:04.648: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Running", Reason="", readiness=false. Elapsed: 22.017246438s
    May 16 15:07:06.649: INFO: Pod "pod-subpath-test-configmap-9wpn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.018097633s
    STEP: Saw pod success 05/16/23 15:07:06.649
    May 16 15:07:06.649: INFO: Pod "pod-subpath-test-configmap-9wpn" satisfied condition "Succeeded or Failed"
    May 16 15:07:06.652: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-subpath-test-configmap-9wpn container test-container-subpath-configmap-9wpn: <nil>
    STEP: delete the pod 05/16/23 15:07:06.66
    May 16 15:07:06.669: INFO: Waiting for pod pod-subpath-test-configmap-9wpn to disappear
    May 16 15:07:06.672: INFO: Pod pod-subpath-test-configmap-9wpn no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-9wpn 05/16/23 15:07:06.672
    May 16 15:07:06.672: INFO: Deleting pod "pod-subpath-test-configmap-9wpn" in namespace "subpath-2072"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May 16 15:07:06.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2072" for this suite. 05/16/23 15:07:06.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:07:06.686
May 16 15:07:06.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename configmap 05/16/23 15:07:06.687
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:06.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:06.712
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-9544/configmap-test-2cde7b5d-0521-4713-a6bb-5de717d5ce33 05/16/23 15:07:06.714
STEP: Creating a pod to test consume configMaps 05/16/23 15:07:06.721
May 16 15:07:06.732: INFO: Waiting up to 5m0s for pod "pod-configmaps-0aad39b5-d00c-4b78-b5a9-63ac2321fefd" in namespace "configmap-9544" to be "Succeeded or Failed"
May 16 15:07:06.735: INFO: Pod "pod-configmaps-0aad39b5-d00c-4b78-b5a9-63ac2321fefd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.784069ms
May 16 15:07:08.739: INFO: Pod "pod-configmaps-0aad39b5-d00c-4b78-b5a9-63ac2321fefd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006659101s
May 16 15:07:10.740: INFO: Pod "pod-configmaps-0aad39b5-d00c-4b78-b5a9-63ac2321fefd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007863927s
STEP: Saw pod success 05/16/23 15:07:10.74
May 16 15:07:10.740: INFO: Pod "pod-configmaps-0aad39b5-d00c-4b78-b5a9-63ac2321fefd" satisfied condition "Succeeded or Failed"
May 16 15:07:10.743: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod pod-configmaps-0aad39b5-d00c-4b78-b5a9-63ac2321fefd container env-test: <nil>
STEP: delete the pod 05/16/23 15:07:10.753
May 16 15:07:10.762: INFO: Waiting for pod pod-configmaps-0aad39b5-d00c-4b78-b5a9-63ac2321fefd to disappear
May 16 15:07:10.764: INFO: Pod pod-configmaps-0aad39b5-d00c-4b78-b5a9-63ac2321fefd no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 16 15:07:10.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9544" for this suite. 05/16/23 15:07:10.768
------------------------------
â€¢ [4.088 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:07:06.686
    May 16 15:07:06.686: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename configmap 05/16/23 15:07:06.687
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:06.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:06.712
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-9544/configmap-test-2cde7b5d-0521-4713-a6bb-5de717d5ce33 05/16/23 15:07:06.714
    STEP: Creating a pod to test consume configMaps 05/16/23 15:07:06.721
    May 16 15:07:06.732: INFO: Waiting up to 5m0s for pod "pod-configmaps-0aad39b5-d00c-4b78-b5a9-63ac2321fefd" in namespace "configmap-9544" to be "Succeeded or Failed"
    May 16 15:07:06.735: INFO: Pod "pod-configmaps-0aad39b5-d00c-4b78-b5a9-63ac2321fefd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.784069ms
    May 16 15:07:08.739: INFO: Pod "pod-configmaps-0aad39b5-d00c-4b78-b5a9-63ac2321fefd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006659101s
    May 16 15:07:10.740: INFO: Pod "pod-configmaps-0aad39b5-d00c-4b78-b5a9-63ac2321fefd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007863927s
    STEP: Saw pod success 05/16/23 15:07:10.74
    May 16 15:07:10.740: INFO: Pod "pod-configmaps-0aad39b5-d00c-4b78-b5a9-63ac2321fefd" satisfied condition "Succeeded or Failed"
    May 16 15:07:10.743: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod pod-configmaps-0aad39b5-d00c-4b78-b5a9-63ac2321fefd container env-test: <nil>
    STEP: delete the pod 05/16/23 15:07:10.753
    May 16 15:07:10.762: INFO: Waiting for pod pod-configmaps-0aad39b5-d00c-4b78-b5a9-63ac2321fefd to disappear
    May 16 15:07:10.764: INFO: Pod pod-configmaps-0aad39b5-d00c-4b78-b5a9-63ac2321fefd no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 16 15:07:10.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9544" for this suite. 05/16/23 15:07:10.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:07:10.774
May 16 15:07:10.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename services 05/16/23 15:07:10.775
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:10.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:10.798
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 05/16/23 15:07:10.8
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 16 15:07:10.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5385" for this suite. 05/16/23 15:07:10.814
------------------------------
â€¢ [0.045 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:07:10.774
    May 16 15:07:10.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename services 05/16/23 15:07:10.775
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:10.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:10.798
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 05/16/23 15:07:10.8
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 16 15:07:10.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5385" for this suite. 05/16/23 15:07:10.814
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:07:10.82
May 16 15:07:10.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename var-expansion 05/16/23 15:07:10.82
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:10.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:10.843
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 05/16/23 15:07:10.848
May 16 15:07:10.863: INFO: Waiting up to 5m0s for pod "var-expansion-a435042c-724c-4212-86fb-10562348157d" in namespace "var-expansion-1501" to be "Succeeded or Failed"
May 16 15:07:10.866: INFO: Pod "var-expansion-a435042c-724c-4212-86fb-10562348157d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.286671ms
May 16 15:07:12.870: INFO: Pod "var-expansion-a435042c-724c-4212-86fb-10562348157d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006608487s
May 16 15:07:14.870: INFO: Pod "var-expansion-a435042c-724c-4212-86fb-10562348157d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007036018s
STEP: Saw pod success 05/16/23 15:07:14.87
May 16 15:07:14.870: INFO: Pod "var-expansion-a435042c-724c-4212-86fb-10562348157d" satisfied condition "Succeeded or Failed"
May 16 15:07:14.873: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod var-expansion-a435042c-724c-4212-86fb-10562348157d container dapi-container: <nil>
STEP: delete the pod 05/16/23 15:07:14.878
May 16 15:07:14.888: INFO: Waiting for pod var-expansion-a435042c-724c-4212-86fb-10562348157d to disappear
May 16 15:07:14.891: INFO: Pod var-expansion-a435042c-724c-4212-86fb-10562348157d no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May 16 15:07:14.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1501" for this suite. 05/16/23 15:07:14.895
------------------------------
â€¢ [4.081 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:07:10.82
    May 16 15:07:10.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename var-expansion 05/16/23 15:07:10.82
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:10.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:10.843
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 05/16/23 15:07:10.848
    May 16 15:07:10.863: INFO: Waiting up to 5m0s for pod "var-expansion-a435042c-724c-4212-86fb-10562348157d" in namespace "var-expansion-1501" to be "Succeeded or Failed"
    May 16 15:07:10.866: INFO: Pod "var-expansion-a435042c-724c-4212-86fb-10562348157d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.286671ms
    May 16 15:07:12.870: INFO: Pod "var-expansion-a435042c-724c-4212-86fb-10562348157d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006608487s
    May 16 15:07:14.870: INFO: Pod "var-expansion-a435042c-724c-4212-86fb-10562348157d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007036018s
    STEP: Saw pod success 05/16/23 15:07:14.87
    May 16 15:07:14.870: INFO: Pod "var-expansion-a435042c-724c-4212-86fb-10562348157d" satisfied condition "Succeeded or Failed"
    May 16 15:07:14.873: INFO: Trying to get logs from node ip-10-0-132-142.eu-central-1.compute.internal pod var-expansion-a435042c-724c-4212-86fb-10562348157d container dapi-container: <nil>
    STEP: delete the pod 05/16/23 15:07:14.878
    May 16 15:07:14.888: INFO: Waiting for pod var-expansion-a435042c-724c-4212-86fb-10562348157d to disappear
    May 16 15:07:14.891: INFO: Pod var-expansion-a435042c-724c-4212-86fb-10562348157d no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May 16 15:07:14.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1501" for this suite. 05/16/23 15:07:14.895
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:07:14.902
May 16 15:07:14.902: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubectl 05/16/23 15:07:14.902
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:14.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:14.922
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/16/23 15:07:14.925
May 16 15:07:14.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3429 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May 16 15:07:14.987: INFO: stderr: ""
May 16 15:07:14.987: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 05/16/23 15:07:14.987
May 16 15:07:14.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3429 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
May 16 15:07:15.868: INFO: stderr: ""
May 16 15:07:15.868: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/16/23 15:07:15.868
May 16 15:07:15.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3429 delete pods e2e-test-httpd-pod'
May 16 15:07:17.981: INFO: stderr: ""
May 16 15:07:17.981: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 16 15:07:17.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3429" for this suite. 05/16/23 15:07:17.985
------------------------------
â€¢ [3.089 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:07:14.902
    May 16 15:07:14.902: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubectl 05/16/23 15:07:14.902
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:14.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:14.922
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/16/23 15:07:14.925
    May 16 15:07:14.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3429 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    May 16 15:07:14.987: INFO: stderr: ""
    May 16 15:07:14.987: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 05/16/23 15:07:14.987
    May 16 15:07:14.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3429 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    May 16 15:07:15.868: INFO: stderr: ""
    May 16 15:07:15.868: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/16/23 15:07:15.868
    May 16 15:07:15.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3429 delete pods e2e-test-httpd-pod'
    May 16 15:07:17.981: INFO: stderr: ""
    May 16 15:07:17.981: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 16 15:07:17.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3429" for this suite. 05/16/23 15:07:17.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:07:17.991
May 16 15:07:17.991: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename security-context 05/16/23 15:07:17.992
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:18.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:18.013
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/16/23 15:07:18.015
May 16 15:07:18.032: INFO: Waiting up to 5m0s for pod "security-context-a3f18e85-557e-4c73-af89-3d029697ca77" in namespace "security-context-5196" to be "Succeeded or Failed"
May 16 15:07:18.039: INFO: Pod "security-context-a3f18e85-557e-4c73-af89-3d029697ca77": Phase="Pending", Reason="", readiness=false. Elapsed: 7.188462ms
May 16 15:07:20.043: INFO: Pod "security-context-a3f18e85-557e-4c73-af89-3d029697ca77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011633941s
May 16 15:07:22.043: INFO: Pod "security-context-a3f18e85-557e-4c73-af89-3d029697ca77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01129454s
STEP: Saw pod success 05/16/23 15:07:22.043
May 16 15:07:22.043: INFO: Pod "security-context-a3f18e85-557e-4c73-af89-3d029697ca77" satisfied condition "Succeeded or Failed"
May 16 15:07:22.046: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod security-context-a3f18e85-557e-4c73-af89-3d029697ca77 container test-container: <nil>
STEP: delete the pod 05/16/23 15:07:22.052
May 16 15:07:22.063: INFO: Waiting for pod security-context-a3f18e85-557e-4c73-af89-3d029697ca77 to disappear
May 16 15:07:22.066: INFO: Pod security-context-a3f18e85-557e-4c73-af89-3d029697ca77 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May 16 15:07:22.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-5196" for this suite. 05/16/23 15:07:22.069
------------------------------
â€¢ [4.083 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:07:17.991
    May 16 15:07:17.991: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename security-context 05/16/23 15:07:17.992
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:18.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:18.013
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 05/16/23 15:07:18.015
    May 16 15:07:18.032: INFO: Waiting up to 5m0s for pod "security-context-a3f18e85-557e-4c73-af89-3d029697ca77" in namespace "security-context-5196" to be "Succeeded or Failed"
    May 16 15:07:18.039: INFO: Pod "security-context-a3f18e85-557e-4c73-af89-3d029697ca77": Phase="Pending", Reason="", readiness=false. Elapsed: 7.188462ms
    May 16 15:07:20.043: INFO: Pod "security-context-a3f18e85-557e-4c73-af89-3d029697ca77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011633941s
    May 16 15:07:22.043: INFO: Pod "security-context-a3f18e85-557e-4c73-af89-3d029697ca77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01129454s
    STEP: Saw pod success 05/16/23 15:07:22.043
    May 16 15:07:22.043: INFO: Pod "security-context-a3f18e85-557e-4c73-af89-3d029697ca77" satisfied condition "Succeeded or Failed"
    May 16 15:07:22.046: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod security-context-a3f18e85-557e-4c73-af89-3d029697ca77 container test-container: <nil>
    STEP: delete the pod 05/16/23 15:07:22.052
    May 16 15:07:22.063: INFO: Waiting for pod security-context-a3f18e85-557e-4c73-af89-3d029697ca77 to disappear
    May 16 15:07:22.066: INFO: Pod security-context-a3f18e85-557e-4c73-af89-3d029697ca77 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May 16 15:07:22.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-5196" for this suite. 05/16/23 15:07:22.069
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:07:22.075
May 16 15:07:22.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename runtimeclass 05/16/23 15:07:22.075
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:22.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:22.094
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
May 16 15:07:22.118: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5443 to be scheduled
May 16 15:07:22.122: INFO: 1 pods are not scheduled: [runtimeclass-5443/test-runtimeclass-runtimeclass-5443-preconfigured-handler-hf55t(9be78711-fc50-4e70-913d-041b633270b2)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May 16 15:07:24.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5443" for this suite. 05/16/23 15:07:24.135
------------------------------
â€¢ [2.066 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:07:22.075
    May 16 15:07:22.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename runtimeclass 05/16/23 15:07:22.075
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:22.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:22.094
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    May 16 15:07:22.118: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5443 to be scheduled
    May 16 15:07:22.122: INFO: 1 pods are not scheduled: [runtimeclass-5443/test-runtimeclass-runtimeclass-5443-preconfigured-handler-hf55t(9be78711-fc50-4e70-913d-041b633270b2)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May 16 15:07:24.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5443" for this suite. 05/16/23 15:07:24.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:07:24.141
May 16 15:07:24.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename downward-api 05/16/23 15:07:24.141
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:24.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:24.165
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 05/16/23 15:07:24.173
May 16 15:07:24.188: INFO: Waiting up to 5m0s for pod "downwardapi-volume-918bc1d7-5c7c-427e-9662-afdb98f73233" in namespace "downward-api-3267" to be "Succeeded or Failed"
May 16 15:07:24.195: INFO: Pod "downwardapi-volume-918bc1d7-5c7c-427e-9662-afdb98f73233": Phase="Pending", Reason="", readiness=false. Elapsed: 6.522193ms
May 16 15:07:26.199: INFO: Pod "downwardapi-volume-918bc1d7-5c7c-427e-9662-afdb98f73233": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010789509s
May 16 15:07:28.198: INFO: Pod "downwardapi-volume-918bc1d7-5c7c-427e-9662-afdb98f73233": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009742073s
STEP: Saw pod success 05/16/23 15:07:28.198
May 16 15:07:28.198: INFO: Pod "downwardapi-volume-918bc1d7-5c7c-427e-9662-afdb98f73233" satisfied condition "Succeeded or Failed"
May 16 15:07:28.201: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-918bc1d7-5c7c-427e-9662-afdb98f73233 container client-container: <nil>
STEP: delete the pod 05/16/23 15:07:28.206
May 16 15:07:28.215: INFO: Waiting for pod downwardapi-volume-918bc1d7-5c7c-427e-9662-afdb98f73233 to disappear
May 16 15:07:28.218: INFO: Pod downwardapi-volume-918bc1d7-5c7c-427e-9662-afdb98f73233 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 16 15:07:28.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3267" for this suite. 05/16/23 15:07:28.223
------------------------------
â€¢ [4.089 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:07:24.141
    May 16 15:07:24.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename downward-api 05/16/23 15:07:24.141
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:24.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:24.165
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 05/16/23 15:07:24.173
    May 16 15:07:24.188: INFO: Waiting up to 5m0s for pod "downwardapi-volume-918bc1d7-5c7c-427e-9662-afdb98f73233" in namespace "downward-api-3267" to be "Succeeded or Failed"
    May 16 15:07:24.195: INFO: Pod "downwardapi-volume-918bc1d7-5c7c-427e-9662-afdb98f73233": Phase="Pending", Reason="", readiness=false. Elapsed: 6.522193ms
    May 16 15:07:26.199: INFO: Pod "downwardapi-volume-918bc1d7-5c7c-427e-9662-afdb98f73233": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010789509s
    May 16 15:07:28.198: INFO: Pod "downwardapi-volume-918bc1d7-5c7c-427e-9662-afdb98f73233": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009742073s
    STEP: Saw pod success 05/16/23 15:07:28.198
    May 16 15:07:28.198: INFO: Pod "downwardapi-volume-918bc1d7-5c7c-427e-9662-afdb98f73233" satisfied condition "Succeeded or Failed"
    May 16 15:07:28.201: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-918bc1d7-5c7c-427e-9662-afdb98f73233 container client-container: <nil>
    STEP: delete the pod 05/16/23 15:07:28.206
    May 16 15:07:28.215: INFO: Waiting for pod downwardapi-volume-918bc1d7-5c7c-427e-9662-afdb98f73233 to disappear
    May 16 15:07:28.218: INFO: Pod downwardapi-volume-918bc1d7-5c7c-427e-9662-afdb98f73233 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 16 15:07:28.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3267" for this suite. 05/16/23 15:07:28.223
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:07:28.23
May 16 15:07:28.230: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename services 05/16/23 15:07:28.231
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:28.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:28.251
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-6633 05/16/23 15:07:28.257
STEP: creating service affinity-nodeport-transition in namespace services-6633 05/16/23 15:07:28.257
STEP: creating replication controller affinity-nodeport-transition in namespace services-6633 05/16/23 15:07:28.277
I0516 15:07:28.288793      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6633, replica count: 3
I0516 15:07:31.340738      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 16 15:07:31.350: INFO: Creating new exec pod
May 16 15:07:31.359: INFO: Waiting up to 5m0s for pod "execpod-affinityxspzj" in namespace "services-6633" to be "running"
May 16 15:07:31.362: INFO: Pod "execpod-affinityxspzj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.262971ms
May 16 15:07:33.367: INFO: Pod "execpod-affinityxspzj": Phase="Running", Reason="", readiness=true. Elapsed: 2.007846694s
May 16 15:07:33.367: INFO: Pod "execpod-affinityxspzj" satisfied condition "running"
May 16 15:07:34.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6633 exec execpod-affinityxspzj -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
May 16 15:07:35.496: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May 16 15:07:35.496: INFO: stdout: ""
May 16 15:07:35.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6633 exec execpod-affinityxspzj -- /bin/sh -x -c nc -v -z -w 2 172.30.180.176 80'
May 16 15:07:35.616: INFO: stderr: "+ nc -v -z -w 2 172.30.180.176 80\nConnection to 172.30.180.176 80 port [tcp/http] succeeded!\n"
May 16 15:07:35.616: INFO: stdout: ""
May 16 15:07:35.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6633 exec execpod-affinityxspzj -- /bin/sh -x -c nc -v -z -w 2 10.0.132.142 30537'
May 16 15:07:36.713: INFO: stderr: "+ nc -v -z -w 2 10.0.132.142 30537\nConnection to 10.0.132.142 30537 port [tcp/*] succeeded!\n"
May 16 15:07:36.713: INFO: stdout: ""
May 16 15:07:36.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6633 exec execpod-affinityxspzj -- /bin/sh -x -c nc -v -z -w 2 10.0.212.246 30537'
May 16 15:07:37.865: INFO: stderr: "+ nc -v -z -w 2 10.0.212.246 30537\nConnection to 10.0.212.246 30537 port [tcp/*] succeeded!\n"
May 16 15:07:37.865: INFO: stdout: ""
May 16 15:07:37.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6633 exec execpod-affinityxspzj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.132.142:30537/ ; done'
May 16 15:07:38.047: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n"
May 16 15:07:38.047: INFO: stdout: "\naffinity-nodeport-transition-nj2xx\naffinity-nodeport-transition-q4m5c\naffinity-nodeport-transition-q4m5c\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-q4m5c\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-q4m5c\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-nj2xx\naffinity-nodeport-transition-nj2xx\naffinity-nodeport-transition-nj2xx\naffinity-nodeport-transition-q4m5c\naffinity-nodeport-transition-q4m5c\naffinity-nodeport-transition-q4m5c"
May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-nj2xx
May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-q4m5c
May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-q4m5c
May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-q4m5c
May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-q4m5c
May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-nj2xx
May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-nj2xx
May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-nj2xx
May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-q4m5c
May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-q4m5c
May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-q4m5c
May 16 15:07:38.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6633 exec execpod-affinityxspzj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.132.142:30537/ ; done'
May 16 15:07:38.240: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n"
May 16 15:07:38.240: INFO: stdout: "\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp"
May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
May 16 15:07:38.240: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6633, will wait for the garbage collector to delete the pods 05/16/23 15:07:38.252
May 16 15:07:38.312: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.448772ms
May 16 15:07:38.412: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.85188ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 16 15:07:40.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6633" for this suite. 05/16/23 15:07:40.053
------------------------------
â€¢ [SLOW TEST] [11.834 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:07:28.23
    May 16 15:07:28.230: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename services 05/16/23 15:07:28.231
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:28.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:28.251
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-6633 05/16/23 15:07:28.257
    STEP: creating service affinity-nodeport-transition in namespace services-6633 05/16/23 15:07:28.257
    STEP: creating replication controller affinity-nodeport-transition in namespace services-6633 05/16/23 15:07:28.277
    I0516 15:07:28.288793      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6633, replica count: 3
    I0516 15:07:31.340738      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 16 15:07:31.350: INFO: Creating new exec pod
    May 16 15:07:31.359: INFO: Waiting up to 5m0s for pod "execpod-affinityxspzj" in namespace "services-6633" to be "running"
    May 16 15:07:31.362: INFO: Pod "execpod-affinityxspzj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.262971ms
    May 16 15:07:33.367: INFO: Pod "execpod-affinityxspzj": Phase="Running", Reason="", readiness=true. Elapsed: 2.007846694s
    May 16 15:07:33.367: INFO: Pod "execpod-affinityxspzj" satisfied condition "running"
    May 16 15:07:34.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6633 exec execpod-affinityxspzj -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    May 16 15:07:35.496: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    May 16 15:07:35.496: INFO: stdout: ""
    May 16 15:07:35.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6633 exec execpod-affinityxspzj -- /bin/sh -x -c nc -v -z -w 2 172.30.180.176 80'
    May 16 15:07:35.616: INFO: stderr: "+ nc -v -z -w 2 172.30.180.176 80\nConnection to 172.30.180.176 80 port [tcp/http] succeeded!\n"
    May 16 15:07:35.616: INFO: stdout: ""
    May 16 15:07:35.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6633 exec execpod-affinityxspzj -- /bin/sh -x -c nc -v -z -w 2 10.0.132.142 30537'
    May 16 15:07:36.713: INFO: stderr: "+ nc -v -z -w 2 10.0.132.142 30537\nConnection to 10.0.132.142 30537 port [tcp/*] succeeded!\n"
    May 16 15:07:36.713: INFO: stdout: ""
    May 16 15:07:36.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6633 exec execpod-affinityxspzj -- /bin/sh -x -c nc -v -z -w 2 10.0.212.246 30537'
    May 16 15:07:37.865: INFO: stderr: "+ nc -v -z -w 2 10.0.212.246 30537\nConnection to 10.0.212.246 30537 port [tcp/*] succeeded!\n"
    May 16 15:07:37.865: INFO: stdout: ""
    May 16 15:07:37.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6633 exec execpod-affinityxspzj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.132.142:30537/ ; done'
    May 16 15:07:38.047: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n"
    May 16 15:07:38.047: INFO: stdout: "\naffinity-nodeport-transition-nj2xx\naffinity-nodeport-transition-q4m5c\naffinity-nodeport-transition-q4m5c\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-q4m5c\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-q4m5c\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-nj2xx\naffinity-nodeport-transition-nj2xx\naffinity-nodeport-transition-nj2xx\naffinity-nodeport-transition-q4m5c\naffinity-nodeport-transition-q4m5c\naffinity-nodeport-transition-q4m5c"
    May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-nj2xx
    May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-q4m5c
    May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-q4m5c
    May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-q4m5c
    May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-q4m5c
    May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-nj2xx
    May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-nj2xx
    May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-nj2xx
    May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-q4m5c
    May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-q4m5c
    May 16 15:07:38.047: INFO: Received response from host: affinity-nodeport-transition-q4m5c
    May 16 15:07:38.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-6633 exec execpod-affinityxspzj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.132.142:30537/ ; done'
    May 16 15:07:38.240: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.132.142:30537/\n"
    May 16 15:07:38.240: INFO: stdout: "\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp\naffinity-nodeport-transition-5mmjp"
    May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.240: INFO: Received response from host: affinity-nodeport-transition-5mmjp
    May 16 15:07:38.240: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6633, will wait for the garbage collector to delete the pods 05/16/23 15:07:38.252
    May 16 15:07:38.312: INFO: Deleting ReplicationController affinity-nodeport-transition took: 6.448772ms
    May 16 15:07:38.412: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.85188ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 16 15:07:40.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6633" for this suite. 05/16/23 15:07:40.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:07:40.065
May 16 15:07:40.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename podtemplate 05/16/23 15:07:40.068
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:40.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:40.098
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 05/16/23 15:07:40.104
W0516 15:07:41.112194      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:07:41.112: INFO: created test-podtemplate-1
May 16 15:07:41.117: INFO: created test-podtemplate-2
May 16 15:07:41.121: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 05/16/23 15:07:41.121
STEP: delete collection of pod templates 05/16/23 15:07:41.125
May 16 15:07:41.125: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 05/16/23 15:07:41.139
May 16 15:07:41.139: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
May 16 15:07:41.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-6140" for this suite. 05/16/23 15:07:41.146
------------------------------
â€¢ [1.086 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:07:40.065
    May 16 15:07:40.065: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename podtemplate 05/16/23 15:07:40.068
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:40.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:40.098
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 05/16/23 15:07:40.104
    W0516 15:07:41.112194      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:07:41.112: INFO: created test-podtemplate-1
    May 16 15:07:41.117: INFO: created test-podtemplate-2
    May 16 15:07:41.121: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 05/16/23 15:07:41.121
    STEP: delete collection of pod templates 05/16/23 15:07:41.125
    May 16 15:07:41.125: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 05/16/23 15:07:41.139
    May 16 15:07:41.139: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    May 16 15:07:41.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-6140" for this suite. 05/16/23 15:07:41.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:07:41.152
May 16 15:07:41.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename downward-api 05/16/23 15:07:41.153
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:41.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:41.173
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 05/16/23 15:07:41.179
May 16 15:07:41.193: INFO: Waiting up to 5m0s for pod "annotationupdate7a29c1d3-a40d-4767-b525-0e30359964a5" in namespace "downward-api-741" to be "running and ready"
May 16 15:07:41.203: INFO: Pod "annotationupdate7a29c1d3-a40d-4767-b525-0e30359964a5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.527982ms
May 16 15:07:41.203: INFO: The phase of Pod annotationupdate7a29c1d3-a40d-4767-b525-0e30359964a5 is Pending, waiting for it to be Running (with Ready = true)
May 16 15:07:43.206: INFO: Pod "annotationupdate7a29c1d3-a40d-4767-b525-0e30359964a5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013341905s
May 16 15:07:43.206: INFO: The phase of Pod annotationupdate7a29c1d3-a40d-4767-b525-0e30359964a5 is Running (Ready = true)
May 16 15:07:43.206: INFO: Pod "annotationupdate7a29c1d3-a40d-4767-b525-0e30359964a5" satisfied condition "running and ready"
May 16 15:07:43.733: INFO: Successfully updated pod "annotationupdate7a29c1d3-a40d-4767-b525-0e30359964a5"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 16 15:07:47.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-741" for this suite. 05/16/23 15:07:47.756
------------------------------
â€¢ [SLOW TEST] [6.610 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:07:41.152
    May 16 15:07:41.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename downward-api 05/16/23 15:07:41.153
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:41.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:41.173
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 05/16/23 15:07:41.179
    May 16 15:07:41.193: INFO: Waiting up to 5m0s for pod "annotationupdate7a29c1d3-a40d-4767-b525-0e30359964a5" in namespace "downward-api-741" to be "running and ready"
    May 16 15:07:41.203: INFO: Pod "annotationupdate7a29c1d3-a40d-4767-b525-0e30359964a5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.527982ms
    May 16 15:07:41.203: INFO: The phase of Pod annotationupdate7a29c1d3-a40d-4767-b525-0e30359964a5 is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:07:43.206: INFO: Pod "annotationupdate7a29c1d3-a40d-4767-b525-0e30359964a5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013341905s
    May 16 15:07:43.206: INFO: The phase of Pod annotationupdate7a29c1d3-a40d-4767-b525-0e30359964a5 is Running (Ready = true)
    May 16 15:07:43.206: INFO: Pod "annotationupdate7a29c1d3-a40d-4767-b525-0e30359964a5" satisfied condition "running and ready"
    May 16 15:07:43.733: INFO: Successfully updated pod "annotationupdate7a29c1d3-a40d-4767-b525-0e30359964a5"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 16 15:07:47.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-741" for this suite. 05/16/23 15:07:47.756
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:07:47.762
May 16 15:07:47.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename configmap 05/16/23 15:07:47.763
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:47.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:47.784
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-ddceea15-6fe3-4c8e-bfc0-2e959a625ecd 05/16/23 15:07:47.788
STEP: Creating a pod to test consume configMaps 05/16/23 15:07:47.795
May 16 15:07:47.806: INFO: Waiting up to 5m0s for pod "pod-configmaps-b52c2131-c04c-408d-afce-e3a66ca235f6" in namespace "configmap-1320" to be "Succeeded or Failed"
May 16 15:07:47.813: INFO: Pod "pod-configmaps-b52c2131-c04c-408d-afce-e3a66ca235f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.468149ms
May 16 15:07:49.816: INFO: Pod "pod-configmaps-b52c2131-c04c-408d-afce-e3a66ca235f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009748922s
May 16 15:07:51.818: INFO: Pod "pod-configmaps-b52c2131-c04c-408d-afce-e3a66ca235f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011331933s
STEP: Saw pod success 05/16/23 15:07:51.818
May 16 15:07:51.818: INFO: Pod "pod-configmaps-b52c2131-c04c-408d-afce-e3a66ca235f6" satisfied condition "Succeeded or Failed"
May 16 15:07:51.821: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-configmaps-b52c2131-c04c-408d-afce-e3a66ca235f6 container agnhost-container: <nil>
STEP: delete the pod 05/16/23 15:07:51.826
May 16 15:07:51.835: INFO: Waiting for pod pod-configmaps-b52c2131-c04c-408d-afce-e3a66ca235f6 to disappear
May 16 15:07:51.837: INFO: Pod pod-configmaps-b52c2131-c04c-408d-afce-e3a66ca235f6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 16 15:07:51.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1320" for this suite. 05/16/23 15:07:51.841
------------------------------
â€¢ [4.087 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:07:47.762
    May 16 15:07:47.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename configmap 05/16/23 15:07:47.763
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:47.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:47.784
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-ddceea15-6fe3-4c8e-bfc0-2e959a625ecd 05/16/23 15:07:47.788
    STEP: Creating a pod to test consume configMaps 05/16/23 15:07:47.795
    May 16 15:07:47.806: INFO: Waiting up to 5m0s for pod "pod-configmaps-b52c2131-c04c-408d-afce-e3a66ca235f6" in namespace "configmap-1320" to be "Succeeded or Failed"
    May 16 15:07:47.813: INFO: Pod "pod-configmaps-b52c2131-c04c-408d-afce-e3a66ca235f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.468149ms
    May 16 15:07:49.816: INFO: Pod "pod-configmaps-b52c2131-c04c-408d-afce-e3a66ca235f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009748922s
    May 16 15:07:51.818: INFO: Pod "pod-configmaps-b52c2131-c04c-408d-afce-e3a66ca235f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011331933s
    STEP: Saw pod success 05/16/23 15:07:51.818
    May 16 15:07:51.818: INFO: Pod "pod-configmaps-b52c2131-c04c-408d-afce-e3a66ca235f6" satisfied condition "Succeeded or Failed"
    May 16 15:07:51.821: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-configmaps-b52c2131-c04c-408d-afce-e3a66ca235f6 container agnhost-container: <nil>
    STEP: delete the pod 05/16/23 15:07:51.826
    May 16 15:07:51.835: INFO: Waiting for pod pod-configmaps-b52c2131-c04c-408d-afce-e3a66ca235f6 to disappear
    May 16 15:07:51.837: INFO: Pod pod-configmaps-b52c2131-c04c-408d-afce-e3a66ca235f6 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 16 15:07:51.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1320" for this suite. 05/16/23 15:07:51.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:07:51.85
May 16 15:07:51.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename container-runtime 05/16/23 15:07:51.85
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:51.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:51.867
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 05/16/23 15:07:51.87
W0516 15:07:51.884859      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: wait for the container to reach Succeeded 05/16/23 15:07:51.884
STEP: get the container status 05/16/23 15:07:54.903
STEP: the container should be terminated 05/16/23 15:07:54.906
STEP: the termination message should be set 05/16/23 15:07:54.906
May 16 15:07:54.906: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 05/16/23 15:07:54.906
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May 16 15:07:54.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2488" for this suite. 05/16/23 15:07:54.924
------------------------------
â€¢ [3.082 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:07:51.85
    May 16 15:07:51.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename container-runtime 05/16/23 15:07:51.85
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:51.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:51.867
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 05/16/23 15:07:51.87
    W0516 15:07:51.884859      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: wait for the container to reach Succeeded 05/16/23 15:07:51.884
    STEP: get the container status 05/16/23 15:07:54.903
    STEP: the container should be terminated 05/16/23 15:07:54.906
    STEP: the termination message should be set 05/16/23 15:07:54.906
    May 16 15:07:54.906: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 05/16/23 15:07:54.906
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May 16 15:07:54.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2488" for this suite. 05/16/23 15:07:54.924
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:07:54.932
May 16 15:07:54.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename statefulset 05/16/23 15:07:54.933
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:54.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:54.956
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2380 05/16/23 15:07:54.959
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-2380 05/16/23 15:07:54.97
May 16 15:07:54.986: INFO: Found 0 stateful pods, waiting for 1
May 16 15:08:04.991: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 05/16/23 15:08:04.997
STEP: updating a scale subresource 05/16/23 15:08:04.999
STEP: verifying the statefulset Spec.Replicas was modified 05/16/23 15:08:05.005
STEP: Patch a scale subresource 05/16/23 15:08:05.008
STEP: verifying the statefulset Spec.Replicas was modified 05/16/23 15:08:05.014
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May 16 15:08:05.018: INFO: Deleting all statefulset in ns statefulset-2380
May 16 15:08:05.023: INFO: Scaling statefulset ss to 0
May 16 15:08:15.051: INFO: Waiting for statefulset status.replicas updated to 0
May 16 15:08:15.054: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May 16 15:08:15.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2380" for this suite. 05/16/23 15:08:15.075
------------------------------
â€¢ [SLOW TEST] [20.149 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:07:54.932
    May 16 15:07:54.932: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename statefulset 05/16/23 15:07:54.933
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:07:54.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:07:54.956
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2380 05/16/23 15:07:54.959
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-2380 05/16/23 15:07:54.97
    May 16 15:07:54.986: INFO: Found 0 stateful pods, waiting for 1
    May 16 15:08:04.991: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 05/16/23 15:08:04.997
    STEP: updating a scale subresource 05/16/23 15:08:04.999
    STEP: verifying the statefulset Spec.Replicas was modified 05/16/23 15:08:05.005
    STEP: Patch a scale subresource 05/16/23 15:08:05.008
    STEP: verifying the statefulset Spec.Replicas was modified 05/16/23 15:08:05.014
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May 16 15:08:05.018: INFO: Deleting all statefulset in ns statefulset-2380
    May 16 15:08:05.023: INFO: Scaling statefulset ss to 0
    May 16 15:08:15.051: INFO: Waiting for statefulset status.replicas updated to 0
    May 16 15:08:15.054: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May 16 15:08:15.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2380" for this suite. 05/16/23 15:08:15.075
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:08:15.081
May 16 15:08:15.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename sysctl 05/16/23 15:08:15.082
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:08:15.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:08:15.104
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 05/16/23 15:08:15.108
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May 16 15:08:15.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-2098" for this suite. 05/16/23 15:08:15.127
------------------------------
â€¢ [0.055 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:08:15.081
    May 16 15:08:15.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename sysctl 05/16/23 15:08:15.082
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:08:15.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:08:15.104
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 05/16/23 15:08:15.108
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May 16 15:08:15.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-2098" for this suite. 05/16/23 15:08:15.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:08:15.137
May 16 15:08:15.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename container-probe 05/16/23 15:08:15.138
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:08:15.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:08:15.169
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-6d52efe8-c7e4-435c-8fa1-4715b479525c in namespace container-probe-789 05/16/23 15:08:15.171
May 16 15:08:15.182: INFO: Waiting up to 5m0s for pod "liveness-6d52efe8-c7e4-435c-8fa1-4715b479525c" in namespace "container-probe-789" to be "not pending"
May 16 15:08:15.185: INFO: Pod "liveness-6d52efe8-c7e4-435c-8fa1-4715b479525c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.196622ms
May 16 15:08:17.188: INFO: Pod "liveness-6d52efe8-c7e4-435c-8fa1-4715b479525c": Phase="Running", Reason="", readiness=true. Elapsed: 2.006776576s
May 16 15:08:17.188: INFO: Pod "liveness-6d52efe8-c7e4-435c-8fa1-4715b479525c" satisfied condition "not pending"
May 16 15:08:17.188: INFO: Started pod liveness-6d52efe8-c7e4-435c-8fa1-4715b479525c in namespace container-probe-789
STEP: checking the pod's current state and verifying that restartCount is present 05/16/23 15:08:17.188
May 16 15:08:17.191: INFO: Initial restart count of pod liveness-6d52efe8-c7e4-435c-8fa1-4715b479525c is 0
May 16 15:08:37.235: INFO: Restart count of pod container-probe-789/liveness-6d52efe8-c7e4-435c-8fa1-4715b479525c is now 1 (20.043485245s elapsed)
STEP: deleting the pod 05/16/23 15:08:37.235
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May 16 15:08:37.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-789" for this suite. 05/16/23 15:08:37.249
------------------------------
â€¢ [SLOW TEST] [22.118 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:08:15.137
    May 16 15:08:15.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename container-probe 05/16/23 15:08:15.138
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:08:15.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:08:15.169
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-6d52efe8-c7e4-435c-8fa1-4715b479525c in namespace container-probe-789 05/16/23 15:08:15.171
    May 16 15:08:15.182: INFO: Waiting up to 5m0s for pod "liveness-6d52efe8-c7e4-435c-8fa1-4715b479525c" in namespace "container-probe-789" to be "not pending"
    May 16 15:08:15.185: INFO: Pod "liveness-6d52efe8-c7e4-435c-8fa1-4715b479525c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.196622ms
    May 16 15:08:17.188: INFO: Pod "liveness-6d52efe8-c7e4-435c-8fa1-4715b479525c": Phase="Running", Reason="", readiness=true. Elapsed: 2.006776576s
    May 16 15:08:17.188: INFO: Pod "liveness-6d52efe8-c7e4-435c-8fa1-4715b479525c" satisfied condition "not pending"
    May 16 15:08:17.188: INFO: Started pod liveness-6d52efe8-c7e4-435c-8fa1-4715b479525c in namespace container-probe-789
    STEP: checking the pod's current state and verifying that restartCount is present 05/16/23 15:08:17.188
    May 16 15:08:17.191: INFO: Initial restart count of pod liveness-6d52efe8-c7e4-435c-8fa1-4715b479525c is 0
    May 16 15:08:37.235: INFO: Restart count of pod container-probe-789/liveness-6d52efe8-c7e4-435c-8fa1-4715b479525c is now 1 (20.043485245s elapsed)
    STEP: deleting the pod 05/16/23 15:08:37.235
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May 16 15:08:37.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-789" for this suite. 05/16/23 15:08:37.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:08:37.256
May 16 15:08:37.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubectl 05/16/23 15:08:37.256
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:08:37.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:08:37.284
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 05/16/23 15:08:37.288
May 16 15:08:37.288: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6927 proxy --unix-socket=/tmp/kubectl-proxy-unix3869069417/test'
STEP: retrieving proxy /api/ output 05/16/23 15:08:37.319
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 16 15:08:37.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6927" for this suite. 05/16/23 15:08:37.326
------------------------------
â€¢ [0.080 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:08:37.256
    May 16 15:08:37.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubectl 05/16/23 15:08:37.256
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:08:37.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:08:37.284
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 05/16/23 15:08:37.288
    May 16 15:08:37.288: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6927 proxy --unix-socket=/tmp/kubectl-proxy-unix3869069417/test'
    STEP: retrieving proxy /api/ output 05/16/23 15:08:37.319
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 16 15:08:37.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6927" for this suite. 05/16/23 15:08:37.326
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:08:37.335
May 16 15:08:37.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename proxy 05/16/23 15:08:37.336
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:08:37.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:08:37.355
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
May 16 15:08:37.359: INFO: Creating pod...
W0516 15:08:37.371398      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:08:37.371: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5787" to be "running"
May 16 15:08:37.378: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.948423ms
May 16 15:08:39.382: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.011311576s
May 16 15:08:39.382: INFO: Pod "agnhost" satisfied condition "running"
May 16 15:08:39.382: INFO: Creating service...
May 16 15:08:39.392: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/pods/agnhost/proxy?method=DELETE
May 16 15:08:39.401: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 16 15:08:39.401: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/pods/agnhost/proxy?method=OPTIONS
May 16 15:08:39.413: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 16 15:08:39.413: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/pods/agnhost/proxy?method=PATCH
May 16 15:08:39.418: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 16 15:08:39.418: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/pods/agnhost/proxy?method=POST
May 16 15:08:39.421: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 16 15:08:39.421: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/pods/agnhost/proxy?method=PUT
May 16 15:08:39.424: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May 16 15:08:39.424: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/services/e2e-proxy-test-service/proxy?method=DELETE
May 16 15:08:39.430: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 16 15:08:39.430: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/services/e2e-proxy-test-service/proxy?method=OPTIONS
May 16 15:08:39.436: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 16 15:08:39.437: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/services/e2e-proxy-test-service/proxy?method=PATCH
May 16 15:08:39.442: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 16 15:08:39.442: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/services/e2e-proxy-test-service/proxy?method=POST
May 16 15:08:39.447: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 16 15:08:39.447: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/services/e2e-proxy-test-service/proxy?method=PUT
May 16 15:08:39.454: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May 16 15:08:39.454: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/pods/agnhost/proxy?method=GET
May 16 15:08:39.456: INFO: http.Client request:GET StatusCode:301
May 16 15:08:39.456: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/services/e2e-proxy-test-service/proxy?method=GET
May 16 15:08:39.460: INFO: http.Client request:GET StatusCode:301
May 16 15:08:39.460: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/pods/agnhost/proxy?method=HEAD
May 16 15:08:39.463: INFO: http.Client request:HEAD StatusCode:301
May 16 15:08:39.463: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/services/e2e-proxy-test-service/proxy?method=HEAD
May 16 15:08:39.469: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
May 16 15:08:39.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-5787" for this suite. 05/16/23 15:08:39.473
------------------------------
â€¢ [2.144 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:08:37.335
    May 16 15:08:37.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename proxy 05/16/23 15:08:37.336
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:08:37.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:08:37.355
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    May 16 15:08:37.359: INFO: Creating pod...
    W0516 15:08:37.371398      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:08:37.371: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5787" to be "running"
    May 16 15:08:37.378: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.948423ms
    May 16 15:08:39.382: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.011311576s
    May 16 15:08:39.382: INFO: Pod "agnhost" satisfied condition "running"
    May 16 15:08:39.382: INFO: Creating service...
    May 16 15:08:39.392: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/pods/agnhost/proxy?method=DELETE
    May 16 15:08:39.401: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May 16 15:08:39.401: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/pods/agnhost/proxy?method=OPTIONS
    May 16 15:08:39.413: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May 16 15:08:39.413: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/pods/agnhost/proxy?method=PATCH
    May 16 15:08:39.418: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May 16 15:08:39.418: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/pods/agnhost/proxy?method=POST
    May 16 15:08:39.421: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May 16 15:08:39.421: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/pods/agnhost/proxy?method=PUT
    May 16 15:08:39.424: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May 16 15:08:39.424: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/services/e2e-proxy-test-service/proxy?method=DELETE
    May 16 15:08:39.430: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May 16 15:08:39.430: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/services/e2e-proxy-test-service/proxy?method=OPTIONS
    May 16 15:08:39.436: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May 16 15:08:39.437: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/services/e2e-proxy-test-service/proxy?method=PATCH
    May 16 15:08:39.442: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May 16 15:08:39.442: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/services/e2e-proxy-test-service/proxy?method=POST
    May 16 15:08:39.447: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May 16 15:08:39.447: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/services/e2e-proxy-test-service/proxy?method=PUT
    May 16 15:08:39.454: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May 16 15:08:39.454: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/pods/agnhost/proxy?method=GET
    May 16 15:08:39.456: INFO: http.Client request:GET StatusCode:301
    May 16 15:08:39.456: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/services/e2e-proxy-test-service/proxy?method=GET
    May 16 15:08:39.460: INFO: http.Client request:GET StatusCode:301
    May 16 15:08:39.460: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/pods/agnhost/proxy?method=HEAD
    May 16 15:08:39.463: INFO: http.Client request:HEAD StatusCode:301
    May 16 15:08:39.463: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-5787/services/e2e-proxy-test-service/proxy?method=HEAD
    May 16 15:08:39.469: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    May 16 15:08:39.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-5787" for this suite. 05/16/23 15:08:39.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:08:39.48
May 16 15:08:39.480: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename watch 05/16/23 15:08:39.48
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:08:39.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:08:39.503
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 05/16/23 15:08:39.508
STEP: starting a background goroutine to produce watch events 05/16/23 15:08:39.513
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 05/16/23 15:08:39.513
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
May 16 15:08:42.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-3843" for this suite. 05/16/23 15:08:42.338
------------------------------
â€¢ [2.907 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:08:39.48
    May 16 15:08:39.480: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename watch 05/16/23 15:08:39.48
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:08:39.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:08:39.503
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 05/16/23 15:08:39.508
    STEP: starting a background goroutine to produce watch events 05/16/23 15:08:39.513
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 05/16/23 15:08:39.513
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    May 16 15:08:42.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-3843" for this suite. 05/16/23 15:08:42.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:08:42.387
May 16 15:08:42.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename crd-publish-openapi 05/16/23 15:08:42.388
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:08:42.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:08:42.408
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
May 16 15:08:42.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/16/23 15:08:45.465
May 16 15:08:45.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-4987 --namespace=crd-publish-openapi-4987 create -f -'
May 16 15:08:46.978: INFO: stderr: ""
May 16 15:08:46.978: INFO: stdout: "e2e-test-crd-publish-openapi-3221-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 16 15:08:46.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-4987 --namespace=crd-publish-openapi-4987 delete e2e-test-crd-publish-openapi-3221-crds test-cr'
May 16 15:08:47.031: INFO: stderr: ""
May 16 15:08:47.031: INFO: stdout: "e2e-test-crd-publish-openapi-3221-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 16 15:08:47.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-4987 --namespace=crd-publish-openapi-4987 apply -f -'
May 16 15:08:47.293: INFO: stderr: ""
May 16 15:08:47.293: INFO: stdout: "e2e-test-crd-publish-openapi-3221-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 16 15:08:47.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-4987 --namespace=crd-publish-openapi-4987 delete e2e-test-crd-publish-openapi-3221-crds test-cr'
May 16 15:08:47.364: INFO: stderr: ""
May 16 15:08:47.364: INFO: stdout: "e2e-test-crd-publish-openapi-3221-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 05/16/23 15:08:47.364
May 16 15:08:47.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-4987 explain e2e-test-crd-publish-openapi-3221-crds'
May 16 15:08:48.337: INFO: stderr: ""
May 16 15:08:48.337: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3221-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 15:08:51.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4987" for this suite. 05/16/23 15:08:51.793
------------------------------
â€¢ [SLOW TEST] [9.412 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:08:42.387
    May 16 15:08:42.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename crd-publish-openapi 05/16/23 15:08:42.388
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:08:42.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:08:42.408
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    May 16 15:08:42.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/16/23 15:08:45.465
    May 16 15:08:45.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-4987 --namespace=crd-publish-openapi-4987 create -f -'
    May 16 15:08:46.978: INFO: stderr: ""
    May 16 15:08:46.978: INFO: stdout: "e2e-test-crd-publish-openapi-3221-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    May 16 15:08:46.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-4987 --namespace=crd-publish-openapi-4987 delete e2e-test-crd-publish-openapi-3221-crds test-cr'
    May 16 15:08:47.031: INFO: stderr: ""
    May 16 15:08:47.031: INFO: stdout: "e2e-test-crd-publish-openapi-3221-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    May 16 15:08:47.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-4987 --namespace=crd-publish-openapi-4987 apply -f -'
    May 16 15:08:47.293: INFO: stderr: ""
    May 16 15:08:47.293: INFO: stdout: "e2e-test-crd-publish-openapi-3221-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    May 16 15:08:47.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-4987 --namespace=crd-publish-openapi-4987 delete e2e-test-crd-publish-openapi-3221-crds test-cr'
    May 16 15:08:47.364: INFO: stderr: ""
    May 16 15:08:47.364: INFO: stdout: "e2e-test-crd-publish-openapi-3221-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 05/16/23 15:08:47.364
    May 16 15:08:47.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-4987 explain e2e-test-crd-publish-openapi-3221-crds'
    May 16 15:08:48.337: INFO: stderr: ""
    May 16 15:08:48.337: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3221-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 15:08:51.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4987" for this suite. 05/16/23 15:08:51.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:08:51.8
May 16 15:08:51.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename ingressclass 05/16/23 15:08:51.8
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:08:51.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:08:51.821
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 05/16/23 15:08:51.823
STEP: getting /apis/networking.k8s.io 05/16/23 15:08:51.826
STEP: getting /apis/networking.k8s.iov1 05/16/23 15:08:51.829
STEP: creating 05/16/23 15:08:51.83
STEP: getting 05/16/23 15:08:51.844
STEP: listing 05/16/23 15:08:51.847
STEP: watching 05/16/23 15:08:51.85
May 16 15:08:51.850: INFO: starting watch
STEP: patching 05/16/23 15:08:51.851
STEP: updating 05/16/23 15:08:51.858
May 16 15:08:51.862: INFO: waiting for watch events with expected annotations
May 16 15:08:51.863: INFO: saw patched and updated annotations
STEP: deleting 05/16/23 15:08:51.863
STEP: deleting a collection 05/16/23 15:08:51.876
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
May 16 15:08:51.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-8995" for this suite. 05/16/23 15:08:51.895
------------------------------
â€¢ [0.104 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:08:51.8
    May 16 15:08:51.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename ingressclass 05/16/23 15:08:51.8
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:08:51.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:08:51.821
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 05/16/23 15:08:51.823
    STEP: getting /apis/networking.k8s.io 05/16/23 15:08:51.826
    STEP: getting /apis/networking.k8s.iov1 05/16/23 15:08:51.829
    STEP: creating 05/16/23 15:08:51.83
    STEP: getting 05/16/23 15:08:51.844
    STEP: listing 05/16/23 15:08:51.847
    STEP: watching 05/16/23 15:08:51.85
    May 16 15:08:51.850: INFO: starting watch
    STEP: patching 05/16/23 15:08:51.851
    STEP: updating 05/16/23 15:08:51.858
    May 16 15:08:51.862: INFO: waiting for watch events with expected annotations
    May 16 15:08:51.863: INFO: saw patched and updated annotations
    STEP: deleting 05/16/23 15:08:51.863
    STEP: deleting a collection 05/16/23 15:08:51.876
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    May 16 15:08:51.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-8995" for this suite. 05/16/23 15:08:51.895
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:08:51.904
May 16 15:08:51.904: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename container-probe 05/16/23 15:08:51.905
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:08:51.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:08:51.925
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-26ade6e3-db70-4c52-b71d-856c2f01592c in namespace container-probe-507 05/16/23 15:08:51.928
May 16 15:08:51.949: INFO: Waiting up to 5m0s for pod "liveness-26ade6e3-db70-4c52-b71d-856c2f01592c" in namespace "container-probe-507" to be "not pending"
May 16 15:08:51.952: INFO: Pod "liveness-26ade6e3-db70-4c52-b71d-856c2f01592c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369771ms
May 16 15:08:53.956: INFO: Pod "liveness-26ade6e3-db70-4c52-b71d-856c2f01592c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007425571s
May 16 15:08:53.956: INFO: Pod "liveness-26ade6e3-db70-4c52-b71d-856c2f01592c" satisfied condition "not pending"
May 16 15:08:53.956: INFO: Started pod liveness-26ade6e3-db70-4c52-b71d-856c2f01592c in namespace container-probe-507
STEP: checking the pod's current state and verifying that restartCount is present 05/16/23 15:08:53.956
May 16 15:08:53.959: INFO: Initial restart count of pod liveness-26ade6e3-db70-4c52-b71d-856c2f01592c is 0
STEP: deleting the pod 05/16/23 15:12:54.466
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May 16 15:12:54.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-507" for this suite. 05/16/23 15:12:54.48
------------------------------
â€¢ [SLOW TEST] [242.582 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:08:51.904
    May 16 15:08:51.904: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename container-probe 05/16/23 15:08:51.905
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:08:51.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:08:51.925
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-26ade6e3-db70-4c52-b71d-856c2f01592c in namespace container-probe-507 05/16/23 15:08:51.928
    May 16 15:08:51.949: INFO: Waiting up to 5m0s for pod "liveness-26ade6e3-db70-4c52-b71d-856c2f01592c" in namespace "container-probe-507" to be "not pending"
    May 16 15:08:51.952: INFO: Pod "liveness-26ade6e3-db70-4c52-b71d-856c2f01592c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.369771ms
    May 16 15:08:53.956: INFO: Pod "liveness-26ade6e3-db70-4c52-b71d-856c2f01592c": Phase="Running", Reason="", readiness=true. Elapsed: 2.007425571s
    May 16 15:08:53.956: INFO: Pod "liveness-26ade6e3-db70-4c52-b71d-856c2f01592c" satisfied condition "not pending"
    May 16 15:08:53.956: INFO: Started pod liveness-26ade6e3-db70-4c52-b71d-856c2f01592c in namespace container-probe-507
    STEP: checking the pod's current state and verifying that restartCount is present 05/16/23 15:08:53.956
    May 16 15:08:53.959: INFO: Initial restart count of pod liveness-26ade6e3-db70-4c52-b71d-856c2f01592c is 0
    STEP: deleting the pod 05/16/23 15:12:54.466
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May 16 15:12:54.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-507" for this suite. 05/16/23 15:12:54.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:12:54.487
May 16 15:12:54.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename var-expansion 05/16/23 15:12:54.488
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:12:54.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:12:54.517
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 05/16/23 15:12:54.521
May 16 15:12:54.539: INFO: Waiting up to 5m0s for pod "var-expansion-492945c1-0d36-4271-92de-31a77d3aaf32" in namespace "var-expansion-8314" to be "Succeeded or Failed"
May 16 15:12:54.542: INFO: Pod "var-expansion-492945c1-0d36-4271-92de-31a77d3aaf32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.396261ms
May 16 15:12:56.546: INFO: Pod "var-expansion-492945c1-0d36-4271-92de-31a77d3aaf32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007020452s
May 16 15:12:58.545: INFO: Pod "var-expansion-492945c1-0d36-4271-92de-31a77d3aaf32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005816769s
STEP: Saw pod success 05/16/23 15:12:58.545
May 16 15:12:58.545: INFO: Pod "var-expansion-492945c1-0d36-4271-92de-31a77d3aaf32" satisfied condition "Succeeded or Failed"
May 16 15:12:58.548: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod var-expansion-492945c1-0d36-4271-92de-31a77d3aaf32 container dapi-container: <nil>
STEP: delete the pod 05/16/23 15:12:58.565
May 16 15:12:58.574: INFO: Waiting for pod var-expansion-492945c1-0d36-4271-92de-31a77d3aaf32 to disappear
May 16 15:12:58.578: INFO: Pod var-expansion-492945c1-0d36-4271-92de-31a77d3aaf32 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May 16 15:12:58.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8314" for this suite. 05/16/23 15:12:58.582
------------------------------
â€¢ [4.101 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:12:54.487
    May 16 15:12:54.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename var-expansion 05/16/23 15:12:54.488
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:12:54.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:12:54.517
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 05/16/23 15:12:54.521
    May 16 15:12:54.539: INFO: Waiting up to 5m0s for pod "var-expansion-492945c1-0d36-4271-92de-31a77d3aaf32" in namespace "var-expansion-8314" to be "Succeeded or Failed"
    May 16 15:12:54.542: INFO: Pod "var-expansion-492945c1-0d36-4271-92de-31a77d3aaf32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.396261ms
    May 16 15:12:56.546: INFO: Pod "var-expansion-492945c1-0d36-4271-92de-31a77d3aaf32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007020452s
    May 16 15:12:58.545: INFO: Pod "var-expansion-492945c1-0d36-4271-92de-31a77d3aaf32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005816769s
    STEP: Saw pod success 05/16/23 15:12:58.545
    May 16 15:12:58.545: INFO: Pod "var-expansion-492945c1-0d36-4271-92de-31a77d3aaf32" satisfied condition "Succeeded or Failed"
    May 16 15:12:58.548: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod var-expansion-492945c1-0d36-4271-92de-31a77d3aaf32 container dapi-container: <nil>
    STEP: delete the pod 05/16/23 15:12:58.565
    May 16 15:12:58.574: INFO: Waiting for pod var-expansion-492945c1-0d36-4271-92de-31a77d3aaf32 to disappear
    May 16 15:12:58.578: INFO: Pod var-expansion-492945c1-0d36-4271-92de-31a77d3aaf32 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May 16 15:12:58.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8314" for this suite. 05/16/23 15:12:58.582
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:12:58.588
May 16 15:12:58.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename limitrange 05/16/23 15:12:58.589
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:12:58.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:12:58.671
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 05/16/23 15:12:58.674
STEP: Setting up watch 05/16/23 15:12:58.674
STEP: Submitting a LimitRange 05/16/23 15:12:58.779
STEP: Verifying LimitRange creation was observed 05/16/23 15:12:58.784
STEP: Fetching the LimitRange to ensure it has proper values 05/16/23 15:12:58.784
May 16 15:12:58.787: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 16 15:12:58.787: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 05/16/23 15:12:58.787
STEP: Ensuring Pod has resource requirements applied from LimitRange 05/16/23 15:12:58.794
May 16 15:12:58.796: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 16 15:12:58.796: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 05/16/23 15:12:58.796
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 05/16/23 15:12:58.805
May 16 15:12:58.810: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May 16 15:12:58.810: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 05/16/23 15:12:58.81
STEP: Failing to create a Pod with more than max resources 05/16/23 15:12:58.813
STEP: Updating a LimitRange 05/16/23 15:12:58.819
STEP: Verifying LimitRange updating is effective 05/16/23 15:12:58.825
STEP: Creating a Pod with less than former min resources 05/16/23 15:13:00.829
STEP: Failing to create a Pod with more than max resources 05/16/23 15:13:00.835
STEP: Deleting a LimitRange 05/16/23 15:13:00.845
STEP: Verifying the LimitRange was deleted 05/16/23 15:13:00.858
May 16 15:13:05.862: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 05/16/23 15:13:05.862
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
May 16 15:13:05.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-5014" for this suite. 05/16/23 15:13:05.875
------------------------------
â€¢ [SLOW TEST] [7.293 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:12:58.588
    May 16 15:12:58.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename limitrange 05/16/23 15:12:58.589
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:12:58.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:12:58.671
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 05/16/23 15:12:58.674
    STEP: Setting up watch 05/16/23 15:12:58.674
    STEP: Submitting a LimitRange 05/16/23 15:12:58.779
    STEP: Verifying LimitRange creation was observed 05/16/23 15:12:58.784
    STEP: Fetching the LimitRange to ensure it has proper values 05/16/23 15:12:58.784
    May 16 15:12:58.787: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    May 16 15:12:58.787: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 05/16/23 15:12:58.787
    STEP: Ensuring Pod has resource requirements applied from LimitRange 05/16/23 15:12:58.794
    May 16 15:12:58.796: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    May 16 15:12:58.796: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 05/16/23 15:12:58.796
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 05/16/23 15:12:58.805
    May 16 15:12:58.810: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    May 16 15:12:58.810: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 05/16/23 15:12:58.81
    STEP: Failing to create a Pod with more than max resources 05/16/23 15:12:58.813
    STEP: Updating a LimitRange 05/16/23 15:12:58.819
    STEP: Verifying LimitRange updating is effective 05/16/23 15:12:58.825
    STEP: Creating a Pod with less than former min resources 05/16/23 15:13:00.829
    STEP: Failing to create a Pod with more than max resources 05/16/23 15:13:00.835
    STEP: Deleting a LimitRange 05/16/23 15:13:00.845
    STEP: Verifying the LimitRange was deleted 05/16/23 15:13:00.858
    May 16 15:13:05.862: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 05/16/23 15:13:05.862
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    May 16 15:13:05.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-5014" for this suite. 05/16/23 15:13:05.875
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:13:05.881
May 16 15:13:05.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename hostport 05/16/23 15:13:05.882
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:05.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:13:05.904
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 05/16/23 15:13:05.921
May 16 15:13:05.937: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2305" to be "running and ready"
May 16 15:13:05.956: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.812283ms
May 16 15:13:05.956: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 16 15:13:07.961: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.023342692s
May 16 15:13:07.961: INFO: The phase of Pod pod1 is Running (Ready = true)
May 16 15:13:07.961: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.212.246 on the node which pod1 resides and expect scheduled 05/16/23 15:13:07.961
May 16 15:13:07.969: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2305" to be "running and ready"
May 16 15:13:07.971: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.440611ms
May 16 15:13:07.971: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 16 15:13:09.976: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.007072336s
May 16 15:13:09.976: INFO: The phase of Pod pod2 is Running (Ready = false)
May 16 15:13:11.975: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.006416541s
May 16 15:13:11.975: INFO: The phase of Pod pod2 is Running (Ready = true)
May 16 15:13:11.975: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.212.246 but use UDP protocol on the node which pod2 resides 05/16/23 15:13:11.975
May 16 15:13:11.985: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2305" to be "running and ready"
May 16 15:13:11.987: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.724421ms
May 16 15:13:11.987: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
May 16 15:13:13.992: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.007246438s
May 16 15:13:13.992: INFO: The phase of Pod pod3 is Running (Ready = false)
May 16 15:13:15.991: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.006767478s
May 16 15:13:15.991: INFO: The phase of Pod pod3 is Running (Ready = true)
May 16 15:13:15.991: INFO: Pod "pod3" satisfied condition "running and ready"
May 16 15:13:15.998: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2305" to be "running and ready"
May 16 15:13:16.001: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.693755ms
May 16 15:13:16.001: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
May 16 15:13:18.006: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.007279326s
May 16 15:13:18.006: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
May 16 15:13:18.006: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 05/16/23 15:13:18.009
May 16 15:13:18.009: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.212.246 http://127.0.0.1:54323/hostname] Namespace:hostport-2305 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:13:18.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:13:18.009: INFO: ExecWithOptions: Clientset creation
May 16 15:13:18.009: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-2305/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.212.246+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.212.246, port: 54323 05/16/23 15:13:18.107
May 16 15:13:18.107: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.212.246:54323/hostname] Namespace:hostport-2305 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:13:18.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:13:18.107: INFO: ExecWithOptions: Clientset creation
May 16 15:13:18.107: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-2305/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.212.246%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.212.246, port: 54323 UDP 05/16/23 15:13:18.181
May 16 15:13:18.181: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.212.246 54323] Namespace:hostport-2305 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:13:18.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:13:18.182: INFO: ExecWithOptions: Clientset creation
May 16 15:13:18.182: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-2305/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.212.246+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
May 16 15:13:23.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-2305" for this suite. 05/16/23 15:13:23.264
------------------------------
â€¢ [SLOW TEST] [17.388 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:13:05.881
    May 16 15:13:05.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename hostport 05/16/23 15:13:05.882
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:05.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:13:05.904
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 05/16/23 15:13:05.921
    May 16 15:13:05.937: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2305" to be "running and ready"
    May 16 15:13:05.956: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.812283ms
    May 16 15:13:05.956: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:13:07.961: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.023342692s
    May 16 15:13:07.961: INFO: The phase of Pod pod1 is Running (Ready = true)
    May 16 15:13:07.961: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.212.246 on the node which pod1 resides and expect scheduled 05/16/23 15:13:07.961
    May 16 15:13:07.969: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2305" to be "running and ready"
    May 16 15:13:07.971: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.440611ms
    May 16 15:13:07.971: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:13:09.976: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.007072336s
    May 16 15:13:09.976: INFO: The phase of Pod pod2 is Running (Ready = false)
    May 16 15:13:11.975: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.006416541s
    May 16 15:13:11.975: INFO: The phase of Pod pod2 is Running (Ready = true)
    May 16 15:13:11.975: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.212.246 but use UDP protocol on the node which pod2 resides 05/16/23 15:13:11.975
    May 16 15:13:11.985: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2305" to be "running and ready"
    May 16 15:13:11.987: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.724421ms
    May 16 15:13:11.987: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:13:13.992: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.007246438s
    May 16 15:13:13.992: INFO: The phase of Pod pod3 is Running (Ready = false)
    May 16 15:13:15.991: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.006767478s
    May 16 15:13:15.991: INFO: The phase of Pod pod3 is Running (Ready = true)
    May 16 15:13:15.991: INFO: Pod "pod3" satisfied condition "running and ready"
    May 16 15:13:15.998: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2305" to be "running and ready"
    May 16 15:13:16.001: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.693755ms
    May 16 15:13:16.001: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:13:18.006: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.007279326s
    May 16 15:13:18.006: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    May 16 15:13:18.006: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 05/16/23 15:13:18.009
    May 16 15:13:18.009: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.212.246 http://127.0.0.1:54323/hostname] Namespace:hostport-2305 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:13:18.009: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:13:18.009: INFO: ExecWithOptions: Clientset creation
    May 16 15:13:18.009: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-2305/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.212.246+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.212.246, port: 54323 05/16/23 15:13:18.107
    May 16 15:13:18.107: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.212.246:54323/hostname] Namespace:hostport-2305 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:13:18.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:13:18.107: INFO: ExecWithOptions: Clientset creation
    May 16 15:13:18.107: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-2305/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.212.246%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.212.246, port: 54323 UDP 05/16/23 15:13:18.181
    May 16 15:13:18.181: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.212.246 54323] Namespace:hostport-2305 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:13:18.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:13:18.182: INFO: ExecWithOptions: Clientset creation
    May 16 15:13:18.182: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-2305/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.212.246+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    May 16 15:13:23.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-2305" for this suite. 05/16/23 15:13:23.264
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:13:23.27
May 16 15:13:23.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 15:13:23.271
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:23.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:13:23.289
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-9df6d7e9-831f-4065-bef5-13ef44aee166 05/16/23 15:13:23.291
STEP: Creating a pod to test consume secrets 05/16/23 15:13:23.3
May 16 15:13:23.315: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f863158b-a1fe-48ec-b250-bdb65157f648" in namespace "projected-5650" to be "Succeeded or Failed"
May 16 15:13:23.320: INFO: Pod "pod-projected-secrets-f863158b-a1fe-48ec-b250-bdb65157f648": Phase="Pending", Reason="", readiness=false. Elapsed: 5.021557ms
May 16 15:13:25.324: INFO: Pod "pod-projected-secrets-f863158b-a1fe-48ec-b250-bdb65157f648": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009039205s
May 16 15:13:27.324: INFO: Pod "pod-projected-secrets-f863158b-a1fe-48ec-b250-bdb65157f648": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00899841s
STEP: Saw pod success 05/16/23 15:13:27.324
May 16 15:13:27.324: INFO: Pod "pod-projected-secrets-f863158b-a1fe-48ec-b250-bdb65157f648" satisfied condition "Succeeded or Failed"
May 16 15:13:27.328: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-secrets-f863158b-a1fe-48ec-b250-bdb65157f648 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/16/23 15:13:27.334
May 16 15:13:27.343: INFO: Waiting for pod pod-projected-secrets-f863158b-a1fe-48ec-b250-bdb65157f648 to disappear
May 16 15:13:27.346: INFO: Pod pod-projected-secrets-f863158b-a1fe-48ec-b250-bdb65157f648 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May 16 15:13:27.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5650" for this suite. 05/16/23 15:13:27.35
------------------------------
â€¢ [4.086 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:13:23.27
    May 16 15:13:23.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 15:13:23.271
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:23.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:13:23.289
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-9df6d7e9-831f-4065-bef5-13ef44aee166 05/16/23 15:13:23.291
    STEP: Creating a pod to test consume secrets 05/16/23 15:13:23.3
    May 16 15:13:23.315: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f863158b-a1fe-48ec-b250-bdb65157f648" in namespace "projected-5650" to be "Succeeded or Failed"
    May 16 15:13:23.320: INFO: Pod "pod-projected-secrets-f863158b-a1fe-48ec-b250-bdb65157f648": Phase="Pending", Reason="", readiness=false. Elapsed: 5.021557ms
    May 16 15:13:25.324: INFO: Pod "pod-projected-secrets-f863158b-a1fe-48ec-b250-bdb65157f648": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009039205s
    May 16 15:13:27.324: INFO: Pod "pod-projected-secrets-f863158b-a1fe-48ec-b250-bdb65157f648": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00899841s
    STEP: Saw pod success 05/16/23 15:13:27.324
    May 16 15:13:27.324: INFO: Pod "pod-projected-secrets-f863158b-a1fe-48ec-b250-bdb65157f648" satisfied condition "Succeeded or Failed"
    May 16 15:13:27.328: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-secrets-f863158b-a1fe-48ec-b250-bdb65157f648 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/16/23 15:13:27.334
    May 16 15:13:27.343: INFO: Waiting for pod pod-projected-secrets-f863158b-a1fe-48ec-b250-bdb65157f648 to disappear
    May 16 15:13:27.346: INFO: Pod pod-projected-secrets-f863158b-a1fe-48ec-b250-bdb65157f648 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May 16 15:13:27.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5650" for this suite. 05/16/23 15:13:27.35
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:13:27.356
May 16 15:13:27.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename svcaccounts 05/16/23 15:13:27.357
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:27.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:13:27.376
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
May 16 15:13:27.415: INFO: created pod pod-service-account-defaultsa
May 16 15:13:27.415: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 16 15:13:27.425: INFO: created pod pod-service-account-mountsa
May 16 15:13:27.425: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 16 15:13:27.433: INFO: created pod pod-service-account-nomountsa
May 16 15:13:27.433: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 16 15:13:27.442: INFO: created pod pod-service-account-defaultsa-mountspec
May 16 15:13:27.442: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 16 15:13:27.455: INFO: created pod pod-service-account-mountsa-mountspec
May 16 15:13:27.455: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 16 15:13:27.466: INFO: created pod pod-service-account-nomountsa-mountspec
May 16 15:13:27.466: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 16 15:13:27.480: INFO: created pod pod-service-account-defaultsa-nomountspec
May 16 15:13:27.480: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 16 15:13:27.497: INFO: created pod pod-service-account-mountsa-nomountspec
May 16 15:13:27.497: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 16 15:13:27.513: INFO: created pod pod-service-account-nomountsa-nomountspec
May 16 15:13:27.513: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May 16 15:13:27.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9678" for this suite. 05/16/23 15:13:27.52
------------------------------
â€¢ [0.184 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:13:27.356
    May 16 15:13:27.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename svcaccounts 05/16/23 15:13:27.357
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:27.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:13:27.376
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    May 16 15:13:27.415: INFO: created pod pod-service-account-defaultsa
    May 16 15:13:27.415: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    May 16 15:13:27.425: INFO: created pod pod-service-account-mountsa
    May 16 15:13:27.425: INFO: pod pod-service-account-mountsa service account token volume mount: true
    May 16 15:13:27.433: INFO: created pod pod-service-account-nomountsa
    May 16 15:13:27.433: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    May 16 15:13:27.442: INFO: created pod pod-service-account-defaultsa-mountspec
    May 16 15:13:27.442: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    May 16 15:13:27.455: INFO: created pod pod-service-account-mountsa-mountspec
    May 16 15:13:27.455: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    May 16 15:13:27.466: INFO: created pod pod-service-account-nomountsa-mountspec
    May 16 15:13:27.466: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    May 16 15:13:27.480: INFO: created pod pod-service-account-defaultsa-nomountspec
    May 16 15:13:27.480: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    May 16 15:13:27.497: INFO: created pod pod-service-account-mountsa-nomountspec
    May 16 15:13:27.497: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    May 16 15:13:27.513: INFO: created pod pod-service-account-nomountsa-nomountspec
    May 16 15:13:27.513: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May 16 15:13:27.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9678" for this suite. 05/16/23 15:13:27.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:13:27.541
May 16 15:13:27.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename custom-resource-definition 05/16/23 15:13:27.541
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:27.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:13:27.579
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
May 16 15:13:27.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 15:13:34.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-4503" for this suite. 05/16/23 15:13:34.348
------------------------------
â€¢ [SLOW TEST] [6.813 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:13:27.541
    May 16 15:13:27.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename custom-resource-definition 05/16/23 15:13:27.541
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:27.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:13:27.579
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    May 16 15:13:27.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 15:13:34.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-4503" for this suite. 05/16/23 15:13:34.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:13:34.355
May 16 15:13:34.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubectl 05/16/23 15:13:34.355
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:34.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:13:34.381
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 05/16/23 15:13:34.385
May 16 15:13:34.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2178 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
May 16 15:13:34.456: INFO: stderr: ""
May 16 15:13:34.456: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 05/16/23 15:13:34.456
May 16 15:13:34.456: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 16 15:13:34.456: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2178" to be "running and ready, or succeeded"
May 16 15:13:34.460: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.839832ms
May 16 15:13:34.460: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-10-0-212-246.eu-central-1.compute.internal' to be 'Running' but was 'Pending'
May 16 15:13:36.464: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.008151239s
May 16 15:13:36.464: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 16 15:13:36.464: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 05/16/23 15:13:36.464
May 16 15:13:36.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2178 logs logs-generator logs-generator'
May 16 15:13:36.522: INFO: stderr: ""
May 16 15:13:36.522: INFO: stdout: "I0516 15:13:35.065140       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/ggmx 227\nI0516 15:13:35.265481       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/bg2 341\nI0516 15:13:35.465780       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/g4j 410\nI0516 15:13:35.666100       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/w95q 464\nI0516 15:13:35.865236       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/gwv 412\nI0516 15:13:36.065541       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/9xfh 482\nI0516 15:13:36.265843       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/7lm9 324\nI0516 15:13:36.466145       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/5kj2 519\n"
STEP: limiting log lines 05/16/23 15:13:36.522
May 16 15:13:36.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2178 logs logs-generator logs-generator --tail=1'
May 16 15:13:36.572: INFO: stderr: ""
May 16 15:13:36.572: INFO: stdout: "I0516 15:13:36.466145       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/5kj2 519\n"
May 16 15:13:36.572: INFO: got output "I0516 15:13:36.466145       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/5kj2 519\n"
STEP: limiting log bytes 05/16/23 15:13:36.572
May 16 15:13:36.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2178 logs logs-generator logs-generator --limit-bytes=1'
May 16 15:13:36.627: INFO: stderr: ""
May 16 15:13:36.627: INFO: stdout: "I"
May 16 15:13:36.627: INFO: got output "I"
STEP: exposing timestamps 05/16/23 15:13:36.627
May 16 15:13:36.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2178 logs logs-generator logs-generator --tail=1 --timestamps'
May 16 15:13:36.682: INFO: stderr: ""
May 16 15:13:36.682: INFO: stdout: "2023-05-16T15:13:36.665486994Z I0516 15:13:36.665451       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/4sw6 318\n"
May 16 15:13:36.682: INFO: got output "2023-05-16T15:13:36.665486994Z I0516 15:13:36.665451       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/4sw6 318\n"
STEP: restricting to a time range 05/16/23 15:13:36.682
May 16 15:13:39.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2178 logs logs-generator logs-generator --since=1s'
May 16 15:13:39.234: INFO: stderr: ""
May 16 15:13:39.234: INFO: stdout: "I0516 15:13:38.265618       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/ffw9 530\nI0516 15:13:38.465922       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/thb 383\nI0516 15:13:38.665174       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/hkq 305\nI0516 15:13:38.865480       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/jzn5 406\nI0516 15:13:39.065782       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/jcrn 577\n"
May 16 15:13:39.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2178 logs logs-generator logs-generator --since=24h'
May 16 15:13:39.285: INFO: stderr: ""
May 16 15:13:39.285: INFO: stdout: "I0516 15:13:35.065140       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/ggmx 227\nI0516 15:13:35.265481       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/bg2 341\nI0516 15:13:35.465780       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/g4j 410\nI0516 15:13:35.666100       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/w95q 464\nI0516 15:13:35.865236       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/gwv 412\nI0516 15:13:36.065541       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/9xfh 482\nI0516 15:13:36.265843       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/7lm9 324\nI0516 15:13:36.466145       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/5kj2 519\nI0516 15:13:36.665451       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/4sw6 318\nI0516 15:13:36.865751       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/n56 540\nI0516 15:13:37.066016       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/cwx 310\nI0516 15:13:37.265252       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/z6r4 240\nI0516 15:13:37.465556       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/8zhl 305\nI0516 15:13:37.665856       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/mbht 595\nI0516 15:13:37.866082       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/z2q 275\nI0516 15:13:38.065322       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/dtc 399\nI0516 15:13:38.265618       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/ffw9 530\nI0516 15:13:38.465922       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/thb 383\nI0516 15:13:38.665174       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/hkq 305\nI0516 15:13:38.865480       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/jzn5 406\nI0516 15:13:39.065782       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/jcrn 577\nI0516 15:13:39.266085       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/bkvw 387\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
May 16 15:13:39.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2178 delete pod logs-generator'
May 16 15:13:40.704: INFO: stderr: ""
May 16 15:13:40.704: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 16 15:13:40.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2178" for this suite. 05/16/23 15:13:40.709
------------------------------
â€¢ [SLOW TEST] [6.363 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:13:34.355
    May 16 15:13:34.355: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubectl 05/16/23 15:13:34.355
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:34.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:13:34.381
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 05/16/23 15:13:34.385
    May 16 15:13:34.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2178 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    May 16 15:13:34.456: INFO: stderr: ""
    May 16 15:13:34.456: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 05/16/23 15:13:34.456
    May 16 15:13:34.456: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    May 16 15:13:34.456: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2178" to be "running and ready, or succeeded"
    May 16 15:13:34.460: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.839832ms
    May 16 15:13:34.460: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-10-0-212-246.eu-central-1.compute.internal' to be 'Running' but was 'Pending'
    May 16 15:13:36.464: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.008151239s
    May 16 15:13:36.464: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    May 16 15:13:36.464: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 05/16/23 15:13:36.464
    May 16 15:13:36.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2178 logs logs-generator logs-generator'
    May 16 15:13:36.522: INFO: stderr: ""
    May 16 15:13:36.522: INFO: stdout: "I0516 15:13:35.065140       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/ggmx 227\nI0516 15:13:35.265481       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/bg2 341\nI0516 15:13:35.465780       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/g4j 410\nI0516 15:13:35.666100       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/w95q 464\nI0516 15:13:35.865236       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/gwv 412\nI0516 15:13:36.065541       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/9xfh 482\nI0516 15:13:36.265843       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/7lm9 324\nI0516 15:13:36.466145       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/5kj2 519\n"
    STEP: limiting log lines 05/16/23 15:13:36.522
    May 16 15:13:36.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2178 logs logs-generator logs-generator --tail=1'
    May 16 15:13:36.572: INFO: stderr: ""
    May 16 15:13:36.572: INFO: stdout: "I0516 15:13:36.466145       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/5kj2 519\n"
    May 16 15:13:36.572: INFO: got output "I0516 15:13:36.466145       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/5kj2 519\n"
    STEP: limiting log bytes 05/16/23 15:13:36.572
    May 16 15:13:36.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2178 logs logs-generator logs-generator --limit-bytes=1'
    May 16 15:13:36.627: INFO: stderr: ""
    May 16 15:13:36.627: INFO: stdout: "I"
    May 16 15:13:36.627: INFO: got output "I"
    STEP: exposing timestamps 05/16/23 15:13:36.627
    May 16 15:13:36.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2178 logs logs-generator logs-generator --tail=1 --timestamps'
    May 16 15:13:36.682: INFO: stderr: ""
    May 16 15:13:36.682: INFO: stdout: "2023-05-16T15:13:36.665486994Z I0516 15:13:36.665451       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/4sw6 318\n"
    May 16 15:13:36.682: INFO: got output "2023-05-16T15:13:36.665486994Z I0516 15:13:36.665451       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/4sw6 318\n"
    STEP: restricting to a time range 05/16/23 15:13:36.682
    May 16 15:13:39.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2178 logs logs-generator logs-generator --since=1s'
    May 16 15:13:39.234: INFO: stderr: ""
    May 16 15:13:39.234: INFO: stdout: "I0516 15:13:38.265618       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/ffw9 530\nI0516 15:13:38.465922       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/thb 383\nI0516 15:13:38.665174       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/hkq 305\nI0516 15:13:38.865480       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/jzn5 406\nI0516 15:13:39.065782       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/jcrn 577\n"
    May 16 15:13:39.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2178 logs logs-generator logs-generator --since=24h'
    May 16 15:13:39.285: INFO: stderr: ""
    May 16 15:13:39.285: INFO: stdout: "I0516 15:13:35.065140       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/ggmx 227\nI0516 15:13:35.265481       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/bg2 341\nI0516 15:13:35.465780       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/g4j 410\nI0516 15:13:35.666100       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/w95q 464\nI0516 15:13:35.865236       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/gwv 412\nI0516 15:13:36.065541       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/9xfh 482\nI0516 15:13:36.265843       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/7lm9 324\nI0516 15:13:36.466145       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/5kj2 519\nI0516 15:13:36.665451       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/4sw6 318\nI0516 15:13:36.865751       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/n56 540\nI0516 15:13:37.066016       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/cwx 310\nI0516 15:13:37.265252       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/z6r4 240\nI0516 15:13:37.465556       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/8zhl 305\nI0516 15:13:37.665856       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/mbht 595\nI0516 15:13:37.866082       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/z2q 275\nI0516 15:13:38.065322       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/dtc 399\nI0516 15:13:38.265618       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/ffw9 530\nI0516 15:13:38.465922       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/thb 383\nI0516 15:13:38.665174       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/hkq 305\nI0516 15:13:38.865480       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/jzn5 406\nI0516 15:13:39.065782       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/jcrn 577\nI0516 15:13:39.266085       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/bkvw 387\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    May 16 15:13:39.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-2178 delete pod logs-generator'
    May 16 15:13:40.704: INFO: stderr: ""
    May 16 15:13:40.704: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 16 15:13:40.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2178" for this suite. 05/16/23 15:13:40.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:13:40.719
May 16 15:13:40.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename namespaces 05/16/23 15:13:40.72
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:40.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:13:40.74
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 05/16/23 15:13:40.744
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:40.77
STEP: Creating a service in the namespace 05/16/23 15:13:40.775
STEP: Deleting the namespace 05/16/23 15:13:40.796
STEP: Waiting for the namespace to be removed. 05/16/23 15:13:40.811
STEP: Recreating the namespace 05/16/23 15:13:46.816
STEP: Verifying there is no service in the namespace 05/16/23 15:13:46.832
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 15:13:46.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4002" for this suite. 05/16/23 15:13:46.849
STEP: Destroying namespace "nsdeletetest-1725" for this suite. 05/16/23 15:13:46.858
May 16 15:13:46.861: INFO: Namespace nsdeletetest-1725 was already deleted
STEP: Destroying namespace "nsdeletetest-6351" for this suite. 05/16/23 15:13:46.861
------------------------------
â€¢ [SLOW TEST] [6.148 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:13:40.719
    May 16 15:13:40.719: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename namespaces 05/16/23 15:13:40.72
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:40.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:13:40.74
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 05/16/23 15:13:40.744
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:40.77
    STEP: Creating a service in the namespace 05/16/23 15:13:40.775
    STEP: Deleting the namespace 05/16/23 15:13:40.796
    STEP: Waiting for the namespace to be removed. 05/16/23 15:13:40.811
    STEP: Recreating the namespace 05/16/23 15:13:46.816
    STEP: Verifying there is no service in the namespace 05/16/23 15:13:46.832
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 15:13:46.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4002" for this suite. 05/16/23 15:13:46.849
    STEP: Destroying namespace "nsdeletetest-1725" for this suite. 05/16/23 15:13:46.858
    May 16 15:13:46.861: INFO: Namespace nsdeletetest-1725 was already deleted
    STEP: Destroying namespace "nsdeletetest-6351" for this suite. 05/16/23 15:13:46.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:13:46.868
May 16 15:13:46.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename replication-controller 05/16/23 15:13:46.869
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:46.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:13:46.895
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 05/16/23 15:13:46.898
W0516 15:13:46.905667      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: When the matched label of one of its pods change 05/16/23 15:13:46.905
May 16 15:13:46.909: INFO: Pod name pod-release: Found 0 pods out of 1
May 16 15:13:51.912: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 05/16/23 15:13:51.922
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May 16 15:13:52.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2966" for this suite. 05/16/23 15:13:52.934
------------------------------
â€¢ [SLOW TEST] [6.072 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:13:46.868
    May 16 15:13:46.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename replication-controller 05/16/23 15:13:46.869
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:46.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:13:46.895
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 05/16/23 15:13:46.898
    W0516 15:13:46.905667      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: When the matched label of one of its pods change 05/16/23 15:13:46.905
    May 16 15:13:46.909: INFO: Pod name pod-release: Found 0 pods out of 1
    May 16 15:13:51.912: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 05/16/23 15:13:51.922
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May 16 15:13:52.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2966" for this suite. 05/16/23 15:13:52.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:13:52.94
May 16 15:13:52.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename security-context-test 05/16/23 15:13:52.941
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:52.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:13:52.961
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
May 16 15:13:52.976: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-89c93ab4-143c-4af3-a6c9-06578badbceb" in namespace "security-context-test-324" to be "Succeeded or Failed"
May 16 15:13:52.980: INFO: Pod "alpine-nnp-false-89c93ab4-143c-4af3-a6c9-06578badbceb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.813994ms
May 16 15:13:54.984: INFO: Pod "alpine-nnp-false-89c93ab4-143c-4af3-a6c9-06578badbceb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007649351s
May 16 15:13:56.984: INFO: Pod "alpine-nnp-false-89c93ab4-143c-4af3-a6c9-06578badbceb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00789517s
May 16 15:13:58.983: INFO: Pod "alpine-nnp-false-89c93ab4-143c-4af3-a6c9-06578badbceb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007561686s
May 16 15:13:58.984: INFO: Pod "alpine-nnp-false-89c93ab4-143c-4af3-a6c9-06578badbceb" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
May 16 15:13:58.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-324" for this suite. 05/16/23 15:13:58.993
------------------------------
â€¢ [SLOW TEST] [6.059 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:13:52.94
    May 16 15:13:52.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename security-context-test 05/16/23 15:13:52.941
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:52.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:13:52.961
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    May 16 15:13:52.976: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-89c93ab4-143c-4af3-a6c9-06578badbceb" in namespace "security-context-test-324" to be "Succeeded or Failed"
    May 16 15:13:52.980: INFO: Pod "alpine-nnp-false-89c93ab4-143c-4af3-a6c9-06578badbceb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.813994ms
    May 16 15:13:54.984: INFO: Pod "alpine-nnp-false-89c93ab4-143c-4af3-a6c9-06578badbceb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007649351s
    May 16 15:13:56.984: INFO: Pod "alpine-nnp-false-89c93ab4-143c-4af3-a6c9-06578badbceb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00789517s
    May 16 15:13:58.983: INFO: Pod "alpine-nnp-false-89c93ab4-143c-4af3-a6c9-06578badbceb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007561686s
    May 16 15:13:58.984: INFO: Pod "alpine-nnp-false-89c93ab4-143c-4af3-a6c9-06578badbceb" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    May 16 15:13:58.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-324" for this suite. 05/16/23 15:13:58.993
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:13:58.999
May 16 15:13:59.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename services 05/16/23 15:13:59
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:59.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:13:59.019
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4474 05/16/23 15:13:59.022
STEP: changing the ExternalName service to type=NodePort 05/16/23 15:13:59.028
STEP: creating replication controller externalname-service in namespace services-4474 05/16/23 15:13:59.058
I0516 15:13:59.065807      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4474, replica count: 2
I0516 15:14:02.116928      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 16 15:14:02.116: INFO: Creating new exec pod
May 16 15:14:02.126: INFO: Waiting up to 5m0s for pod "execpod9nhp5" in namespace "services-4474" to be "running"
May 16 15:14:02.128: INFO: Pod "execpod9nhp5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.741577ms
May 16 15:14:04.132: INFO: Pod "execpod9nhp5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006152186s
May 16 15:14:04.132: INFO: Pod "execpod9nhp5" satisfied condition "running"
May 16 15:14:05.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-4474 exec execpod9nhp5 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
May 16 15:14:05.258: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 16 15:14:05.258: INFO: stdout: ""
May 16 15:14:05.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-4474 exec execpod9nhp5 -- /bin/sh -x -c nc -v -z -w 2 172.30.13.4 80'
May 16 15:14:05.355: INFO: stderr: "+ nc -v -z -w 2 172.30.13.4 80\nConnection to 172.30.13.4 80 port [tcp/http] succeeded!\n"
May 16 15:14:05.355: INFO: stdout: ""
May 16 15:14:05.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-4474 exec execpod9nhp5 -- /bin/sh -x -c nc -v -z -w 2 10.0.161.164 30578'
May 16 15:14:05.461: INFO: stderr: "+ nc -v -z -w 2 10.0.161.164 30578\nConnection to 10.0.161.164 30578 port [tcp/*] succeeded!\n"
May 16 15:14:05.461: INFO: stdout: ""
May 16 15:14:05.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-4474 exec execpod9nhp5 -- /bin/sh -x -c nc -v -z -w 2 10.0.212.246 30578'
May 16 15:14:05.561: INFO: stderr: "+ nc -v -z -w 2 10.0.212.246 30578\nConnection to 10.0.212.246 30578 port [tcp/*] succeeded!\n"
May 16 15:14:05.561: INFO: stdout: ""
May 16 15:14:05.561: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 16 15:14:05.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4474" for this suite. 05/16/23 15:14:05.605
------------------------------
â€¢ [SLOW TEST] [6.618 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:13:58.999
    May 16 15:13:59.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename services 05/16/23 15:13:59
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:13:59.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:13:59.019
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4474 05/16/23 15:13:59.022
    STEP: changing the ExternalName service to type=NodePort 05/16/23 15:13:59.028
    STEP: creating replication controller externalname-service in namespace services-4474 05/16/23 15:13:59.058
    I0516 15:13:59.065807      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4474, replica count: 2
    I0516 15:14:02.116928      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 16 15:14:02.116: INFO: Creating new exec pod
    May 16 15:14:02.126: INFO: Waiting up to 5m0s for pod "execpod9nhp5" in namespace "services-4474" to be "running"
    May 16 15:14:02.128: INFO: Pod "execpod9nhp5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.741577ms
    May 16 15:14:04.132: INFO: Pod "execpod9nhp5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006152186s
    May 16 15:14:04.132: INFO: Pod "execpod9nhp5" satisfied condition "running"
    May 16 15:14:05.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-4474 exec execpod9nhp5 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    May 16 15:14:05.258: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    May 16 15:14:05.258: INFO: stdout: ""
    May 16 15:14:05.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-4474 exec execpod9nhp5 -- /bin/sh -x -c nc -v -z -w 2 172.30.13.4 80'
    May 16 15:14:05.355: INFO: stderr: "+ nc -v -z -w 2 172.30.13.4 80\nConnection to 172.30.13.4 80 port [tcp/http] succeeded!\n"
    May 16 15:14:05.355: INFO: stdout: ""
    May 16 15:14:05.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-4474 exec execpod9nhp5 -- /bin/sh -x -c nc -v -z -w 2 10.0.161.164 30578'
    May 16 15:14:05.461: INFO: stderr: "+ nc -v -z -w 2 10.0.161.164 30578\nConnection to 10.0.161.164 30578 port [tcp/*] succeeded!\n"
    May 16 15:14:05.461: INFO: stdout: ""
    May 16 15:14:05.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-4474 exec execpod9nhp5 -- /bin/sh -x -c nc -v -z -w 2 10.0.212.246 30578'
    May 16 15:14:05.561: INFO: stderr: "+ nc -v -z -w 2 10.0.212.246 30578\nConnection to 10.0.212.246 30578 port [tcp/*] succeeded!\n"
    May 16 15:14:05.561: INFO: stdout: ""
    May 16 15:14:05.561: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 16 15:14:05.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4474" for this suite. 05/16/23 15:14:05.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:14:05.618
May 16 15:14:05.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubelet-test 05/16/23 15:14:05.619
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:05.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:05.646
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
May 16 15:14:10.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7297" for this suite. 05/16/23 15:14:10.677
------------------------------
â€¢ [SLOW TEST] [5.065 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:14:05.618
    May 16 15:14:05.618: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubelet-test 05/16/23 15:14:05.619
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:05.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:05.646
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    May 16 15:14:10.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7297" for this suite. 05/16/23 15:14:10.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:14:10.684
May 16 15:14:10.684: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename webhook 05/16/23 15:14:10.684
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:10.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:10.706
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/16/23 15:14:10.729
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 15:14:11.284
STEP: Deploying the webhook pod 05/16/23 15:14:11.294
STEP: Wait for the deployment to be ready 05/16/23 15:14:11.306
May 16 15:14:11.314: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 16 15:14:13.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 15, 14, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 14, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 14, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 14, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/16/23 15:14:15.327
STEP: Verifying the service has paired with the endpoint 05/16/23 15:14:15.337
May 16 15:14:16.337: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 05/16/23 15:14:16.392
STEP: Creating a configMap that should be mutated 05/16/23 15:14:16.402
STEP: Deleting the collection of validation webhooks 05/16/23 15:14:16.422
STEP: Creating a configMap that should not be mutated 05/16/23 15:14:16.47
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 15:14:16.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5053" for this suite. 05/16/23 15:14:16.529
STEP: Destroying namespace "webhook-5053-markers" for this suite. 05/16/23 15:14:16.537
------------------------------
â€¢ [SLOW TEST] [5.868 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:14:10.684
    May 16 15:14:10.684: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename webhook 05/16/23 15:14:10.684
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:10.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:10.706
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/16/23 15:14:10.729
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 15:14:11.284
    STEP: Deploying the webhook pod 05/16/23 15:14:11.294
    STEP: Wait for the deployment to be ready 05/16/23 15:14:11.306
    May 16 15:14:11.314: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May 16 15:14:13.323: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 15, 14, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 14, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 14, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 14, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/16/23 15:14:15.327
    STEP: Verifying the service has paired with the endpoint 05/16/23 15:14:15.337
    May 16 15:14:16.337: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 05/16/23 15:14:16.392
    STEP: Creating a configMap that should be mutated 05/16/23 15:14:16.402
    STEP: Deleting the collection of validation webhooks 05/16/23 15:14:16.422
    STEP: Creating a configMap that should not be mutated 05/16/23 15:14:16.47
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 15:14:16.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5053" for this suite. 05/16/23 15:14:16.529
    STEP: Destroying namespace "webhook-5053-markers" for this suite. 05/16/23 15:14:16.537
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:14:16.552
May 16 15:14:16.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubectl 05/16/23 15:14:16.553
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:16.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:16.578
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/16/23 15:14:16.586
May 16 15:14:16.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6273 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
May 16 15:14:16.649: INFO: stderr: ""
May 16 15:14:16.649: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 05/16/23 15:14:16.649
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
May 16 15:14:16.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6273 delete pods e2e-test-httpd-pod'
May 16 15:14:18.842: INFO: stderr: ""
May 16 15:14:18.842: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 16 15:14:18.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6273" for this suite. 05/16/23 15:14:18.846
------------------------------
â€¢ [2.303 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:14:16.552
    May 16 15:14:16.553: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubectl 05/16/23 15:14:16.553
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:16.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:16.578
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 05/16/23 15:14:16.586
    May 16 15:14:16.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6273 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    May 16 15:14:16.649: INFO: stderr: ""
    May 16 15:14:16.649: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 05/16/23 15:14:16.649
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    May 16 15:14:16.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6273 delete pods e2e-test-httpd-pod'
    May 16 15:14:18.842: INFO: stderr: ""
    May 16 15:14:18.842: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 16 15:14:18.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6273" for this suite. 05/16/23 15:14:18.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:14:18.856
May 16 15:14:18.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename emptydir 05/16/23 15:14:18.857
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:18.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:18.88
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 05/16/23 15:14:18.884
W0516 15:14:18.898934      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:14:18.899: INFO: Waiting up to 5m0s for pod "pod-ed80ea1a-65fc-48b7-8819-fa22d4d02997" in namespace "emptydir-888" to be "Succeeded or Failed"
May 16 15:14:18.908: INFO: Pod "pod-ed80ea1a-65fc-48b7-8819-fa22d4d02997": Phase="Pending", Reason="", readiness=false. Elapsed: 9.872164ms
May 16 15:14:20.912: INFO: Pod "pod-ed80ea1a-65fc-48b7-8819-fa22d4d02997": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013778867s
May 16 15:14:22.914: INFO: Pod "pod-ed80ea1a-65fc-48b7-8819-fa22d4d02997": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015669099s
STEP: Saw pod success 05/16/23 15:14:22.914
May 16 15:14:22.914: INFO: Pod "pod-ed80ea1a-65fc-48b7-8819-fa22d4d02997" satisfied condition "Succeeded or Failed"
May 16 15:14:22.917: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-ed80ea1a-65fc-48b7-8819-fa22d4d02997 container test-container: <nil>
STEP: delete the pod 05/16/23 15:14:22.922
May 16 15:14:22.932: INFO: Waiting for pod pod-ed80ea1a-65fc-48b7-8819-fa22d4d02997 to disappear
May 16 15:14:22.934: INFO: Pod pod-ed80ea1a-65fc-48b7-8819-fa22d4d02997 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 16 15:14:22.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-888" for this suite. 05/16/23 15:14:22.939
------------------------------
â€¢ [4.088 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:14:18.856
    May 16 15:14:18.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename emptydir 05/16/23 15:14:18.857
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:18.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:18.88
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 05/16/23 15:14:18.884
    W0516 15:14:18.898934      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:14:18.899: INFO: Waiting up to 5m0s for pod "pod-ed80ea1a-65fc-48b7-8819-fa22d4d02997" in namespace "emptydir-888" to be "Succeeded or Failed"
    May 16 15:14:18.908: INFO: Pod "pod-ed80ea1a-65fc-48b7-8819-fa22d4d02997": Phase="Pending", Reason="", readiness=false. Elapsed: 9.872164ms
    May 16 15:14:20.912: INFO: Pod "pod-ed80ea1a-65fc-48b7-8819-fa22d4d02997": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013778867s
    May 16 15:14:22.914: INFO: Pod "pod-ed80ea1a-65fc-48b7-8819-fa22d4d02997": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015669099s
    STEP: Saw pod success 05/16/23 15:14:22.914
    May 16 15:14:22.914: INFO: Pod "pod-ed80ea1a-65fc-48b7-8819-fa22d4d02997" satisfied condition "Succeeded or Failed"
    May 16 15:14:22.917: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-ed80ea1a-65fc-48b7-8819-fa22d4d02997 container test-container: <nil>
    STEP: delete the pod 05/16/23 15:14:22.922
    May 16 15:14:22.932: INFO: Waiting for pod pod-ed80ea1a-65fc-48b7-8819-fa22d4d02997 to disappear
    May 16 15:14:22.934: INFO: Pod pod-ed80ea1a-65fc-48b7-8819-fa22d4d02997 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 16 15:14:22.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-888" for this suite. 05/16/23 15:14:22.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:14:22.945
May 16 15:14:22.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename daemonsets 05/16/23 15:14:22.946
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:22.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:22.965
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
May 16 15:14:22.990: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 05/16/23 15:14:22.996
May 16 15:14:23.000: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 15:14:23.000: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 05/16/23 15:14:23
May 16 15:14:23.030: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 15:14:23.030: INFO: Node ip-10-0-212-246.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 15:14:24.033: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 15:14:24.033: INFO: Node ip-10-0-212-246.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 15:14:25.034: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 16 15:14:25.034: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 05/16/23 15:14:25.037
May 16 15:14:25.056: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 16 15:14:25.056: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
May 16 15:14:26.060: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 15:14:26.060: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 05/16/23 15:14:26.06
May 16 15:14:26.068: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 15:14:26.068: INFO: Node ip-10-0-212-246.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 15:14:27.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 15:14:27.074: INFO: Node ip-10-0-212-246.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 15:14:28.072: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 15:14:28.072: INFO: Node ip-10-0-212-246.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 15:14:29.072: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 16 15:14:29.072: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/16/23 15:14:29.078
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2210, will wait for the garbage collector to delete the pods 05/16/23 15:14:29.078
May 16 15:14:29.137: INFO: Deleting DaemonSet.extensions daemon-set took: 6.181014ms
May 16 15:14:29.238: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.820279ms
May 16 15:14:31.842: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 15:14:31.842: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 16 15:14:31.845: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"84357"},"items":null}

May 16 15:14:31.848: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"84357"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 15:14:31.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2210" for this suite. 05/16/23 15:14:31.883
------------------------------
â€¢ [SLOW TEST] [8.944 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:14:22.945
    May 16 15:14:22.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename daemonsets 05/16/23 15:14:22.946
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:22.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:22.965
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    May 16 15:14:22.990: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 05/16/23 15:14:22.996
    May 16 15:14:23.000: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 15:14:23.000: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 05/16/23 15:14:23
    May 16 15:14:23.030: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 15:14:23.030: INFO: Node ip-10-0-212-246.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 15:14:24.033: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 15:14:24.033: INFO: Node ip-10-0-212-246.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 15:14:25.034: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 16 15:14:25.034: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 05/16/23 15:14:25.037
    May 16 15:14:25.056: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 16 15:14:25.056: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    May 16 15:14:26.060: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 15:14:26.060: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 05/16/23 15:14:26.06
    May 16 15:14:26.068: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 15:14:26.068: INFO: Node ip-10-0-212-246.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 15:14:27.074: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 15:14:27.074: INFO: Node ip-10-0-212-246.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 15:14:28.072: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 15:14:28.072: INFO: Node ip-10-0-212-246.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 15:14:29.072: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 16 15:14:29.072: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/16/23 15:14:29.078
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2210, will wait for the garbage collector to delete the pods 05/16/23 15:14:29.078
    May 16 15:14:29.137: INFO: Deleting DaemonSet.extensions daemon-set took: 6.181014ms
    May 16 15:14:29.238: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.820279ms
    May 16 15:14:31.842: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 15:14:31.842: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May 16 15:14:31.845: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"84357"},"items":null}

    May 16 15:14:31.848: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"84357"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 15:14:31.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2210" for this suite. 05/16/23 15:14:31.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:14:31.89
May 16 15:14:31.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename endpointslice 05/16/23 15:14:31.891
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:31.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:31.917
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
May 16 15:14:33.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-6766" for this suite. 05/16/23 15:14:33.993
------------------------------
â€¢ [2.113 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:14:31.89
    May 16 15:14:31.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename endpointslice 05/16/23 15:14:31.891
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:31.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:31.917
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    May 16 15:14:33.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-6766" for this suite. 05/16/23 15:14:33.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:14:34.003
May 16 15:14:34.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename emptydir 05/16/23 15:14:34.004
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:34.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:34.026
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 05/16/23 15:14:34.032
W0516 15:14:34.047635      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:14:34.047: INFO: Waiting up to 5m0s for pod "pod-358ec9f8-f16d-430d-a740-865850adc3d2" in namespace "emptydir-3503" to be "Succeeded or Failed"
May 16 15:14:34.053: INFO: Pod "pod-358ec9f8-f16d-430d-a740-865850adc3d2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.469534ms
May 16 15:14:36.056: INFO: Pod "pod-358ec9f8-f16d-430d-a740-865850adc3d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009258328s
May 16 15:14:38.056: INFO: Pod "pod-358ec9f8-f16d-430d-a740-865850adc3d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009068174s
STEP: Saw pod success 05/16/23 15:14:38.056
May 16 15:14:38.056: INFO: Pod "pod-358ec9f8-f16d-430d-a740-865850adc3d2" satisfied condition "Succeeded or Failed"
May 16 15:14:38.059: INFO: Trying to get logs from node ip-10-0-212-246.eu-central-1.compute.internal pod pod-358ec9f8-f16d-430d-a740-865850adc3d2 container test-container: <nil>
STEP: delete the pod 05/16/23 15:14:38.064
May 16 15:14:38.074: INFO: Waiting for pod pod-358ec9f8-f16d-430d-a740-865850adc3d2 to disappear
May 16 15:14:38.076: INFO: Pod pod-358ec9f8-f16d-430d-a740-865850adc3d2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 16 15:14:38.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3503" for this suite. 05/16/23 15:14:38.08
------------------------------
â€¢ [4.091 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:14:34.003
    May 16 15:14:34.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename emptydir 05/16/23 15:14:34.004
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:34.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:34.026
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 05/16/23 15:14:34.032
    W0516 15:14:34.047635      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:14:34.047: INFO: Waiting up to 5m0s for pod "pod-358ec9f8-f16d-430d-a740-865850adc3d2" in namespace "emptydir-3503" to be "Succeeded or Failed"
    May 16 15:14:34.053: INFO: Pod "pod-358ec9f8-f16d-430d-a740-865850adc3d2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.469534ms
    May 16 15:14:36.056: INFO: Pod "pod-358ec9f8-f16d-430d-a740-865850adc3d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009258328s
    May 16 15:14:38.056: INFO: Pod "pod-358ec9f8-f16d-430d-a740-865850adc3d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009068174s
    STEP: Saw pod success 05/16/23 15:14:38.056
    May 16 15:14:38.056: INFO: Pod "pod-358ec9f8-f16d-430d-a740-865850adc3d2" satisfied condition "Succeeded or Failed"
    May 16 15:14:38.059: INFO: Trying to get logs from node ip-10-0-212-246.eu-central-1.compute.internal pod pod-358ec9f8-f16d-430d-a740-865850adc3d2 container test-container: <nil>
    STEP: delete the pod 05/16/23 15:14:38.064
    May 16 15:14:38.074: INFO: Waiting for pod pod-358ec9f8-f16d-430d-a740-865850adc3d2 to disappear
    May 16 15:14:38.076: INFO: Pod pod-358ec9f8-f16d-430d-a740-865850adc3d2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 16 15:14:38.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3503" for this suite. 05/16/23 15:14:38.08
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:14:38.095
May 16 15:14:38.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename events 05/16/23 15:14:38.096
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:38.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:38.113
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 05/16/23 15:14:38.116
May 16 15:14:38.125: INFO: created test-event-1
May 16 15:14:38.130: INFO: created test-event-2
May 16 15:14:38.134: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 05/16/23 15:14:38.134
STEP: delete collection of events 05/16/23 15:14:38.142
May 16 15:14:38.142: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 05/16/23 15:14:38.171
May 16 15:14:38.171: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
May 16 15:14:38.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-243" for this suite. 05/16/23 15:14:38.178
------------------------------
â€¢ [0.089 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:14:38.095
    May 16 15:14:38.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename events 05/16/23 15:14:38.096
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:38.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:38.113
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 05/16/23 15:14:38.116
    May 16 15:14:38.125: INFO: created test-event-1
    May 16 15:14:38.130: INFO: created test-event-2
    May 16 15:14:38.134: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 05/16/23 15:14:38.134
    STEP: delete collection of events 05/16/23 15:14:38.142
    May 16 15:14:38.142: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 05/16/23 15:14:38.171
    May 16 15:14:38.171: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    May 16 15:14:38.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-243" for this suite. 05/16/23 15:14:38.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:14:38.185
May 16 15:14:38.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename statefulset 05/16/23 15:14:38.186
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:38.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:38.212
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6673 05/16/23 15:14:38.215
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-6673 05/16/23 15:14:38.228
May 16 15:14:38.236: INFO: Found 0 stateful pods, waiting for 1
May 16 15:14:48.241: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 05/16/23 15:14:48.246
STEP: Getting /status 05/16/23 15:14:48.256
May 16 15:14:48.259: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 05/16/23 15:14:48.259
May 16 15:14:48.267: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 05/16/23 15:14:48.267
May 16 15:14:48.269: INFO: Observed &StatefulSet event: ADDED
May 16 15:14:48.269: INFO: Found Statefulset ss in namespace statefulset-6673 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 16 15:14:48.269: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 05/16/23 15:14:48.269
May 16 15:14:48.269: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May 16 15:14:48.274: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 05/16/23 15:14:48.274
May 16 15:14:48.275: INFO: Observed &StatefulSet event: ADDED
May 16 15:14:48.275: INFO: Observed Statefulset ss in namespace statefulset-6673 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 16 15:14:48.275: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May 16 15:14:48.275: INFO: Deleting all statefulset in ns statefulset-6673
May 16 15:14:48.279: INFO: Scaling statefulset ss to 0
May 16 15:14:58.294: INFO: Waiting for statefulset status.replicas updated to 0
May 16 15:14:58.297: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May 16 15:14:58.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6673" for this suite. 05/16/23 15:14:58.312
------------------------------
â€¢ [SLOW TEST] [20.133 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:14:38.185
    May 16 15:14:38.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename statefulset 05/16/23 15:14:38.186
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:38.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:38.212
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6673 05/16/23 15:14:38.215
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-6673 05/16/23 15:14:38.228
    May 16 15:14:38.236: INFO: Found 0 stateful pods, waiting for 1
    May 16 15:14:48.241: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 05/16/23 15:14:48.246
    STEP: Getting /status 05/16/23 15:14:48.256
    May 16 15:14:48.259: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 05/16/23 15:14:48.259
    May 16 15:14:48.267: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 05/16/23 15:14:48.267
    May 16 15:14:48.269: INFO: Observed &StatefulSet event: ADDED
    May 16 15:14:48.269: INFO: Found Statefulset ss in namespace statefulset-6673 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May 16 15:14:48.269: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 05/16/23 15:14:48.269
    May 16 15:14:48.269: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May 16 15:14:48.274: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 05/16/23 15:14:48.274
    May 16 15:14:48.275: INFO: Observed &StatefulSet event: ADDED
    May 16 15:14:48.275: INFO: Observed Statefulset ss in namespace statefulset-6673 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May 16 15:14:48.275: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May 16 15:14:48.275: INFO: Deleting all statefulset in ns statefulset-6673
    May 16 15:14:48.279: INFO: Scaling statefulset ss to 0
    May 16 15:14:58.294: INFO: Waiting for statefulset status.replicas updated to 0
    May 16 15:14:58.297: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May 16 15:14:58.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6673" for this suite. 05/16/23 15:14:58.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:14:58.318
May 16 15:14:58.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename server-version 05/16/23 15:14:58.319
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:58.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:58.34
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 05/16/23 15:14:58.343
STEP: Confirm major version 05/16/23 15:14:58.344
May 16 15:14:58.345: INFO: Major version: 1
STEP: Confirm minor version 05/16/23 15:14:58.345
May 16 15:14:58.345: INFO: cleanMinorVersion: 26
May 16 15:14:58.345: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
May 16 15:14:58.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-3548" for this suite. 05/16/23 15:14:58.35
------------------------------
â€¢ [0.037 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:14:58.318
    May 16 15:14:58.318: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename server-version 05/16/23 15:14:58.319
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:58.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:58.34
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 05/16/23 15:14:58.343
    STEP: Confirm major version 05/16/23 15:14:58.344
    May 16 15:14:58.345: INFO: Major version: 1
    STEP: Confirm minor version 05/16/23 15:14:58.345
    May 16 15:14:58.345: INFO: cleanMinorVersion: 26
    May 16 15:14:58.345: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    May 16 15:14:58.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-3548" for this suite. 05/16/23 15:14:58.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:14:58.356
May 16 15:14:58.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename webhook 05/16/23 15:14:58.356
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:58.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:58.379
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/16/23 15:14:58.402
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 15:14:58.606
STEP: Deploying the webhook pod 05/16/23 15:14:58.612
STEP: Wait for the deployment to be ready 05/16/23 15:14:58.623
May 16 15:14:58.630: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/16/23 15:15:00.643
STEP: Verifying the service has paired with the endpoint 05/16/23 15:15:00.653
May 16 15:15:01.654: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 05/16/23 15:15:01.658
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 05/16/23 15:15:01.659
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 05/16/23 15:15:01.659
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 05/16/23 15:15:01.659
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 05/16/23 15:15:01.66
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 05/16/23 15:15:01.66
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 05/16/23 15:15:01.661
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 15:15:01.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3996" for this suite. 05/16/23 15:15:01.717
STEP: Destroying namespace "webhook-3996-markers" for this suite. 05/16/23 15:15:01.728
------------------------------
â€¢ [3.383 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:14:58.356
    May 16 15:14:58.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename webhook 05/16/23 15:14:58.356
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:14:58.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:14:58.379
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/16/23 15:14:58.402
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 15:14:58.606
    STEP: Deploying the webhook pod 05/16/23 15:14:58.612
    STEP: Wait for the deployment to be ready 05/16/23 15:14:58.623
    May 16 15:14:58.630: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/16/23 15:15:00.643
    STEP: Verifying the service has paired with the endpoint 05/16/23 15:15:00.653
    May 16 15:15:01.654: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 05/16/23 15:15:01.658
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 05/16/23 15:15:01.659
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 05/16/23 15:15:01.659
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 05/16/23 15:15:01.659
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 05/16/23 15:15:01.66
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 05/16/23 15:15:01.66
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 05/16/23 15:15:01.661
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 15:15:01.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3996" for this suite. 05/16/23 15:15:01.717
    STEP: Destroying namespace "webhook-3996-markers" for this suite. 05/16/23 15:15:01.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:15:01.739
May 16 15:15:01.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename init-container 05/16/23 15:15:01.74
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:15:01.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:15:01.763
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 05/16/23 15:15:01.767
May 16 15:15:01.767: INFO: PodSpec: initContainers in spec.initContainers
May 16 15:15:44.985: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-0cae1822-589e-4385-abfc-852d074901b0", GenerateName:"", Namespace:"init-container-989", SelfLink:"", UID:"20b12a77-659d-4729-a4f7-201ea57ea69d", ResourceVersion:"85367", Generation:0, CreationTimestamp:time.Date(2023, time.May, 16, 15, 15, 1, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"767428260"}, Annotations:map[string]string{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.129.2.147/23\"],\"mac_address\":\"0a:58:0a:81:02:93\",\"gateway_ips\":[\"10.129.2.1\"],\"ip_address\":\"10.129.2.147/23\",\"gateway_ip\":\"10.129.2.1\"}}", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.129.2.147\"\n    ],\n    \"mac\": \"0a:58:0a:81:02:93\",\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 16, 15, 15, 1, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0013c3980), Subresource:""}, v1.ManagedFieldsEntry{Manager:"ip-10-0-130-191", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 16, 15, 15, 1, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0013c39c8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 16, 15, 15, 2, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0013c39f8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 16, 15, 15, 44, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0013c3a40), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-zfgss", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0032fd120), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zfgss", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00cc9e6c0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zfgss", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00cc9e720), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zfgss", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00cc9e660), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc009a6c1e0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-212-246.eu-central-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00cc9d260), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc009a6c290)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc009a6c2b0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc009a6c2cc), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc009a6c2d0), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc008a78ac0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 16, 15, 15, 1, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 16, 15, 15, 1, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 16, 15, 15, 1, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 16, 15, 15, 1, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.212.246", PodIP:"10.129.2.147", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.129.2.147"}}, StartTime:time.Date(2023, time.May, 16, 15, 15, 1, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00cc9d340)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00cc9d3b0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://6d309ff19326a74f074092c6e39859cff81e340b8e4f010bd048075b0aa2d168", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0032fd1e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0032fd1a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc009a6c34f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
May 16 15:15:44.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-989" for this suite. 05/16/23 15:15:44.99
------------------------------
â€¢ [SLOW TEST] [43.258 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:15:01.739
    May 16 15:15:01.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename init-container 05/16/23 15:15:01.74
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:15:01.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:15:01.763
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 05/16/23 15:15:01.767
    May 16 15:15:01.767: INFO: PodSpec: initContainers in spec.initContainers
    May 16 15:15:44.985: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-0cae1822-589e-4385-abfc-852d074901b0", GenerateName:"", Namespace:"init-container-989", SelfLink:"", UID:"20b12a77-659d-4729-a4f7-201ea57ea69d", ResourceVersion:"85367", Generation:0, CreationTimestamp:time.Date(2023, time.May, 16, 15, 15, 1, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"767428260"}, Annotations:map[string]string{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.129.2.147/23\"],\"mac_address\":\"0a:58:0a:81:02:93\",\"gateway_ips\":[\"10.129.2.1\"],\"ip_address\":\"10.129.2.147/23\",\"gateway_ip\":\"10.129.2.1\"}}", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.129.2.147\"\n    ],\n    \"mac\": \"0a:58:0a:81:02:93\",\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 16, 15, 15, 1, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0013c3980), Subresource:""}, v1.ManagedFieldsEntry{Manager:"ip-10-0-130-191", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 16, 15, 15, 1, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0013c39c8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 16, 15, 15, 2, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0013c39f8), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 16, 15, 15, 44, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0013c3a40), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-zfgss", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0032fd120), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zfgss", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00cc9e6c0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zfgss", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00cc9e720), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-zfgss", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00cc9e660), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc009a6c1e0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-212-246.eu-central-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00cc9d260), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc009a6c290)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc009a6c2b0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc009a6c2cc), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc009a6c2d0), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc008a78ac0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 16, 15, 15, 1, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 16, 15, 15, 1, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 16, 15, 15, 1, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 16, 15, 15, 1, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.212.246", PodIP:"10.129.2.147", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.129.2.147"}}, StartTime:time.Date(2023, time.May, 16, 15, 15, 1, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00cc9d340)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00cc9d3b0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://6d309ff19326a74f074092c6e39859cff81e340b8e4f010bd048075b0aa2d168", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0032fd1e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0032fd1a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc009a6c34f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    May 16 15:15:44.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-989" for this suite. 05/16/23 15:15:44.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:15:44.997
May 16 15:15:44.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 15:15:44.998
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:15:45.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:15:45.02
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-ffeb38ca-80ad-469b-94c7-bb01c8672232 05/16/23 15:15:45.024
STEP: Creating a pod to test consume secrets 05/16/23 15:15:45.033
May 16 15:15:45.049: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-09126864-c72d-4d08-be82-b4c692acf935" in namespace "projected-8884" to be "Succeeded or Failed"
May 16 15:15:45.053: INFO: Pod "pod-projected-secrets-09126864-c72d-4d08-be82-b4c692acf935": Phase="Pending", Reason="", readiness=false. Elapsed: 4.530269ms
May 16 15:15:47.058: INFO: Pod "pod-projected-secrets-09126864-c72d-4d08-be82-b4c692acf935": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008846136s
May 16 15:15:49.057: INFO: Pod "pod-projected-secrets-09126864-c72d-4d08-be82-b4c692acf935": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00867152s
STEP: Saw pod success 05/16/23 15:15:49.057
May 16 15:15:49.058: INFO: Pod "pod-projected-secrets-09126864-c72d-4d08-be82-b4c692acf935" satisfied condition "Succeeded or Failed"
May 16 15:15:49.061: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-secrets-09126864-c72d-4d08-be82-b4c692acf935 container secret-volume-test: <nil>
STEP: delete the pod 05/16/23 15:15:49.066
May 16 15:15:49.075: INFO: Waiting for pod pod-projected-secrets-09126864-c72d-4d08-be82-b4c692acf935 to disappear
May 16 15:15:49.082: INFO: Pod pod-projected-secrets-09126864-c72d-4d08-be82-b4c692acf935 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May 16 15:15:49.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8884" for this suite. 05/16/23 15:15:49.086
------------------------------
â€¢ [4.095 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:15:44.997
    May 16 15:15:44.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 15:15:44.998
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:15:45.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:15:45.02
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-ffeb38ca-80ad-469b-94c7-bb01c8672232 05/16/23 15:15:45.024
    STEP: Creating a pod to test consume secrets 05/16/23 15:15:45.033
    May 16 15:15:45.049: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-09126864-c72d-4d08-be82-b4c692acf935" in namespace "projected-8884" to be "Succeeded or Failed"
    May 16 15:15:45.053: INFO: Pod "pod-projected-secrets-09126864-c72d-4d08-be82-b4c692acf935": Phase="Pending", Reason="", readiness=false. Elapsed: 4.530269ms
    May 16 15:15:47.058: INFO: Pod "pod-projected-secrets-09126864-c72d-4d08-be82-b4c692acf935": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008846136s
    May 16 15:15:49.057: INFO: Pod "pod-projected-secrets-09126864-c72d-4d08-be82-b4c692acf935": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00867152s
    STEP: Saw pod success 05/16/23 15:15:49.057
    May 16 15:15:49.058: INFO: Pod "pod-projected-secrets-09126864-c72d-4d08-be82-b4c692acf935" satisfied condition "Succeeded or Failed"
    May 16 15:15:49.061: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-secrets-09126864-c72d-4d08-be82-b4c692acf935 container secret-volume-test: <nil>
    STEP: delete the pod 05/16/23 15:15:49.066
    May 16 15:15:49.075: INFO: Waiting for pod pod-projected-secrets-09126864-c72d-4d08-be82-b4c692acf935 to disappear
    May 16 15:15:49.082: INFO: Pod pod-projected-secrets-09126864-c72d-4d08-be82-b4c692acf935 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May 16 15:15:49.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8884" for this suite. 05/16/23 15:15:49.086
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:15:49.093
May 16 15:15:49.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 15:15:49.093
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:15:49.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:15:49.114
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-96989210-7a0d-4a7b-abc6-14fded952adb 05/16/23 15:15:49.118
STEP: Creating a pod to test consume secrets 05/16/23 15:15:49.124
May 16 15:15:49.151: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c4430097-4ff3-4336-8e17-ce3ade30b15b" in namespace "projected-5697" to be "Succeeded or Failed"
May 16 15:15:49.155: INFO: Pod "pod-projected-secrets-c4430097-4ff3-4336-8e17-ce3ade30b15b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.084491ms
May 16 15:15:51.160: INFO: Pod "pod-projected-secrets-c4430097-4ff3-4336-8e17-ce3ade30b15b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008229589s
May 16 15:15:53.160: INFO: Pod "pod-projected-secrets-c4430097-4ff3-4336-8e17-ce3ade30b15b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008532495s
STEP: Saw pod success 05/16/23 15:15:53.16
May 16 15:15:53.160: INFO: Pod "pod-projected-secrets-c4430097-4ff3-4336-8e17-ce3ade30b15b" satisfied condition "Succeeded or Failed"
May 16 15:15:53.163: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-secrets-c4430097-4ff3-4336-8e17-ce3ade30b15b container projected-secret-volume-test: <nil>
STEP: delete the pod 05/16/23 15:15:53.168
May 16 15:15:53.178: INFO: Waiting for pod pod-projected-secrets-c4430097-4ff3-4336-8e17-ce3ade30b15b to disappear
May 16 15:15:53.180: INFO: Pod pod-projected-secrets-c4430097-4ff3-4336-8e17-ce3ade30b15b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May 16 15:15:53.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5697" for this suite. 05/16/23 15:15:53.185
------------------------------
â€¢ [4.098 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:15:49.093
    May 16 15:15:49.093: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 15:15:49.093
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:15:49.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:15:49.114
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-96989210-7a0d-4a7b-abc6-14fded952adb 05/16/23 15:15:49.118
    STEP: Creating a pod to test consume secrets 05/16/23 15:15:49.124
    May 16 15:15:49.151: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c4430097-4ff3-4336-8e17-ce3ade30b15b" in namespace "projected-5697" to be "Succeeded or Failed"
    May 16 15:15:49.155: INFO: Pod "pod-projected-secrets-c4430097-4ff3-4336-8e17-ce3ade30b15b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.084491ms
    May 16 15:15:51.160: INFO: Pod "pod-projected-secrets-c4430097-4ff3-4336-8e17-ce3ade30b15b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008229589s
    May 16 15:15:53.160: INFO: Pod "pod-projected-secrets-c4430097-4ff3-4336-8e17-ce3ade30b15b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008532495s
    STEP: Saw pod success 05/16/23 15:15:53.16
    May 16 15:15:53.160: INFO: Pod "pod-projected-secrets-c4430097-4ff3-4336-8e17-ce3ade30b15b" satisfied condition "Succeeded or Failed"
    May 16 15:15:53.163: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-secrets-c4430097-4ff3-4336-8e17-ce3ade30b15b container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/16/23 15:15:53.168
    May 16 15:15:53.178: INFO: Waiting for pod pod-projected-secrets-c4430097-4ff3-4336-8e17-ce3ade30b15b to disappear
    May 16 15:15:53.180: INFO: Pod pod-projected-secrets-c4430097-4ff3-4336-8e17-ce3ade30b15b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May 16 15:15:53.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5697" for this suite. 05/16/23 15:15:53.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:15:53.191
May 16 15:15:53.191: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename job 05/16/23 15:15:53.192
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:15:53.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:15:53.209
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 05/16/23 15:15:53.214
W0516 15:15:53.224293      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions 05/16/23 15:15:53.224
STEP: Ensuring pods with index for job exist 05/16/23 15:16:01.228
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May 16 15:16:01.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1070" for this suite. 05/16/23 15:16:01.235
------------------------------
â€¢ [SLOW TEST] [8.049 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:15:53.191
    May 16 15:15:53.191: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename job 05/16/23 15:15:53.192
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:15:53.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:15:53.209
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 05/16/23 15:15:53.214
    W0516 15:15:53.224293      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring job reaches completions 05/16/23 15:15:53.224
    STEP: Ensuring pods with index for job exist 05/16/23 15:16:01.228
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May 16 15:16:01.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1070" for this suite. 05/16/23 15:16:01.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:16:01.241
May 16 15:16:01.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename crd-publish-openapi 05/16/23 15:16:01.242
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:01.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:01.262
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 05/16/23 15:16:01.266
May 16 15:16:01.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 05/16/23 15:16:11.335
May 16 15:16:11.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:16:14.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 15:16:25.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6212" for this suite. 05/16/23 15:16:25.78
------------------------------
â€¢ [SLOW TEST] [24.544 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:16:01.241
    May 16 15:16:01.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename crd-publish-openapi 05/16/23 15:16:01.242
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:01.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:01.262
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 05/16/23 15:16:01.266
    May 16 15:16:01.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 05/16/23 15:16:11.335
    May 16 15:16:11.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:16:14.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 15:16:25.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6212" for this suite. 05/16/23 15:16:25.78
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:16:25.786
May 16 15:16:25.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename svcaccounts 05/16/23 15:16:25.787
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:25.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:25.81
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 05/16/23 15:16:25.812
STEP: watching for the ServiceAccount to be added 05/16/23 15:16:25.822
STEP: patching the ServiceAccount 05/16/23 15:16:25.823
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 05/16/23 15:16:25.831
STEP: deleting the ServiceAccount 05/16/23 15:16:25.844
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May 16 15:16:25.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1008" for this suite. 05/16/23 15:16:25.883
------------------------------
â€¢ [0.108 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:16:25.786
    May 16 15:16:25.786: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename svcaccounts 05/16/23 15:16:25.787
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:25.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:25.81
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 05/16/23 15:16:25.812
    STEP: watching for the ServiceAccount to be added 05/16/23 15:16:25.822
    STEP: patching the ServiceAccount 05/16/23 15:16:25.823
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 05/16/23 15:16:25.831
    STEP: deleting the ServiceAccount 05/16/23 15:16:25.844
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May 16 15:16:25.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1008" for this suite. 05/16/23 15:16:25.883
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:16:25.895
May 16 15:16:25.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename namespaces 05/16/23 15:16:25.895
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:25.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:25.914
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 05/16/23 15:16:25.917
STEP: patching the Namespace 05/16/23 15:16:25.945
STEP: get the Namespace and ensuring it has the label 05/16/23 15:16:25.971
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 15:16:25.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5214" for this suite. 05/16/23 15:16:25.979
STEP: Destroying namespace "nspatchtest-7592236f-afb1-42ae-88f7-d34e1b90ab35-8289" for this suite. 05/16/23 15:16:25.993
------------------------------
â€¢ [0.107 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:16:25.895
    May 16 15:16:25.895: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename namespaces 05/16/23 15:16:25.895
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:25.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:25.914
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 05/16/23 15:16:25.917
    STEP: patching the Namespace 05/16/23 15:16:25.945
    STEP: get the Namespace and ensuring it has the label 05/16/23 15:16:25.971
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 15:16:25.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5214" for this suite. 05/16/23 15:16:25.979
    STEP: Destroying namespace "nspatchtest-7592236f-afb1-42ae-88f7-d34e1b90ab35-8289" for this suite. 05/16/23 15:16:25.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:16:26.002
May 16 15:16:26.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubectl 05/16/23 15:16:26.003
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:26.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:26.024
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 05/16/23 15:16:26.026
May 16 15:16:26.026: INFO: namespace kubectl-6230
May 16 15:16:26.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6230 create -f -'
May 16 15:16:27.057: INFO: stderr: ""
May 16 15:16:27.057: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/16/23 15:16:27.057
May 16 15:16:28.061: INFO: Selector matched 1 pods for map[app:agnhost]
May 16 15:16:28.061: INFO: Found 0 / 1
May 16 15:16:29.061: INFO: Selector matched 1 pods for map[app:agnhost]
May 16 15:16:29.061: INFO: Found 1 / 1
May 16 15:16:29.061: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 16 15:16:29.064: INFO: Selector matched 1 pods for map[app:agnhost]
May 16 15:16:29.064: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 16 15:16:29.064: INFO: wait on agnhost-primary startup in kubectl-6230 
May 16 15:16:29.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6230 logs agnhost-primary-flzlm agnhost-primary'
May 16 15:16:29.120: INFO: stderr: ""
May 16 15:16:29.120: INFO: stdout: "Paused\n"
STEP: exposing RC 05/16/23 15:16:29.12
May 16 15:16:29.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6230 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May 16 15:16:29.178: INFO: stderr: ""
May 16 15:16:29.178: INFO: stdout: "service/rm2 exposed\n"
May 16 15:16:29.182: INFO: Service rm2 in namespace kubectl-6230 found.
STEP: exposing service 05/16/23 15:16:31.189
May 16 15:16:31.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6230 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May 16 15:16:31.248: INFO: stderr: ""
May 16 15:16:31.248: INFO: stdout: "service/rm3 exposed\n"
May 16 15:16:31.255: INFO: Service rm3 in namespace kubectl-6230 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 16 15:16:33.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6230" for this suite. 05/16/23 15:16:33.266
------------------------------
â€¢ [SLOW TEST] [7.269 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:16:26.002
    May 16 15:16:26.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubectl 05/16/23 15:16:26.003
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:26.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:26.024
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 05/16/23 15:16:26.026
    May 16 15:16:26.026: INFO: namespace kubectl-6230
    May 16 15:16:26.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6230 create -f -'
    May 16 15:16:27.057: INFO: stderr: ""
    May 16 15:16:27.057: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/16/23 15:16:27.057
    May 16 15:16:28.061: INFO: Selector matched 1 pods for map[app:agnhost]
    May 16 15:16:28.061: INFO: Found 0 / 1
    May 16 15:16:29.061: INFO: Selector matched 1 pods for map[app:agnhost]
    May 16 15:16:29.061: INFO: Found 1 / 1
    May 16 15:16:29.061: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    May 16 15:16:29.064: INFO: Selector matched 1 pods for map[app:agnhost]
    May 16 15:16:29.064: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May 16 15:16:29.064: INFO: wait on agnhost-primary startup in kubectl-6230 
    May 16 15:16:29.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6230 logs agnhost-primary-flzlm agnhost-primary'
    May 16 15:16:29.120: INFO: stderr: ""
    May 16 15:16:29.120: INFO: stdout: "Paused\n"
    STEP: exposing RC 05/16/23 15:16:29.12
    May 16 15:16:29.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6230 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    May 16 15:16:29.178: INFO: stderr: ""
    May 16 15:16:29.178: INFO: stdout: "service/rm2 exposed\n"
    May 16 15:16:29.182: INFO: Service rm2 in namespace kubectl-6230 found.
    STEP: exposing service 05/16/23 15:16:31.189
    May 16 15:16:31.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6230 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    May 16 15:16:31.248: INFO: stderr: ""
    May 16 15:16:31.248: INFO: stdout: "service/rm3 exposed\n"
    May 16 15:16:31.255: INFO: Service rm3 in namespace kubectl-6230 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 16 15:16:33.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6230" for this suite. 05/16/23 15:16:33.266
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:16:33.272
May 16 15:16:33.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename emptydir 05/16/23 15:16:33.273
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:33.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:33.294
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 05/16/23 15:16:33.297
May 16 15:16:33.319: INFO: Waiting up to 5m0s for pod "pod-e3122d65-6518-4bf9-bc50-da5d09f4e46c" in namespace "emptydir-8391" to be "Succeeded or Failed"
May 16 15:16:33.321: INFO: Pod "pod-e3122d65-6518-4bf9-bc50-da5d09f4e46c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.856554ms
May 16 15:16:35.326: INFO: Pod "pod-e3122d65-6518-4bf9-bc50-da5d09f4e46c": Phase="Running", Reason="", readiness=false. Elapsed: 2.007591696s
May 16 15:16:37.325: INFO: Pod "pod-e3122d65-6518-4bf9-bc50-da5d09f4e46c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006775809s
STEP: Saw pod success 05/16/23 15:16:37.325
May 16 15:16:37.325: INFO: Pod "pod-e3122d65-6518-4bf9-bc50-da5d09f4e46c" satisfied condition "Succeeded or Failed"
May 16 15:16:37.329: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-e3122d65-6518-4bf9-bc50-da5d09f4e46c container test-container: <nil>
STEP: delete the pod 05/16/23 15:16:37.339
May 16 15:16:37.352: INFO: Waiting for pod pod-e3122d65-6518-4bf9-bc50-da5d09f4e46c to disappear
May 16 15:16:37.354: INFO: Pod pod-e3122d65-6518-4bf9-bc50-da5d09f4e46c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 16 15:16:37.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8391" for this suite. 05/16/23 15:16:37.359
------------------------------
â€¢ [4.092 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:16:33.272
    May 16 15:16:33.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename emptydir 05/16/23 15:16:33.273
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:33.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:33.294
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 05/16/23 15:16:33.297
    May 16 15:16:33.319: INFO: Waiting up to 5m0s for pod "pod-e3122d65-6518-4bf9-bc50-da5d09f4e46c" in namespace "emptydir-8391" to be "Succeeded or Failed"
    May 16 15:16:33.321: INFO: Pod "pod-e3122d65-6518-4bf9-bc50-da5d09f4e46c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.856554ms
    May 16 15:16:35.326: INFO: Pod "pod-e3122d65-6518-4bf9-bc50-da5d09f4e46c": Phase="Running", Reason="", readiness=false. Elapsed: 2.007591696s
    May 16 15:16:37.325: INFO: Pod "pod-e3122d65-6518-4bf9-bc50-da5d09f4e46c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006775809s
    STEP: Saw pod success 05/16/23 15:16:37.325
    May 16 15:16:37.325: INFO: Pod "pod-e3122d65-6518-4bf9-bc50-da5d09f4e46c" satisfied condition "Succeeded or Failed"
    May 16 15:16:37.329: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-e3122d65-6518-4bf9-bc50-da5d09f4e46c container test-container: <nil>
    STEP: delete the pod 05/16/23 15:16:37.339
    May 16 15:16:37.352: INFO: Waiting for pod pod-e3122d65-6518-4bf9-bc50-da5d09f4e46c to disappear
    May 16 15:16:37.354: INFO: Pod pod-e3122d65-6518-4bf9-bc50-da5d09f4e46c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 16 15:16:37.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8391" for this suite. 05/16/23 15:16:37.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:16:37.365
May 16 15:16:37.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename sched-pred 05/16/23 15:16:37.366
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:37.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:37.384
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
May 16 15:16:37.387: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 16 15:16:37.397: INFO: Waiting for terminating namespaces to be deleted...
May 16 15:16:37.402: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-132-142.eu-central-1.compute.internal before test
May 16 15:16:37.426: INFO: aws-ebs-csi-driver-node-27d5z from openshift-cluster-csi-drivers started at 2023-05-16 13:52:17 +0000 UTC (3 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container csi-driver ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 16 15:16:37.427: INFO: tuned-j9bjl from openshift-cluster-node-tuning-operator started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container tuned ready: true, restart count 0
May 16 15:16:37.427: INFO: dns-default-9lcb7 from openshift-dns started at 2023-05-16 13:53:17 +0000 UTC (2 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container dns ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.427: INFO: node-resolver-b47gh from openshift-dns started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 16 15:16:37.427: INFO: image-registry-b549d4f89-4ck4v from openshift-image-registry started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container registry ready: true, restart count 0
May 16 15:16:37.427: INFO: node-ca-px455 from openshift-image-registry started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container node-ca ready: true, restart count 0
May 16 15:16:37.427: INFO: ingress-canary-4hv85 from openshift-ingress-canary started at 2023-05-16 13:53:17 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May 16 15:16:37.427: INFO: router-default-8f95c5-srcbl from openshift-ingress started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container router ready: true, restart count 0
May 16 15:16:37.427: INFO: machine-config-daemon-l6bpj from openshift-machine-config-operator started at 2023-05-16 13:52:17 +0000 UTC (2 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 15:16:37.427: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (6 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container alertmanager ready: true, restart count 1
May 16 15:16:37.427: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container config-reloader ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 16 15:16:37.427: INFO: node-exporter-pltn9 from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container node-exporter ready: true, restart count 0
May 16 15:16:37.427: INFO: prometheus-adapter-7767c6bd47-bfq4d from openshift-monitoring started at 2023-05-16 13:53:50 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 16 15:16:37.427: INFO: prometheus-operator-admission-webhook-5d679565bb-8lkws from openshift-monitoring started at 2023-05-16 14:48:21 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
May 16 15:16:37.427: INFO: telemeter-client-859dbffbb4-mt92v from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (3 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container reload ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container telemeter-client ready: true, restart count 0
May 16 15:16:37.427: INFO: thanos-querier-5c9d876b5c-xdt6m from openshift-monitoring started at 2023-05-16 13:53:54 +0000 UTC (6 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container thanos-query ready: true, restart count 0
May 16 15:16:37.427: INFO: multus-2wjqr from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container kube-multus ready: true, restart count 0
May 16 15:16:37.427: INFO: multus-additional-cni-plugins-z2h4l from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May 16 15:16:37.427: INFO: network-metrics-daemon-rqdtf from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (2 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 16 15:16:37.427: INFO: network-check-target-28wts from openshift-network-diagnostics started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container network-check-target-container ready: true, restart count 0
May 16 15:16:37.427: INFO: ovnkube-node-npwgq from openshift-ovn-kubernetes started at 2023-05-16 13:52:17 +0000 UTC (5 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container ovn-acl-logging ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container ovn-controller ready: true, restart count 1
May 16 15:16:37.427: INFO: 	Container ovnkube-node ready: true, restart count 0
May 16 15:16:37.427: INFO: sonobuoy from sonobuoy started at 2023-05-16 14:11:07 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 16 15:16:37.427: INFO: sonobuoy-e2e-job-840dd79630d44353 from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container e2e ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 15:16:37.427: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-7zsf6 from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
May 16 15:16:37.427: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 15:16:37.427: INFO: 	Container systemd-logs ready: true, restart count 0
May 16 15:16:37.427: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-161-164.eu-central-1.compute.internal before test
May 16 15:16:37.468: INFO: agnhost-primary-flzlm from kubectl-6230 started at 2023-05-16 15:16:27 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container agnhost-primary ready: true, restart count 0
May 16 15:16:37.468: INFO: aws-ebs-csi-driver-node-9m6vn from openshift-cluster-csi-drivers started at 2023-05-16 13:44:25 +0000 UTC (3 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container csi-driver ready: true, restart count 0
May 16 15:16:37.468: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 16 15:16:37.468: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 16 15:16:37.468: INFO: tuned-6n5b7 from openshift-cluster-node-tuning-operator started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container tuned ready: true, restart count 0
May 16 15:16:37.468: INFO: dns-default-pwtdq from openshift-dns started at 2023-05-16 14:48:42 +0000 UTC (2 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container dns ready: true, restart count 0
May 16 15:16:37.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.468: INFO: node-resolver-jbmqq from openshift-dns started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 16 15:16:37.468: INFO: node-ca-gw7rz from openshift-image-registry started at 2023-05-16 13:50:21 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container node-ca ready: true, restart count 0
May 16 15:16:37.468: INFO: ingress-canary-mvqst from openshift-ingress-canary started at 2023-05-16 14:48:23 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May 16 15:16:37.468: INFO: machine-config-daemon-qhdsv from openshift-machine-config-operator started at 2023-05-16 13:44:25 +0000 UTC (2 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 16 15:16:37.468: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 15:16:37.468: INFO: node-exporter-l2zvs from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.468: INFO: 	Container node-exporter ready: true, restart count 0
May 16 15:16:37.468: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-05-16 14:48:24 +0000 UTC (6 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container config-reloader ready: true, restart count 0
May 16 15:16:37.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.468: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May 16 15:16:37.468: INFO: 	Container prometheus ready: true, restart count 0
May 16 15:16:37.468: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 16 15:16:37.468: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 16 15:16:37.468: INFO: multus-additional-cni-plugins-jkn7l from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May 16 15:16:37.468: INFO: multus-mc6qb from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container kube-multus ready: true, restart count 0
May 16 15:16:37.468: INFO: network-metrics-daemon-n4p75 from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (2 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.468: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 16 15:16:37.468: INFO: network-check-source-7f6b75fdb6-lxqsg from openshift-network-diagnostics started at 2023-05-16 14:48:21 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container check-endpoints ready: true, restart count 0
May 16 15:16:37.468: INFO: network-check-target-bprb5 from openshift-network-diagnostics started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container network-check-target-container ready: true, restart count 0
May 16 15:16:37.468: INFO: collect-profiles-28070820-8vnf5 from openshift-operator-lifecycle-manager started at 2023-05-16 15:00:00 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container collect-profiles ready: false, restart count 0
May 16 15:16:37.468: INFO: collect-profiles-28070835-9g48l from openshift-operator-lifecycle-manager started at 2023-05-16 15:15:00 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container collect-profiles ready: false, restart count 0
May 16 15:16:37.468: INFO: ovnkube-node-w8qn8 from openshift-ovn-kubernetes started at 2023-05-16 13:44:25 +0000 UTC (5 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.468: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
May 16 15:16:37.468: INFO: 	Container ovn-acl-logging ready: true, restart count 0
May 16 15:16:37.468: INFO: 	Container ovn-controller ready: true, restart count 1
May 16 15:16:37.468: INFO: 	Container ovnkube-node ready: true, restart count 0
May 16 15:16:37.468: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-5r4tm from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
May 16 15:16:37.468: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 15:16:37.468: INFO: 	Container systemd-logs ready: true, restart count 0
May 16 15:16:37.468: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-212-246.eu-central-1.compute.internal before test
May 16 15:16:37.496: INFO: aws-ebs-csi-driver-node-b4s28 from openshift-cluster-csi-drivers started at 2023-05-16 13:52:19 +0000 UTC (3 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container csi-driver ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 16 15:16:37.496: INFO: tuned-8x9rx from openshift-cluster-node-tuning-operator started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container tuned ready: true, restart count 0
May 16 15:16:37.496: INFO: dns-default-dbkcs from openshift-dns started at 2023-05-16 13:53:17 +0000 UTC (2 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container dns ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.496: INFO: node-resolver-gdzz2 from openshift-dns started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 16 15:16:37.496: INFO: image-registry-b549d4f89-hs4qn from openshift-image-registry started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container registry ready: true, restart count 0
May 16 15:16:37.496: INFO: node-ca-nsxtf from openshift-image-registry started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container node-ca ready: true, restart count 0
May 16 15:16:37.496: INFO: ingress-canary-wphz5 from openshift-ingress-canary started at 2023-05-16 13:53:17 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May 16 15:16:37.496: INFO: router-default-8f95c5-bbmm5 from openshift-ingress started at 2023-05-16 14:48:21 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container router ready: true, restart count 0
May 16 15:16:37.496: INFO: machine-config-daemon-z7sw6 from openshift-machine-config-operator started at 2023-05-16 13:52:19 +0000 UTC (2 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 15:16:37.496: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (6 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container alertmanager ready: true, restart count 1
May 16 15:16:37.496: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container config-reloader ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 16 15:16:37.496: INFO: kube-state-metrics-6ccfb58dc4-wgwts from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (3 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 16 15:16:37.496: INFO: node-exporter-jrknj from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container node-exporter ready: true, restart count 0
May 16 15:16:37.496: INFO: openshift-state-metrics-7d7f8b4cf8-9lbg2 from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (3 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container openshift-state-metrics ready: true, restart count 0
May 16 15:16:37.496: INFO: prometheus-adapter-7767c6bd47-bqlnh from openshift-monitoring started at 2023-05-16 13:53:50 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 16 15:16:37.496: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-05-16 13:53:57 +0000 UTC (6 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container config-reloader ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container prometheus ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 16 15:16:37.496: INFO: prometheus-operator-admission-webhook-5d679565bb-96qsx from openshift-monitoring started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
May 16 15:16:37.496: INFO: thanos-querier-5c9d876b5c-wwd7q from openshift-monitoring started at 2023-05-16 13:53:54 +0000 UTC (6 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container thanos-query ready: true, restart count 0
May 16 15:16:37.496: INFO: multus-additional-cni-plugins-wxcnn from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May 16 15:16:37.496: INFO: multus-rpn9n from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container kube-multus ready: true, restart count 0
May 16 15:16:37.496: INFO: network-metrics-daemon-7sqls from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (2 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 16 15:16:37.496: INFO: network-check-target-8pcqz from openshift-network-diagnostics started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container network-check-target-container ready: true, restart count 0
May 16 15:16:37.496: INFO: ovnkube-node-kqvdm from openshift-ovn-kubernetes started at 2023-05-16 13:52:19 +0000 UTC (5 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container ovn-acl-logging ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container ovn-controller ready: true, restart count 1
May 16 15:16:37.496: INFO: 	Container ovnkube-node ready: true, restart count 0
May 16 15:16:37.496: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-gsqlr from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
May 16 15:16:37.496: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 15:16:37.496: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node ip-10-0-132-142.eu-central-1.compute.internal 05/16/23 15:16:37.551
STEP: verifying the node has the label node ip-10-0-161-164.eu-central-1.compute.internal 05/16/23 15:16:37.569
STEP: verifying the node has the label node ip-10-0-212-246.eu-central-1.compute.internal 05/16/23 15:16:37.581
May 16 15:16:37.631: INFO: Pod agnhost-primary-flzlm requesting resource cpu=0m on Node ip-10-0-161-164.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod aws-ebs-csi-driver-node-27d5z requesting resource cpu=30m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod aws-ebs-csi-driver-node-9m6vn requesting resource cpu=30m on Node ip-10-0-161-164.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod aws-ebs-csi-driver-node-b4s28 requesting resource cpu=30m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod tuned-6n5b7 requesting resource cpu=10m on Node ip-10-0-161-164.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod tuned-8x9rx requesting resource cpu=10m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod tuned-j9bjl requesting resource cpu=10m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod dns-default-9lcb7 requesting resource cpu=60m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod dns-default-dbkcs requesting resource cpu=60m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod dns-default-pwtdq requesting resource cpu=60m on Node ip-10-0-161-164.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod node-resolver-b47gh requesting resource cpu=5m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod node-resolver-gdzz2 requesting resource cpu=5m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod node-resolver-jbmqq requesting resource cpu=5m on Node ip-10-0-161-164.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod image-registry-b549d4f89-4ck4v requesting resource cpu=100m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod image-registry-b549d4f89-hs4qn requesting resource cpu=100m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod node-ca-gw7rz requesting resource cpu=10m on Node ip-10-0-161-164.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod node-ca-nsxtf requesting resource cpu=10m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod node-ca-px455 requesting resource cpu=10m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod ingress-canary-4hv85 requesting resource cpu=10m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod ingress-canary-mvqst requesting resource cpu=10m on Node ip-10-0-161-164.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod ingress-canary-wphz5 requesting resource cpu=10m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod router-default-8f95c5-bbmm5 requesting resource cpu=100m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod router-default-8f95c5-srcbl requesting resource cpu=100m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod machine-config-daemon-l6bpj requesting resource cpu=40m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod machine-config-daemon-qhdsv requesting resource cpu=40m on Node ip-10-0-161-164.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod machine-config-daemon-z7sw6 requesting resource cpu=40m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod kube-state-metrics-6ccfb58dc4-wgwts requesting resource cpu=4m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod node-exporter-jrknj requesting resource cpu=9m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod node-exporter-l2zvs requesting resource cpu=9m on Node ip-10-0-161-164.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod node-exporter-pltn9 requesting resource cpu=9m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod openshift-state-metrics-7d7f8b4cf8-9lbg2 requesting resource cpu=3m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod prometheus-adapter-7767c6bd47-bfq4d requesting resource cpu=1m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod prometheus-adapter-7767c6bd47-bqlnh requesting resource cpu=1m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node ip-10-0-161-164.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod prometheus-operator-admission-webhook-5d679565bb-8lkws requesting resource cpu=5m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod prometheus-operator-admission-webhook-5d679565bb-96qsx requesting resource cpu=5m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod telemeter-client-859dbffbb4-mt92v requesting resource cpu=3m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod thanos-querier-5c9d876b5c-wwd7q requesting resource cpu=15m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod thanos-querier-5c9d876b5c-xdt6m requesting resource cpu=15m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod multus-2wjqr requesting resource cpu=10m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod multus-additional-cni-plugins-jkn7l requesting resource cpu=10m on Node ip-10-0-161-164.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod multus-additional-cni-plugins-wxcnn requesting resource cpu=10m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod multus-additional-cni-plugins-z2h4l requesting resource cpu=10m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod multus-mc6qb requesting resource cpu=10m on Node ip-10-0-161-164.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod multus-rpn9n requesting resource cpu=10m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod network-metrics-daemon-7sqls requesting resource cpu=20m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod network-metrics-daemon-n4p75 requesting resource cpu=20m on Node ip-10-0-161-164.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod network-metrics-daemon-rqdtf requesting resource cpu=20m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod network-check-source-7f6b75fdb6-lxqsg requesting resource cpu=10m on Node ip-10-0-161-164.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod network-check-target-28wts requesting resource cpu=10m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod network-check-target-8pcqz requesting resource cpu=10m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod network-check-target-bprb5 requesting resource cpu=10m on Node ip-10-0-161-164.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod ovnkube-node-kqvdm requesting resource cpu=50m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod ovnkube-node-npwgq requesting resource cpu=50m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod ovnkube-node-w8qn8 requesting resource cpu=50m on Node ip-10-0-161-164.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod sonobuoy-e2e-job-840dd79630d44353 requesting resource cpu=0m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-5r4tm requesting resource cpu=0m on Node ip-10-0-161-164.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-7zsf6 requesting resource cpu=0m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.631: INFO: Pod sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-gsqlr requesting resource cpu=0m on Node ip-10-0-212-246.eu-central-1.compute.internal
STEP: Starting Pods to consume most of the cluster CPU. 05/16/23 15:16:37.631
May 16 15:16:37.631: INFO: Creating a pod which consumes cpu=2198m on Node ip-10-0-161-164.eu-central-1.compute.internal
May 16 15:16:37.642: INFO: Creating a pod which consumes cpu=2039m on Node ip-10-0-212-246.eu-central-1.compute.internal
May 16 15:16:37.654: INFO: Creating a pod which consumes cpu=2095m on Node ip-10-0-132-142.eu-central-1.compute.internal
May 16 15:16:37.663: INFO: Waiting up to 5m0s for pod "filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128" in namespace "sched-pred-9029" to be "running"
May 16 15:16:37.666: INFO: Pod "filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128": Phase="Pending", Reason="", readiness=false. Elapsed: 2.794423ms
May 16 15:16:39.669: INFO: Pod "filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128": Phase="Running", Reason="", readiness=true. Elapsed: 2.005841189s
May 16 15:16:39.669: INFO: Pod "filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128" satisfied condition "running"
May 16 15:16:39.669: INFO: Waiting up to 5m0s for pod "filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7" in namespace "sched-pred-9029" to be "running"
May 16 15:16:39.671: INFO: Pod "filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.266376ms
May 16 15:16:39.671: INFO: Pod "filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7" satisfied condition "running"
May 16 15:16:39.671: INFO: Waiting up to 5m0s for pod "filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd" in namespace "sched-pred-9029" to be "running"
May 16 15:16:39.674: INFO: Pod "filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd": Phase="Running", Reason="", readiness=true. Elapsed: 3.156016ms
May 16 15:16:39.674: INFO: Pod "filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 05/16/23 15:16:39.674
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd.175fa8a19937ae5c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9029/filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd to ip-10-0-132-142.eu-central-1.compute.internal] 05/16/23 15:16:39.677
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd.175fa8a1b548c1d1], Reason = [AddedInterface], Message = [Add eth0 [10.128.2.165/23] from ovn-kubernetes] 05/16/23 15:16:39.677
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd.175fa8a1b66e5bc9], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 05/16/23 15:16:39.677
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd.175fa8a1bc4cdd71], Reason = [Created], Message = [Created container filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd] 05/16/23 15:16:39.677
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd.175fa8a1bebbe82a], Reason = [Started], Message = [Started container filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd] 05/16/23 15:16:39.677
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128.175fa8a197b0e77c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9029/filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128 to ip-10-0-161-164.eu-central-1.compute.internal] 05/16/23 15:16:39.677
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128.175fa8a1bfdd74db], Reason = [AddedInterface], Message = [Add eth0 [10.131.1.63/23] from ovn-kubernetes] 05/16/23 15:16:39.677
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128.175fa8a1c1061830], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 05/16/23 15:16:39.677
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128.175fa8a1c71be5ad], Reason = [Created], Message = [Created container filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128] 05/16/23 15:16:39.677
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128.175fa8a1c812e209], Reason = [Started], Message = [Started container filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128] 05/16/23 15:16:39.678
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7.175fa8a1985a04bd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9029/filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7 to ip-10-0-212-246.eu-central-1.compute.internal] 05/16/23 15:16:39.678
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7.175fa8a1c0db3fba], Reason = [AddedInterface], Message = [Add eth0 [10.129.2.148/23] from ovn-kubernetes] 05/16/23 15:16:39.678
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7.175fa8a1c31903f0], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 05/16/23 15:16:39.678
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7.175fa8a1c8a77040], Reason = [Created], Message = [Created container filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7] 05/16/23 15:16:39.678
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7.175fa8a1c9e2a664], Reason = [Started], Message = [Started container filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7] 05/16/23 15:16:39.678
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.175fa8a2112fe707], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] 05/16/23 15:16:39.692
STEP: removing the label node off the node ip-10-0-212-246.eu-central-1.compute.internal 05/16/23 15:16:40.689
STEP: verifying the node doesn't have the label node 05/16/23 15:16:40.699
STEP: removing the label node off the node ip-10-0-132-142.eu-central-1.compute.internal 05/16/23 15:16:40.705
STEP: verifying the node doesn't have the label node 05/16/23 15:16:40.724
STEP: removing the label node off the node ip-10-0-161-164.eu-central-1.compute.internal 05/16/23 15:16:40.727
STEP: verifying the node doesn't have the label node 05/16/23 15:16:40.744
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 15:16:40.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9029" for this suite. 05/16/23 15:16:40.752
------------------------------
â€¢ [3.394 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:16:37.365
    May 16 15:16:37.365: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename sched-pred 05/16/23 15:16:37.366
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:37.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:37.384
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    May 16 15:16:37.387: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May 16 15:16:37.397: INFO: Waiting for terminating namespaces to be deleted...
    May 16 15:16:37.402: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-132-142.eu-central-1.compute.internal before test
    May 16 15:16:37.426: INFO: aws-ebs-csi-driver-node-27d5z from openshift-cluster-csi-drivers started at 2023-05-16 13:52:17 +0000 UTC (3 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container csi-driver ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    May 16 15:16:37.427: INFO: tuned-j9bjl from openshift-cluster-node-tuning-operator started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container tuned ready: true, restart count 0
    May 16 15:16:37.427: INFO: dns-default-9lcb7 from openshift-dns started at 2023-05-16 13:53:17 +0000 UTC (2 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container dns ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.427: INFO: node-resolver-b47gh from openshift-dns started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May 16 15:16:37.427: INFO: image-registry-b549d4f89-4ck4v from openshift-image-registry started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container registry ready: true, restart count 0
    May 16 15:16:37.427: INFO: node-ca-px455 from openshift-image-registry started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container node-ca ready: true, restart count 0
    May 16 15:16:37.427: INFO: ingress-canary-4hv85 from openshift-ingress-canary started at 2023-05-16 13:53:17 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May 16 15:16:37.427: INFO: router-default-8f95c5-srcbl from openshift-ingress started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container router ready: true, restart count 0
    May 16 15:16:37.427: INFO: machine-config-daemon-l6bpj from openshift-machine-config-operator started at 2023-05-16 13:52:17 +0000 UTC (2 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container machine-config-daemon ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 15:16:37.427: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (6 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container alertmanager ready: true, restart count 1
    May 16 15:16:37.427: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container config-reloader ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May 16 15:16:37.427: INFO: node-exporter-pltn9 from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container node-exporter ready: true, restart count 0
    May 16 15:16:37.427: INFO: prometheus-adapter-7767c6bd47-bfq4d from openshift-monitoring started at 2023-05-16 13:53:50 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container prometheus-adapter ready: true, restart count 0
    May 16 15:16:37.427: INFO: prometheus-operator-admission-webhook-5d679565bb-8lkws from openshift-monitoring started at 2023-05-16 14:48:21 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    May 16 15:16:37.427: INFO: telemeter-client-859dbffbb4-mt92v from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (3 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container reload ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container telemeter-client ready: true, restart count 0
    May 16 15:16:37.427: INFO: thanos-querier-5c9d876b5c-xdt6m from openshift-monitoring started at 2023-05-16 13:53:54 +0000 UTC (6 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container thanos-query ready: true, restart count 0
    May 16 15:16:37.427: INFO: multus-2wjqr from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container kube-multus ready: true, restart count 0
    May 16 15:16:37.427: INFO: multus-additional-cni-plugins-z2h4l from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May 16 15:16:37.427: INFO: network-metrics-daemon-rqdtf from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (2 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May 16 15:16:37.427: INFO: network-check-target-28wts from openshift-network-diagnostics started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container network-check-target-container ready: true, restart count 0
    May 16 15:16:37.427: INFO: ovnkube-node-npwgq from openshift-ovn-kubernetes started at 2023-05-16 13:52:17 +0000 UTC (5 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container ovn-controller ready: true, restart count 1
    May 16 15:16:37.427: INFO: 	Container ovnkube-node ready: true, restart count 0
    May 16 15:16:37.427: INFO: sonobuoy from sonobuoy started at 2023-05-16 14:11:07 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May 16 15:16:37.427: INFO: sonobuoy-e2e-job-840dd79630d44353 from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container e2e ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 16 15:16:37.427: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-7zsf6 from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
    May 16 15:16:37.427: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 16 15:16:37.427: INFO: 	Container systemd-logs ready: true, restart count 0
    May 16 15:16:37.427: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-161-164.eu-central-1.compute.internal before test
    May 16 15:16:37.468: INFO: agnhost-primary-flzlm from kubectl-6230 started at 2023-05-16 15:16:27 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container agnhost-primary ready: true, restart count 0
    May 16 15:16:37.468: INFO: aws-ebs-csi-driver-node-9m6vn from openshift-cluster-csi-drivers started at 2023-05-16 13:44:25 +0000 UTC (3 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container csi-driver ready: true, restart count 0
    May 16 15:16:37.468: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    May 16 15:16:37.468: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    May 16 15:16:37.468: INFO: tuned-6n5b7 from openshift-cluster-node-tuning-operator started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container tuned ready: true, restart count 0
    May 16 15:16:37.468: INFO: dns-default-pwtdq from openshift-dns started at 2023-05-16 14:48:42 +0000 UTC (2 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container dns ready: true, restart count 0
    May 16 15:16:37.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.468: INFO: node-resolver-jbmqq from openshift-dns started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May 16 15:16:37.468: INFO: node-ca-gw7rz from openshift-image-registry started at 2023-05-16 13:50:21 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container node-ca ready: true, restart count 0
    May 16 15:16:37.468: INFO: ingress-canary-mvqst from openshift-ingress-canary started at 2023-05-16 14:48:23 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May 16 15:16:37.468: INFO: machine-config-daemon-qhdsv from openshift-machine-config-operator started at 2023-05-16 13:44:25 +0000 UTC (2 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container machine-config-daemon ready: true, restart count 0
    May 16 15:16:37.468: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 15:16:37.468: INFO: node-exporter-l2zvs from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.468: INFO: 	Container node-exporter ready: true, restart count 0
    May 16 15:16:37.468: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-05-16 14:48:24 +0000 UTC (6 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container config-reloader ready: true, restart count 0
    May 16 15:16:37.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.468: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    May 16 15:16:37.468: INFO: 	Container prometheus ready: true, restart count 0
    May 16 15:16:37.468: INFO: 	Container prometheus-proxy ready: true, restart count 0
    May 16 15:16:37.468: INFO: 	Container thanos-sidecar ready: true, restart count 0
    May 16 15:16:37.468: INFO: multus-additional-cni-plugins-jkn7l from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May 16 15:16:37.468: INFO: multus-mc6qb from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container kube-multus ready: true, restart count 0
    May 16 15:16:37.468: INFO: network-metrics-daemon-n4p75 from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (2 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.468: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May 16 15:16:37.468: INFO: network-check-source-7f6b75fdb6-lxqsg from openshift-network-diagnostics started at 2023-05-16 14:48:21 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container check-endpoints ready: true, restart count 0
    May 16 15:16:37.468: INFO: network-check-target-bprb5 from openshift-network-diagnostics started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container network-check-target-container ready: true, restart count 0
    May 16 15:16:37.468: INFO: collect-profiles-28070820-8vnf5 from openshift-operator-lifecycle-manager started at 2023-05-16 15:00:00 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container collect-profiles ready: false, restart count 0
    May 16 15:16:37.468: INFO: collect-profiles-28070835-9g48l from openshift-operator-lifecycle-manager started at 2023-05-16 15:15:00 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container collect-profiles ready: false, restart count 0
    May 16 15:16:37.468: INFO: ovnkube-node-w8qn8 from openshift-ovn-kubernetes started at 2023-05-16 13:44:25 +0000 UTC (5 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.468: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    May 16 15:16:37.468: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    May 16 15:16:37.468: INFO: 	Container ovn-controller ready: true, restart count 1
    May 16 15:16:37.468: INFO: 	Container ovnkube-node ready: true, restart count 0
    May 16 15:16:37.468: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-5r4tm from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
    May 16 15:16:37.468: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 16 15:16:37.468: INFO: 	Container systemd-logs ready: true, restart count 0
    May 16 15:16:37.468: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-212-246.eu-central-1.compute.internal before test
    May 16 15:16:37.496: INFO: aws-ebs-csi-driver-node-b4s28 from openshift-cluster-csi-drivers started at 2023-05-16 13:52:19 +0000 UTC (3 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container csi-driver ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    May 16 15:16:37.496: INFO: tuned-8x9rx from openshift-cluster-node-tuning-operator started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container tuned ready: true, restart count 0
    May 16 15:16:37.496: INFO: dns-default-dbkcs from openshift-dns started at 2023-05-16 13:53:17 +0000 UTC (2 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container dns ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.496: INFO: node-resolver-gdzz2 from openshift-dns started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May 16 15:16:37.496: INFO: image-registry-b549d4f89-hs4qn from openshift-image-registry started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container registry ready: true, restart count 0
    May 16 15:16:37.496: INFO: node-ca-nsxtf from openshift-image-registry started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container node-ca ready: true, restart count 0
    May 16 15:16:37.496: INFO: ingress-canary-wphz5 from openshift-ingress-canary started at 2023-05-16 13:53:17 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May 16 15:16:37.496: INFO: router-default-8f95c5-bbmm5 from openshift-ingress started at 2023-05-16 14:48:21 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container router ready: true, restart count 0
    May 16 15:16:37.496: INFO: machine-config-daemon-z7sw6 from openshift-machine-config-operator started at 2023-05-16 13:52:19 +0000 UTC (2 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container machine-config-daemon ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 15:16:37.496: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (6 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container alertmanager ready: true, restart count 1
    May 16 15:16:37.496: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container config-reloader ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May 16 15:16:37.496: INFO: kube-state-metrics-6ccfb58dc4-wgwts from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (3 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container kube-state-metrics ready: true, restart count 0
    May 16 15:16:37.496: INFO: node-exporter-jrknj from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container node-exporter ready: true, restart count 0
    May 16 15:16:37.496: INFO: openshift-state-metrics-7d7f8b4cf8-9lbg2 from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (3 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    May 16 15:16:37.496: INFO: prometheus-adapter-7767c6bd47-bqlnh from openshift-monitoring started at 2023-05-16 13:53:50 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container prometheus-adapter ready: true, restart count 0
    May 16 15:16:37.496: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-05-16 13:53:57 +0000 UTC (6 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container config-reloader ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container prometheus ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container prometheus-proxy ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container thanos-sidecar ready: true, restart count 0
    May 16 15:16:37.496: INFO: prometheus-operator-admission-webhook-5d679565bb-96qsx from openshift-monitoring started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    May 16 15:16:37.496: INFO: thanos-querier-5c9d876b5c-wwd7q from openshift-monitoring started at 2023-05-16 13:53:54 +0000 UTC (6 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container thanos-query ready: true, restart count 0
    May 16 15:16:37.496: INFO: multus-additional-cni-plugins-wxcnn from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May 16 15:16:37.496: INFO: multus-rpn9n from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container kube-multus ready: true, restart count 0
    May 16 15:16:37.496: INFO: network-metrics-daemon-7sqls from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (2 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May 16 15:16:37.496: INFO: network-check-target-8pcqz from openshift-network-diagnostics started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container network-check-target-container ready: true, restart count 0
    May 16 15:16:37.496: INFO: ovnkube-node-kqvdm from openshift-ovn-kubernetes started at 2023-05-16 13:52:19 +0000 UTC (5 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container ovn-controller ready: true, restart count 1
    May 16 15:16:37.496: INFO: 	Container ovnkube-node ready: true, restart count 0
    May 16 15:16:37.496: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-gsqlr from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
    May 16 15:16:37.496: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 16 15:16:37.496: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node ip-10-0-132-142.eu-central-1.compute.internal 05/16/23 15:16:37.551
    STEP: verifying the node has the label node ip-10-0-161-164.eu-central-1.compute.internal 05/16/23 15:16:37.569
    STEP: verifying the node has the label node ip-10-0-212-246.eu-central-1.compute.internal 05/16/23 15:16:37.581
    May 16 15:16:37.631: INFO: Pod agnhost-primary-flzlm requesting resource cpu=0m on Node ip-10-0-161-164.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod aws-ebs-csi-driver-node-27d5z requesting resource cpu=30m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod aws-ebs-csi-driver-node-9m6vn requesting resource cpu=30m on Node ip-10-0-161-164.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod aws-ebs-csi-driver-node-b4s28 requesting resource cpu=30m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod tuned-6n5b7 requesting resource cpu=10m on Node ip-10-0-161-164.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod tuned-8x9rx requesting resource cpu=10m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod tuned-j9bjl requesting resource cpu=10m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod dns-default-9lcb7 requesting resource cpu=60m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod dns-default-dbkcs requesting resource cpu=60m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod dns-default-pwtdq requesting resource cpu=60m on Node ip-10-0-161-164.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod node-resolver-b47gh requesting resource cpu=5m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod node-resolver-gdzz2 requesting resource cpu=5m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod node-resolver-jbmqq requesting resource cpu=5m on Node ip-10-0-161-164.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod image-registry-b549d4f89-4ck4v requesting resource cpu=100m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod image-registry-b549d4f89-hs4qn requesting resource cpu=100m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod node-ca-gw7rz requesting resource cpu=10m on Node ip-10-0-161-164.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod node-ca-nsxtf requesting resource cpu=10m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod node-ca-px455 requesting resource cpu=10m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod ingress-canary-4hv85 requesting resource cpu=10m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod ingress-canary-mvqst requesting resource cpu=10m on Node ip-10-0-161-164.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod ingress-canary-wphz5 requesting resource cpu=10m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod router-default-8f95c5-bbmm5 requesting resource cpu=100m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod router-default-8f95c5-srcbl requesting resource cpu=100m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod machine-config-daemon-l6bpj requesting resource cpu=40m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod machine-config-daemon-qhdsv requesting resource cpu=40m on Node ip-10-0-161-164.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod machine-config-daemon-z7sw6 requesting resource cpu=40m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod kube-state-metrics-6ccfb58dc4-wgwts requesting resource cpu=4m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod node-exporter-jrknj requesting resource cpu=9m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod node-exporter-l2zvs requesting resource cpu=9m on Node ip-10-0-161-164.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod node-exporter-pltn9 requesting resource cpu=9m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod openshift-state-metrics-7d7f8b4cf8-9lbg2 requesting resource cpu=3m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod prometheus-adapter-7767c6bd47-bfq4d requesting resource cpu=1m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod prometheus-adapter-7767c6bd47-bqlnh requesting resource cpu=1m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node ip-10-0-161-164.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod prometheus-operator-admission-webhook-5d679565bb-8lkws requesting resource cpu=5m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod prometheus-operator-admission-webhook-5d679565bb-96qsx requesting resource cpu=5m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod telemeter-client-859dbffbb4-mt92v requesting resource cpu=3m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod thanos-querier-5c9d876b5c-wwd7q requesting resource cpu=15m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod thanos-querier-5c9d876b5c-xdt6m requesting resource cpu=15m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod multus-2wjqr requesting resource cpu=10m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod multus-additional-cni-plugins-jkn7l requesting resource cpu=10m on Node ip-10-0-161-164.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod multus-additional-cni-plugins-wxcnn requesting resource cpu=10m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod multus-additional-cni-plugins-z2h4l requesting resource cpu=10m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod multus-mc6qb requesting resource cpu=10m on Node ip-10-0-161-164.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod multus-rpn9n requesting resource cpu=10m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod network-metrics-daemon-7sqls requesting resource cpu=20m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod network-metrics-daemon-n4p75 requesting resource cpu=20m on Node ip-10-0-161-164.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod network-metrics-daemon-rqdtf requesting resource cpu=20m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod network-check-source-7f6b75fdb6-lxqsg requesting resource cpu=10m on Node ip-10-0-161-164.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod network-check-target-28wts requesting resource cpu=10m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod network-check-target-8pcqz requesting resource cpu=10m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod network-check-target-bprb5 requesting resource cpu=10m on Node ip-10-0-161-164.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod ovnkube-node-kqvdm requesting resource cpu=50m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod ovnkube-node-npwgq requesting resource cpu=50m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod ovnkube-node-w8qn8 requesting resource cpu=50m on Node ip-10-0-161-164.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod sonobuoy-e2e-job-840dd79630d44353 requesting resource cpu=0m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-5r4tm requesting resource cpu=0m on Node ip-10-0-161-164.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-7zsf6 requesting resource cpu=0m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.631: INFO: Pod sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-gsqlr requesting resource cpu=0m on Node ip-10-0-212-246.eu-central-1.compute.internal
    STEP: Starting Pods to consume most of the cluster CPU. 05/16/23 15:16:37.631
    May 16 15:16:37.631: INFO: Creating a pod which consumes cpu=2198m on Node ip-10-0-161-164.eu-central-1.compute.internal
    May 16 15:16:37.642: INFO: Creating a pod which consumes cpu=2039m on Node ip-10-0-212-246.eu-central-1.compute.internal
    May 16 15:16:37.654: INFO: Creating a pod which consumes cpu=2095m on Node ip-10-0-132-142.eu-central-1.compute.internal
    May 16 15:16:37.663: INFO: Waiting up to 5m0s for pod "filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128" in namespace "sched-pred-9029" to be "running"
    May 16 15:16:37.666: INFO: Pod "filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128": Phase="Pending", Reason="", readiness=false. Elapsed: 2.794423ms
    May 16 15:16:39.669: INFO: Pod "filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128": Phase="Running", Reason="", readiness=true. Elapsed: 2.005841189s
    May 16 15:16:39.669: INFO: Pod "filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128" satisfied condition "running"
    May 16 15:16:39.669: INFO: Waiting up to 5m0s for pod "filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7" in namespace "sched-pred-9029" to be "running"
    May 16 15:16:39.671: INFO: Pod "filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.266376ms
    May 16 15:16:39.671: INFO: Pod "filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7" satisfied condition "running"
    May 16 15:16:39.671: INFO: Waiting up to 5m0s for pod "filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd" in namespace "sched-pred-9029" to be "running"
    May 16 15:16:39.674: INFO: Pod "filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd": Phase="Running", Reason="", readiness=true. Elapsed: 3.156016ms
    May 16 15:16:39.674: INFO: Pod "filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 05/16/23 15:16:39.674
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd.175fa8a19937ae5c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9029/filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd to ip-10-0-132-142.eu-central-1.compute.internal] 05/16/23 15:16:39.677
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd.175fa8a1b548c1d1], Reason = [AddedInterface], Message = [Add eth0 [10.128.2.165/23] from ovn-kubernetes] 05/16/23 15:16:39.677
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd.175fa8a1b66e5bc9], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 05/16/23 15:16:39.677
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd.175fa8a1bc4cdd71], Reason = [Created], Message = [Created container filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd] 05/16/23 15:16:39.677
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd.175fa8a1bebbe82a], Reason = [Started], Message = [Started container filler-pod-104e1ff2-0b46-4065-b6fc-514f74c9fcfd] 05/16/23 15:16:39.677
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128.175fa8a197b0e77c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9029/filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128 to ip-10-0-161-164.eu-central-1.compute.internal] 05/16/23 15:16:39.677
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128.175fa8a1bfdd74db], Reason = [AddedInterface], Message = [Add eth0 [10.131.1.63/23] from ovn-kubernetes] 05/16/23 15:16:39.677
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128.175fa8a1c1061830], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 05/16/23 15:16:39.677
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128.175fa8a1c71be5ad], Reason = [Created], Message = [Created container filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128] 05/16/23 15:16:39.677
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128.175fa8a1c812e209], Reason = [Started], Message = [Started container filler-pod-48bf84c3-d48a-4c49-af64-393580fd5128] 05/16/23 15:16:39.678
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7.175fa8a1985a04bd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9029/filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7 to ip-10-0-212-246.eu-central-1.compute.internal] 05/16/23 15:16:39.678
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7.175fa8a1c0db3fba], Reason = [AddedInterface], Message = [Add eth0 [10.129.2.148/23] from ovn-kubernetes] 05/16/23 15:16:39.678
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7.175fa8a1c31903f0], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 05/16/23 15:16:39.678
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7.175fa8a1c8a77040], Reason = [Created], Message = [Created container filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7] 05/16/23 15:16:39.678
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7.175fa8a1c9e2a664], Reason = [Started], Message = [Started container filler-pod-c621f49f-40a8-4b9d-bcb1-270a7e66f2f7] 05/16/23 15:16:39.678
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.175fa8a2112fe707], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] 05/16/23 15:16:39.692
    STEP: removing the label node off the node ip-10-0-212-246.eu-central-1.compute.internal 05/16/23 15:16:40.689
    STEP: verifying the node doesn't have the label node 05/16/23 15:16:40.699
    STEP: removing the label node off the node ip-10-0-132-142.eu-central-1.compute.internal 05/16/23 15:16:40.705
    STEP: verifying the node doesn't have the label node 05/16/23 15:16:40.724
    STEP: removing the label node off the node ip-10-0-161-164.eu-central-1.compute.internal 05/16/23 15:16:40.727
    STEP: verifying the node doesn't have the label node 05/16/23 15:16:40.744
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 15:16:40.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9029" for this suite. 05/16/23 15:16:40.752
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:16:40.76
May 16 15:16:40.760: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename svcaccounts 05/16/23 15:16:40.76
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:40.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:40.786
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  05/16/23 15:16:40.789
May 16 15:16:40.803: INFO: Waiting up to 5m0s for pod "test-pod-8778901c-e3f8-4282-99ca-76b3f3c60141" in namespace "svcaccounts-246" to be "Succeeded or Failed"
May 16 15:16:40.808: INFO: Pod "test-pod-8778901c-e3f8-4282-99ca-76b3f3c60141": Phase="Pending", Reason="", readiness=false. Elapsed: 5.658692ms
May 16 15:16:42.813: INFO: Pod "test-pod-8778901c-e3f8-4282-99ca-76b3f3c60141": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010421794s
May 16 15:16:44.812: INFO: Pod "test-pod-8778901c-e3f8-4282-99ca-76b3f3c60141": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009093829s
STEP: Saw pod success 05/16/23 15:16:44.812
May 16 15:16:44.812: INFO: Pod "test-pod-8778901c-e3f8-4282-99ca-76b3f3c60141" satisfied condition "Succeeded or Failed"
May 16 15:16:44.815: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod test-pod-8778901c-e3f8-4282-99ca-76b3f3c60141 container agnhost-container: <nil>
STEP: delete the pod 05/16/23 15:16:44.822
May 16 15:16:44.832: INFO: Waiting for pod test-pod-8778901c-e3f8-4282-99ca-76b3f3c60141 to disappear
May 16 15:16:44.834: INFO: Pod test-pod-8778901c-e3f8-4282-99ca-76b3f3c60141 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
May 16 15:16:44.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-246" for this suite. 05/16/23 15:16:44.847
------------------------------
â€¢ [4.095 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:16:40.76
    May 16 15:16:40.760: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename svcaccounts 05/16/23 15:16:40.76
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:40.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:40.786
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  05/16/23 15:16:40.789
    May 16 15:16:40.803: INFO: Waiting up to 5m0s for pod "test-pod-8778901c-e3f8-4282-99ca-76b3f3c60141" in namespace "svcaccounts-246" to be "Succeeded or Failed"
    May 16 15:16:40.808: INFO: Pod "test-pod-8778901c-e3f8-4282-99ca-76b3f3c60141": Phase="Pending", Reason="", readiness=false. Elapsed: 5.658692ms
    May 16 15:16:42.813: INFO: Pod "test-pod-8778901c-e3f8-4282-99ca-76b3f3c60141": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010421794s
    May 16 15:16:44.812: INFO: Pod "test-pod-8778901c-e3f8-4282-99ca-76b3f3c60141": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009093829s
    STEP: Saw pod success 05/16/23 15:16:44.812
    May 16 15:16:44.812: INFO: Pod "test-pod-8778901c-e3f8-4282-99ca-76b3f3c60141" satisfied condition "Succeeded or Failed"
    May 16 15:16:44.815: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod test-pod-8778901c-e3f8-4282-99ca-76b3f3c60141 container agnhost-container: <nil>
    STEP: delete the pod 05/16/23 15:16:44.822
    May 16 15:16:44.832: INFO: Waiting for pod test-pod-8778901c-e3f8-4282-99ca-76b3f3c60141 to disappear
    May 16 15:16:44.834: INFO: Pod test-pod-8778901c-e3f8-4282-99ca-76b3f3c60141 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    May 16 15:16:44.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-246" for this suite. 05/16/23 15:16:44.847
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:16:44.854
May 16 15:16:44.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename downward-api 05/16/23 15:16:44.855
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:44.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:44.872
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 05/16/23 15:16:44.875
May 16 15:16:44.889: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d561623-4793-4ecc-85d3-f1c7bd994c82" in namespace "downward-api-4611" to be "Succeeded or Failed"
May 16 15:16:44.893: INFO: Pod "downwardapi-volume-5d561623-4793-4ecc-85d3-f1c7bd994c82": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07651ms
May 16 15:16:46.897: INFO: Pod "downwardapi-volume-5d561623-4793-4ecc-85d3-f1c7bd994c82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007778867s
May 16 15:16:48.898: INFO: Pod "downwardapi-volume-5d561623-4793-4ecc-85d3-f1c7bd994c82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008529879s
STEP: Saw pod success 05/16/23 15:16:48.898
May 16 15:16:48.898: INFO: Pod "downwardapi-volume-5d561623-4793-4ecc-85d3-f1c7bd994c82" satisfied condition "Succeeded or Failed"
May 16 15:16:48.901: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-5d561623-4793-4ecc-85d3-f1c7bd994c82 container client-container: <nil>
STEP: delete the pod 05/16/23 15:16:48.906
May 16 15:16:48.915: INFO: Waiting for pod downwardapi-volume-5d561623-4793-4ecc-85d3-f1c7bd994c82 to disappear
May 16 15:16:48.917: INFO: Pod downwardapi-volume-5d561623-4793-4ecc-85d3-f1c7bd994c82 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 16 15:16:48.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4611" for this suite. 05/16/23 15:16:48.92
------------------------------
â€¢ [4.071 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:16:44.854
    May 16 15:16:44.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename downward-api 05/16/23 15:16:44.855
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:44.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:44.872
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 05/16/23 15:16:44.875
    May 16 15:16:44.889: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d561623-4793-4ecc-85d3-f1c7bd994c82" in namespace "downward-api-4611" to be "Succeeded or Failed"
    May 16 15:16:44.893: INFO: Pod "downwardapi-volume-5d561623-4793-4ecc-85d3-f1c7bd994c82": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07651ms
    May 16 15:16:46.897: INFO: Pod "downwardapi-volume-5d561623-4793-4ecc-85d3-f1c7bd994c82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007778867s
    May 16 15:16:48.898: INFO: Pod "downwardapi-volume-5d561623-4793-4ecc-85d3-f1c7bd994c82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008529879s
    STEP: Saw pod success 05/16/23 15:16:48.898
    May 16 15:16:48.898: INFO: Pod "downwardapi-volume-5d561623-4793-4ecc-85d3-f1c7bd994c82" satisfied condition "Succeeded or Failed"
    May 16 15:16:48.901: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-5d561623-4793-4ecc-85d3-f1c7bd994c82 container client-container: <nil>
    STEP: delete the pod 05/16/23 15:16:48.906
    May 16 15:16:48.915: INFO: Waiting for pod downwardapi-volume-5d561623-4793-4ecc-85d3-f1c7bd994c82 to disappear
    May 16 15:16:48.917: INFO: Pod downwardapi-volume-5d561623-4793-4ecc-85d3-f1c7bd994c82 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 16 15:16:48.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4611" for this suite. 05/16/23 15:16:48.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:16:48.926
May 16 15:16:48.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename containers 05/16/23 15:16:48.926
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:48.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:48.945
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 05/16/23 15:16:48.948
W0516 15:16:48.962109      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:16:48.962: INFO: Waiting up to 5m0s for pod "client-containers-44845b41-4582-4d5a-8b7d-5e85c3b4f1b0" in namespace "containers-8842" to be "Succeeded or Failed"
May 16 15:16:48.966: INFO: Pod "client-containers-44845b41-4582-4d5a-8b7d-5e85c3b4f1b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.215035ms
May 16 15:16:50.972: INFO: Pod "client-containers-44845b41-4582-4d5a-8b7d-5e85c3b4f1b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009978088s
May 16 15:16:52.970: INFO: Pod "client-containers-44845b41-4582-4d5a-8b7d-5e85c3b4f1b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008615468s
STEP: Saw pod success 05/16/23 15:16:52.97
May 16 15:16:52.970: INFO: Pod "client-containers-44845b41-4582-4d5a-8b7d-5e85c3b4f1b0" satisfied condition "Succeeded or Failed"
May 16 15:16:52.973: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod client-containers-44845b41-4582-4d5a-8b7d-5e85c3b4f1b0 container agnhost-container: <nil>
STEP: delete the pod 05/16/23 15:16:52.979
May 16 15:16:52.989: INFO: Waiting for pod client-containers-44845b41-4582-4d5a-8b7d-5e85c3b4f1b0 to disappear
May 16 15:16:52.992: INFO: Pod client-containers-44845b41-4582-4d5a-8b7d-5e85c3b4f1b0 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
May 16 15:16:52.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-8842" for this suite. 05/16/23 15:16:52.997
------------------------------
â€¢ [4.077 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:16:48.926
    May 16 15:16:48.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename containers 05/16/23 15:16:48.926
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:48.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:48.945
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 05/16/23 15:16:48.948
    W0516 15:16:48.962109      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:16:48.962: INFO: Waiting up to 5m0s for pod "client-containers-44845b41-4582-4d5a-8b7d-5e85c3b4f1b0" in namespace "containers-8842" to be "Succeeded or Failed"
    May 16 15:16:48.966: INFO: Pod "client-containers-44845b41-4582-4d5a-8b7d-5e85c3b4f1b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.215035ms
    May 16 15:16:50.972: INFO: Pod "client-containers-44845b41-4582-4d5a-8b7d-5e85c3b4f1b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009978088s
    May 16 15:16:52.970: INFO: Pod "client-containers-44845b41-4582-4d5a-8b7d-5e85c3b4f1b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008615468s
    STEP: Saw pod success 05/16/23 15:16:52.97
    May 16 15:16:52.970: INFO: Pod "client-containers-44845b41-4582-4d5a-8b7d-5e85c3b4f1b0" satisfied condition "Succeeded or Failed"
    May 16 15:16:52.973: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod client-containers-44845b41-4582-4d5a-8b7d-5e85c3b4f1b0 container agnhost-container: <nil>
    STEP: delete the pod 05/16/23 15:16:52.979
    May 16 15:16:52.989: INFO: Waiting for pod client-containers-44845b41-4582-4d5a-8b7d-5e85c3b4f1b0 to disappear
    May 16 15:16:52.992: INFO: Pod client-containers-44845b41-4582-4d5a-8b7d-5e85c3b4f1b0 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    May 16 15:16:52.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-8842" for this suite. 05/16/23 15:16:52.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:16:53.003
May 16 15:16:53.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename container-probe 05/16/23 15:16:53.004
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:53.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:53.023
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-27f64994-1b57-4c5b-9f3e-2875623696f0 in namespace container-probe-2076 05/16/23 15:16:53.025
May 16 15:16:53.037: INFO: Waiting up to 5m0s for pod "test-webserver-27f64994-1b57-4c5b-9f3e-2875623696f0" in namespace "container-probe-2076" to be "not pending"
May 16 15:16:53.039: INFO: Pod "test-webserver-27f64994-1b57-4c5b-9f3e-2875623696f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.429275ms
May 16 15:16:55.044: INFO: Pod "test-webserver-27f64994-1b57-4c5b-9f3e-2875623696f0": Phase="Running", Reason="", readiness=true. Elapsed: 2.007372076s
May 16 15:16:55.044: INFO: Pod "test-webserver-27f64994-1b57-4c5b-9f3e-2875623696f0" satisfied condition "not pending"
May 16 15:16:55.044: INFO: Started pod test-webserver-27f64994-1b57-4c5b-9f3e-2875623696f0 in namespace container-probe-2076
STEP: checking the pod's current state and verifying that restartCount is present 05/16/23 15:16:55.044
May 16 15:16:55.047: INFO: Initial restart count of pod test-webserver-27f64994-1b57-4c5b-9f3e-2875623696f0 is 0
STEP: deleting the pod 05/16/23 15:20:55.578
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May 16 15:20:55.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2076" for this suite. 05/16/23 15:20:55.597
------------------------------
â€¢ [SLOW TEST] [242.601 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:16:53.003
    May 16 15:16:53.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename container-probe 05/16/23 15:16:53.004
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:16:53.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:16:53.023
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-27f64994-1b57-4c5b-9f3e-2875623696f0 in namespace container-probe-2076 05/16/23 15:16:53.025
    May 16 15:16:53.037: INFO: Waiting up to 5m0s for pod "test-webserver-27f64994-1b57-4c5b-9f3e-2875623696f0" in namespace "container-probe-2076" to be "not pending"
    May 16 15:16:53.039: INFO: Pod "test-webserver-27f64994-1b57-4c5b-9f3e-2875623696f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.429275ms
    May 16 15:16:55.044: INFO: Pod "test-webserver-27f64994-1b57-4c5b-9f3e-2875623696f0": Phase="Running", Reason="", readiness=true. Elapsed: 2.007372076s
    May 16 15:16:55.044: INFO: Pod "test-webserver-27f64994-1b57-4c5b-9f3e-2875623696f0" satisfied condition "not pending"
    May 16 15:16:55.044: INFO: Started pod test-webserver-27f64994-1b57-4c5b-9f3e-2875623696f0 in namespace container-probe-2076
    STEP: checking the pod's current state and verifying that restartCount is present 05/16/23 15:16:55.044
    May 16 15:16:55.047: INFO: Initial restart count of pod test-webserver-27f64994-1b57-4c5b-9f3e-2875623696f0 is 0
    STEP: deleting the pod 05/16/23 15:20:55.578
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May 16 15:20:55.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2076" for this suite. 05/16/23 15:20:55.597
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:20:55.604
May 16 15:20:55.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename downward-api 05/16/23 15:20:55.604
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:20:55.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:20:55.63
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 05/16/23 15:20:55.633
May 16 15:20:55.654: INFO: Waiting up to 5m0s for pod "downward-api-c94a1b1f-adff-4758-9ef1-a07ff4605166" in namespace "downward-api-6490" to be "Succeeded or Failed"
May 16 15:20:55.659: INFO: Pod "downward-api-c94a1b1f-adff-4758-9ef1-a07ff4605166": Phase="Pending", Reason="", readiness=false. Elapsed: 5.126613ms
May 16 15:20:57.677: INFO: Pod "downward-api-c94a1b1f-adff-4758-9ef1-a07ff4605166": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022640101s
May 16 15:20:59.664: INFO: Pod "downward-api-c94a1b1f-adff-4758-9ef1-a07ff4605166": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010188184s
STEP: Saw pod success 05/16/23 15:20:59.664
May 16 15:20:59.664: INFO: Pod "downward-api-c94a1b1f-adff-4758-9ef1-a07ff4605166" satisfied condition "Succeeded or Failed"
May 16 15:20:59.667: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downward-api-c94a1b1f-adff-4758-9ef1-a07ff4605166 container dapi-container: <nil>
STEP: delete the pod 05/16/23 15:20:59.677
May 16 15:20:59.686: INFO: Waiting for pod downward-api-c94a1b1f-adff-4758-9ef1-a07ff4605166 to disappear
May 16 15:20:59.688: INFO: Pod downward-api-c94a1b1f-adff-4758-9ef1-a07ff4605166 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May 16 15:20:59.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6490" for this suite. 05/16/23 15:20:59.692
------------------------------
â€¢ [4.094 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:20:55.604
    May 16 15:20:55.604: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename downward-api 05/16/23 15:20:55.604
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:20:55.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:20:55.63
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 05/16/23 15:20:55.633
    May 16 15:20:55.654: INFO: Waiting up to 5m0s for pod "downward-api-c94a1b1f-adff-4758-9ef1-a07ff4605166" in namespace "downward-api-6490" to be "Succeeded or Failed"
    May 16 15:20:55.659: INFO: Pod "downward-api-c94a1b1f-adff-4758-9ef1-a07ff4605166": Phase="Pending", Reason="", readiness=false. Elapsed: 5.126613ms
    May 16 15:20:57.677: INFO: Pod "downward-api-c94a1b1f-adff-4758-9ef1-a07ff4605166": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022640101s
    May 16 15:20:59.664: INFO: Pod "downward-api-c94a1b1f-adff-4758-9ef1-a07ff4605166": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010188184s
    STEP: Saw pod success 05/16/23 15:20:59.664
    May 16 15:20:59.664: INFO: Pod "downward-api-c94a1b1f-adff-4758-9ef1-a07ff4605166" satisfied condition "Succeeded or Failed"
    May 16 15:20:59.667: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downward-api-c94a1b1f-adff-4758-9ef1-a07ff4605166 container dapi-container: <nil>
    STEP: delete the pod 05/16/23 15:20:59.677
    May 16 15:20:59.686: INFO: Waiting for pod downward-api-c94a1b1f-adff-4758-9ef1-a07ff4605166 to disappear
    May 16 15:20:59.688: INFO: Pod downward-api-c94a1b1f-adff-4758-9ef1-a07ff4605166 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May 16 15:20:59.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6490" for this suite. 05/16/23 15:20:59.692
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:20:59.698
May 16 15:20:59.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename namespaces 05/16/23 15:20:59.699
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:20:59.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:20:59.72
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 05/16/23 15:20:59.723
May 16 15:20:59.726: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 05/16/23 15:20:59.726
May 16 15:20:59.732: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 05/16/23 15:20:59.732
May 16 15:20:59.742: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 15:20:59.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6430" for this suite. 05/16/23 15:20:59.748
------------------------------
â€¢ [0.062 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:20:59.698
    May 16 15:20:59.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename namespaces 05/16/23 15:20:59.699
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:20:59.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:20:59.72
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 05/16/23 15:20:59.723
    May 16 15:20:59.726: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 05/16/23 15:20:59.726
    May 16 15:20:59.732: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 05/16/23 15:20:59.732
    May 16 15:20:59.742: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 15:20:59.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6430" for this suite. 05/16/23 15:20:59.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:20:59.764
May 16 15:20:59.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename downward-api 05/16/23 15:20:59.765
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:20:59.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:20:59.784
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 05/16/23 15:20:59.787
May 16 15:20:59.798: INFO: Waiting up to 5m0s for pod "downward-api-19e21991-f04f-4562-a3a6-4b1d600cc408" in namespace "downward-api-6213" to be "Succeeded or Failed"
May 16 15:20:59.804: INFO: Pod "downward-api-19e21991-f04f-4562-a3a6-4b1d600cc408": Phase="Pending", Reason="", readiness=false. Elapsed: 5.481459ms
May 16 15:21:01.807: INFO: Pod "downward-api-19e21991-f04f-4562-a3a6-4b1d600cc408": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008688291s
May 16 15:21:03.807: INFO: Pod "downward-api-19e21991-f04f-4562-a3a6-4b1d600cc408": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008883447s
STEP: Saw pod success 05/16/23 15:21:03.807
May 16 15:21:03.807: INFO: Pod "downward-api-19e21991-f04f-4562-a3a6-4b1d600cc408" satisfied condition "Succeeded or Failed"
May 16 15:21:03.811: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downward-api-19e21991-f04f-4562-a3a6-4b1d600cc408 container dapi-container: <nil>
STEP: delete the pod 05/16/23 15:21:03.817
May 16 15:21:03.825: INFO: Waiting for pod downward-api-19e21991-f04f-4562-a3a6-4b1d600cc408 to disappear
May 16 15:21:03.828: INFO: Pod downward-api-19e21991-f04f-4562-a3a6-4b1d600cc408 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May 16 15:21:03.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6213" for this suite. 05/16/23 15:21:03.832
------------------------------
â€¢ [4.074 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:20:59.764
    May 16 15:20:59.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename downward-api 05/16/23 15:20:59.765
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:20:59.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:20:59.784
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 05/16/23 15:20:59.787
    May 16 15:20:59.798: INFO: Waiting up to 5m0s for pod "downward-api-19e21991-f04f-4562-a3a6-4b1d600cc408" in namespace "downward-api-6213" to be "Succeeded or Failed"
    May 16 15:20:59.804: INFO: Pod "downward-api-19e21991-f04f-4562-a3a6-4b1d600cc408": Phase="Pending", Reason="", readiness=false. Elapsed: 5.481459ms
    May 16 15:21:01.807: INFO: Pod "downward-api-19e21991-f04f-4562-a3a6-4b1d600cc408": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008688291s
    May 16 15:21:03.807: INFO: Pod "downward-api-19e21991-f04f-4562-a3a6-4b1d600cc408": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008883447s
    STEP: Saw pod success 05/16/23 15:21:03.807
    May 16 15:21:03.807: INFO: Pod "downward-api-19e21991-f04f-4562-a3a6-4b1d600cc408" satisfied condition "Succeeded or Failed"
    May 16 15:21:03.811: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downward-api-19e21991-f04f-4562-a3a6-4b1d600cc408 container dapi-container: <nil>
    STEP: delete the pod 05/16/23 15:21:03.817
    May 16 15:21:03.825: INFO: Waiting for pod downward-api-19e21991-f04f-4562-a3a6-4b1d600cc408 to disappear
    May 16 15:21:03.828: INFO: Pod downward-api-19e21991-f04f-4562-a3a6-4b1d600cc408 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May 16 15:21:03.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6213" for this suite. 05/16/23 15:21:03.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:21:03.838
May 16 15:21:03.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename sched-preemption 05/16/23 15:21:03.839
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:21:03.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:21:03.859
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
May 16 15:21:03.878: INFO: Waiting up to 1m0s for all nodes to be ready
May 16 15:22:04.000: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:22:04.005
May 16 15:22:04.005: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename sched-preemption-path 05/16/23 15:22:04.006
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:22:04.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:22:04.033
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
May 16 15:22:04.055: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
May 16 15:22:04.062: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
May 16 15:22:04.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 15:22:04.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-6395" for this suite. 05/16/23 15:22:04.148
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1160" for this suite. 05/16/23 15:22:04.156
------------------------------
â€¢ [SLOW TEST] [60.324 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:21:03.838
    May 16 15:21:03.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename sched-preemption 05/16/23 15:21:03.839
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:21:03.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:21:03.859
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    May 16 15:21:03.878: INFO: Waiting up to 1m0s for all nodes to be ready
    May 16 15:22:04.000: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:22:04.005
    May 16 15:22:04.005: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename sched-preemption-path 05/16/23 15:22:04.006
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:22:04.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:22:04.033
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    May 16 15:22:04.055: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    May 16 15:22:04.062: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    May 16 15:22:04.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 15:22:04.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-6395" for this suite. 05/16/23 15:22:04.148
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1160" for this suite. 05/16/23 15:22:04.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:22:04.162
May 16 15:22:04.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename pod-network-test 05/16/23 15:22:04.163
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:22:04.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:22:04.186
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-2474 05/16/23 15:22:04.189
STEP: creating a selector 05/16/23 15:22:04.189
STEP: Creating the service pods in kubernetes 05/16/23 15:22:04.189
May 16 15:22:04.189: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 16 15:22:04.236: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2474" to be "running and ready"
May 16 15:22:04.245: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.065519ms
May 16 15:22:04.245: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 16 15:22:06.248: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.012898392s
May 16 15:22:06.249: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 15:22:08.249: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013488921s
May 16 15:22:08.249: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 15:22:10.249: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.013210121s
May 16 15:22:10.249: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 15:22:12.249: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013206012s
May 16 15:22:12.249: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 15:22:14.249: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.012904523s
May 16 15:22:14.249: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 15:22:16.249: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.013697743s
May 16 15:22:16.249: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May 16 15:22:16.249: INFO: Pod "netserver-0" satisfied condition "running and ready"
May 16 15:22:16.253: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2474" to be "running and ready"
May 16 15:22:16.255: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.319863ms
May 16 15:22:16.255: INFO: The phase of Pod netserver-1 is Running (Ready = false)
May 16 15:22:18.258: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.005855945s
May 16 15:22:18.258: INFO: The phase of Pod netserver-1 is Running (Ready = false)
May 16 15:22:20.261: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.008033085s
May 16 15:22:20.261: INFO: The phase of Pod netserver-1 is Running (Ready = false)
May 16 15:22:22.258: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.005799126s
May 16 15:22:22.258: INFO: The phase of Pod netserver-1 is Running (Ready = false)
May 16 15:22:24.260: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.007474845s
May 16 15:22:24.260: INFO: The phase of Pod netserver-1 is Running (Ready = false)
May 16 15:22:26.259: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.006373513s
May 16 15:22:26.259: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May 16 15:22:26.259: INFO: Pod "netserver-1" satisfied condition "running and ready"
May 16 15:22:26.261: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2474" to be "running and ready"
May 16 15:22:26.265: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.384156ms
May 16 15:22:26.265: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May 16 15:22:26.265: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/16/23 15:22:26.268
May 16 15:22:26.274: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2474" to be "running"
May 16 15:22:26.276: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.174475ms
May 16 15:22:28.280: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00611558s
May 16 15:22:28.280: INFO: Pod "test-container-pod" satisfied condition "running"
May 16 15:22:28.283: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 16 15:22:28.283: INFO: Breadth first check of 10.128.2.166 on host 10.0.132.142...
May 16 15:22:28.285: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.71:9080/dial?request=hostname&protocol=http&host=10.128.2.166&port=8083&tries=1'] Namespace:pod-network-test-2474 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:22:28.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:22:28.286: INFO: ExecWithOptions: Clientset creation
May 16 15:22:28.286: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-2474/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.1.71%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.2.166%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May 16 15:22:28.360: INFO: Waiting for responses: map[]
May 16 15:22:28.360: INFO: reached 10.128.2.166 after 0/1 tries
May 16 15:22:28.360: INFO: Breadth first check of 10.131.1.70 on host 10.0.161.164...
May 16 15:22:28.363: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.71:9080/dial?request=hostname&protocol=http&host=10.131.1.70&port=8083&tries=1'] Namespace:pod-network-test-2474 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:22:28.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:22:28.364: INFO: ExecWithOptions: Clientset creation
May 16 15:22:28.364: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-2474/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.1.71%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.131.1.70%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May 16 15:22:28.438: INFO: Waiting for responses: map[]
May 16 15:22:28.438: INFO: reached 10.131.1.70 after 0/1 tries
May 16 15:22:28.438: INFO: Breadth first check of 10.129.2.149 on host 10.0.212.246...
May 16 15:22:28.442: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.71:9080/dial?request=hostname&protocol=http&host=10.129.2.149&port=8083&tries=1'] Namespace:pod-network-test-2474 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:22:28.442: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:22:28.442: INFO: ExecWithOptions: Clientset creation
May 16 15:22:28.442: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-2474/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.1.71%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.129.2.149%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
May 16 15:22:28.494: INFO: Waiting for responses: map[]
May 16 15:22:28.494: INFO: reached 10.129.2.149 after 0/1 tries
May 16 15:22:28.494: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
May 16 15:22:28.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2474" for this suite. 05/16/23 15:22:28.499
------------------------------
â€¢ [SLOW TEST] [24.342 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:22:04.162
    May 16 15:22:04.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename pod-network-test 05/16/23 15:22:04.163
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:22:04.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:22:04.186
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-2474 05/16/23 15:22:04.189
    STEP: creating a selector 05/16/23 15:22:04.189
    STEP: Creating the service pods in kubernetes 05/16/23 15:22:04.189
    May 16 15:22:04.189: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May 16 15:22:04.236: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2474" to be "running and ready"
    May 16 15:22:04.245: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.065519ms
    May 16 15:22:04.245: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:22:06.248: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.012898392s
    May 16 15:22:06.249: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 15:22:08.249: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013488921s
    May 16 15:22:08.249: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 15:22:10.249: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.013210121s
    May 16 15:22:10.249: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 15:22:12.249: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013206012s
    May 16 15:22:12.249: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 15:22:14.249: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.012904523s
    May 16 15:22:14.249: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 15:22:16.249: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.013697743s
    May 16 15:22:16.249: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May 16 15:22:16.249: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May 16 15:22:16.253: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2474" to be "running and ready"
    May 16 15:22:16.255: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.319863ms
    May 16 15:22:16.255: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    May 16 15:22:18.258: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.005855945s
    May 16 15:22:18.258: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    May 16 15:22:20.261: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.008033085s
    May 16 15:22:20.261: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    May 16 15:22:22.258: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.005799126s
    May 16 15:22:22.258: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    May 16 15:22:24.260: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.007474845s
    May 16 15:22:24.260: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    May 16 15:22:26.259: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.006373513s
    May 16 15:22:26.259: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May 16 15:22:26.259: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May 16 15:22:26.261: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2474" to be "running and ready"
    May 16 15:22:26.265: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.384156ms
    May 16 15:22:26.265: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May 16 15:22:26.265: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/16/23 15:22:26.268
    May 16 15:22:26.274: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2474" to be "running"
    May 16 15:22:26.276: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.174475ms
    May 16 15:22:28.280: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00611558s
    May 16 15:22:28.280: INFO: Pod "test-container-pod" satisfied condition "running"
    May 16 15:22:28.283: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May 16 15:22:28.283: INFO: Breadth first check of 10.128.2.166 on host 10.0.132.142...
    May 16 15:22:28.285: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.71:9080/dial?request=hostname&protocol=http&host=10.128.2.166&port=8083&tries=1'] Namespace:pod-network-test-2474 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:22:28.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:22:28.286: INFO: ExecWithOptions: Clientset creation
    May 16 15:22:28.286: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-2474/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.1.71%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.2.166%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May 16 15:22:28.360: INFO: Waiting for responses: map[]
    May 16 15:22:28.360: INFO: reached 10.128.2.166 after 0/1 tries
    May 16 15:22:28.360: INFO: Breadth first check of 10.131.1.70 on host 10.0.161.164...
    May 16 15:22:28.363: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.71:9080/dial?request=hostname&protocol=http&host=10.131.1.70&port=8083&tries=1'] Namespace:pod-network-test-2474 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:22:28.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:22:28.364: INFO: ExecWithOptions: Clientset creation
    May 16 15:22:28.364: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-2474/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.1.71%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.131.1.70%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May 16 15:22:28.438: INFO: Waiting for responses: map[]
    May 16 15:22:28.438: INFO: reached 10.131.1.70 after 0/1 tries
    May 16 15:22:28.438: INFO: Breadth first check of 10.129.2.149 on host 10.0.212.246...
    May 16 15:22:28.442: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.131.1.71:9080/dial?request=hostname&protocol=http&host=10.129.2.149&port=8083&tries=1'] Namespace:pod-network-test-2474 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:22:28.442: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:22:28.442: INFO: ExecWithOptions: Clientset creation
    May 16 15:22:28.442: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-2474/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.131.1.71%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.129.2.149%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    May 16 15:22:28.494: INFO: Waiting for responses: map[]
    May 16 15:22:28.494: INFO: reached 10.129.2.149 after 0/1 tries
    May 16 15:22:28.494: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    May 16 15:22:28.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2474" for this suite. 05/16/23 15:22:28.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:22:28.505
May 16 15:22:28.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename disruption 05/16/23 15:22:28.506
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:22:28.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:22:28.523
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 05/16/23 15:22:28.525
STEP: Waiting for the pdb to be processed 05/16/23 15:22:28.531
STEP: First trying to evict a pod which shouldn't be evictable 05/16/23 15:22:28.545
STEP: Waiting for all pods to be running 05/16/23 15:22:28.545
May 16 15:22:28.552: INFO: pods: 0 < 3
STEP: locating a running pod 05/16/23 15:22:30.556
STEP: Updating the pdb to allow a pod to be evicted 05/16/23 15:22:30.565
STEP: Waiting for the pdb to be processed 05/16/23 15:22:30.571
STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/16/23 15:22:32.579
STEP: Waiting for all pods to be running 05/16/23 15:22:32.579
STEP: Waiting for the pdb to observed all healthy pods 05/16/23 15:22:32.582
STEP: Patching the pdb to disallow a pod to be evicted 05/16/23 15:22:32.605
STEP: Waiting for the pdb to be processed 05/16/23 15:22:32.62
STEP: Waiting for all pods to be running 05/16/23 15:22:34.629
STEP: locating a running pod 05/16/23 15:22:34.632
STEP: Deleting the pdb to allow a pod to be evicted 05/16/23 15:22:34.639
STEP: Waiting for the pdb to be deleted 05/16/23 15:22:34.645
STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/16/23 15:22:34.648
STEP: Waiting for all pods to be running 05/16/23 15:22:34.648
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May 16 15:22:34.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6295" for this suite. 05/16/23 15:22:34.669
------------------------------
â€¢ [SLOW TEST] [6.170 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:22:28.505
    May 16 15:22:28.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename disruption 05/16/23 15:22:28.506
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:22:28.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:22:28.523
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 05/16/23 15:22:28.525
    STEP: Waiting for the pdb to be processed 05/16/23 15:22:28.531
    STEP: First trying to evict a pod which shouldn't be evictable 05/16/23 15:22:28.545
    STEP: Waiting for all pods to be running 05/16/23 15:22:28.545
    May 16 15:22:28.552: INFO: pods: 0 < 3
    STEP: locating a running pod 05/16/23 15:22:30.556
    STEP: Updating the pdb to allow a pod to be evicted 05/16/23 15:22:30.565
    STEP: Waiting for the pdb to be processed 05/16/23 15:22:30.571
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/16/23 15:22:32.579
    STEP: Waiting for all pods to be running 05/16/23 15:22:32.579
    STEP: Waiting for the pdb to observed all healthy pods 05/16/23 15:22:32.582
    STEP: Patching the pdb to disallow a pod to be evicted 05/16/23 15:22:32.605
    STEP: Waiting for the pdb to be processed 05/16/23 15:22:32.62
    STEP: Waiting for all pods to be running 05/16/23 15:22:34.629
    STEP: locating a running pod 05/16/23 15:22:34.632
    STEP: Deleting the pdb to allow a pod to be evicted 05/16/23 15:22:34.639
    STEP: Waiting for the pdb to be deleted 05/16/23 15:22:34.645
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 05/16/23 15:22:34.648
    STEP: Waiting for all pods to be running 05/16/23 15:22:34.648
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May 16 15:22:34.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6295" for this suite. 05/16/23 15:22:34.669
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:22:34.675
May 16 15:22:34.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename podtemplate 05/16/23 15:22:34.676
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:22:34.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:22:34.699
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
W0516 15:22:34.712583      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
May 16 15:22:34.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-2844" for this suite. 05/16/23 15:22:34.749
------------------------------
â€¢ [0.084 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:22:34.675
    May 16 15:22:34.675: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename podtemplate 05/16/23 15:22:34.676
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:22:34.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:22:34.699
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    W0516 15:22:34.712583      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    May 16 15:22:34.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-2844" for this suite. 05/16/23 15:22:34.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:22:34.76
May 16 15:22:34.760: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename prestop 05/16/23 15:22:34.761
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:22:34.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:22:34.784
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-6718 05/16/23 15:22:34.786
STEP: Waiting for pods to come up. 05/16/23 15:22:34.797
May 16 15:22:34.797: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6718" to be "running"
May 16 15:22:34.801: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.98802ms
May 16 15:22:36.806: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.008815253s
May 16 15:22:36.806: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-6718 05/16/23 15:22:36.808
May 16 15:22:36.815: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6718" to be "running"
May 16 15:22:36.819: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.688664ms
May 16 15:22:38.823: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.007607553s
May 16 15:22:38.823: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 05/16/23 15:22:38.823
May 16 15:22:43.836: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 05/16/23 15:22:43.836
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
May 16 15:22:43.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-6718" for this suite. 05/16/23 15:22:43.853
------------------------------
â€¢ [SLOW TEST] [9.099 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:22:34.76
    May 16 15:22:34.760: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename prestop 05/16/23 15:22:34.761
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:22:34.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:22:34.784
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-6718 05/16/23 15:22:34.786
    STEP: Waiting for pods to come up. 05/16/23 15:22:34.797
    May 16 15:22:34.797: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6718" to be "running"
    May 16 15:22:34.801: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.98802ms
    May 16 15:22:36.806: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.008815253s
    May 16 15:22:36.806: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-6718 05/16/23 15:22:36.808
    May 16 15:22:36.815: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6718" to be "running"
    May 16 15:22:36.819: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.688664ms
    May 16 15:22:38.823: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.007607553s
    May 16 15:22:38.823: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 05/16/23 15:22:38.823
    May 16 15:22:43.836: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 05/16/23 15:22:43.836
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    May 16 15:22:43.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-6718" for this suite. 05/16/23 15:22:43.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:22:43.859
May 16 15:22:43.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename proxy 05/16/23 15:22:43.86
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:22:43.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:22:43.88
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
May 16 15:22:43.883: INFO: Creating pod...
May 16 15:22:43.897: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8632" to be "running"
May 16 15:22:43.900: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.664586ms
May 16 15:22:45.905: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007427101s
May 16 15:22:45.905: INFO: Pod "agnhost" satisfied condition "running"
May 16 15:22:45.905: INFO: Creating service...
May 16 15:22:45.913: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/pods/agnhost/proxy/some/path/with/DELETE
May 16 15:22:45.920: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 16 15:22:45.920: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/pods/agnhost/proxy/some/path/with/GET
May 16 15:22:45.924: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May 16 15:22:45.924: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/pods/agnhost/proxy/some/path/with/HEAD
May 16 15:22:45.928: INFO: http.Client request:HEAD | StatusCode:200
May 16 15:22:45.928: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/pods/agnhost/proxy/some/path/with/OPTIONS
May 16 15:22:45.935: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 16 15:22:45.935: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/pods/agnhost/proxy/some/path/with/PATCH
May 16 15:22:45.941: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 16 15:22:45.941: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/pods/agnhost/proxy/some/path/with/POST
May 16 15:22:45.948: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 16 15:22:45.948: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/pods/agnhost/proxy/some/path/with/PUT
May 16 15:22:45.951: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May 16 15:22:45.951: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/services/test-service/proxy/some/path/with/DELETE
May 16 15:22:45.956: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 16 15:22:45.956: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/services/test-service/proxy/some/path/with/GET
May 16 15:22:45.962: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May 16 15:22:45.962: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/services/test-service/proxy/some/path/with/HEAD
May 16 15:22:45.968: INFO: http.Client request:HEAD | StatusCode:200
May 16 15:22:45.968: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/services/test-service/proxy/some/path/with/OPTIONS
May 16 15:22:45.972: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 16 15:22:45.972: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/services/test-service/proxy/some/path/with/PATCH
May 16 15:22:45.977: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 16 15:22:45.977: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/services/test-service/proxy/some/path/with/POST
May 16 15:22:45.982: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 16 15:22:45.982: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/services/test-service/proxy/some/path/with/PUT
May 16 15:22:45.986: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
May 16 15:22:45.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8632" for this suite. 05/16/23 15:22:45.991
------------------------------
â€¢ [2.139 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:22:43.859
    May 16 15:22:43.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename proxy 05/16/23 15:22:43.86
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:22:43.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:22:43.88
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    May 16 15:22:43.883: INFO: Creating pod...
    May 16 15:22:43.897: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8632" to be "running"
    May 16 15:22:43.900: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.664586ms
    May 16 15:22:45.905: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007427101s
    May 16 15:22:45.905: INFO: Pod "agnhost" satisfied condition "running"
    May 16 15:22:45.905: INFO: Creating service...
    May 16 15:22:45.913: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/pods/agnhost/proxy/some/path/with/DELETE
    May 16 15:22:45.920: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May 16 15:22:45.920: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/pods/agnhost/proxy/some/path/with/GET
    May 16 15:22:45.924: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    May 16 15:22:45.924: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/pods/agnhost/proxy/some/path/with/HEAD
    May 16 15:22:45.928: INFO: http.Client request:HEAD | StatusCode:200
    May 16 15:22:45.928: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/pods/agnhost/proxy/some/path/with/OPTIONS
    May 16 15:22:45.935: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May 16 15:22:45.935: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/pods/agnhost/proxy/some/path/with/PATCH
    May 16 15:22:45.941: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May 16 15:22:45.941: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/pods/agnhost/proxy/some/path/with/POST
    May 16 15:22:45.948: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May 16 15:22:45.948: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/pods/agnhost/proxy/some/path/with/PUT
    May 16 15:22:45.951: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    May 16 15:22:45.951: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/services/test-service/proxy/some/path/with/DELETE
    May 16 15:22:45.956: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    May 16 15:22:45.956: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/services/test-service/proxy/some/path/with/GET
    May 16 15:22:45.962: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    May 16 15:22:45.962: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/services/test-service/proxy/some/path/with/HEAD
    May 16 15:22:45.968: INFO: http.Client request:HEAD | StatusCode:200
    May 16 15:22:45.968: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/services/test-service/proxy/some/path/with/OPTIONS
    May 16 15:22:45.972: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    May 16 15:22:45.972: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/services/test-service/proxy/some/path/with/PATCH
    May 16 15:22:45.977: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    May 16 15:22:45.977: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/services/test-service/proxy/some/path/with/POST
    May 16 15:22:45.982: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    May 16 15:22:45.982: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-8632/services/test-service/proxy/some/path/with/PUT
    May 16 15:22:45.986: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    May 16 15:22:45.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8632" for this suite. 05/16/23 15:22:45.991
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:22:45.999
May 16 15:22:45.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename replicaset 05/16/23 15:22:46
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:22:46.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:22:46.018
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
May 16 15:22:46.044: INFO: Pod name sample-pod: Found 0 pods out of 1
May 16 15:22:51.049: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/16/23 15:22:51.049
STEP: Scaling up "test-rs" replicaset  05/16/23 15:22:51.049
May 16 15:22:51.060: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 05/16/23 15:22:51.06
W0516 15:22:51.070069      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May 16 15:22:51.071: INFO: observed ReplicaSet test-rs in namespace replicaset-5817 with ReadyReplicas 1, AvailableReplicas 1
May 16 15:22:51.084: INFO: observed ReplicaSet test-rs in namespace replicaset-5817 with ReadyReplicas 1, AvailableReplicas 1
May 16 15:22:51.113: INFO: observed ReplicaSet test-rs in namespace replicaset-5817 with ReadyReplicas 1, AvailableReplicas 1
May 16 15:22:51.136: INFO: observed ReplicaSet test-rs in namespace replicaset-5817 with ReadyReplicas 1, AvailableReplicas 1
May 16 15:22:52.629: INFO: observed ReplicaSet test-rs in namespace replicaset-5817 with ReadyReplicas 2, AvailableReplicas 2
May 16 15:22:52.911: INFO: observed Replicaset test-rs in namespace replicaset-5817 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
May 16 15:22:52.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5817" for this suite. 05/16/23 15:22:52.915
------------------------------
â€¢ [SLOW TEST] [6.923 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:22:45.999
    May 16 15:22:45.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename replicaset 05/16/23 15:22:46
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:22:46.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:22:46.018
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    May 16 15:22:46.044: INFO: Pod name sample-pod: Found 0 pods out of 1
    May 16 15:22:51.049: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/16/23 15:22:51.049
    STEP: Scaling up "test-rs" replicaset  05/16/23 15:22:51.049
    May 16 15:22:51.060: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 05/16/23 15:22:51.06
    W0516 15:22:51.070069      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May 16 15:22:51.071: INFO: observed ReplicaSet test-rs in namespace replicaset-5817 with ReadyReplicas 1, AvailableReplicas 1
    May 16 15:22:51.084: INFO: observed ReplicaSet test-rs in namespace replicaset-5817 with ReadyReplicas 1, AvailableReplicas 1
    May 16 15:22:51.113: INFO: observed ReplicaSet test-rs in namespace replicaset-5817 with ReadyReplicas 1, AvailableReplicas 1
    May 16 15:22:51.136: INFO: observed ReplicaSet test-rs in namespace replicaset-5817 with ReadyReplicas 1, AvailableReplicas 1
    May 16 15:22:52.629: INFO: observed ReplicaSet test-rs in namespace replicaset-5817 with ReadyReplicas 2, AvailableReplicas 2
    May 16 15:22:52.911: INFO: observed Replicaset test-rs in namespace replicaset-5817 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    May 16 15:22:52.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5817" for this suite. 05/16/23 15:22:52.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:22:52.922
May 16 15:22:52.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename subpath 05/16/23 15:22:52.923
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:22:52.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:22:52.942
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 05/16/23 15:22:52.945
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-xt72 05/16/23 15:22:52.955
STEP: Creating a pod to test atomic-volume-subpath 05/16/23 15:22:52.955
May 16 15:22:52.974: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-xt72" in namespace "subpath-7684" to be "Succeeded or Failed"
May 16 15:22:52.979: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Pending", Reason="", readiness=false. Elapsed: 5.025505ms
May 16 15:22:54.984: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 2.010141985s
May 16 15:22:56.983: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 4.009377194s
May 16 15:22:58.983: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 6.009794198s
May 16 15:23:00.982: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 8.00887203s
May 16 15:23:02.982: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 10.008686174s
May 16 15:23:04.984: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 12.010778161s
May 16 15:23:06.982: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 14.008773922s
May 16 15:23:08.984: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 16.010521509s
May 16 15:23:10.982: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 18.008880373s
May 16 15:23:12.983: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 20.009580874s
May 16 15:23:14.982: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=false. Elapsed: 22.008588845s
May 16 15:23:16.983: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009907668s
STEP: Saw pod success 05/16/23 15:23:16.983
May 16 15:23:16.984: INFO: Pod "pod-subpath-test-downwardapi-xt72" satisfied condition "Succeeded or Failed"
May 16 15:23:16.986: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-subpath-test-downwardapi-xt72 container test-container-subpath-downwardapi-xt72: <nil>
STEP: delete the pod 05/16/23 15:23:16.994
May 16 15:23:17.004: INFO: Waiting for pod pod-subpath-test-downwardapi-xt72 to disappear
May 16 15:23:17.006: INFO: Pod pod-subpath-test-downwardapi-xt72 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-xt72 05/16/23 15:23:17.006
May 16 15:23:17.006: INFO: Deleting pod "pod-subpath-test-downwardapi-xt72" in namespace "subpath-7684"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
May 16 15:23:17.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7684" for this suite. 05/16/23 15:23:17.014
------------------------------
â€¢ [SLOW TEST] [24.101 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:22:52.922
    May 16 15:22:52.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename subpath 05/16/23 15:22:52.923
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:22:52.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:22:52.942
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 05/16/23 15:22:52.945
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-xt72 05/16/23 15:22:52.955
    STEP: Creating a pod to test atomic-volume-subpath 05/16/23 15:22:52.955
    May 16 15:22:52.974: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-xt72" in namespace "subpath-7684" to be "Succeeded or Failed"
    May 16 15:22:52.979: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Pending", Reason="", readiness=false. Elapsed: 5.025505ms
    May 16 15:22:54.984: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 2.010141985s
    May 16 15:22:56.983: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 4.009377194s
    May 16 15:22:58.983: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 6.009794198s
    May 16 15:23:00.982: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 8.00887203s
    May 16 15:23:02.982: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 10.008686174s
    May 16 15:23:04.984: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 12.010778161s
    May 16 15:23:06.982: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 14.008773922s
    May 16 15:23:08.984: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 16.010521509s
    May 16 15:23:10.982: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 18.008880373s
    May 16 15:23:12.983: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=true. Elapsed: 20.009580874s
    May 16 15:23:14.982: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Running", Reason="", readiness=false. Elapsed: 22.008588845s
    May 16 15:23:16.983: INFO: Pod "pod-subpath-test-downwardapi-xt72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009907668s
    STEP: Saw pod success 05/16/23 15:23:16.983
    May 16 15:23:16.984: INFO: Pod "pod-subpath-test-downwardapi-xt72" satisfied condition "Succeeded or Failed"
    May 16 15:23:16.986: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-subpath-test-downwardapi-xt72 container test-container-subpath-downwardapi-xt72: <nil>
    STEP: delete the pod 05/16/23 15:23:16.994
    May 16 15:23:17.004: INFO: Waiting for pod pod-subpath-test-downwardapi-xt72 to disappear
    May 16 15:23:17.006: INFO: Pod pod-subpath-test-downwardapi-xt72 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-xt72 05/16/23 15:23:17.006
    May 16 15:23:17.006: INFO: Deleting pod "pod-subpath-test-downwardapi-xt72" in namespace "subpath-7684"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    May 16 15:23:17.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7684" for this suite. 05/16/23 15:23:17.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:23:17.023
May 16 15:23:17.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename webhook 05/16/23 15:23:17.024
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:23:17.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:23:17.045
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/16/23 15:23:17.068
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 15:23:17.336
STEP: Deploying the webhook pod 05/16/23 15:23:17.347
STEP: Wait for the deployment to be ready 05/16/23 15:23:17.357
May 16 15:23:17.364: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/16/23 15:23:19.374
STEP: Verifying the service has paired with the endpoint 05/16/23 15:23:19.385
May 16 15:23:20.385: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 05/16/23 15:23:20.387
STEP: create a namespace for the webhook 05/16/23 15:23:20.401
STEP: create a configmap should be unconditionally rejected by the webhook 05/16/23 15:23:20.409
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 15:23:20.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-23" for this suite. 05/16/23 15:23:20.524
STEP: Destroying namespace "webhook-23-markers" for this suite. 05/16/23 15:23:20.532
------------------------------
â€¢ [3.515 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:23:17.023
    May 16 15:23:17.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename webhook 05/16/23 15:23:17.024
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:23:17.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:23:17.045
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/16/23 15:23:17.068
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 15:23:17.336
    STEP: Deploying the webhook pod 05/16/23 15:23:17.347
    STEP: Wait for the deployment to be ready 05/16/23 15:23:17.357
    May 16 15:23:17.364: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/16/23 15:23:19.374
    STEP: Verifying the service has paired with the endpoint 05/16/23 15:23:19.385
    May 16 15:23:20.385: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 05/16/23 15:23:20.387
    STEP: create a namespace for the webhook 05/16/23 15:23:20.401
    STEP: create a configmap should be unconditionally rejected by the webhook 05/16/23 15:23:20.409
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 15:23:20.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-23" for this suite. 05/16/23 15:23:20.524
    STEP: Destroying namespace "webhook-23-markers" for this suite. 05/16/23 15:23:20.532
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:23:20.538
May 16 15:23:20.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename secrets 05/16/23 15:23:20.539
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:23:20.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:23:20.558
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-5edd92e6-44cf-4041-90c3-894351612d8b 05/16/23 15:23:20.561
STEP: Creating a pod to test consume secrets 05/16/23 15:23:20.565
May 16 15:23:20.579: INFO: Waiting up to 5m0s for pod "pod-secrets-fc5c239d-c34d-4376-9e02-77f6390f2e91" in namespace "secrets-290" to be "Succeeded or Failed"
May 16 15:23:20.583: INFO: Pod "pod-secrets-fc5c239d-c34d-4376-9e02-77f6390f2e91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.439119ms
May 16 15:23:22.588: INFO: Pod "pod-secrets-fc5c239d-c34d-4376-9e02-77f6390f2e91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008882437s
May 16 15:23:24.589: INFO: Pod "pod-secrets-fc5c239d-c34d-4376-9e02-77f6390f2e91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009835876s
STEP: Saw pod success 05/16/23 15:23:24.589
May 16 15:23:24.589: INFO: Pod "pod-secrets-fc5c239d-c34d-4376-9e02-77f6390f2e91" satisfied condition "Succeeded or Failed"
May 16 15:23:24.592: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-secrets-fc5c239d-c34d-4376-9e02-77f6390f2e91 container secret-volume-test: <nil>
STEP: delete the pod 05/16/23 15:23:24.597
May 16 15:23:24.606: INFO: Waiting for pod pod-secrets-fc5c239d-c34d-4376-9e02-77f6390f2e91 to disappear
May 16 15:23:24.608: INFO: Pod pod-secrets-fc5c239d-c34d-4376-9e02-77f6390f2e91 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May 16 15:23:24.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-290" for this suite. 05/16/23 15:23:24.611
------------------------------
â€¢ [4.078 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:23:20.538
    May 16 15:23:20.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename secrets 05/16/23 15:23:20.539
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:23:20.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:23:20.558
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-5edd92e6-44cf-4041-90c3-894351612d8b 05/16/23 15:23:20.561
    STEP: Creating a pod to test consume secrets 05/16/23 15:23:20.565
    May 16 15:23:20.579: INFO: Waiting up to 5m0s for pod "pod-secrets-fc5c239d-c34d-4376-9e02-77f6390f2e91" in namespace "secrets-290" to be "Succeeded or Failed"
    May 16 15:23:20.583: INFO: Pod "pod-secrets-fc5c239d-c34d-4376-9e02-77f6390f2e91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.439119ms
    May 16 15:23:22.588: INFO: Pod "pod-secrets-fc5c239d-c34d-4376-9e02-77f6390f2e91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008882437s
    May 16 15:23:24.589: INFO: Pod "pod-secrets-fc5c239d-c34d-4376-9e02-77f6390f2e91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009835876s
    STEP: Saw pod success 05/16/23 15:23:24.589
    May 16 15:23:24.589: INFO: Pod "pod-secrets-fc5c239d-c34d-4376-9e02-77f6390f2e91" satisfied condition "Succeeded or Failed"
    May 16 15:23:24.592: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-secrets-fc5c239d-c34d-4376-9e02-77f6390f2e91 container secret-volume-test: <nil>
    STEP: delete the pod 05/16/23 15:23:24.597
    May 16 15:23:24.606: INFO: Waiting for pod pod-secrets-fc5c239d-c34d-4376-9e02-77f6390f2e91 to disappear
    May 16 15:23:24.608: INFO: Pod pod-secrets-fc5c239d-c34d-4376-9e02-77f6390f2e91 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May 16 15:23:24.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-290" for this suite. 05/16/23 15:23:24.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:23:24.618
May 16 15:23:24.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename dns 05/16/23 15:23:24.619
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:23:24.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:23:24.638
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 05/16/23 15:23:24.64
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 05/16/23 15:23:24.64
STEP: creating a pod to probe DNS 05/16/23 15:23:24.64
STEP: submitting the pod to kubernetes 05/16/23 15:23:24.64
May 16 15:23:24.653: INFO: Waiting up to 15m0s for pod "dns-test-df4d194c-cfb9-49bc-925b-2a86e9ae4e88" in namespace "dns-5619" to be "running"
May 16 15:23:24.660: INFO: Pod "dns-test-df4d194c-cfb9-49bc-925b-2a86e9ae4e88": Phase="Pending", Reason="", readiness=false. Elapsed: 6.981757ms
May 16 15:23:26.663: INFO: Pod "dns-test-df4d194c-cfb9-49bc-925b-2a86e9ae4e88": Phase="Running", Reason="", readiness=true. Elapsed: 2.009987193s
May 16 15:23:26.663: INFO: Pod "dns-test-df4d194c-cfb9-49bc-925b-2a86e9ae4e88" satisfied condition "running"
STEP: retrieving the pod 05/16/23 15:23:26.663
STEP: looking for the results for each expected name from probers 05/16/23 15:23:26.665
May 16 15:23:26.679: INFO: DNS probes using dns-5619/dns-test-df4d194c-cfb9-49bc-925b-2a86e9ae4e88 succeeded

STEP: deleting the pod 05/16/23 15:23:26.679
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May 16 15:23:26.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5619" for this suite. 05/16/23 15:23:26.694
------------------------------
â€¢ [2.083 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:23:24.618
    May 16 15:23:24.619: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename dns 05/16/23 15:23:24.619
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:23:24.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:23:24.638
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     05/16/23 15:23:24.64
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     05/16/23 15:23:24.64
    STEP: creating a pod to probe DNS 05/16/23 15:23:24.64
    STEP: submitting the pod to kubernetes 05/16/23 15:23:24.64
    May 16 15:23:24.653: INFO: Waiting up to 15m0s for pod "dns-test-df4d194c-cfb9-49bc-925b-2a86e9ae4e88" in namespace "dns-5619" to be "running"
    May 16 15:23:24.660: INFO: Pod "dns-test-df4d194c-cfb9-49bc-925b-2a86e9ae4e88": Phase="Pending", Reason="", readiness=false. Elapsed: 6.981757ms
    May 16 15:23:26.663: INFO: Pod "dns-test-df4d194c-cfb9-49bc-925b-2a86e9ae4e88": Phase="Running", Reason="", readiness=true. Elapsed: 2.009987193s
    May 16 15:23:26.663: INFO: Pod "dns-test-df4d194c-cfb9-49bc-925b-2a86e9ae4e88" satisfied condition "running"
    STEP: retrieving the pod 05/16/23 15:23:26.663
    STEP: looking for the results for each expected name from probers 05/16/23 15:23:26.665
    May 16 15:23:26.679: INFO: DNS probes using dns-5619/dns-test-df4d194c-cfb9-49bc-925b-2a86e9ae4e88 succeeded

    STEP: deleting the pod 05/16/23 15:23:26.679
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May 16 15:23:26.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5619" for this suite. 05/16/23 15:23:26.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:23:26.702
May 16 15:23:26.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename proxy 05/16/23 15:23:26.703
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:23:26.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:23:26.724
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 05/16/23 15:23:26.744
STEP: creating replication controller proxy-service-hnp47 in namespace proxy-4923 05/16/23 15:23:26.744
I0516 15:23:26.750935      22 runners.go:193] Created replication controller with name: proxy-service-hnp47, namespace: proxy-4923, replica count: 1
I0516 15:23:27.801423      22 runners.go:193] proxy-service-hnp47 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0516 15:23:28.802089      22 runners.go:193] proxy-service-hnp47 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0516 15:23:29.802227      22 runners.go:193] proxy-service-hnp47 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 16 15:23:29.805: INFO: setup took 3.078397936s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 05/16/23 15:23:29.805
May 16 15:23:29.812: INFO: (0) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 7.541854ms)
May 16 15:23:29.813: INFO: (0) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 7.73135ms)
May 16 15:23:29.813: INFO: (0) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 8.024605ms)
May 16 15:23:29.813: INFO: (0) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 8.48995ms)
May 16 15:23:29.813: INFO: (0) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 8.462335ms)
May 16 15:23:29.814: INFO: (0) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 9.548125ms)
May 16 15:23:29.814: INFO: (0) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 9.417793ms)
May 16 15:23:29.815: INFO: (0) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 9.683093ms)
May 16 15:23:29.815: INFO: (0) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 9.632727ms)
May 16 15:23:29.815: INFO: (0) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 9.659543ms)
May 16 15:23:29.815: INFO: (0) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 9.925289ms)
May 16 15:23:29.817: INFO: (0) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 11.992282ms)
May 16 15:23:29.817: INFO: (0) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 12.099861ms)
May 16 15:23:29.817: INFO: (0) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 12.40181ms)
May 16 15:23:29.817: INFO: (0) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 12.482056ms)
May 16 15:23:29.818: INFO: (0) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 13.131926ms)
May 16 15:23:29.822: INFO: (1) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.299107ms)
May 16 15:23:29.823: INFO: (1) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 4.402933ms)
May 16 15:23:29.823: INFO: (1) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 4.433462ms)
May 16 15:23:29.823: INFO: (1) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 4.350935ms)
May 16 15:23:29.823: INFO: (1) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.471313ms)
May 16 15:23:29.823: INFO: (1) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 4.456628ms)
May 16 15:23:29.823: INFO: (1) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.195883ms)
May 16 15:23:29.824: INFO: (1) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.572427ms)
May 16 15:23:29.824: INFO: (1) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.496452ms)
May 16 15:23:29.824: INFO: (1) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 5.75434ms)
May 16 15:23:29.824: INFO: (1) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 6.204177ms)
May 16 15:23:29.825: INFO: (1) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 6.700718ms)
May 16 15:23:29.825: INFO: (1) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 6.926236ms)
May 16 15:23:29.825: INFO: (1) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 7.056739ms)
May 16 15:23:29.825: INFO: (1) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 7.240291ms)
May 16 15:23:29.826: INFO: (1) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 8.03328ms)
May 16 15:23:29.831: INFO: (2) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.562399ms)
May 16 15:23:29.831: INFO: (2) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.583658ms)
May 16 15:23:29.831: INFO: (2) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 4.78239ms)
May 16 15:23:29.831: INFO: (2) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 4.797151ms)
May 16 15:23:29.831: INFO: (2) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 4.850817ms)
May 16 15:23:29.831: INFO: (2) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 4.875489ms)
May 16 15:23:29.831: INFO: (2) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 4.937507ms)
May 16 15:23:29.832: INFO: (2) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.254442ms)
May 16 15:23:29.832: INFO: (2) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.610656ms)
May 16 15:23:29.832: INFO: (2) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.758917ms)
May 16 15:23:29.832: INFO: (2) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 5.800459ms)
May 16 15:23:29.833: INFO: (2) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 6.23878ms)
May 16 15:23:29.833: INFO: (2) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 6.843364ms)
May 16 15:23:29.834: INFO: (2) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 7.03753ms)
May 16 15:23:29.834: INFO: (2) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 7.413588ms)
May 16 15:23:29.834: INFO: (2) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 7.507563ms)
May 16 15:23:29.838: INFO: (3) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.026496ms)
May 16 15:23:29.839: INFO: (3) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 4.826118ms)
May 16 15:23:29.839: INFO: (3) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.969454ms)
May 16 15:23:29.839: INFO: (3) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.880718ms)
May 16 15:23:29.839: INFO: (3) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 4.884044ms)
May 16 15:23:29.839: INFO: (3) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.177339ms)
May 16 15:23:29.839: INFO: (3) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 5.314462ms)
May 16 15:23:29.840: INFO: (3) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 5.536847ms)
May 16 15:23:29.840: INFO: (3) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 5.640508ms)
May 16 15:23:29.840: INFO: (3) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 5.755712ms)
May 16 15:23:29.840: INFO: (3) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.795425ms)
May 16 15:23:29.840: INFO: (3) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 6.260496ms)
May 16 15:23:29.841: INFO: (3) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 6.545959ms)
May 16 15:23:29.841: INFO: (3) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 7.133985ms)
May 16 15:23:29.842: INFO: (3) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 7.523868ms)
May 16 15:23:29.843: INFO: (3) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 9.071179ms)
May 16 15:23:29.847: INFO: (4) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 3.523738ms)
May 16 15:23:29.848: INFO: (4) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.368788ms)
May 16 15:23:29.848: INFO: (4) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.563047ms)
May 16 15:23:29.848: INFO: (4) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 4.631449ms)
May 16 15:23:29.848: INFO: (4) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 5.099087ms)
May 16 15:23:29.849: INFO: (4) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 5.249126ms)
May 16 15:23:29.849: INFO: (4) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 5.297801ms)
May 16 15:23:29.849: INFO: (4) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 5.583122ms)
May 16 15:23:29.850: INFO: (4) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 6.245766ms)
May 16 15:23:29.850: INFO: (4) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 6.299183ms)
May 16 15:23:29.850: INFO: (4) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 6.551042ms)
May 16 15:23:29.850: INFO: (4) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 6.74962ms)
May 16 15:23:29.850: INFO: (4) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 6.827449ms)
May 16 15:23:29.851: INFO: (4) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 7.382972ms)
May 16 15:23:29.851: INFO: (4) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 7.598025ms)
May 16 15:23:29.851: INFO: (4) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 7.818134ms)
May 16 15:23:29.854: INFO: (5) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 3.212387ms)
May 16 15:23:29.855: INFO: (5) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.017759ms)
May 16 15:23:29.855: INFO: (5) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.234437ms)
May 16 15:23:29.856: INFO: (5) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 4.476822ms)
May 16 15:23:29.856: INFO: (5) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 4.795074ms)
May 16 15:23:29.857: INFO: (5) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.516728ms)
May 16 15:23:29.857: INFO: (5) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 5.598628ms)
May 16 15:23:29.857: INFO: (5) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 6.027466ms)
May 16 15:23:29.858: INFO: (5) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 6.183636ms)
May 16 15:23:29.858: INFO: (5) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 6.330805ms)
May 16 15:23:29.858: INFO: (5) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 6.473152ms)
May 16 15:23:29.859: INFO: (5) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 7.32831ms)
May 16 15:23:29.859: INFO: (5) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 7.52585ms)
May 16 15:23:29.859: INFO: (5) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 7.788885ms)
May 16 15:23:29.860: INFO: (5) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 8.299165ms)
May 16 15:23:29.860: INFO: (5) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 8.86115ms)
May 16 15:23:29.864: INFO: (6) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 3.891495ms)
May 16 15:23:29.865: INFO: (6) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 4.348967ms)
May 16 15:23:29.865: INFO: (6) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.782245ms)
May 16 15:23:29.865: INFO: (6) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.674641ms)
May 16 15:23:29.865: INFO: (6) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.865319ms)
May 16 15:23:29.865: INFO: (6) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 4.877836ms)
May 16 15:23:29.866: INFO: (6) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 5.069176ms)
May 16 15:23:29.866: INFO: (6) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 5.495212ms)
May 16 15:23:29.866: INFO: (6) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 5.975969ms)
May 16 15:23:29.866: INFO: (6) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 6.00051ms)
May 16 15:23:29.866: INFO: (6) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 6.117895ms)
May 16 15:23:29.867: INFO: (6) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 6.185927ms)
May 16 15:23:29.867: INFO: (6) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 6.318226ms)
May 16 15:23:29.867: INFO: (6) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 7.015265ms)
May 16 15:23:29.867: INFO: (6) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 7.02172ms)
May 16 15:23:29.868: INFO: (6) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 7.639341ms)
May 16 15:23:29.873: INFO: (7) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 5.25236ms)
May 16 15:23:29.873: INFO: (7) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 5.349685ms)
May 16 15:23:29.873: INFO: (7) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 5.481975ms)
May 16 15:23:29.874: INFO: (7) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.566047ms)
May 16 15:23:29.874: INFO: (7) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.873394ms)
May 16 15:23:29.874: INFO: (7) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 5.920328ms)
May 16 15:23:29.874: INFO: (7) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 5.934663ms)
May 16 15:23:29.875: INFO: (7) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 6.556855ms)
May 16 15:23:29.875: INFO: (7) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 6.640516ms)
May 16 15:23:29.875: INFO: (7) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 7.242019ms)
May 16 15:23:29.875: INFO: (7) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 7.208506ms)
May 16 15:23:29.875: INFO: (7) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 7.319888ms)
May 16 15:23:29.875: INFO: (7) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 7.474916ms)
May 16 15:23:29.876: INFO: (7) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 7.630942ms)
May 16 15:23:29.876: INFO: (7) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 7.864654ms)
May 16 15:23:29.877: INFO: (7) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 8.384197ms)
May 16 15:23:29.881: INFO: (8) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 4.1071ms)
May 16 15:23:29.881: INFO: (8) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 4.25801ms)
May 16 15:23:29.881: INFO: (8) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.468701ms)
May 16 15:23:29.881: INFO: (8) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.635544ms)
May 16 15:23:29.881: INFO: (8) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.677454ms)
May 16 15:23:29.881: INFO: (8) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 4.737837ms)
May 16 15:23:29.882: INFO: (8) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 5.71701ms)
May 16 15:23:29.882: INFO: (8) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.714591ms)
May 16 15:23:29.882: INFO: (8) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 5.850384ms)
May 16 15:23:29.883: INFO: (8) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.942537ms)
May 16 15:23:29.883: INFO: (8) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 6.09214ms)
May 16 15:23:29.883: INFO: (8) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 6.400182ms)
May 16 15:23:29.883: INFO: (8) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 6.503367ms)
May 16 15:23:29.884: INFO: (8) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 7.212119ms)
May 16 15:23:29.884: INFO: (8) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 7.414437ms)
May 16 15:23:29.885: INFO: (8) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 8.036364ms)
May 16 15:23:29.889: INFO: (9) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 4.629418ms)
May 16 15:23:29.890: INFO: (9) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.577339ms)
May 16 15:23:29.890: INFO: (9) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 5.52261ms)
May 16 15:23:29.890: INFO: (9) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 5.719845ms)
May 16 15:23:29.891: INFO: (9) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 6.451217ms)
May 16 15:23:29.891: INFO: (9) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 6.533246ms)
May 16 15:23:29.891: INFO: (9) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 6.515811ms)
May 16 15:23:29.892: INFO: (9) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 6.868868ms)
May 16 15:23:29.892: INFO: (9) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 6.839935ms)
May 16 15:23:29.892: INFO: (9) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 7.393453ms)
May 16 15:23:29.892: INFO: (9) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 7.363108ms)
May 16 15:23:29.892: INFO: (9) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 7.429712ms)
May 16 15:23:29.892: INFO: (9) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 7.587819ms)
May 16 15:23:29.893: INFO: (9) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 8.603753ms)
May 16 15:23:29.893: INFO: (9) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 8.668023ms)
May 16 15:23:29.894: INFO: (9) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 8.973204ms)
May 16 15:23:29.898: INFO: (10) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 4.505098ms)
May 16 15:23:29.899: INFO: (10) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 4.789327ms)
May 16 15:23:29.899: INFO: (10) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.561993ms)
May 16 15:23:29.899: INFO: (10) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.883956ms)
May 16 15:23:29.899: INFO: (10) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 4.71711ms)
May 16 15:23:29.899: INFO: (10) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 4.818264ms)
May 16 15:23:29.899: INFO: (10) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.797452ms)
May 16 15:23:29.899: INFO: (10) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.865641ms)
May 16 15:23:29.900: INFO: (10) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 5.649191ms)
May 16 15:23:29.900: INFO: (10) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.750309ms)
May 16 15:23:29.900: INFO: (10) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 6.154484ms)
May 16 15:23:29.900: INFO: (10) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 6.504691ms)
May 16 15:23:29.901: INFO: (10) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 6.702184ms)
May 16 15:23:29.901: INFO: (10) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 7.227805ms)
May 16 15:23:29.901: INFO: (10) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 7.114831ms)
May 16 15:23:29.901: INFO: (10) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 7.676145ms)
May 16 15:23:29.906: INFO: (11) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 4.65781ms)
May 16 15:23:29.906: INFO: (11) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 4.773248ms)
May 16 15:23:29.906: INFO: (11) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.755192ms)
May 16 15:23:29.906: INFO: (11) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 4.764205ms)
May 16 15:23:29.907: INFO: (11) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 5.16824ms)
May 16 15:23:29.907: INFO: (11) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 5.359478ms)
May 16 15:23:29.907: INFO: (11) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.45475ms)
May 16 15:23:29.907: INFO: (11) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 5.533894ms)
May 16 15:23:29.907: INFO: (11) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.605372ms)
May 16 15:23:29.907: INFO: (11) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 5.664549ms)
May 16 15:23:29.908: INFO: (11) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 6.074854ms)
May 16 15:23:29.908: INFO: (11) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 6.0961ms)
May 16 15:23:29.908: INFO: (11) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 6.707939ms)
May 16 15:23:29.909: INFO: (11) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 7.19678ms)
May 16 15:23:29.909: INFO: (11) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 7.16952ms)
May 16 15:23:29.910: INFO: (11) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 8.13585ms)
May 16 15:23:29.914: INFO: (12) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 3.878046ms)
May 16 15:23:29.914: INFO: (12) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 4.201323ms)
May 16 15:23:29.914: INFO: (12) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.712402ms)
May 16 15:23:29.914: INFO: (12) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.766944ms)
May 16 15:23:29.915: INFO: (12) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.727824ms)
May 16 15:23:29.915: INFO: (12) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.835557ms)
May 16 15:23:29.915: INFO: (12) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 5.075563ms)
May 16 15:23:29.915: INFO: (12) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 5.680327ms)
May 16 15:23:29.916: INFO: (12) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 5.830762ms)
May 16 15:23:29.916: INFO: (12) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.919767ms)
May 16 15:23:29.916: INFO: (12) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 5.986457ms)
May 16 15:23:29.916: INFO: (12) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 6.547078ms)
May 16 15:23:29.916: INFO: (12) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 6.620866ms)
May 16 15:23:29.917: INFO: (12) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 6.986263ms)
May 16 15:23:29.917: INFO: (12) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 7.311917ms)
May 16 15:23:29.918: INFO: (12) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 7.98607ms)
May 16 15:23:29.923: INFO: (13) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 4.967785ms)
May 16 15:23:29.923: INFO: (13) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 4.922422ms)
May 16 15:23:29.923: INFO: (13) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.987201ms)
May 16 15:23:29.923: INFO: (13) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 4.968263ms)
May 16 15:23:29.923: INFO: (13) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 5.069244ms)
May 16 15:23:29.923: INFO: (13) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 5.019032ms)
May 16 15:23:29.923: INFO: (13) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.281233ms)
May 16 15:23:29.923: INFO: (13) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.451466ms)
May 16 15:23:29.924: INFO: (13) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.609829ms)
May 16 15:23:29.924: INFO: (13) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 6.258446ms)
May 16 15:23:29.924: INFO: (13) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 6.464238ms)
May 16 15:23:29.925: INFO: (13) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 6.736148ms)
May 16 15:23:29.925: INFO: (13) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 6.67455ms)
May 16 15:23:29.925: INFO: (13) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 7.403698ms)
May 16 15:23:29.925: INFO: (13) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 7.528462ms)
May 16 15:23:29.926: INFO: (13) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 7.583016ms)
May 16 15:23:29.930: INFO: (14) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 4.348694ms)
May 16 15:23:29.930: INFO: (14) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.67343ms)
May 16 15:23:29.930: INFO: (14) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.620195ms)
May 16 15:23:29.930: INFO: (14) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.581104ms)
May 16 15:23:29.930: INFO: (14) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 4.704018ms)
May 16 15:23:29.931: INFO: (14) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.237302ms)
May 16 15:23:29.931: INFO: (14) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 5.546394ms)
May 16 15:23:29.932: INFO: (14) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 5.99854ms)
May 16 15:23:29.932: INFO: (14) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 6.268114ms)
May 16 15:23:29.932: INFO: (14) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 6.383448ms)
May 16 15:23:29.932: INFO: (14) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 6.338811ms)
May 16 15:23:29.932: INFO: (14) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 6.437813ms)
May 16 15:23:29.933: INFO: (14) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 7.393558ms)
May 16 15:23:29.933: INFO: (14) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 7.459617ms)
May 16 15:23:29.933: INFO: (14) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 7.643846ms)
May 16 15:23:29.934: INFO: (14) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 8.417459ms)
May 16 15:23:29.938: INFO: (15) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 3.923624ms)
May 16 15:23:29.938: INFO: (15) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 4.36089ms)
May 16 15:23:29.939: INFO: (15) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 4.348183ms)
May 16 15:23:29.939: INFO: (15) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.478816ms)
May 16 15:23:29.939: INFO: (15) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.572811ms)
May 16 15:23:29.939: INFO: (15) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 4.531945ms)
May 16 15:23:29.939: INFO: (15) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.712781ms)
May 16 15:23:29.940: INFO: (15) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.335998ms)
May 16 15:23:29.940: INFO: (15) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 5.423375ms)
May 16 15:23:29.940: INFO: (15) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 5.445939ms)
May 16 15:23:29.941: INFO: (15) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 6.452438ms)
May 16 15:23:29.941: INFO: (15) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 6.769518ms)
May 16 15:23:29.941: INFO: (15) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 6.888554ms)
May 16 15:23:29.941: INFO: (15) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 7.050292ms)
May 16 15:23:29.941: INFO: (15) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 7.024255ms)
May 16 15:23:29.942: INFO: (15) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 7.87253ms)
May 16 15:23:29.946: INFO: (16) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.332742ms)
May 16 15:23:29.948: INFO: (16) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.72793ms)
May 16 15:23:29.948: INFO: (16) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 5.726164ms)
May 16 15:23:29.948: INFO: (16) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.802872ms)
May 16 15:23:29.948: INFO: (16) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 5.888988ms)
May 16 15:23:29.948: INFO: (16) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 5.855574ms)
May 16 15:23:29.948: INFO: (16) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 6.22583ms)
May 16 15:23:29.948: INFO: (16) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 6.138543ms)
May 16 15:23:29.949: INFO: (16) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 6.690264ms)
May 16 15:23:29.949: INFO: (16) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 7.018328ms)
May 16 15:23:29.949: INFO: (16) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 7.033729ms)
May 16 15:23:29.949: INFO: (16) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 7.209731ms)
May 16 15:23:29.950: INFO: (16) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 7.527327ms)
May 16 15:23:29.950: INFO: (16) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 8.225439ms)
May 16 15:23:29.950: INFO: (16) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 8.237684ms)
May 16 15:23:29.950: INFO: (16) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 8.340773ms)
May 16 15:23:29.955: INFO: (17) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.313477ms)
May 16 15:23:29.955: INFO: (17) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 4.802008ms)
May 16 15:23:29.955: INFO: (17) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.869906ms)
May 16 15:23:29.955: INFO: (17) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 4.944502ms)
May 16 15:23:29.956: INFO: (17) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 5.031442ms)
May 16 15:23:29.956: INFO: (17) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 5.143298ms)
May 16 15:23:29.956: INFO: (17) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.186331ms)
May 16 15:23:29.957: INFO: (17) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 6.193064ms)
May 16 15:23:29.957: INFO: (17) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 6.253407ms)
May 16 15:23:29.957: INFO: (17) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 6.279988ms)
May 16 15:23:29.957: INFO: (17) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 6.506593ms)
May 16 15:23:29.957: INFO: (17) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 6.742162ms)
May 16 15:23:29.958: INFO: (17) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 7.064769ms)
May 16 15:23:29.958: INFO: (17) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 7.336113ms)
May 16 15:23:29.958: INFO: (17) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 7.465617ms)
May 16 15:23:29.958: INFO: (17) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 7.816464ms)
May 16 15:23:29.962: INFO: (18) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 3.693069ms)
May 16 15:23:29.962: INFO: (18) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 3.934916ms)
May 16 15:23:29.967: INFO: (18) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 9.013295ms)
May 16 15:23:29.967: INFO: (18) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 9.179847ms)
May 16 15:23:29.968: INFO: (18) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 9.565041ms)
May 16 15:23:29.968: INFO: (18) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 9.565299ms)
May 16 15:23:29.968: INFO: (18) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 10.16622ms)
May 16 15:23:29.969: INFO: (18) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 10.247905ms)
May 16 15:23:29.969: INFO: (18) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 10.207054ms)
May 16 15:23:29.969: INFO: (18) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 10.202869ms)
May 16 15:23:29.969: INFO: (18) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 10.692245ms)
May 16 15:23:29.970: INFO: (18) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 11.276715ms)
May 16 15:23:29.970: INFO: (18) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 11.168642ms)
May 16 15:23:29.970: INFO: (18) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 11.300345ms)
May 16 15:23:29.970: INFO: (18) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 12.034878ms)
May 16 15:23:29.971: INFO: (18) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 12.207423ms)
May 16 15:23:29.974: INFO: (19) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 2.914314ms)
May 16 15:23:29.975: INFO: (19) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.118272ms)
May 16 15:23:29.975: INFO: (19) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.591029ms)
May 16 15:23:29.975: INFO: (19) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.773423ms)
May 16 15:23:29.976: INFO: (19) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 4.790642ms)
May 16 15:23:29.976: INFO: (19) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.951651ms)
May 16 15:23:29.976: INFO: (19) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.992787ms)
May 16 15:23:29.977: INFO: (19) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 6.203863ms)
May 16 15:23:29.977: INFO: (19) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 6.190217ms)
May 16 15:23:29.977: INFO: (19) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 6.285984ms)
May 16 15:23:29.977: INFO: (19) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 6.160754ms)
May 16 15:23:29.977: INFO: (19) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 6.297267ms)
May 16 15:23:29.977: INFO: (19) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 6.714054ms)
May 16 15:23:29.978: INFO: (19) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 6.93348ms)
May 16 15:23:29.978: INFO: (19) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 7.387336ms)
May 16 15:23:29.978: INFO: (19) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 7.328454ms)
STEP: deleting ReplicationController proxy-service-hnp47 in namespace proxy-4923, will wait for the garbage collector to delete the pods 05/16/23 15:23:29.978
May 16 15:23:30.039: INFO: Deleting ReplicationController proxy-service-hnp47 took: 7.745011ms
May 16 15:23:30.139: INFO: Terminating ReplicationController proxy-service-hnp47 pods took: 100.425653ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
May 16 15:23:33.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-4923" for this suite. 05/16/23 15:23:33.046
------------------------------
â€¢ [SLOW TEST] [6.351 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:23:26.702
    May 16 15:23:26.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename proxy 05/16/23 15:23:26.703
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:23:26.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:23:26.724
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 05/16/23 15:23:26.744
    STEP: creating replication controller proxy-service-hnp47 in namespace proxy-4923 05/16/23 15:23:26.744
    I0516 15:23:26.750935      22 runners.go:193] Created replication controller with name: proxy-service-hnp47, namespace: proxy-4923, replica count: 1
    I0516 15:23:27.801423      22 runners.go:193] proxy-service-hnp47 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0516 15:23:28.802089      22 runners.go:193] proxy-service-hnp47 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0516 15:23:29.802227      22 runners.go:193] proxy-service-hnp47 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    May 16 15:23:29.805: INFO: setup took 3.078397936s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 05/16/23 15:23:29.805
    May 16 15:23:29.812: INFO: (0) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 7.541854ms)
    May 16 15:23:29.813: INFO: (0) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 7.73135ms)
    May 16 15:23:29.813: INFO: (0) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 8.024605ms)
    May 16 15:23:29.813: INFO: (0) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 8.48995ms)
    May 16 15:23:29.813: INFO: (0) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 8.462335ms)
    May 16 15:23:29.814: INFO: (0) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 9.548125ms)
    May 16 15:23:29.814: INFO: (0) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 9.417793ms)
    May 16 15:23:29.815: INFO: (0) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 9.683093ms)
    May 16 15:23:29.815: INFO: (0) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 9.632727ms)
    May 16 15:23:29.815: INFO: (0) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 9.659543ms)
    May 16 15:23:29.815: INFO: (0) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 9.925289ms)
    May 16 15:23:29.817: INFO: (0) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 11.992282ms)
    May 16 15:23:29.817: INFO: (0) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 12.099861ms)
    May 16 15:23:29.817: INFO: (0) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 12.40181ms)
    May 16 15:23:29.817: INFO: (0) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 12.482056ms)
    May 16 15:23:29.818: INFO: (0) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 13.131926ms)
    May 16 15:23:29.822: INFO: (1) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.299107ms)
    May 16 15:23:29.823: INFO: (1) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 4.402933ms)
    May 16 15:23:29.823: INFO: (1) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 4.433462ms)
    May 16 15:23:29.823: INFO: (1) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 4.350935ms)
    May 16 15:23:29.823: INFO: (1) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.471313ms)
    May 16 15:23:29.823: INFO: (1) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 4.456628ms)
    May 16 15:23:29.823: INFO: (1) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.195883ms)
    May 16 15:23:29.824: INFO: (1) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.572427ms)
    May 16 15:23:29.824: INFO: (1) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.496452ms)
    May 16 15:23:29.824: INFO: (1) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 5.75434ms)
    May 16 15:23:29.824: INFO: (1) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 6.204177ms)
    May 16 15:23:29.825: INFO: (1) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 6.700718ms)
    May 16 15:23:29.825: INFO: (1) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 6.926236ms)
    May 16 15:23:29.825: INFO: (1) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 7.056739ms)
    May 16 15:23:29.825: INFO: (1) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 7.240291ms)
    May 16 15:23:29.826: INFO: (1) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 8.03328ms)
    May 16 15:23:29.831: INFO: (2) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.562399ms)
    May 16 15:23:29.831: INFO: (2) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.583658ms)
    May 16 15:23:29.831: INFO: (2) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 4.78239ms)
    May 16 15:23:29.831: INFO: (2) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 4.797151ms)
    May 16 15:23:29.831: INFO: (2) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 4.850817ms)
    May 16 15:23:29.831: INFO: (2) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 4.875489ms)
    May 16 15:23:29.831: INFO: (2) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 4.937507ms)
    May 16 15:23:29.832: INFO: (2) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.254442ms)
    May 16 15:23:29.832: INFO: (2) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.610656ms)
    May 16 15:23:29.832: INFO: (2) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.758917ms)
    May 16 15:23:29.832: INFO: (2) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 5.800459ms)
    May 16 15:23:29.833: INFO: (2) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 6.23878ms)
    May 16 15:23:29.833: INFO: (2) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 6.843364ms)
    May 16 15:23:29.834: INFO: (2) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 7.03753ms)
    May 16 15:23:29.834: INFO: (2) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 7.413588ms)
    May 16 15:23:29.834: INFO: (2) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 7.507563ms)
    May 16 15:23:29.838: INFO: (3) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.026496ms)
    May 16 15:23:29.839: INFO: (3) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 4.826118ms)
    May 16 15:23:29.839: INFO: (3) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.969454ms)
    May 16 15:23:29.839: INFO: (3) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.880718ms)
    May 16 15:23:29.839: INFO: (3) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 4.884044ms)
    May 16 15:23:29.839: INFO: (3) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.177339ms)
    May 16 15:23:29.839: INFO: (3) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 5.314462ms)
    May 16 15:23:29.840: INFO: (3) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 5.536847ms)
    May 16 15:23:29.840: INFO: (3) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 5.640508ms)
    May 16 15:23:29.840: INFO: (3) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 5.755712ms)
    May 16 15:23:29.840: INFO: (3) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.795425ms)
    May 16 15:23:29.840: INFO: (3) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 6.260496ms)
    May 16 15:23:29.841: INFO: (3) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 6.545959ms)
    May 16 15:23:29.841: INFO: (3) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 7.133985ms)
    May 16 15:23:29.842: INFO: (3) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 7.523868ms)
    May 16 15:23:29.843: INFO: (3) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 9.071179ms)
    May 16 15:23:29.847: INFO: (4) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 3.523738ms)
    May 16 15:23:29.848: INFO: (4) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.368788ms)
    May 16 15:23:29.848: INFO: (4) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.563047ms)
    May 16 15:23:29.848: INFO: (4) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 4.631449ms)
    May 16 15:23:29.848: INFO: (4) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 5.099087ms)
    May 16 15:23:29.849: INFO: (4) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 5.249126ms)
    May 16 15:23:29.849: INFO: (4) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 5.297801ms)
    May 16 15:23:29.849: INFO: (4) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 5.583122ms)
    May 16 15:23:29.850: INFO: (4) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 6.245766ms)
    May 16 15:23:29.850: INFO: (4) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 6.299183ms)
    May 16 15:23:29.850: INFO: (4) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 6.551042ms)
    May 16 15:23:29.850: INFO: (4) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 6.74962ms)
    May 16 15:23:29.850: INFO: (4) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 6.827449ms)
    May 16 15:23:29.851: INFO: (4) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 7.382972ms)
    May 16 15:23:29.851: INFO: (4) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 7.598025ms)
    May 16 15:23:29.851: INFO: (4) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 7.818134ms)
    May 16 15:23:29.854: INFO: (5) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 3.212387ms)
    May 16 15:23:29.855: INFO: (5) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.017759ms)
    May 16 15:23:29.855: INFO: (5) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.234437ms)
    May 16 15:23:29.856: INFO: (5) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 4.476822ms)
    May 16 15:23:29.856: INFO: (5) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 4.795074ms)
    May 16 15:23:29.857: INFO: (5) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.516728ms)
    May 16 15:23:29.857: INFO: (5) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 5.598628ms)
    May 16 15:23:29.857: INFO: (5) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 6.027466ms)
    May 16 15:23:29.858: INFO: (5) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 6.183636ms)
    May 16 15:23:29.858: INFO: (5) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 6.330805ms)
    May 16 15:23:29.858: INFO: (5) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 6.473152ms)
    May 16 15:23:29.859: INFO: (5) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 7.32831ms)
    May 16 15:23:29.859: INFO: (5) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 7.52585ms)
    May 16 15:23:29.859: INFO: (5) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 7.788885ms)
    May 16 15:23:29.860: INFO: (5) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 8.299165ms)
    May 16 15:23:29.860: INFO: (5) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 8.86115ms)
    May 16 15:23:29.864: INFO: (6) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 3.891495ms)
    May 16 15:23:29.865: INFO: (6) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 4.348967ms)
    May 16 15:23:29.865: INFO: (6) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.782245ms)
    May 16 15:23:29.865: INFO: (6) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.674641ms)
    May 16 15:23:29.865: INFO: (6) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.865319ms)
    May 16 15:23:29.865: INFO: (6) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 4.877836ms)
    May 16 15:23:29.866: INFO: (6) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 5.069176ms)
    May 16 15:23:29.866: INFO: (6) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 5.495212ms)
    May 16 15:23:29.866: INFO: (6) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 5.975969ms)
    May 16 15:23:29.866: INFO: (6) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 6.00051ms)
    May 16 15:23:29.866: INFO: (6) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 6.117895ms)
    May 16 15:23:29.867: INFO: (6) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 6.185927ms)
    May 16 15:23:29.867: INFO: (6) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 6.318226ms)
    May 16 15:23:29.867: INFO: (6) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 7.015265ms)
    May 16 15:23:29.867: INFO: (6) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 7.02172ms)
    May 16 15:23:29.868: INFO: (6) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 7.639341ms)
    May 16 15:23:29.873: INFO: (7) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 5.25236ms)
    May 16 15:23:29.873: INFO: (7) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 5.349685ms)
    May 16 15:23:29.873: INFO: (7) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 5.481975ms)
    May 16 15:23:29.874: INFO: (7) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.566047ms)
    May 16 15:23:29.874: INFO: (7) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.873394ms)
    May 16 15:23:29.874: INFO: (7) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 5.920328ms)
    May 16 15:23:29.874: INFO: (7) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 5.934663ms)
    May 16 15:23:29.875: INFO: (7) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 6.556855ms)
    May 16 15:23:29.875: INFO: (7) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 6.640516ms)
    May 16 15:23:29.875: INFO: (7) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 7.242019ms)
    May 16 15:23:29.875: INFO: (7) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 7.208506ms)
    May 16 15:23:29.875: INFO: (7) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 7.319888ms)
    May 16 15:23:29.875: INFO: (7) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 7.474916ms)
    May 16 15:23:29.876: INFO: (7) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 7.630942ms)
    May 16 15:23:29.876: INFO: (7) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 7.864654ms)
    May 16 15:23:29.877: INFO: (7) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 8.384197ms)
    May 16 15:23:29.881: INFO: (8) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 4.1071ms)
    May 16 15:23:29.881: INFO: (8) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 4.25801ms)
    May 16 15:23:29.881: INFO: (8) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.468701ms)
    May 16 15:23:29.881: INFO: (8) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.635544ms)
    May 16 15:23:29.881: INFO: (8) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.677454ms)
    May 16 15:23:29.881: INFO: (8) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 4.737837ms)
    May 16 15:23:29.882: INFO: (8) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 5.71701ms)
    May 16 15:23:29.882: INFO: (8) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.714591ms)
    May 16 15:23:29.882: INFO: (8) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 5.850384ms)
    May 16 15:23:29.883: INFO: (8) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.942537ms)
    May 16 15:23:29.883: INFO: (8) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 6.09214ms)
    May 16 15:23:29.883: INFO: (8) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 6.400182ms)
    May 16 15:23:29.883: INFO: (8) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 6.503367ms)
    May 16 15:23:29.884: INFO: (8) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 7.212119ms)
    May 16 15:23:29.884: INFO: (8) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 7.414437ms)
    May 16 15:23:29.885: INFO: (8) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 8.036364ms)
    May 16 15:23:29.889: INFO: (9) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 4.629418ms)
    May 16 15:23:29.890: INFO: (9) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.577339ms)
    May 16 15:23:29.890: INFO: (9) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 5.52261ms)
    May 16 15:23:29.890: INFO: (9) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 5.719845ms)
    May 16 15:23:29.891: INFO: (9) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 6.451217ms)
    May 16 15:23:29.891: INFO: (9) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 6.533246ms)
    May 16 15:23:29.891: INFO: (9) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 6.515811ms)
    May 16 15:23:29.892: INFO: (9) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 6.868868ms)
    May 16 15:23:29.892: INFO: (9) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 6.839935ms)
    May 16 15:23:29.892: INFO: (9) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 7.393453ms)
    May 16 15:23:29.892: INFO: (9) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 7.363108ms)
    May 16 15:23:29.892: INFO: (9) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 7.429712ms)
    May 16 15:23:29.892: INFO: (9) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 7.587819ms)
    May 16 15:23:29.893: INFO: (9) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 8.603753ms)
    May 16 15:23:29.893: INFO: (9) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 8.668023ms)
    May 16 15:23:29.894: INFO: (9) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 8.973204ms)
    May 16 15:23:29.898: INFO: (10) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 4.505098ms)
    May 16 15:23:29.899: INFO: (10) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 4.789327ms)
    May 16 15:23:29.899: INFO: (10) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.561993ms)
    May 16 15:23:29.899: INFO: (10) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.883956ms)
    May 16 15:23:29.899: INFO: (10) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 4.71711ms)
    May 16 15:23:29.899: INFO: (10) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 4.818264ms)
    May 16 15:23:29.899: INFO: (10) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.797452ms)
    May 16 15:23:29.899: INFO: (10) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.865641ms)
    May 16 15:23:29.900: INFO: (10) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 5.649191ms)
    May 16 15:23:29.900: INFO: (10) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.750309ms)
    May 16 15:23:29.900: INFO: (10) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 6.154484ms)
    May 16 15:23:29.900: INFO: (10) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 6.504691ms)
    May 16 15:23:29.901: INFO: (10) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 6.702184ms)
    May 16 15:23:29.901: INFO: (10) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 7.227805ms)
    May 16 15:23:29.901: INFO: (10) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 7.114831ms)
    May 16 15:23:29.901: INFO: (10) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 7.676145ms)
    May 16 15:23:29.906: INFO: (11) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 4.65781ms)
    May 16 15:23:29.906: INFO: (11) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 4.773248ms)
    May 16 15:23:29.906: INFO: (11) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.755192ms)
    May 16 15:23:29.906: INFO: (11) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 4.764205ms)
    May 16 15:23:29.907: INFO: (11) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 5.16824ms)
    May 16 15:23:29.907: INFO: (11) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 5.359478ms)
    May 16 15:23:29.907: INFO: (11) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.45475ms)
    May 16 15:23:29.907: INFO: (11) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 5.533894ms)
    May 16 15:23:29.907: INFO: (11) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.605372ms)
    May 16 15:23:29.907: INFO: (11) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 5.664549ms)
    May 16 15:23:29.908: INFO: (11) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 6.074854ms)
    May 16 15:23:29.908: INFO: (11) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 6.0961ms)
    May 16 15:23:29.908: INFO: (11) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 6.707939ms)
    May 16 15:23:29.909: INFO: (11) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 7.19678ms)
    May 16 15:23:29.909: INFO: (11) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 7.16952ms)
    May 16 15:23:29.910: INFO: (11) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 8.13585ms)
    May 16 15:23:29.914: INFO: (12) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 3.878046ms)
    May 16 15:23:29.914: INFO: (12) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 4.201323ms)
    May 16 15:23:29.914: INFO: (12) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.712402ms)
    May 16 15:23:29.914: INFO: (12) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.766944ms)
    May 16 15:23:29.915: INFO: (12) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.727824ms)
    May 16 15:23:29.915: INFO: (12) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.835557ms)
    May 16 15:23:29.915: INFO: (12) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 5.075563ms)
    May 16 15:23:29.915: INFO: (12) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 5.680327ms)
    May 16 15:23:29.916: INFO: (12) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 5.830762ms)
    May 16 15:23:29.916: INFO: (12) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.919767ms)
    May 16 15:23:29.916: INFO: (12) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 5.986457ms)
    May 16 15:23:29.916: INFO: (12) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 6.547078ms)
    May 16 15:23:29.916: INFO: (12) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 6.620866ms)
    May 16 15:23:29.917: INFO: (12) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 6.986263ms)
    May 16 15:23:29.917: INFO: (12) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 7.311917ms)
    May 16 15:23:29.918: INFO: (12) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 7.98607ms)
    May 16 15:23:29.923: INFO: (13) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 4.967785ms)
    May 16 15:23:29.923: INFO: (13) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 4.922422ms)
    May 16 15:23:29.923: INFO: (13) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.987201ms)
    May 16 15:23:29.923: INFO: (13) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 4.968263ms)
    May 16 15:23:29.923: INFO: (13) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 5.069244ms)
    May 16 15:23:29.923: INFO: (13) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 5.019032ms)
    May 16 15:23:29.923: INFO: (13) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.281233ms)
    May 16 15:23:29.923: INFO: (13) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.451466ms)
    May 16 15:23:29.924: INFO: (13) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.609829ms)
    May 16 15:23:29.924: INFO: (13) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 6.258446ms)
    May 16 15:23:29.924: INFO: (13) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 6.464238ms)
    May 16 15:23:29.925: INFO: (13) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 6.736148ms)
    May 16 15:23:29.925: INFO: (13) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 6.67455ms)
    May 16 15:23:29.925: INFO: (13) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 7.403698ms)
    May 16 15:23:29.925: INFO: (13) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 7.528462ms)
    May 16 15:23:29.926: INFO: (13) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 7.583016ms)
    May 16 15:23:29.930: INFO: (14) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 4.348694ms)
    May 16 15:23:29.930: INFO: (14) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.67343ms)
    May 16 15:23:29.930: INFO: (14) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.620195ms)
    May 16 15:23:29.930: INFO: (14) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.581104ms)
    May 16 15:23:29.930: INFO: (14) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 4.704018ms)
    May 16 15:23:29.931: INFO: (14) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.237302ms)
    May 16 15:23:29.931: INFO: (14) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 5.546394ms)
    May 16 15:23:29.932: INFO: (14) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 5.99854ms)
    May 16 15:23:29.932: INFO: (14) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 6.268114ms)
    May 16 15:23:29.932: INFO: (14) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 6.383448ms)
    May 16 15:23:29.932: INFO: (14) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 6.338811ms)
    May 16 15:23:29.932: INFO: (14) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 6.437813ms)
    May 16 15:23:29.933: INFO: (14) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 7.393558ms)
    May 16 15:23:29.933: INFO: (14) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 7.459617ms)
    May 16 15:23:29.933: INFO: (14) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 7.643846ms)
    May 16 15:23:29.934: INFO: (14) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 8.417459ms)
    May 16 15:23:29.938: INFO: (15) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 3.923624ms)
    May 16 15:23:29.938: INFO: (15) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 4.36089ms)
    May 16 15:23:29.939: INFO: (15) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 4.348183ms)
    May 16 15:23:29.939: INFO: (15) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.478816ms)
    May 16 15:23:29.939: INFO: (15) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.572811ms)
    May 16 15:23:29.939: INFO: (15) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 4.531945ms)
    May 16 15:23:29.939: INFO: (15) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.712781ms)
    May 16 15:23:29.940: INFO: (15) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.335998ms)
    May 16 15:23:29.940: INFO: (15) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 5.423375ms)
    May 16 15:23:29.940: INFO: (15) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 5.445939ms)
    May 16 15:23:29.941: INFO: (15) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 6.452438ms)
    May 16 15:23:29.941: INFO: (15) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 6.769518ms)
    May 16 15:23:29.941: INFO: (15) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 6.888554ms)
    May 16 15:23:29.941: INFO: (15) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 7.050292ms)
    May 16 15:23:29.941: INFO: (15) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 7.024255ms)
    May 16 15:23:29.942: INFO: (15) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 7.87253ms)
    May 16 15:23:29.946: INFO: (16) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.332742ms)
    May 16 15:23:29.948: INFO: (16) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.72793ms)
    May 16 15:23:29.948: INFO: (16) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 5.726164ms)
    May 16 15:23:29.948: INFO: (16) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 5.802872ms)
    May 16 15:23:29.948: INFO: (16) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 5.888988ms)
    May 16 15:23:29.948: INFO: (16) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 5.855574ms)
    May 16 15:23:29.948: INFO: (16) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 6.22583ms)
    May 16 15:23:29.948: INFO: (16) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 6.138543ms)
    May 16 15:23:29.949: INFO: (16) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 6.690264ms)
    May 16 15:23:29.949: INFO: (16) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 7.018328ms)
    May 16 15:23:29.949: INFO: (16) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 7.033729ms)
    May 16 15:23:29.949: INFO: (16) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 7.209731ms)
    May 16 15:23:29.950: INFO: (16) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 7.527327ms)
    May 16 15:23:29.950: INFO: (16) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 8.225439ms)
    May 16 15:23:29.950: INFO: (16) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 8.237684ms)
    May 16 15:23:29.950: INFO: (16) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 8.340773ms)
    May 16 15:23:29.955: INFO: (17) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.313477ms)
    May 16 15:23:29.955: INFO: (17) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 4.802008ms)
    May 16 15:23:29.955: INFO: (17) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.869906ms)
    May 16 15:23:29.955: INFO: (17) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 4.944502ms)
    May 16 15:23:29.956: INFO: (17) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 5.031442ms)
    May 16 15:23:29.956: INFO: (17) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 5.143298ms)
    May 16 15:23:29.956: INFO: (17) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 5.186331ms)
    May 16 15:23:29.957: INFO: (17) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 6.193064ms)
    May 16 15:23:29.957: INFO: (17) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 6.253407ms)
    May 16 15:23:29.957: INFO: (17) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 6.279988ms)
    May 16 15:23:29.957: INFO: (17) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 6.506593ms)
    May 16 15:23:29.957: INFO: (17) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 6.742162ms)
    May 16 15:23:29.958: INFO: (17) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 7.064769ms)
    May 16 15:23:29.958: INFO: (17) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 7.336113ms)
    May 16 15:23:29.958: INFO: (17) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 7.465617ms)
    May 16 15:23:29.958: INFO: (17) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 7.816464ms)
    May 16 15:23:29.962: INFO: (18) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 3.693069ms)
    May 16 15:23:29.962: INFO: (18) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 3.934916ms)
    May 16 15:23:29.967: INFO: (18) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 9.013295ms)
    May 16 15:23:29.967: INFO: (18) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 9.179847ms)
    May 16 15:23:29.968: INFO: (18) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 9.565041ms)
    May 16 15:23:29.968: INFO: (18) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 9.565299ms)
    May 16 15:23:29.968: INFO: (18) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 10.16622ms)
    May 16 15:23:29.969: INFO: (18) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 10.247905ms)
    May 16 15:23:29.969: INFO: (18) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 10.207054ms)
    May 16 15:23:29.969: INFO: (18) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 10.202869ms)
    May 16 15:23:29.969: INFO: (18) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 10.692245ms)
    May 16 15:23:29.970: INFO: (18) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 11.276715ms)
    May 16 15:23:29.970: INFO: (18) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 11.168642ms)
    May 16 15:23:29.970: INFO: (18) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 11.300345ms)
    May 16 15:23:29.970: INFO: (18) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 12.034878ms)
    May 16 15:23:29.971: INFO: (18) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 12.207423ms)
    May 16 15:23:29.974: INFO: (19) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">... (200; 2.914314ms)
    May 16 15:23:29.975: INFO: (19) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.118272ms)
    May 16 15:23:29.975: INFO: (19) /api/v1/namespaces/proxy-4923/pods/http:proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.591029ms)
    May 16 15:23:29.975: INFO: (19) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:460/proxy/: tls baz (200; 4.773423ms)
    May 16 15:23:29.976: INFO: (19) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5/proxy/rewriteme">test</a> (200; 4.790642ms)
    May 16 15:23:29.976: INFO: (19) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:162/proxy/: bar (200; 4.951651ms)
    May 16 15:23:29.976: INFO: (19) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:160/proxy/: foo (200; 4.992787ms)
    May 16 15:23:29.977: INFO: (19) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:443/proxy/tlsrewritem... (200; 6.203863ms)
    May 16 15:23:29.977: INFO: (19) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname2/proxy/: bar (200; 6.190217ms)
    May 16 15:23:29.977: INFO: (19) /api/v1/namespaces/proxy-4923/pods/https:proxy-service-hnp47-2wmj5:462/proxy/: tls qux (200; 6.285984ms)
    May 16 15:23:29.977: INFO: (19) /api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/: <a href="/api/v1/namespaces/proxy-4923/pods/proxy-service-hnp47-2wmj5:1080/proxy/rewriteme">test<... (200; 6.160754ms)
    May 16 15:23:29.977: INFO: (19) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname2/proxy/: tls qux (200; 6.297267ms)
    May 16 15:23:29.977: INFO: (19) /api/v1/namespaces/proxy-4923/services/https:proxy-service-hnp47:tlsportname1/proxy/: tls baz (200; 6.714054ms)
    May 16 15:23:29.978: INFO: (19) /api/v1/namespaces/proxy-4923/services/http:proxy-service-hnp47:portname1/proxy/: foo (200; 6.93348ms)
    May 16 15:23:29.978: INFO: (19) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname2/proxy/: bar (200; 7.387336ms)
    May 16 15:23:29.978: INFO: (19) /api/v1/namespaces/proxy-4923/services/proxy-service-hnp47:portname1/proxy/: foo (200; 7.328454ms)
    STEP: deleting ReplicationController proxy-service-hnp47 in namespace proxy-4923, will wait for the garbage collector to delete the pods 05/16/23 15:23:29.978
    May 16 15:23:30.039: INFO: Deleting ReplicationController proxy-service-hnp47 took: 7.745011ms
    May 16 15:23:30.139: INFO: Terminating ReplicationController proxy-service-hnp47 pods took: 100.425653ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    May 16 15:23:33.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-4923" for this suite. 05/16/23 15:23:33.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:23:33.054
May 16 15:23:33.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename custom-resource-definition 05/16/23 15:23:33.055
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:23:33.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:23:33.084
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
May 16 15:23:33.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 15:23:33.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1229" for this suite. 05/16/23 15:23:33.633
------------------------------
â€¢ [0.585 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:23:33.054
    May 16 15:23:33.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename custom-resource-definition 05/16/23 15:23:33.055
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:23:33.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:23:33.084
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    May 16 15:23:33.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 15:23:33.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1229" for this suite. 05/16/23 15:23:33.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:23:33.639
May 16 15:23:33.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename crd-publish-openapi 05/16/23 15:23:33.64
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:23:33.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:23:33.671
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 05/16/23 15:23:33.674
May 16 15:23:33.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: rename a version 05/16/23 15:23:40.057
STEP: check the new version name is served 05/16/23 15:23:40.068
STEP: check the old version name is removed 05/16/23 15:23:43.315
STEP: check the other version is not changed 05/16/23 15:23:44.146
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 15:23:48.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7166" for this suite. 05/16/23 15:23:48.792
------------------------------
â€¢ [SLOW TEST] [15.158 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:23:33.639
    May 16 15:23:33.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename crd-publish-openapi 05/16/23 15:23:33.64
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:23:33.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:23:33.671
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 05/16/23 15:23:33.674
    May 16 15:23:33.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: rename a version 05/16/23 15:23:40.057
    STEP: check the new version name is served 05/16/23 15:23:40.068
    STEP: check the old version name is removed 05/16/23 15:23:43.315
    STEP: check the other version is not changed 05/16/23 15:23:44.146
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 15:23:48.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7166" for this suite. 05/16/23 15:23:48.792
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:23:48.797
May 16 15:23:48.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename services 05/16/23 15:23:48.798
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:23:48.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:23:48.813
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 16 15:23:48.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2268" for this suite. 05/16/23 15:23:48.83
------------------------------
â€¢ [0.040 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:23:48.797
    May 16 15:23:48.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename services 05/16/23 15:23:48.798
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:23:48.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:23:48.813
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 16 15:23:48.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2268" for this suite. 05/16/23 15:23:48.83
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:23:48.838
May 16 15:23:48.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename statefulset 05/16/23 15:23:48.839
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:23:48.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:23:48.856
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2955 05/16/23 15:23:48.859
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
W0516 15:23:48.881325      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:23:48.883: INFO: Found 0 stateful pods, waiting for 1
May 16 15:23:58.887: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 05/16/23 15:23:58.893
W0516 15:23:58.900324      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
May 16 15:23:58.905: INFO: Found 1 stateful pods, waiting for 2
May 16 15:24:08.911: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 16 15:24:08.911: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 05/16/23 15:24:08.916
STEP: Delete all of the StatefulSets 05/16/23 15:24:08.919
STEP: Verify that StatefulSets have been deleted 05/16/23 15:24:08.925
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May 16 15:24:08.928: INFO: Deleting all statefulset in ns statefulset-2955
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May 16 15:24:08.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2955" for this suite. 05/16/23 15:24:08.944
------------------------------
â€¢ [SLOW TEST] [20.111 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:23:48.838
    May 16 15:23:48.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename statefulset 05/16/23 15:23:48.839
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:23:48.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:23:48.856
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2955 05/16/23 15:23:48.859
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    W0516 15:23:48.881325      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:23:48.883: INFO: Found 0 stateful pods, waiting for 1
    May 16 15:23:58.887: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 05/16/23 15:23:58.893
    W0516 15:23:58.900324      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    May 16 15:23:58.905: INFO: Found 1 stateful pods, waiting for 2
    May 16 15:24:08.911: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    May 16 15:24:08.911: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 05/16/23 15:24:08.916
    STEP: Delete all of the StatefulSets 05/16/23 15:24:08.919
    STEP: Verify that StatefulSets have been deleted 05/16/23 15:24:08.925
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May 16 15:24:08.928: INFO: Deleting all statefulset in ns statefulset-2955
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May 16 15:24:08.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2955" for this suite. 05/16/23 15:24:08.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:24:08.95
May 16 15:24:08.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename pods 05/16/23 15:24:08.951
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:24:08.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:24:08.969
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 05/16/23 15:24:08.972
May 16 15:24:09.983: INFO: Waiting up to 5m0s for pod "pod-klm6k" in namespace "pods-4723" to be "running"
May 16 15:24:09.986: INFO: Pod "pod-klm6k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.63045ms
May 16 15:24:11.989: INFO: Pod "pod-klm6k": Phase="Running", Reason="", readiness=true. Elapsed: 2.00637826s
May 16 15:24:11.989: INFO: Pod "pod-klm6k" satisfied condition "running"
STEP: patching /status 05/16/23 15:24:11.989
May 16 15:24:11.999: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 16 15:24:11.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4723" for this suite. 05/16/23 15:24:12.003
------------------------------
â€¢ [3.062 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:24:08.95
    May 16 15:24:08.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename pods 05/16/23 15:24:08.951
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:24:08.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:24:08.969
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 05/16/23 15:24:08.972
    May 16 15:24:09.983: INFO: Waiting up to 5m0s for pod "pod-klm6k" in namespace "pods-4723" to be "running"
    May 16 15:24:09.986: INFO: Pod "pod-klm6k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.63045ms
    May 16 15:24:11.989: INFO: Pod "pod-klm6k": Phase="Running", Reason="", readiness=true. Elapsed: 2.00637826s
    May 16 15:24:11.989: INFO: Pod "pod-klm6k" satisfied condition "running"
    STEP: patching /status 05/16/23 15:24:11.989
    May 16 15:24:11.999: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 16 15:24:11.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4723" for this suite. 05/16/23 15:24:12.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:24:12.012
May 16 15:24:12.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename webhook 05/16/23 15:24:12.013
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:24:12.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:24:12.03
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/16/23 15:24:12.049
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 15:24:12.312
STEP: Deploying the webhook pod 05/16/23 15:24:12.321
STEP: Wait for the deployment to be ready 05/16/23 15:24:12.329
May 16 15:24:12.336: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 05/16/23 15:24:14.345
STEP: Verifying the service has paired with the endpoint 05/16/23 15:24:14.354
May 16 15:24:15.355: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 05/16/23 15:24:15.357
STEP: Updating a mutating webhook configuration's rules to not include the create operation 05/16/23 15:24:15.373
STEP: Creating a configMap that should not be mutated 05/16/23 15:24:15.378
STEP: Patching a mutating webhook configuration's rules to include the create operation 05/16/23 15:24:15.387
STEP: Creating a configMap that should be mutated 05/16/23 15:24:15.394
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 15:24:15.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3139" for this suite. 05/16/23 15:24:15.47
STEP: Destroying namespace "webhook-3139-markers" for this suite. 05/16/23 15:24:15.477
------------------------------
â€¢ [3.470 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:24:12.012
    May 16 15:24:12.012: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename webhook 05/16/23 15:24:12.013
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:24:12.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:24:12.03
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/16/23 15:24:12.049
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 15:24:12.312
    STEP: Deploying the webhook pod 05/16/23 15:24:12.321
    STEP: Wait for the deployment to be ready 05/16/23 15:24:12.329
    May 16 15:24:12.336: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 05/16/23 15:24:14.345
    STEP: Verifying the service has paired with the endpoint 05/16/23 15:24:14.354
    May 16 15:24:15.355: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 05/16/23 15:24:15.357
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 05/16/23 15:24:15.373
    STEP: Creating a configMap that should not be mutated 05/16/23 15:24:15.378
    STEP: Patching a mutating webhook configuration's rules to include the create operation 05/16/23 15:24:15.387
    STEP: Creating a configMap that should be mutated 05/16/23 15:24:15.394
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 15:24:15.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3139" for this suite. 05/16/23 15:24:15.47
    STEP: Destroying namespace "webhook-3139-markers" for this suite. 05/16/23 15:24:15.477
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:24:15.483
May 16 15:24:15.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename crd-watch 05/16/23 15:24:15.484
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:24:15.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:24:15.511
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
May 16 15:24:15.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Creating first CR  05/16/23 15:24:18.057
May 16 15:24:18.062: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-16T15:24:18Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-16T15:24:18Z]] name:name1 resourceVersion:91360 uid:da6b7f12-66c1-4e67-8854-c92605c72b1c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 05/16/23 15:24:28.062
May 16 15:24:28.068: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-16T15:24:28Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-16T15:24:28Z]] name:name2 resourceVersion:91472 uid:02d32ead-5c93-4671-90ad-647eb01f5af3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 05/16/23 15:24:38.07
May 16 15:24:38.075: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-16T15:24:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-16T15:24:38Z]] name:name1 resourceVersion:91531 uid:da6b7f12-66c1-4e67-8854-c92605c72b1c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 05/16/23 15:24:48.076
May 16 15:24:48.082: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-16T15:24:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-16T15:24:48Z]] name:name2 resourceVersion:91592 uid:02d32ead-5c93-4671-90ad-647eb01f5af3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 05/16/23 15:24:58.082
May 16 15:24:58.089: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-16T15:24:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-16T15:24:38Z]] name:name1 resourceVersion:91644 uid:da6b7f12-66c1-4e67-8854-c92605c72b1c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 05/16/23 15:25:08.091
May 16 15:25:08.097: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-16T15:24:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-16T15:24:48Z]] name:name2 resourceVersion:91699 uid:02d32ead-5c93-4671-90ad-647eb01f5af3] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 15:25:18.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-3074" for this suite. 05/16/23 15:25:18.625
------------------------------
â€¢ [SLOW TEST] [63.187 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:24:15.483
    May 16 15:24:15.483: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename crd-watch 05/16/23 15:24:15.484
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:24:15.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:24:15.511
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    May 16 15:24:15.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Creating first CR  05/16/23 15:24:18.057
    May 16 15:24:18.062: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-16T15:24:18Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-16T15:24:18Z]] name:name1 resourceVersion:91360 uid:da6b7f12-66c1-4e67-8854-c92605c72b1c] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 05/16/23 15:24:28.062
    May 16 15:24:28.068: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-16T15:24:28Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-16T15:24:28Z]] name:name2 resourceVersion:91472 uid:02d32ead-5c93-4671-90ad-647eb01f5af3] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 05/16/23 15:24:38.07
    May 16 15:24:38.075: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-16T15:24:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-16T15:24:38Z]] name:name1 resourceVersion:91531 uid:da6b7f12-66c1-4e67-8854-c92605c72b1c] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 05/16/23 15:24:48.076
    May 16 15:24:48.082: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-16T15:24:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-16T15:24:48Z]] name:name2 resourceVersion:91592 uid:02d32ead-5c93-4671-90ad-647eb01f5af3] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 05/16/23 15:24:58.082
    May 16 15:24:58.089: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-16T15:24:18Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-16T15:24:38Z]] name:name1 resourceVersion:91644 uid:da6b7f12-66c1-4e67-8854-c92605c72b1c] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 05/16/23 15:25:08.091
    May 16 15:25:08.097: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-16T15:24:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-16T15:24:48Z]] name:name2 resourceVersion:91699 uid:02d32ead-5c93-4671-90ad-647eb01f5af3] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 15:25:18.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-3074" for this suite. 05/16/23 15:25:18.625
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:25:18.671
May 16 15:25:18.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename statefulset 05/16/23 15:25:18.672
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:25:18.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:25:18.753
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5508 05/16/23 15:25:18.755
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 05/16/23 15:25:18.76
W0516 15:25:18.765508      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:25:18.768: INFO: Found 0 stateful pods, waiting for 3
May 16 15:25:28.772: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 16 15:25:28.772: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 16 15:25:28.772: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 05/16/23 15:25:28.78
May 16 15:25:28.797: INFO: Updating stateful set ss2
STEP: Creating a new revision 05/16/23 15:25:28.797
STEP: Not applying an update when the partition is greater than the number of replicas 05/16/23 15:25:38.81
STEP: Performing a canary update 05/16/23 15:25:38.81
May 16 15:25:38.827: INFO: Updating stateful set ss2
May 16 15:25:38.832: INFO: Waiting for Pod statefulset-5508/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 05/16/23 15:25:48.838
May 16 15:25:48.872: INFO: Found 1 stateful pods, waiting for 3
May 16 15:25:58.876: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 16 15:25:58.876: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 16 15:25:58.876: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 05/16/23 15:25:58.881
May 16 15:25:58.899: INFO: Updating stateful set ss2
May 16 15:25:58.903: INFO: Waiting for Pod statefulset-5508/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
May 16 15:26:08.934: INFO: Updating stateful set ss2
May 16 15:26:08.940: INFO: Waiting for StatefulSet statefulset-5508/ss2 to complete update
May 16 15:26:08.940: INFO: Waiting for Pod statefulset-5508/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May 16 15:26:18.946: INFO: Deleting all statefulset in ns statefulset-5508
May 16 15:26:18.948: INFO: Scaling statefulset ss2 to 0
May 16 15:26:28.962: INFO: Waiting for statefulset status.replicas updated to 0
May 16 15:26:28.965: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May 16 15:26:28.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5508" for this suite. 05/16/23 15:26:28.994
------------------------------
â€¢ [SLOW TEST] [70.339 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:25:18.671
    May 16 15:25:18.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename statefulset 05/16/23 15:25:18.672
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:25:18.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:25:18.753
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5508 05/16/23 15:25:18.755
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 05/16/23 15:25:18.76
    W0516 15:25:18.765508      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:25:18.768: INFO: Found 0 stateful pods, waiting for 3
    May 16 15:25:28.772: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May 16 15:25:28.772: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May 16 15:25:28.772: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 05/16/23 15:25:28.78
    May 16 15:25:28.797: INFO: Updating stateful set ss2
    STEP: Creating a new revision 05/16/23 15:25:28.797
    STEP: Not applying an update when the partition is greater than the number of replicas 05/16/23 15:25:38.81
    STEP: Performing a canary update 05/16/23 15:25:38.81
    May 16 15:25:38.827: INFO: Updating stateful set ss2
    May 16 15:25:38.832: INFO: Waiting for Pod statefulset-5508/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 05/16/23 15:25:48.838
    May 16 15:25:48.872: INFO: Found 1 stateful pods, waiting for 3
    May 16 15:25:58.876: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May 16 15:25:58.876: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May 16 15:25:58.876: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 05/16/23 15:25:58.881
    May 16 15:25:58.899: INFO: Updating stateful set ss2
    May 16 15:25:58.903: INFO: Waiting for Pod statefulset-5508/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    May 16 15:26:08.934: INFO: Updating stateful set ss2
    May 16 15:26:08.940: INFO: Waiting for StatefulSet statefulset-5508/ss2 to complete update
    May 16 15:26:08.940: INFO: Waiting for Pod statefulset-5508/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May 16 15:26:18.946: INFO: Deleting all statefulset in ns statefulset-5508
    May 16 15:26:18.948: INFO: Scaling statefulset ss2 to 0
    May 16 15:26:28.962: INFO: Waiting for statefulset status.replicas updated to 0
    May 16 15:26:28.965: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May 16 15:26:28.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5508" for this suite. 05/16/23 15:26:28.994
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:26:29.01
May 16 15:26:29.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename events 05/16/23 15:26:29.01
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:26:29.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:26:29.026
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 05/16/23 15:26:29.028
STEP: get a list of Events with a label in the current namespace 05/16/23 15:26:29.054
STEP: delete a list of events 05/16/23 15:26:29.062
May 16 15:26:29.062: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 05/16/23 15:26:29.093
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
May 16 15:26:29.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1013" for this suite. 05/16/23 15:26:29.1
------------------------------
â€¢ [0.095 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:26:29.01
    May 16 15:26:29.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename events 05/16/23 15:26:29.01
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:26:29.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:26:29.026
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 05/16/23 15:26:29.028
    STEP: get a list of Events with a label in the current namespace 05/16/23 15:26:29.054
    STEP: delete a list of events 05/16/23 15:26:29.062
    May 16 15:26:29.062: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 05/16/23 15:26:29.093
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    May 16 15:26:29.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1013" for this suite. 05/16/23 15:26:29.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:26:29.106
May 16 15:26:29.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename resourcequota 05/16/23 15:26:29.106
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:26:29.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:26:29.124
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 05/16/23 15:26:29.127
STEP: Creating a ResourceQuota 05/16/23 15:26:34.129
STEP: Ensuring resource quota status is calculated 05/16/23 15:26:34.133
STEP: Creating a Service 05/16/23 15:26:36.137
STEP: Creating a NodePort Service 05/16/23 15:26:36.151
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 05/16/23 15:26:36.181
STEP: Ensuring resource quota status captures service creation 05/16/23 15:26:36.212
STEP: Deleting Services 05/16/23 15:26:38.217
STEP: Ensuring resource quota status released usage 05/16/23 15:26:38.267
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 16 15:26:40.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4232" for this suite. 05/16/23 15:26:40.275
------------------------------
â€¢ [SLOW TEST] [11.175 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:26:29.106
    May 16 15:26:29.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename resourcequota 05/16/23 15:26:29.106
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:26:29.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:26:29.124
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 05/16/23 15:26:29.127
    STEP: Creating a ResourceQuota 05/16/23 15:26:34.129
    STEP: Ensuring resource quota status is calculated 05/16/23 15:26:34.133
    STEP: Creating a Service 05/16/23 15:26:36.137
    STEP: Creating a NodePort Service 05/16/23 15:26:36.151
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 05/16/23 15:26:36.181
    STEP: Ensuring resource quota status captures service creation 05/16/23 15:26:36.212
    STEP: Deleting Services 05/16/23 15:26:38.217
    STEP: Ensuring resource quota status released usage 05/16/23 15:26:38.267
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 16 15:26:40.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4232" for this suite. 05/16/23 15:26:40.275
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:26:40.28
May 16 15:26:40.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 15:26:40.281
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:26:40.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:26:40.296
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 05/16/23 15:26:40.3
W0516 15:26:40.315361      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:26:40.315: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54bb7dd2-4d57-49ec-94cb-fec97fea1947" in namespace "projected-6239" to be "Succeeded or Failed"
May 16 15:26:40.326: INFO: Pod "downwardapi-volume-54bb7dd2-4d57-49ec-94cb-fec97fea1947": Phase="Pending", Reason="", readiness=false. Elapsed: 11.322514ms
May 16 15:26:42.330: INFO: Pod "downwardapi-volume-54bb7dd2-4d57-49ec-94cb-fec97fea1947": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015406873s
May 16 15:26:44.330: INFO: Pod "downwardapi-volume-54bb7dd2-4d57-49ec-94cb-fec97fea1947": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015305455s
STEP: Saw pod success 05/16/23 15:26:44.33
May 16 15:26:44.330: INFO: Pod "downwardapi-volume-54bb7dd2-4d57-49ec-94cb-fec97fea1947" satisfied condition "Succeeded or Failed"
May 16 15:26:44.333: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-54bb7dd2-4d57-49ec-94cb-fec97fea1947 container client-container: <nil>
STEP: delete the pod 05/16/23 15:26:44.344
May 16 15:26:44.354: INFO: Waiting for pod downwardapi-volume-54bb7dd2-4d57-49ec-94cb-fec97fea1947 to disappear
May 16 15:26:44.357: INFO: Pod downwardapi-volume-54bb7dd2-4d57-49ec-94cb-fec97fea1947 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 16 15:26:44.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6239" for this suite. 05/16/23 15:26:44.36
------------------------------
â€¢ [4.085 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:26:40.28
    May 16 15:26:40.280: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 15:26:40.281
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:26:40.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:26:40.296
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 05/16/23 15:26:40.3
    W0516 15:26:40.315361      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:26:40.315: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54bb7dd2-4d57-49ec-94cb-fec97fea1947" in namespace "projected-6239" to be "Succeeded or Failed"
    May 16 15:26:40.326: INFO: Pod "downwardapi-volume-54bb7dd2-4d57-49ec-94cb-fec97fea1947": Phase="Pending", Reason="", readiness=false. Elapsed: 11.322514ms
    May 16 15:26:42.330: INFO: Pod "downwardapi-volume-54bb7dd2-4d57-49ec-94cb-fec97fea1947": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015406873s
    May 16 15:26:44.330: INFO: Pod "downwardapi-volume-54bb7dd2-4d57-49ec-94cb-fec97fea1947": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015305455s
    STEP: Saw pod success 05/16/23 15:26:44.33
    May 16 15:26:44.330: INFO: Pod "downwardapi-volume-54bb7dd2-4d57-49ec-94cb-fec97fea1947" satisfied condition "Succeeded or Failed"
    May 16 15:26:44.333: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-54bb7dd2-4d57-49ec-94cb-fec97fea1947 container client-container: <nil>
    STEP: delete the pod 05/16/23 15:26:44.344
    May 16 15:26:44.354: INFO: Waiting for pod downwardapi-volume-54bb7dd2-4d57-49ec-94cb-fec97fea1947 to disappear
    May 16 15:26:44.357: INFO: Pod downwardapi-volume-54bb7dd2-4d57-49ec-94cb-fec97fea1947 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 16 15:26:44.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6239" for this suite. 05/16/23 15:26:44.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:26:44.366
May 16 15:26:44.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename pods 05/16/23 15:26:44.366
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:26:44.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:26:44.381
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 05/16/23 15:26:44.383
May 16 15:26:44.397: INFO: created test-pod-1
May 16 15:26:44.411: INFO: created test-pod-2
May 16 15:26:44.421: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 05/16/23 15:26:44.421
May 16 15:26:44.421: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5666' to be running and ready
May 16 15:26:44.444: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May 16 15:26:44.444: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May 16 15:26:44.444: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
May 16 15:26:44.444: INFO: 0 / 3 pods in namespace 'pods-5666' are running and ready (0 seconds elapsed)
May 16 15:26:44.444: INFO: expected 0 pod replicas in namespace 'pods-5666', 0 are Running and Ready.
May 16 15:26:44.444: INFO: POD         NODE                                           PHASE    GRACE  CONDITIONS
May 16 15:26:44.444: INFO: test-pod-1  ip-10-0-161-164.eu-central-1.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 15:26:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 15:26:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 15:26:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 15:26:44 +0000 UTC  }]
May 16 15:26:44.444: INFO: test-pod-2  ip-10-0-161-164.eu-central-1.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 15:26:44 +0000 UTC  }]
May 16 15:26:44.444: INFO: test-pod-3  ip-10-0-132-142.eu-central-1.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 15:26:44 +0000 UTC  }]
May 16 15:26:44.444: INFO: 
May 16 15:26:46.452: INFO: 3 / 3 pods in namespace 'pods-5666' are running and ready (2 seconds elapsed)
May 16 15:26:46.452: INFO: expected 0 pod replicas in namespace 'pods-5666', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 05/16/23 15:26:46.469
May 16 15:26:46.472: INFO: Pod quantity 3 is different from expected quantity 0
May 16 15:26:47.476: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 16 15:26:48.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5666" for this suite. 05/16/23 15:26:48.479
------------------------------
â€¢ [4.120 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:26:44.366
    May 16 15:26:44.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename pods 05/16/23 15:26:44.366
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:26:44.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:26:44.381
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 05/16/23 15:26:44.383
    May 16 15:26:44.397: INFO: created test-pod-1
    May 16 15:26:44.411: INFO: created test-pod-2
    May 16 15:26:44.421: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 05/16/23 15:26:44.421
    May 16 15:26:44.421: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5666' to be running and ready
    May 16 15:26:44.444: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May 16 15:26:44.444: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May 16 15:26:44.444: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    May 16 15:26:44.444: INFO: 0 / 3 pods in namespace 'pods-5666' are running and ready (0 seconds elapsed)
    May 16 15:26:44.444: INFO: expected 0 pod replicas in namespace 'pods-5666', 0 are Running and Ready.
    May 16 15:26:44.444: INFO: POD         NODE                                           PHASE    GRACE  CONDITIONS
    May 16 15:26:44.444: INFO: test-pod-1  ip-10-0-161-164.eu-central-1.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 15:26:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 15:26:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-16 15:26:44 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 15:26:44 +0000 UTC  }]
    May 16 15:26:44.444: INFO: test-pod-2  ip-10-0-161-164.eu-central-1.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 15:26:44 +0000 UTC  }]
    May 16 15:26:44.444: INFO: test-pod-3  ip-10-0-132-142.eu-central-1.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-16 15:26:44 +0000 UTC  }]
    May 16 15:26:44.444: INFO: 
    May 16 15:26:46.452: INFO: 3 / 3 pods in namespace 'pods-5666' are running and ready (2 seconds elapsed)
    May 16 15:26:46.452: INFO: expected 0 pod replicas in namespace 'pods-5666', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 05/16/23 15:26:46.469
    May 16 15:26:46.472: INFO: Pod quantity 3 is different from expected quantity 0
    May 16 15:26:47.476: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 16 15:26:48.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5666" for this suite. 05/16/23 15:26:48.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:26:48.486
May 16 15:26:48.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename secrets 05/16/23 15:26:48.487
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:26:48.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:26:48.505
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-606/secret-test-3b6e6c39-9f5b-49cd-a2ec-f9068db2e1de 05/16/23 15:26:48.507
STEP: Creating a pod to test consume secrets 05/16/23 15:26:48.512
May 16 15:26:48.525: INFO: Waiting up to 5m0s for pod "pod-configmaps-78e2e1e0-17e9-492f-a7bb-5dc0eab101bf" in namespace "secrets-606" to be "Succeeded or Failed"
May 16 15:26:48.528: INFO: Pod "pod-configmaps-78e2e1e0-17e9-492f-a7bb-5dc0eab101bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.642214ms
May 16 15:26:50.532: INFO: Pod "pod-configmaps-78e2e1e0-17e9-492f-a7bb-5dc0eab101bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006566035s
May 16 15:26:52.532: INFO: Pod "pod-configmaps-78e2e1e0-17e9-492f-a7bb-5dc0eab101bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006618963s
STEP: Saw pod success 05/16/23 15:26:52.532
May 16 15:26:52.532: INFO: Pod "pod-configmaps-78e2e1e0-17e9-492f-a7bb-5dc0eab101bf" satisfied condition "Succeeded or Failed"
May 16 15:26:52.535: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-configmaps-78e2e1e0-17e9-492f-a7bb-5dc0eab101bf container env-test: <nil>
STEP: delete the pod 05/16/23 15:26:52.541
May 16 15:26:52.550: INFO: Waiting for pod pod-configmaps-78e2e1e0-17e9-492f-a7bb-5dc0eab101bf to disappear
May 16 15:26:52.553: INFO: Pod pod-configmaps-78e2e1e0-17e9-492f-a7bb-5dc0eab101bf no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
May 16 15:26:52.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-606" for this suite. 05/16/23 15:26:52.558
------------------------------
â€¢ [4.077 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:26:48.486
    May 16 15:26:48.487: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename secrets 05/16/23 15:26:48.487
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:26:48.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:26:48.505
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-606/secret-test-3b6e6c39-9f5b-49cd-a2ec-f9068db2e1de 05/16/23 15:26:48.507
    STEP: Creating a pod to test consume secrets 05/16/23 15:26:48.512
    May 16 15:26:48.525: INFO: Waiting up to 5m0s for pod "pod-configmaps-78e2e1e0-17e9-492f-a7bb-5dc0eab101bf" in namespace "secrets-606" to be "Succeeded or Failed"
    May 16 15:26:48.528: INFO: Pod "pod-configmaps-78e2e1e0-17e9-492f-a7bb-5dc0eab101bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.642214ms
    May 16 15:26:50.532: INFO: Pod "pod-configmaps-78e2e1e0-17e9-492f-a7bb-5dc0eab101bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006566035s
    May 16 15:26:52.532: INFO: Pod "pod-configmaps-78e2e1e0-17e9-492f-a7bb-5dc0eab101bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006618963s
    STEP: Saw pod success 05/16/23 15:26:52.532
    May 16 15:26:52.532: INFO: Pod "pod-configmaps-78e2e1e0-17e9-492f-a7bb-5dc0eab101bf" satisfied condition "Succeeded or Failed"
    May 16 15:26:52.535: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-configmaps-78e2e1e0-17e9-492f-a7bb-5dc0eab101bf container env-test: <nil>
    STEP: delete the pod 05/16/23 15:26:52.541
    May 16 15:26:52.550: INFO: Waiting for pod pod-configmaps-78e2e1e0-17e9-492f-a7bb-5dc0eab101bf to disappear
    May 16 15:26:52.553: INFO: Pod pod-configmaps-78e2e1e0-17e9-492f-a7bb-5dc0eab101bf no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    May 16 15:26:52.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-606" for this suite. 05/16/23 15:26:52.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:26:52.564
May 16 15:26:52.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename resourcequota 05/16/23 15:26:52.564
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:26:52.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:26:52.58
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 05/16/23 15:26:52.582
STEP: Creating a ResourceQuota 05/16/23 15:26:57.585
STEP: Ensuring resource quota status is calculated 05/16/23 15:26:57.589
STEP: Creating a ReplicaSet 05/16/23 15:26:59.592
STEP: Ensuring resource quota status captures replicaset creation 05/16/23 15:26:59.617
STEP: Deleting a ReplicaSet 05/16/23 15:27:01.621
STEP: Ensuring resource quota status released usage 05/16/23 15:27:01.626
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 16 15:27:03.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-41" for this suite. 05/16/23 15:27:03.633
------------------------------
â€¢ [SLOW TEST] [11.074 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:26:52.564
    May 16 15:26:52.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename resourcequota 05/16/23 15:26:52.564
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:26:52.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:26:52.58
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 05/16/23 15:26:52.582
    STEP: Creating a ResourceQuota 05/16/23 15:26:57.585
    STEP: Ensuring resource quota status is calculated 05/16/23 15:26:57.589
    STEP: Creating a ReplicaSet 05/16/23 15:26:59.592
    STEP: Ensuring resource quota status captures replicaset creation 05/16/23 15:26:59.617
    STEP: Deleting a ReplicaSet 05/16/23 15:27:01.621
    STEP: Ensuring resource quota status released usage 05/16/23 15:27:01.626
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 16 15:27:03.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-41" for this suite. 05/16/23 15:27:03.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:27:03.639
May 16 15:27:03.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename deployment 05/16/23 15:27:03.639
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:27:03.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:27:03.654
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 05/16/23 15:27:03.671
May 16 15:27:03.671: INFO: Creating simple deployment test-deployment-98s8q
May 16 15:27:03.685: INFO: deployment "test-deployment-98s8q" doesn't have the required revision set
STEP: Getting /status 05/16/23 15:27:05.695
May 16 15:27:05.698: INFO: Deployment test-deployment-98s8q has Conditions: [{Available True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-98s8q-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 05/16/23 15:27:05.698
May 16 15:27:05.705: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 27, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 27, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 27, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 27, 3, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-98s8q-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 05/16/23 15:27:05.705
May 16 15:27:05.706: INFO: Observed &Deployment event: ADDED
May 16 15:27:05.706: INFO: Observed Deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-98s8q-54bc444df"}
May 16 15:27:05.706: INFO: Observed &Deployment event: MODIFIED
May 16 15:27:05.706: INFO: Observed Deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-98s8q-54bc444df"}
May 16 15:27:05.706: INFO: Observed Deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 16 15:27:05.706: INFO: Observed &Deployment event: MODIFIED
May 16 15:27:05.706: INFO: Observed Deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 16 15:27:05.706: INFO: Observed Deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-98s8q-54bc444df" is progressing.}
May 16 15:27:05.706: INFO: Observed &Deployment event: MODIFIED
May 16 15:27:05.706: INFO: Observed Deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 16 15:27:05.706: INFO: Observed Deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-98s8q-54bc444df" has successfully progressed.}
May 16 15:27:05.706: INFO: Observed &Deployment event: MODIFIED
May 16 15:27:05.706: INFO: Observed Deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 16 15:27:05.706: INFO: Observed Deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-98s8q-54bc444df" has successfully progressed.}
May 16 15:27:05.706: INFO: Found Deployment test-deployment-98s8q in namespace deployment-3244 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 16 15:27:05.706: INFO: Deployment test-deployment-98s8q has an updated status
STEP: patching the Statefulset Status 05/16/23 15:27:05.706
May 16 15:27:05.706: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May 16 15:27:05.712: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 05/16/23 15:27:05.712
May 16 15:27:05.713: INFO: Observed &Deployment event: ADDED
May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-98s8q-54bc444df"}
May 16 15:27:05.713: INFO: Observed &Deployment event: MODIFIED
May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-98s8q-54bc444df"}
May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 16 15:27:05.713: INFO: Observed &Deployment event: MODIFIED
May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-98s8q-54bc444df" is progressing.}
May 16 15:27:05.713: INFO: Observed &Deployment event: MODIFIED
May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-98s8q-54bc444df" has successfully progressed.}
May 16 15:27:05.713: INFO: Observed &Deployment event: MODIFIED
May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-98s8q-54bc444df" has successfully progressed.}
May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 16 15:27:05.713: INFO: Observed &Deployment event: MODIFIED
May 16 15:27:05.713: INFO: Found deployment test-deployment-98s8q in namespace deployment-3244 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
May 16 15:27:05.713: INFO: Deployment test-deployment-98s8q has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 16 15:27:05.718: INFO: Deployment "test-deployment-98s8q":
&Deployment{ObjectMeta:{test-deployment-98s8q  deployment-3244  3bf26f92-3e23-4fbd-8616-9b4d12510991 93296 1 2023-05-16 15:27:03 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-16 15:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:27:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-05-16 15:27:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004397878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 16 15:27:05.720: INFO: New ReplicaSet "test-deployment-98s8q-54bc444df" of Deployment "test-deployment-98s8q":
&ReplicaSet{ObjectMeta:{test-deployment-98s8q-54bc444df  deployment-3244  e30ccbf8-4fa9-480e-a2f2-d241ea6b8fdd 93288 1 2023-05-16 15:27:03 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-98s8q 3bf26f92-3e23-4fbd-8616-9b4d12510991 0xc004e56600 0xc004e56601}] [] [{kube-controller-manager Update apps/v1 2023-05-16 15:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bf26f92-3e23-4fbd-8616-9b4d12510991\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:27:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e566a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 16 15:27:05.723: INFO: Pod "test-deployment-98s8q-54bc444df-6s9lp" is available:
&Pod{ObjectMeta:{test-deployment-98s8q-54bc444df-6s9lp test-deployment-98s8q-54bc444df- deployment-3244  e181aa6b-1204-4e65-bb6f-7f69ac8526ae 93287 0 2023-05-16 15:27:03 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.1.94/23"],"mac_address":"0a:58:0a:83:01:5e","gateway_ips":["10.131.0.1"],"ip_address":"10.131.1.94/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.1.94"
    ],
    "mac": "0a:58:0a:83:01:5e",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-98s8q-54bc444df e30ccbf8-4fa9-480e-a2f2-d241ea6b8fdd 0xc004e56a57 0xc004e56a58}] [] [{ip-10-0-130-191 Update v1 2023-05-16 15:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 15:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e30ccbf8-4fa9-480e-a2f2-d241ea6b8fdd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 15:27:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-05-16 15:27:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lj6nk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lj6nk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c47,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:27:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:27:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:27:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:27:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:10.131.1.94,StartTime:2023-05-16 15:27:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 15:27:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://6729334bf1262d542be63288290ff41e7c572edaa7c37b853035431be88d363a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.1.94,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May 16 15:27:05.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3244" for this suite. 05/16/23 15:27:05.726
------------------------------
â€¢ [2.093 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:27:03.639
    May 16 15:27:03.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename deployment 05/16/23 15:27:03.639
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:27:03.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:27:03.654
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 05/16/23 15:27:03.671
    May 16 15:27:03.671: INFO: Creating simple deployment test-deployment-98s8q
    May 16 15:27:03.685: INFO: deployment "test-deployment-98s8q" doesn't have the required revision set
    STEP: Getting /status 05/16/23 15:27:05.695
    May 16 15:27:05.698: INFO: Deployment test-deployment-98s8q has Conditions: [{Available True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-98s8q-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 05/16/23 15:27:05.698
    May 16 15:27:05.705: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 27, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 27, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 27, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 27, 3, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-98s8q-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 05/16/23 15:27:05.705
    May 16 15:27:05.706: INFO: Observed &Deployment event: ADDED
    May 16 15:27:05.706: INFO: Observed Deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-98s8q-54bc444df"}
    May 16 15:27:05.706: INFO: Observed &Deployment event: MODIFIED
    May 16 15:27:05.706: INFO: Observed Deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-98s8q-54bc444df"}
    May 16 15:27:05.706: INFO: Observed Deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May 16 15:27:05.706: INFO: Observed &Deployment event: MODIFIED
    May 16 15:27:05.706: INFO: Observed Deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May 16 15:27:05.706: INFO: Observed Deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-98s8q-54bc444df" is progressing.}
    May 16 15:27:05.706: INFO: Observed &Deployment event: MODIFIED
    May 16 15:27:05.706: INFO: Observed Deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May 16 15:27:05.706: INFO: Observed Deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-98s8q-54bc444df" has successfully progressed.}
    May 16 15:27:05.706: INFO: Observed &Deployment event: MODIFIED
    May 16 15:27:05.706: INFO: Observed Deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May 16 15:27:05.706: INFO: Observed Deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-98s8q-54bc444df" has successfully progressed.}
    May 16 15:27:05.706: INFO: Found Deployment test-deployment-98s8q in namespace deployment-3244 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May 16 15:27:05.706: INFO: Deployment test-deployment-98s8q has an updated status
    STEP: patching the Statefulset Status 05/16/23 15:27:05.706
    May 16 15:27:05.706: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    May 16 15:27:05.712: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 05/16/23 15:27:05.712
    May 16 15:27:05.713: INFO: Observed &Deployment event: ADDED
    May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-98s8q-54bc444df"}
    May 16 15:27:05.713: INFO: Observed &Deployment event: MODIFIED
    May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-98s8q-54bc444df"}
    May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May 16 15:27:05.713: INFO: Observed &Deployment event: MODIFIED
    May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:03 +0000 UTC 2023-05-16 15:27:03 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-98s8q-54bc444df" is progressing.}
    May 16 15:27:05.713: INFO: Observed &Deployment event: MODIFIED
    May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-98s8q-54bc444df" has successfully progressed.}
    May 16 15:27:05.713: INFO: Observed &Deployment event: MODIFIED
    May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-16 15:27:04 +0000 UTC 2023-05-16 15:27:03 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-98s8q-54bc444df" has successfully progressed.}
    May 16 15:27:05.713: INFO: Observed deployment test-deployment-98s8q in namespace deployment-3244 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    May 16 15:27:05.713: INFO: Observed &Deployment event: MODIFIED
    May 16 15:27:05.713: INFO: Found deployment test-deployment-98s8q in namespace deployment-3244 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    May 16 15:27:05.713: INFO: Deployment test-deployment-98s8q has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 16 15:27:05.718: INFO: Deployment "test-deployment-98s8q":
    &Deployment{ObjectMeta:{test-deployment-98s8q  deployment-3244  3bf26f92-3e23-4fbd-8616-9b4d12510991 93296 1 2023-05-16 15:27:03 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-16 15:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:27:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-05-16 15:27:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004397878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May 16 15:27:05.720: INFO: New ReplicaSet "test-deployment-98s8q-54bc444df" of Deployment "test-deployment-98s8q":
    &ReplicaSet{ObjectMeta:{test-deployment-98s8q-54bc444df  deployment-3244  e30ccbf8-4fa9-480e-a2f2-d241ea6b8fdd 93288 1 2023-05-16 15:27:03 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-98s8q 3bf26f92-3e23-4fbd-8616-9b4d12510991 0xc004e56600 0xc004e56601}] [] [{kube-controller-manager Update apps/v1 2023-05-16 15:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bf26f92-3e23-4fbd-8616-9b4d12510991\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:27:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e566a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May 16 15:27:05.723: INFO: Pod "test-deployment-98s8q-54bc444df-6s9lp" is available:
    &Pod{ObjectMeta:{test-deployment-98s8q-54bc444df-6s9lp test-deployment-98s8q-54bc444df- deployment-3244  e181aa6b-1204-4e65-bb6f-7f69ac8526ae 93287 0 2023-05-16 15:27:03 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.1.94/23"],"mac_address":"0a:58:0a:83:01:5e","gateway_ips":["10.131.0.1"],"ip_address":"10.131.1.94/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.1.94"
        ],
        "mac": "0a:58:0a:83:01:5e",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-98s8q-54bc444df e30ccbf8-4fa9-480e-a2f2-d241ea6b8fdd 0xc004e56a57 0xc004e56a58}] [] [{ip-10-0-130-191 Update v1 2023-05-16 15:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 15:27:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e30ccbf8-4fa9-480e-a2f2-d241ea6b8fdd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 15:27:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-05-16 15:27:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lj6nk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lj6nk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c47,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:27:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:27:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:27:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:27:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:10.131.1.94,StartTime:2023-05-16 15:27:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 15:27:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://6729334bf1262d542be63288290ff41e7c572edaa7c37b853035431be88d363a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.1.94,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May 16 15:27:05.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3244" for this suite. 05/16/23 15:27:05.726
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:27:05.732
May 16 15:27:05.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename disruption 05/16/23 15:27:05.733
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:27:05.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:27:05.75
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:27:05.752
May 16 15:27:05.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename disruption-2 05/16/23 15:27:05.752
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:27:05.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:27:05.774
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 05/16/23 15:27:05.782
STEP: Waiting for the pdb to be processed 05/16/23 15:27:05.802
STEP: Waiting for the pdb to be processed 05/16/23 15:27:07.815
STEP: listing a collection of PDBs across all namespaces 05/16/23 15:27:09.821
STEP: listing a collection of PDBs in namespace disruption-9945 05/16/23 15:27:09.824
STEP: deleting a collection of PDBs 05/16/23 15:27:09.827
STEP: Waiting for the PDB collection to be deleted 05/16/23 15:27:09.837
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
May 16 15:27:09.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May 16 15:27:09.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-2673" for this suite. 05/16/23 15:27:09.846
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9945" for this suite. 05/16/23 15:27:09.851
------------------------------
â€¢ [4.124 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:27:05.732
    May 16 15:27:05.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename disruption 05/16/23 15:27:05.733
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:27:05.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:27:05.75
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:27:05.752
    May 16 15:27:05.752: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename disruption-2 05/16/23 15:27:05.752
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:27:05.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:27:05.774
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 05/16/23 15:27:05.782
    STEP: Waiting for the pdb to be processed 05/16/23 15:27:05.802
    STEP: Waiting for the pdb to be processed 05/16/23 15:27:07.815
    STEP: listing a collection of PDBs across all namespaces 05/16/23 15:27:09.821
    STEP: listing a collection of PDBs in namespace disruption-9945 05/16/23 15:27:09.824
    STEP: deleting a collection of PDBs 05/16/23 15:27:09.827
    STEP: Waiting for the PDB collection to be deleted 05/16/23 15:27:09.837
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    May 16 15:27:09.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May 16 15:27:09.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-2673" for this suite. 05/16/23 15:27:09.846
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9945" for this suite. 05/16/23 15:27:09.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:27:09.857
May 16 15:27:09.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubectl 05/16/23 15:27:09.858
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:27:09.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:27:09.873
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 05/16/23 15:27:09.875
May 16 15:27:09.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-1520 create -f -'
May 16 15:27:11.410: INFO: stderr: ""
May 16 15:27:11.410: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 05/16/23 15:27:11.41
May 16 15:27:11.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-1520 diff -f -'
May 16 15:27:11.671: INFO: rc: 1
May 16 15:27:11.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-1520 delete -f -'
May 16 15:27:11.718: INFO: stderr: ""
May 16 15:27:11.718: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 16 15:27:11.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1520" for this suite. 05/16/23 15:27:11.724
------------------------------
â€¢ [1.874 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:27:09.857
    May 16 15:27:09.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubectl 05/16/23 15:27:09.858
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:27:09.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:27:09.873
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 05/16/23 15:27:09.875
    May 16 15:27:09.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-1520 create -f -'
    May 16 15:27:11.410: INFO: stderr: ""
    May 16 15:27:11.410: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 05/16/23 15:27:11.41
    May 16 15:27:11.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-1520 diff -f -'
    May 16 15:27:11.671: INFO: rc: 1
    May 16 15:27:11.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-1520 delete -f -'
    May 16 15:27:11.718: INFO: stderr: ""
    May 16 15:27:11.718: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 16 15:27:11.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1520" for this suite. 05/16/23 15:27:11.724
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:27:11.731
May 16 15:27:11.731: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename statefulset 05/16/23 15:27:11.731
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:27:11.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:27:11.748
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-373 05/16/23 15:27:11.751
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 05/16/23 15:27:11.77
W0516 15:27:11.778533      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:27:11.783: INFO: Found 0 stateful pods, waiting for 3
May 16 15:27:21.787: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 16 15:27:21.787: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 16 15:27:21.787: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 16 15:27:21.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-373 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 16 15:27:21.902: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 16 15:27:21.902: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 16 15:27:21.902: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 05/16/23 15:27:31.914
May 16 15:27:31.931: INFO: Updating stateful set ss2
STEP: Creating a new revision 05/16/23 15:27:31.931
STEP: Updating Pods in reverse ordinal order 05/16/23 15:27:41.944
May 16 15:27:41.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-373 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 16 15:27:42.075: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 16 15:27:42.075: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 16 15:27:42.075: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 05/16/23 15:27:52.092
May 16 15:27:52.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-373 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 16 15:27:52.203: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 16 15:27:52.203: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 16 15:27:52.203: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 16 15:28:02.242: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 05/16/23 15:28:12.259
May 16 15:28:12.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-373 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 16 15:28:12.389: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 16 15:28:12.389: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 16 15:28:12.389: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
May 16 15:28:22.425: INFO: Deleting all statefulset in ns statefulset-373
May 16 15:28:22.434: INFO: Scaling statefulset ss2 to 0
May 16 15:28:32.470: INFO: Waiting for statefulset status.replicas updated to 0
May 16 15:28:32.480: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
May 16 15:28:32.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-373" for this suite. 05/16/23 15:28:32.557
------------------------------
â€¢ [SLOW TEST] [80.839 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:27:11.731
    May 16 15:27:11.731: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename statefulset 05/16/23 15:27:11.731
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:27:11.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:27:11.748
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-373 05/16/23 15:27:11.751
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 05/16/23 15:27:11.77
    W0516 15:27:11.778533      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:27:11.783: INFO: Found 0 stateful pods, waiting for 3
    May 16 15:27:21.787: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    May 16 15:27:21.787: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    May 16 15:27:21.787: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    May 16 15:27:21.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-373 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 16 15:27:21.902: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 16 15:27:21.902: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 16 15:27:21.902: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 05/16/23 15:27:31.914
    May 16 15:27:31.931: INFO: Updating stateful set ss2
    STEP: Creating a new revision 05/16/23 15:27:31.931
    STEP: Updating Pods in reverse ordinal order 05/16/23 15:27:41.944
    May 16 15:27:41.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-373 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 16 15:27:42.075: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 16 15:27:42.075: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 16 15:27:42.075: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 05/16/23 15:27:52.092
    May 16 15:27:52.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-373 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    May 16 15:27:52.203: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    May 16 15:27:52.203: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    May 16 15:27:52.203: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    May 16 15:28:02.242: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 05/16/23 15:28:12.259
    May 16 15:28:12.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=statefulset-373 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    May 16 15:28:12.389: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    May 16 15:28:12.389: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    May 16 15:28:12.389: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    May 16 15:28:22.425: INFO: Deleting all statefulset in ns statefulset-373
    May 16 15:28:22.434: INFO: Scaling statefulset ss2 to 0
    May 16 15:28:32.470: INFO: Waiting for statefulset status.replicas updated to 0
    May 16 15:28:32.480: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    May 16 15:28:32.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-373" for this suite. 05/16/23 15:28:32.557
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:28:32.57
May 16 15:28:32.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename replication-controller 05/16/23 15:28:32.571
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:28:32.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:28:32.593
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d 05/16/23 15:28:32.609
W0516 15:28:33.614982      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:28:33.617: INFO: Pod name my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d: Found 0 pods out of 1
May 16 15:28:38.621: INFO: Pod name my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d: Found 1 pods out of 1
May 16 15:28:38.621: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d" are running
May 16 15:28:38.621: INFO: Waiting up to 5m0s for pod "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d-kbqls" in namespace "replication-controller-7214" to be "running"
May 16 15:28:38.624: INFO: Pod "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d-kbqls": Phase="Running", Reason="", readiness=true. Elapsed: 2.997933ms
May 16 15:28:38.625: INFO: Pod "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d-kbqls" satisfied condition "running"
May 16 15:28:38.625: INFO: Pod "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d-kbqls" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-16 15:28:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-16 15:28:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-16 15:28:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-16 15:28:33 +0000 UTC Reason: Message:}])
May 16 15:28:38.625: INFO: Trying to dial the pod
May 16 15:28:43.637: INFO: Controller my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d: Got expected result from replica 1 [my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d-kbqls]: "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d-kbqls", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May 16 15:28:43.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7214" for this suite. 05/16/23 15:28:43.64
------------------------------
â€¢ [SLOW TEST] [11.076 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:28:32.57
    May 16 15:28:32.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename replication-controller 05/16/23 15:28:32.571
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:28:32.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:28:32.593
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d 05/16/23 15:28:32.609
    W0516 15:28:33.614982      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:28:33.617: INFO: Pod name my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d: Found 0 pods out of 1
    May 16 15:28:38.621: INFO: Pod name my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d: Found 1 pods out of 1
    May 16 15:28:38.621: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d" are running
    May 16 15:28:38.621: INFO: Waiting up to 5m0s for pod "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d-kbqls" in namespace "replication-controller-7214" to be "running"
    May 16 15:28:38.624: INFO: Pod "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d-kbqls": Phase="Running", Reason="", readiness=true. Elapsed: 2.997933ms
    May 16 15:28:38.625: INFO: Pod "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d-kbqls" satisfied condition "running"
    May 16 15:28:38.625: INFO: Pod "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d-kbqls" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-16 15:28:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-16 15:28:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-16 15:28:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-16 15:28:33 +0000 UTC Reason: Message:}])
    May 16 15:28:38.625: INFO: Trying to dial the pod
    May 16 15:28:43.637: INFO: Controller my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d: Got expected result from replica 1 [my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d-kbqls]: "my-hostname-basic-d7b51952-9dc7-4c50-baea-a4dac1557f1d-kbqls", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May 16 15:28:43.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7214" for this suite. 05/16/23 15:28:43.64
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:28:43.646
May 16 15:28:43.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename secrets 05/16/23 15:28:43.647
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:28:43.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:28:43.665
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-f417c84e-f174-4fb9-a2e4-cf3820f1070f 05/16/23 15:28:43.668
STEP: Creating a pod to test consume secrets 05/16/23 15:28:43.674
May 16 15:28:43.685: INFO: Waiting up to 5m0s for pod "pod-secrets-422906fd-0f18-474c-8ee7-1a36336165c6" in namespace "secrets-3068" to be "Succeeded or Failed"
May 16 15:28:43.687: INFO: Pod "pod-secrets-422906fd-0f18-474c-8ee7-1a36336165c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.369063ms
May 16 15:28:45.691: INFO: Pod "pod-secrets-422906fd-0f18-474c-8ee7-1a36336165c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00562115s
May 16 15:28:47.691: INFO: Pod "pod-secrets-422906fd-0f18-474c-8ee7-1a36336165c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005577496s
STEP: Saw pod success 05/16/23 15:28:47.691
May 16 15:28:47.691: INFO: Pod "pod-secrets-422906fd-0f18-474c-8ee7-1a36336165c6" satisfied condition "Succeeded or Failed"
May 16 15:28:47.693: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-secrets-422906fd-0f18-474c-8ee7-1a36336165c6 container secret-volume-test: <nil>
STEP: delete the pod 05/16/23 15:28:47.702
May 16 15:28:47.713: INFO: Waiting for pod pod-secrets-422906fd-0f18-474c-8ee7-1a36336165c6 to disappear
May 16 15:28:47.715: INFO: Pod pod-secrets-422906fd-0f18-474c-8ee7-1a36336165c6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May 16 15:28:47.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3068" for this suite. 05/16/23 15:28:47.719
------------------------------
â€¢ [4.078 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:28:43.646
    May 16 15:28:43.646: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename secrets 05/16/23 15:28:43.647
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:28:43.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:28:43.665
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-f417c84e-f174-4fb9-a2e4-cf3820f1070f 05/16/23 15:28:43.668
    STEP: Creating a pod to test consume secrets 05/16/23 15:28:43.674
    May 16 15:28:43.685: INFO: Waiting up to 5m0s for pod "pod-secrets-422906fd-0f18-474c-8ee7-1a36336165c6" in namespace "secrets-3068" to be "Succeeded or Failed"
    May 16 15:28:43.687: INFO: Pod "pod-secrets-422906fd-0f18-474c-8ee7-1a36336165c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.369063ms
    May 16 15:28:45.691: INFO: Pod "pod-secrets-422906fd-0f18-474c-8ee7-1a36336165c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00562115s
    May 16 15:28:47.691: INFO: Pod "pod-secrets-422906fd-0f18-474c-8ee7-1a36336165c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005577496s
    STEP: Saw pod success 05/16/23 15:28:47.691
    May 16 15:28:47.691: INFO: Pod "pod-secrets-422906fd-0f18-474c-8ee7-1a36336165c6" satisfied condition "Succeeded or Failed"
    May 16 15:28:47.693: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-secrets-422906fd-0f18-474c-8ee7-1a36336165c6 container secret-volume-test: <nil>
    STEP: delete the pod 05/16/23 15:28:47.702
    May 16 15:28:47.713: INFO: Waiting for pod pod-secrets-422906fd-0f18-474c-8ee7-1a36336165c6 to disappear
    May 16 15:28:47.715: INFO: Pod pod-secrets-422906fd-0f18-474c-8ee7-1a36336165c6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May 16 15:28:47.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3068" for this suite. 05/16/23 15:28:47.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:28:47.725
May 16 15:28:47.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename dns 05/16/23 15:28:47.725
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:28:47.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:28:47.741
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5751.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5751.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 05/16/23 15:28:47.742
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5751.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5751.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 05/16/23 15:28:47.743
STEP: creating a pod to probe /etc/hosts 05/16/23 15:28:47.743
STEP: submitting the pod to kubernetes 05/16/23 15:28:47.743
W0516 15:28:47.756918      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:28:47.756: INFO: Waiting up to 15m0s for pod "dns-test-24a96989-16c3-4997-8a58-c8652eb86371" in namespace "dns-5751" to be "running"
May 16 15:28:47.761: INFO: Pod "dns-test-24a96989-16c3-4997-8a58-c8652eb86371": Phase="Pending", Reason="", readiness=false. Elapsed: 4.80405ms
May 16 15:28:49.765: INFO: Pod "dns-test-24a96989-16c3-4997-8a58-c8652eb86371": Phase="Running", Reason="", readiness=true. Elapsed: 2.008137451s
May 16 15:28:49.765: INFO: Pod "dns-test-24a96989-16c3-4997-8a58-c8652eb86371" satisfied condition "running"
STEP: retrieving the pod 05/16/23 15:28:49.765
STEP: looking for the results for each expected name from probers 05/16/23 15:28:49.767
May 16 15:28:49.781: INFO: DNS probes using dns-5751/dns-test-24a96989-16c3-4997-8a58-c8652eb86371 succeeded

STEP: deleting the pod 05/16/23 15:28:49.781
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May 16 15:28:49.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5751" for this suite. 05/16/23 15:28:49.797
------------------------------
â€¢ [2.077 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:28:47.725
    May 16 15:28:47.725: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename dns 05/16/23 15:28:47.725
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:28:47.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:28:47.741
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5751.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5751.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     05/16/23 15:28:47.742
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5751.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5751.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     05/16/23 15:28:47.743
    STEP: creating a pod to probe /etc/hosts 05/16/23 15:28:47.743
    STEP: submitting the pod to kubernetes 05/16/23 15:28:47.743
    W0516 15:28:47.756918      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:28:47.756: INFO: Waiting up to 15m0s for pod "dns-test-24a96989-16c3-4997-8a58-c8652eb86371" in namespace "dns-5751" to be "running"
    May 16 15:28:47.761: INFO: Pod "dns-test-24a96989-16c3-4997-8a58-c8652eb86371": Phase="Pending", Reason="", readiness=false. Elapsed: 4.80405ms
    May 16 15:28:49.765: INFO: Pod "dns-test-24a96989-16c3-4997-8a58-c8652eb86371": Phase="Running", Reason="", readiness=true. Elapsed: 2.008137451s
    May 16 15:28:49.765: INFO: Pod "dns-test-24a96989-16c3-4997-8a58-c8652eb86371" satisfied condition "running"
    STEP: retrieving the pod 05/16/23 15:28:49.765
    STEP: looking for the results for each expected name from probers 05/16/23 15:28:49.767
    May 16 15:28:49.781: INFO: DNS probes using dns-5751/dns-test-24a96989-16c3-4997-8a58-c8652eb86371 succeeded

    STEP: deleting the pod 05/16/23 15:28:49.781
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May 16 15:28:49.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5751" for this suite. 05/16/23 15:28:49.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:28:49.803
May 16 15:28:49.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename container-lifecycle-hook 05/16/23 15:28:49.804
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:28:49.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:28:49.827
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 05/16/23 15:28:49.835
W0516 15:28:49.861668      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:28:49.861: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6716" to be "running and ready"
May 16 15:28:49.869: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.066607ms
May 16 15:28:49.869: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 16 15:28:51.873: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.011708155s
May 16 15:28:51.873: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
May 16 15:28:51.873: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 05/16/23 15:28:51.876
May 16 15:28:51.882: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6716" to be "running and ready"
May 16 15:28:51.885: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.687853ms
May 16 15:28:51.885: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 16 15:28:53.888: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006305954s
May 16 15:28:53.888: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
May 16 15:28:53.888: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 05/16/23 15:28:53.891
STEP: delete the pod with lifecycle hook 05/16/23 15:28:53.901
May 16 15:28:53.907: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 16 15:28:53.910: INFO: Pod pod-with-poststart-exec-hook still exists
May 16 15:28:55.910: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 16 15:28:55.914: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
May 16 15:28:55.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6716" for this suite. 05/16/23 15:28:55.918
------------------------------
â€¢ [SLOW TEST] [6.122 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:28:49.803
    May 16 15:28:49.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename container-lifecycle-hook 05/16/23 15:28:49.804
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:28:49.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:28:49.827
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 05/16/23 15:28:49.835
    W0516 15:28:49.861668      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:28:49.861: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6716" to be "running and ready"
    May 16 15:28:49.869: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.066607ms
    May 16 15:28:49.869: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:28:51.873: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.011708155s
    May 16 15:28:51.873: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    May 16 15:28:51.873: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 05/16/23 15:28:51.876
    May 16 15:28:51.882: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6716" to be "running and ready"
    May 16 15:28:51.885: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.687853ms
    May 16 15:28:51.885: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:28:53.888: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006305954s
    May 16 15:28:53.888: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    May 16 15:28:53.888: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 05/16/23 15:28:53.891
    STEP: delete the pod with lifecycle hook 05/16/23 15:28:53.901
    May 16 15:28:53.907: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May 16 15:28:53.910: INFO: Pod pod-with-poststart-exec-hook still exists
    May 16 15:28:55.910: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    May 16 15:28:55.914: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    May 16 15:28:55.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6716" for this suite. 05/16/23 15:28:55.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:28:55.926
May 16 15:28:55.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename emptydir 05/16/23 15:28:55.926
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:28:55.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:28:55.945
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 05/16/23 15:28:55.947
W0516 15:28:55.963262      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:28:55.963: INFO: Waiting up to 5m0s for pod "pod-5f76a345-3c55-4505-99b2-d6501459d7d2" in namespace "emptydir-9816" to be "Succeeded or Failed"
May 16 15:28:55.966: INFO: Pod "pod-5f76a345-3c55-4505-99b2-d6501459d7d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.795715ms
May 16 15:28:57.969: INFO: Pod "pod-5f76a345-3c55-4505-99b2-d6501459d7d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005963277s
May 16 15:28:59.970: INFO: Pod "pod-5f76a345-3c55-4505-99b2-d6501459d7d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006751791s
STEP: Saw pod success 05/16/23 15:28:59.97
May 16 15:28:59.970: INFO: Pod "pod-5f76a345-3c55-4505-99b2-d6501459d7d2" satisfied condition "Succeeded or Failed"
May 16 15:28:59.972: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-5f76a345-3c55-4505-99b2-d6501459d7d2 container test-container: <nil>
STEP: delete the pod 05/16/23 15:28:59.977
May 16 15:28:59.986: INFO: Waiting for pod pod-5f76a345-3c55-4505-99b2-d6501459d7d2 to disappear
May 16 15:28:59.988: INFO: Pod pod-5f76a345-3c55-4505-99b2-d6501459d7d2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 16 15:28:59.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9816" for this suite. 05/16/23 15:28:59.992
------------------------------
â€¢ [4.072 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:28:55.926
    May 16 15:28:55.926: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename emptydir 05/16/23 15:28:55.926
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:28:55.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:28:55.945
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 05/16/23 15:28:55.947
    W0516 15:28:55.963262      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:28:55.963: INFO: Waiting up to 5m0s for pod "pod-5f76a345-3c55-4505-99b2-d6501459d7d2" in namespace "emptydir-9816" to be "Succeeded or Failed"
    May 16 15:28:55.966: INFO: Pod "pod-5f76a345-3c55-4505-99b2-d6501459d7d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.795715ms
    May 16 15:28:57.969: INFO: Pod "pod-5f76a345-3c55-4505-99b2-d6501459d7d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005963277s
    May 16 15:28:59.970: INFO: Pod "pod-5f76a345-3c55-4505-99b2-d6501459d7d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006751791s
    STEP: Saw pod success 05/16/23 15:28:59.97
    May 16 15:28:59.970: INFO: Pod "pod-5f76a345-3c55-4505-99b2-d6501459d7d2" satisfied condition "Succeeded or Failed"
    May 16 15:28:59.972: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-5f76a345-3c55-4505-99b2-d6501459d7d2 container test-container: <nil>
    STEP: delete the pod 05/16/23 15:28:59.977
    May 16 15:28:59.986: INFO: Waiting for pod pod-5f76a345-3c55-4505-99b2-d6501459d7d2 to disappear
    May 16 15:28:59.988: INFO: Pod pod-5f76a345-3c55-4505-99b2-d6501459d7d2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 16 15:28:59.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9816" for this suite. 05/16/23 15:28:59.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:28:59.998
May 16 15:28:59.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename resourcequota 05/16/23 15:28:59.998
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:00.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:00.014
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 05/16/23 15:29:00.016
STEP: Creating a ResourceQuota 05/16/23 15:29:05.019
STEP: Ensuring resource quota status is calculated 05/16/23 15:29:05.023
STEP: Creating a ReplicationController 05/16/23 15:29:07.028
STEP: Ensuring resource quota status captures replication controller creation 05/16/23 15:29:07.05
STEP: Deleting a ReplicationController 05/16/23 15:29:09.053
STEP: Ensuring resource quota status released usage 05/16/23 15:29:09.059
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 16 15:29:11.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8848" for this suite. 05/16/23 15:29:11.066
------------------------------
â€¢ [SLOW TEST] [11.073 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:28:59.998
    May 16 15:28:59.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename resourcequota 05/16/23 15:28:59.998
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:00.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:00.014
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 05/16/23 15:29:00.016
    STEP: Creating a ResourceQuota 05/16/23 15:29:05.019
    STEP: Ensuring resource quota status is calculated 05/16/23 15:29:05.023
    STEP: Creating a ReplicationController 05/16/23 15:29:07.028
    STEP: Ensuring resource quota status captures replication controller creation 05/16/23 15:29:07.05
    STEP: Deleting a ReplicationController 05/16/23 15:29:09.053
    STEP: Ensuring resource quota status released usage 05/16/23 15:29:09.059
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 16 15:29:11.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8848" for this suite. 05/16/23 15:29:11.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:29:11.072
May 16 15:29:11.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 15:29:11.072
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:11.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:11.089
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 05/16/23 15:29:11.091
May 16 15:29:11.119: INFO: Waiting up to 5m0s for pod "labelsupdate719fcd47-983d-4a57-976c-18635ed98dbb" in namespace "projected-9053" to be "running and ready"
May 16 15:29:11.122: INFO: Pod "labelsupdate719fcd47-983d-4a57-976c-18635ed98dbb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.041924ms
May 16 15:29:11.122: INFO: The phase of Pod labelsupdate719fcd47-983d-4a57-976c-18635ed98dbb is Pending, waiting for it to be Running (with Ready = true)
May 16 15:29:13.125: INFO: Pod "labelsupdate719fcd47-983d-4a57-976c-18635ed98dbb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005945237s
May 16 15:29:13.125: INFO: The phase of Pod labelsupdate719fcd47-983d-4a57-976c-18635ed98dbb is Running (Ready = true)
May 16 15:29:13.125: INFO: Pod "labelsupdate719fcd47-983d-4a57-976c-18635ed98dbb" satisfied condition "running and ready"
May 16 15:29:13.644: INFO: Successfully updated pod "labelsupdate719fcd47-983d-4a57-976c-18635ed98dbb"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 16 15:29:17.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9053" for this suite. 05/16/23 15:29:17.668
------------------------------
â€¢ [SLOW TEST] [6.602 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:29:11.072
    May 16 15:29:11.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 15:29:11.072
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:11.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:11.089
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 05/16/23 15:29:11.091
    May 16 15:29:11.119: INFO: Waiting up to 5m0s for pod "labelsupdate719fcd47-983d-4a57-976c-18635ed98dbb" in namespace "projected-9053" to be "running and ready"
    May 16 15:29:11.122: INFO: Pod "labelsupdate719fcd47-983d-4a57-976c-18635ed98dbb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.041924ms
    May 16 15:29:11.122: INFO: The phase of Pod labelsupdate719fcd47-983d-4a57-976c-18635ed98dbb is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:29:13.125: INFO: Pod "labelsupdate719fcd47-983d-4a57-976c-18635ed98dbb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005945237s
    May 16 15:29:13.125: INFO: The phase of Pod labelsupdate719fcd47-983d-4a57-976c-18635ed98dbb is Running (Ready = true)
    May 16 15:29:13.125: INFO: Pod "labelsupdate719fcd47-983d-4a57-976c-18635ed98dbb" satisfied condition "running and ready"
    May 16 15:29:13.644: INFO: Successfully updated pod "labelsupdate719fcd47-983d-4a57-976c-18635ed98dbb"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 16 15:29:17.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9053" for this suite. 05/16/23 15:29:17.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:29:17.674
May 16 15:29:17.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubectl 05/16/23 15:29:17.675
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:17.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:17.692
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 05/16/23 15:29:17.693
May 16 15:29:17.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-1794 cluster-info'
May 16 15:29:17.744: INFO: stderr: ""
May 16 15:29:17.744: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.30.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 16 15:29:17.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1794" for this suite. 05/16/23 15:29:17.748
------------------------------
â€¢ [0.079 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:29:17.674
    May 16 15:29:17.674: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubectl 05/16/23 15:29:17.675
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:17.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:17.692
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 05/16/23 15:29:17.693
    May 16 15:29:17.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-1794 cluster-info'
    May 16 15:29:17.744: INFO: stderr: ""
    May 16 15:29:17.744: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.30.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 16 15:29:17.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1794" for this suite. 05/16/23 15:29:17.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:29:17.754
May 16 15:29:17.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename downward-api 05/16/23 15:29:17.755
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:17.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:17.77
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 05/16/23 15:29:17.771
W0516 15:29:17.786538      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:29:17.786: INFO: Waiting up to 5m0s for pod "downwardapi-volume-73552921-af96-4c30-ae63-bcecd54b09f8" in namespace "downward-api-268" to be "Succeeded or Failed"
May 16 15:29:17.789: INFO: Pod "downwardapi-volume-73552921-af96-4c30-ae63-bcecd54b09f8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.024845ms
May 16 15:29:19.792: INFO: Pod "downwardapi-volume-73552921-af96-4c30-ae63-bcecd54b09f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005998778s
May 16 15:29:21.793: INFO: Pod "downwardapi-volume-73552921-af96-4c30-ae63-bcecd54b09f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006804212s
STEP: Saw pod success 05/16/23 15:29:21.793
May 16 15:29:21.793: INFO: Pod "downwardapi-volume-73552921-af96-4c30-ae63-bcecd54b09f8" satisfied condition "Succeeded or Failed"
May 16 15:29:21.796: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-73552921-af96-4c30-ae63-bcecd54b09f8 container client-container: <nil>
STEP: delete the pod 05/16/23 15:29:21.801
May 16 15:29:21.813: INFO: Waiting for pod downwardapi-volume-73552921-af96-4c30-ae63-bcecd54b09f8 to disappear
May 16 15:29:21.815: INFO: Pod downwardapi-volume-73552921-af96-4c30-ae63-bcecd54b09f8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 16 15:29:21.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-268" for this suite. 05/16/23 15:29:21.819
------------------------------
â€¢ [4.070 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:29:17.754
    May 16 15:29:17.754: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename downward-api 05/16/23 15:29:17.755
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:17.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:17.77
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 05/16/23 15:29:17.771
    W0516 15:29:17.786538      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:29:17.786: INFO: Waiting up to 5m0s for pod "downwardapi-volume-73552921-af96-4c30-ae63-bcecd54b09f8" in namespace "downward-api-268" to be "Succeeded or Failed"
    May 16 15:29:17.789: INFO: Pod "downwardapi-volume-73552921-af96-4c30-ae63-bcecd54b09f8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.024845ms
    May 16 15:29:19.792: INFO: Pod "downwardapi-volume-73552921-af96-4c30-ae63-bcecd54b09f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005998778s
    May 16 15:29:21.793: INFO: Pod "downwardapi-volume-73552921-af96-4c30-ae63-bcecd54b09f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006804212s
    STEP: Saw pod success 05/16/23 15:29:21.793
    May 16 15:29:21.793: INFO: Pod "downwardapi-volume-73552921-af96-4c30-ae63-bcecd54b09f8" satisfied condition "Succeeded or Failed"
    May 16 15:29:21.796: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-73552921-af96-4c30-ae63-bcecd54b09f8 container client-container: <nil>
    STEP: delete the pod 05/16/23 15:29:21.801
    May 16 15:29:21.813: INFO: Waiting for pod downwardapi-volume-73552921-af96-4c30-ae63-bcecd54b09f8 to disappear
    May 16 15:29:21.815: INFO: Pod downwardapi-volume-73552921-af96-4c30-ae63-bcecd54b09f8 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 16 15:29:21.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-268" for this suite. 05/16/23 15:29:21.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:29:21.825
May 16 15:29:21.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename runtimeclass 05/16/23 15:29:21.826
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:21.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:21.841
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-9798-delete-me 05/16/23 15:29:21.85
STEP: Waiting for the RuntimeClass to disappear 05/16/23 15:29:21.859
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
May 16 15:29:21.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9798" for this suite. 05/16/23 15:29:21.87
------------------------------
â€¢ [0.052 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:29:21.825
    May 16 15:29:21.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename runtimeclass 05/16/23 15:29:21.826
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:21.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:21.841
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-9798-delete-me 05/16/23 15:29:21.85
    STEP: Waiting for the RuntimeClass to disappear 05/16/23 15:29:21.859
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    May 16 15:29:21.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9798" for this suite. 05/16/23 15:29:21.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:29:21.877
May 16 15:29:21.877: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename webhook 05/16/23 15:29:21.878
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:21.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:21.893
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/16/23 15:29:21.959
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 15:29:22.291
STEP: Deploying the webhook pod 05/16/23 15:29:22.299
STEP: Wait for the deployment to be ready 05/16/23 15:29:22.309
May 16 15:29:22.318: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 16 15:29:24.327: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 15, 29, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 29, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 29, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 29, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 05/16/23 15:29:26.331
STEP: Verifying the service has paired with the endpoint 05/16/23 15:29:26.34
May 16 15:29:27.341: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 05/16/23 15:29:27.343
STEP: create a pod 05/16/23 15:29:27.355
May 16 15:29:27.362: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2276" to be "running"
May 16 15:29:27.365: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.603215ms
May 16 15:29:29.368: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005849029s
May 16 15:29:29.368: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 05/16/23 15:29:29.368
May 16 15:29:29.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=webhook-2276 attach --namespace=webhook-2276 to-be-attached-pod -i -c=container1'
May 16 15:29:29.426: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 15:29:29.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2276" for this suite. 05/16/23 15:29:29.478
STEP: Destroying namespace "webhook-2276-markers" for this suite. 05/16/23 15:29:29.49
------------------------------
â€¢ [SLOW TEST] [7.619 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:29:21.877
    May 16 15:29:21.877: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename webhook 05/16/23 15:29:21.878
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:21.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:21.893
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/16/23 15:29:21.959
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 15:29:22.291
    STEP: Deploying the webhook pod 05/16/23 15:29:22.299
    STEP: Wait for the deployment to be ready 05/16/23 15:29:22.309
    May 16 15:29:22.318: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    May 16 15:29:24.327: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 16, 15, 29, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 29, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 29, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 29, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 05/16/23 15:29:26.331
    STEP: Verifying the service has paired with the endpoint 05/16/23 15:29:26.34
    May 16 15:29:27.341: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 05/16/23 15:29:27.343
    STEP: create a pod 05/16/23 15:29:27.355
    May 16 15:29:27.362: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-2276" to be "running"
    May 16 15:29:27.365: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.603215ms
    May 16 15:29:29.368: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005849029s
    May 16 15:29:29.368: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 05/16/23 15:29:29.368
    May 16 15:29:29.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=webhook-2276 attach --namespace=webhook-2276 to-be-attached-pod -i -c=container1'
    May 16 15:29:29.426: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 15:29:29.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2276" for this suite. 05/16/23 15:29:29.478
    STEP: Destroying namespace "webhook-2276-markers" for this suite. 05/16/23 15:29:29.49
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:29:29.496
May 16 15:29:29.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename webhook 05/16/23 15:29:29.497
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:29.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:29.521
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 05/16/23 15:29:29.558
STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 15:29:29.831
STEP: Deploying the webhook pod 05/16/23 15:29:29.838
STEP: Wait for the deployment to be ready 05/16/23 15:29:29.847
May 16 15:29:29.853: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 05/16/23 15:29:31.861
STEP: Verifying the service has paired with the endpoint 05/16/23 15:29:31.871
May 16 15:29:32.872: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 05/16/23 15:29:32.875
STEP: create a pod that should be updated by the webhook 05/16/23 15:29:32.889
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 15:29:32.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1498" for this suite. 05/16/23 15:29:32.971
STEP: Destroying namespace "webhook-1498-markers" for this suite. 05/16/23 15:29:32.979
------------------------------
â€¢ [3.499 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:29:29.496
    May 16 15:29:29.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename webhook 05/16/23 15:29:29.497
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:29.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:29.521
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 05/16/23 15:29:29.558
    STEP: Create role binding to let webhook read extension-apiserver-authentication 05/16/23 15:29:29.831
    STEP: Deploying the webhook pod 05/16/23 15:29:29.838
    STEP: Wait for the deployment to be ready 05/16/23 15:29:29.847
    May 16 15:29:29.853: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 05/16/23 15:29:31.861
    STEP: Verifying the service has paired with the endpoint 05/16/23 15:29:31.871
    May 16 15:29:32.872: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 05/16/23 15:29:32.875
    STEP: create a pod that should be updated by the webhook 05/16/23 15:29:32.889
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 15:29:32.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1498" for this suite. 05/16/23 15:29:32.971
    STEP: Destroying namespace "webhook-1498-markers" for this suite. 05/16/23 15:29:32.979
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:29:32.995
May 16 15:29:32.995: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename services 05/16/23 15:29:32.996
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:33.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:33.022
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-8534 05/16/23 15:29:33.024
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8534 to expose endpoints map[] 05/16/23 15:29:33.062
May 16 15:29:33.086: INFO: successfully validated that service endpoint-test2 in namespace services-8534 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8534 05/16/23 15:29:33.086
May 16 15:29:33.108: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8534" to be "running and ready"
May 16 15:29:33.114: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.523398ms
May 16 15:29:33.114: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 16 15:29:35.119: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011350494s
May 16 15:29:35.119: INFO: The phase of Pod pod1 is Running (Ready = true)
May 16 15:29:35.119: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8534 to expose endpoints map[pod1:[80]] 05/16/23 15:29:35.123
May 16 15:29:35.132: INFO: successfully validated that service endpoint-test2 in namespace services-8534 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 05/16/23 15:29:35.132
May 16 15:29:35.132: INFO: Creating new exec pod
May 16 15:29:35.137: INFO: Waiting up to 5m0s for pod "execpod99jrf" in namespace "services-8534" to be "running"
May 16 15:29:35.140: INFO: Pod "execpod99jrf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.681965ms
May 16 15:29:37.144: INFO: Pod "execpod99jrf": Phase="Running", Reason="", readiness=true. Elapsed: 2.006596918s
May 16 15:29:37.144: INFO: Pod "execpod99jrf" satisfied condition "running"
May 16 15:29:38.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-8534 exec execpod99jrf -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
May 16 15:29:38.259: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May 16 15:29:38.259: INFO: stdout: ""
May 16 15:29:38.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-8534 exec execpod99jrf -- /bin/sh -x -c nc -v -z -w 2 172.30.217.203 80'
May 16 15:29:38.367: INFO: stderr: "+ nc -v -z -w 2 172.30.217.203 80\nConnection to 172.30.217.203 80 port [tcp/http] succeeded!\n"
May 16 15:29:38.367: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-8534 05/16/23 15:29:38.367
May 16 15:29:38.374: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8534" to be "running and ready"
May 16 15:29:38.377: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.494974ms
May 16 15:29:38.377: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 16 15:29:40.380: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005358149s
May 16 15:29:40.380: INFO: The phase of Pod pod2 is Running (Ready = true)
May 16 15:29:40.380: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8534 to expose endpoints map[pod1:[80] pod2:[80]] 05/16/23 15:29:40.382
May 16 15:29:40.392: INFO: successfully validated that service endpoint-test2 in namespace services-8534 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 05/16/23 15:29:40.392
May 16 15:29:41.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-8534 exec execpod99jrf -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
May 16 15:29:41.499: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May 16 15:29:41.499: INFO: stdout: ""
May 16 15:29:41.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-8534 exec execpod99jrf -- /bin/sh -x -c nc -v -z -w 2 172.30.217.203 80'
May 16 15:29:41.604: INFO: stderr: "+ nc -v -z -w 2 172.30.217.203 80\nConnection to 172.30.217.203 80 port [tcp/http] succeeded!\n"
May 16 15:29:41.604: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-8534 05/16/23 15:29:41.604
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8534 to expose endpoints map[pod2:[80]] 05/16/23 15:29:41.613
May 16 15:29:41.627: INFO: successfully validated that service endpoint-test2 in namespace services-8534 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 05/16/23 15:29:41.627
May 16 15:29:42.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-8534 exec execpod99jrf -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
May 16 15:29:42.760: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May 16 15:29:42.760: INFO: stdout: ""
May 16 15:29:42.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-8534 exec execpod99jrf -- /bin/sh -x -c nc -v -z -w 2 172.30.217.203 80'
May 16 15:29:42.881: INFO: stderr: "+ nc -v -z -w 2 172.30.217.203 80\nConnection to 172.30.217.203 80 port [tcp/http] succeeded!\n"
May 16 15:29:42.881: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-8534 05/16/23 15:29:42.881
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8534 to expose endpoints map[] 05/16/23 15:29:42.895
May 16 15:29:42.903: INFO: successfully validated that service endpoint-test2 in namespace services-8534 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 16 15:29:42.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8534" for this suite. 05/16/23 15:29:42.955
------------------------------
â€¢ [SLOW TEST] [9.973 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:29:32.995
    May 16 15:29:32.995: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename services 05/16/23 15:29:32.996
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:33.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:33.022
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-8534 05/16/23 15:29:33.024
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8534 to expose endpoints map[] 05/16/23 15:29:33.062
    May 16 15:29:33.086: INFO: successfully validated that service endpoint-test2 in namespace services-8534 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-8534 05/16/23 15:29:33.086
    May 16 15:29:33.108: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8534" to be "running and ready"
    May 16 15:29:33.114: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.523398ms
    May 16 15:29:33.114: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:29:35.119: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011350494s
    May 16 15:29:35.119: INFO: The phase of Pod pod1 is Running (Ready = true)
    May 16 15:29:35.119: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8534 to expose endpoints map[pod1:[80]] 05/16/23 15:29:35.123
    May 16 15:29:35.132: INFO: successfully validated that service endpoint-test2 in namespace services-8534 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 05/16/23 15:29:35.132
    May 16 15:29:35.132: INFO: Creating new exec pod
    May 16 15:29:35.137: INFO: Waiting up to 5m0s for pod "execpod99jrf" in namespace "services-8534" to be "running"
    May 16 15:29:35.140: INFO: Pod "execpod99jrf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.681965ms
    May 16 15:29:37.144: INFO: Pod "execpod99jrf": Phase="Running", Reason="", readiness=true. Elapsed: 2.006596918s
    May 16 15:29:37.144: INFO: Pod "execpod99jrf" satisfied condition "running"
    May 16 15:29:38.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-8534 exec execpod99jrf -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    May 16 15:29:38.259: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May 16 15:29:38.259: INFO: stdout: ""
    May 16 15:29:38.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-8534 exec execpod99jrf -- /bin/sh -x -c nc -v -z -w 2 172.30.217.203 80'
    May 16 15:29:38.367: INFO: stderr: "+ nc -v -z -w 2 172.30.217.203 80\nConnection to 172.30.217.203 80 port [tcp/http] succeeded!\n"
    May 16 15:29:38.367: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-8534 05/16/23 15:29:38.367
    May 16 15:29:38.374: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8534" to be "running and ready"
    May 16 15:29:38.377: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.494974ms
    May 16 15:29:38.377: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:29:40.380: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005358149s
    May 16 15:29:40.380: INFO: The phase of Pod pod2 is Running (Ready = true)
    May 16 15:29:40.380: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8534 to expose endpoints map[pod1:[80] pod2:[80]] 05/16/23 15:29:40.382
    May 16 15:29:40.392: INFO: successfully validated that service endpoint-test2 in namespace services-8534 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 05/16/23 15:29:40.392
    May 16 15:29:41.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-8534 exec execpod99jrf -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    May 16 15:29:41.499: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May 16 15:29:41.499: INFO: stdout: ""
    May 16 15:29:41.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-8534 exec execpod99jrf -- /bin/sh -x -c nc -v -z -w 2 172.30.217.203 80'
    May 16 15:29:41.604: INFO: stderr: "+ nc -v -z -w 2 172.30.217.203 80\nConnection to 172.30.217.203 80 port [tcp/http] succeeded!\n"
    May 16 15:29:41.604: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-8534 05/16/23 15:29:41.604
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8534 to expose endpoints map[pod2:[80]] 05/16/23 15:29:41.613
    May 16 15:29:41.627: INFO: successfully validated that service endpoint-test2 in namespace services-8534 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 05/16/23 15:29:41.627
    May 16 15:29:42.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-8534 exec execpod99jrf -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    May 16 15:29:42.760: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    May 16 15:29:42.760: INFO: stdout: ""
    May 16 15:29:42.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-8534 exec execpod99jrf -- /bin/sh -x -c nc -v -z -w 2 172.30.217.203 80'
    May 16 15:29:42.881: INFO: stderr: "+ nc -v -z -w 2 172.30.217.203 80\nConnection to 172.30.217.203 80 port [tcp/http] succeeded!\n"
    May 16 15:29:42.881: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-8534 05/16/23 15:29:42.881
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8534 to expose endpoints map[] 05/16/23 15:29:42.895
    May 16 15:29:42.903: INFO: successfully validated that service endpoint-test2 in namespace services-8534 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 16 15:29:42.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8534" for this suite. 05/16/23 15:29:42.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:29:42.968
May 16 15:29:42.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 15:29:42.969
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:42.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:42.993
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-fc497e22-8211-4e74-b647-38e03b6327be 05/16/23 15:29:42.995
STEP: Creating a pod to test consume configMaps 05/16/23 15:29:43
W0516 15:29:43.011262      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:29:43.011: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5420529d-01d5-4c07-acc5-6ac4a221e1b8" in namespace "projected-7893" to be "Succeeded or Failed"
May 16 15:29:43.016: INFO: Pod "pod-projected-configmaps-5420529d-01d5-4c07-acc5-6ac4a221e1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.798247ms
May 16 15:29:45.019: INFO: Pod "pod-projected-configmaps-5420529d-01d5-4c07-acc5-6ac4a221e1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008233222s
May 16 15:29:47.019: INFO: Pod "pod-projected-configmaps-5420529d-01d5-4c07-acc5-6ac4a221e1b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008572988s
STEP: Saw pod success 05/16/23 15:29:47.019
May 16 15:29:47.020: INFO: Pod "pod-projected-configmaps-5420529d-01d5-4c07-acc5-6ac4a221e1b8" satisfied condition "Succeeded or Failed"
May 16 15:29:47.022: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-configmaps-5420529d-01d5-4c07-acc5-6ac4a221e1b8 container agnhost-container: <nil>
STEP: delete the pod 05/16/23 15:29:47.027
May 16 15:29:47.040: INFO: Waiting for pod pod-projected-configmaps-5420529d-01d5-4c07-acc5-6ac4a221e1b8 to disappear
May 16 15:29:47.042: INFO: Pod pod-projected-configmaps-5420529d-01d5-4c07-acc5-6ac4a221e1b8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May 16 15:29:47.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7893" for this suite. 05/16/23 15:29:47.046
------------------------------
â€¢ [4.083 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:29:42.968
    May 16 15:29:42.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 15:29:42.969
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:42.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:42.993
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-fc497e22-8211-4e74-b647-38e03b6327be 05/16/23 15:29:42.995
    STEP: Creating a pod to test consume configMaps 05/16/23 15:29:43
    W0516 15:29:43.011262      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:29:43.011: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5420529d-01d5-4c07-acc5-6ac4a221e1b8" in namespace "projected-7893" to be "Succeeded or Failed"
    May 16 15:29:43.016: INFO: Pod "pod-projected-configmaps-5420529d-01d5-4c07-acc5-6ac4a221e1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.798247ms
    May 16 15:29:45.019: INFO: Pod "pod-projected-configmaps-5420529d-01d5-4c07-acc5-6ac4a221e1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008233222s
    May 16 15:29:47.019: INFO: Pod "pod-projected-configmaps-5420529d-01d5-4c07-acc5-6ac4a221e1b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008572988s
    STEP: Saw pod success 05/16/23 15:29:47.019
    May 16 15:29:47.020: INFO: Pod "pod-projected-configmaps-5420529d-01d5-4c07-acc5-6ac4a221e1b8" satisfied condition "Succeeded or Failed"
    May 16 15:29:47.022: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-configmaps-5420529d-01d5-4c07-acc5-6ac4a221e1b8 container agnhost-container: <nil>
    STEP: delete the pod 05/16/23 15:29:47.027
    May 16 15:29:47.040: INFO: Waiting for pod pod-projected-configmaps-5420529d-01d5-4c07-acc5-6ac4a221e1b8 to disappear
    May 16 15:29:47.042: INFO: Pod pod-projected-configmaps-5420529d-01d5-4c07-acc5-6ac4a221e1b8 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May 16 15:29:47.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7893" for this suite. 05/16/23 15:29:47.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:29:47.052
May 16 15:29:47.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename sched-pred 05/16/23 15:29:47.052
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:47.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:47.071
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
May 16 15:29:47.072: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 16 15:29:47.085: INFO: Waiting for terminating namespaces to be deleted...
May 16 15:29:47.093: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-132-142.eu-central-1.compute.internal before test
May 16 15:29:47.125: INFO: aws-ebs-csi-driver-node-27d5z from openshift-cluster-csi-drivers started at 2023-05-16 13:52:17 +0000 UTC (3 container statuses recorded)
May 16 15:29:47.125: INFO: 	Container csi-driver ready: true, restart count 0
May 16 15:29:47.125: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 16 15:29:47.125: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 16 15:29:47.125: INFO: tuned-j9bjl from openshift-cluster-node-tuning-operator started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.125: INFO: 	Container tuned ready: true, restart count 0
May 16 15:29:47.125: INFO: dns-default-9lcb7 from openshift-dns started at 2023-05-16 13:53:17 +0000 UTC (2 container statuses recorded)
May 16 15:29:47.125: INFO: 	Container dns ready: true, restart count 0
May 16 15:29:47.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.125: INFO: node-resolver-b47gh from openshift-dns started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.125: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 16 15:29:47.125: INFO: image-registry-b549d4f89-4ck4v from openshift-image-registry started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.125: INFO: 	Container registry ready: true, restart count 0
May 16 15:29:47.125: INFO: node-ca-px455 from openshift-image-registry started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.125: INFO: 	Container node-ca ready: true, restart count 0
May 16 15:29:47.125: INFO: ingress-canary-4hv85 from openshift-ingress-canary started at 2023-05-16 13:53:17 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.125: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May 16 15:29:47.125: INFO: router-default-8f95c5-srcbl from openshift-ingress started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.125: INFO: 	Container router ready: true, restart count 0
May 16 15:29:47.125: INFO: machine-config-daemon-l6bpj from openshift-machine-config-operator started at 2023-05-16 13:52:17 +0000 UTC (2 container statuses recorded)
May 16 15:29:47.125: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 16 15:29:47.125: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 15:29:47.125: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (6 container statuses recorded)
May 16 15:29:47.125: INFO: 	Container alertmanager ready: true, restart count 1
May 16 15:29:47.125: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 16 15:29:47.125: INFO: 	Container config-reloader ready: true, restart count 0
May 16 15:29:47.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.125: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
May 16 15:29:47.125: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 16 15:29:47.125: INFO: node-exporter-pltn9 from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
May 16 15:29:47.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.125: INFO: 	Container node-exporter ready: true, restart count 0
May 16 15:29:47.125: INFO: prometheus-adapter-7767c6bd47-bfq4d from openshift-monitoring started at 2023-05-16 13:53:50 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.125: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 16 15:29:47.125: INFO: prometheus-operator-admission-webhook-5d679565bb-8lkws from openshift-monitoring started at 2023-05-16 14:48:21 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.125: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
May 16 15:29:47.125: INFO: telemeter-client-859dbffbb4-mt92v from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (3 container statuses recorded)
May 16 15:29:47.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.125: INFO: 	Container reload ready: true, restart count 0
May 16 15:29:47.125: INFO: 	Container telemeter-client ready: true, restart count 0
May 16 15:29:47.125: INFO: thanos-querier-5c9d876b5c-xdt6m from openshift-monitoring started at 2023-05-16 13:53:54 +0000 UTC (6 container statuses recorded)
May 16 15:29:47.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.125: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
May 16 15:29:47.125: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May 16 15:29:47.125: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 15:29:47.125: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 16 15:29:47.125: INFO: 	Container thanos-query ready: true, restart count 0
May 16 15:29:47.125: INFO: multus-2wjqr from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.125: INFO: 	Container kube-multus ready: true, restart count 0
May 16 15:29:47.125: INFO: multus-additional-cni-plugins-z2h4l from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.126: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May 16 15:29:47.126: INFO: network-metrics-daemon-rqdtf from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (2 container statuses recorded)
May 16 15:29:47.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.126: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 16 15:29:47.126: INFO: network-check-target-28wts from openshift-network-diagnostics started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.126: INFO: 	Container network-check-target-container ready: true, restart count 0
May 16 15:29:47.126: INFO: ovnkube-node-npwgq from openshift-ovn-kubernetes started at 2023-05-16 13:52:17 +0000 UTC (5 container statuses recorded)
May 16 15:29:47.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.126: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
May 16 15:29:47.126: INFO: 	Container ovn-acl-logging ready: true, restart count 0
May 16 15:29:47.126: INFO: 	Container ovn-controller ready: true, restart count 1
May 16 15:29:47.126: INFO: 	Container ovnkube-node ready: true, restart count 0
May 16 15:29:47.126: INFO: sonobuoy from sonobuoy started at 2023-05-16 14:11:07 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.126: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 16 15:29:47.126: INFO: sonobuoy-e2e-job-840dd79630d44353 from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
May 16 15:29:47.126: INFO: 	Container e2e ready: true, restart count 0
May 16 15:29:47.126: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 15:29:47.126: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-7zsf6 from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
May 16 15:29:47.126: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 15:29:47.126: INFO: 	Container systemd-logs ready: true, restart count 0
May 16 15:29:47.126: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-161-164.eu-central-1.compute.internal before test
May 16 15:29:47.147: INFO: aws-ebs-csi-driver-node-9m6vn from openshift-cluster-csi-drivers started at 2023-05-16 13:44:25 +0000 UTC (3 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container csi-driver ready: true, restart count 0
May 16 15:29:47.147: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 16 15:29:47.147: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 16 15:29:47.147: INFO: tuned-6n5b7 from openshift-cluster-node-tuning-operator started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container tuned ready: true, restart count 0
May 16 15:29:47.147: INFO: dns-default-pwtdq from openshift-dns started at 2023-05-16 14:48:42 +0000 UTC (2 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container dns ready: true, restart count 0
May 16 15:29:47.147: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.147: INFO: node-resolver-jbmqq from openshift-dns started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 16 15:29:47.147: INFO: node-ca-gw7rz from openshift-image-registry started at 2023-05-16 13:50:21 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container node-ca ready: true, restart count 0
May 16 15:29:47.147: INFO: ingress-canary-mvqst from openshift-ingress-canary started at 2023-05-16 14:48:23 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May 16 15:29:47.147: INFO: machine-config-daemon-qhdsv from openshift-machine-config-operator started at 2023-05-16 13:44:25 +0000 UTC (2 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 16 15:29:47.147: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 15:29:47.147: INFO: node-exporter-l2zvs from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.147: INFO: 	Container node-exporter ready: true, restart count 0
May 16 15:29:47.147: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-05-16 14:48:24 +0000 UTC (6 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container config-reloader ready: true, restart count 0
May 16 15:29:47.147: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.147: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May 16 15:29:47.147: INFO: 	Container prometheus ready: true, restart count 0
May 16 15:29:47.147: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 16 15:29:47.147: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 16 15:29:47.147: INFO: multus-additional-cni-plugins-jkn7l from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May 16 15:29:47.147: INFO: multus-mc6qb from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container kube-multus ready: true, restart count 0
May 16 15:29:47.147: INFO: network-metrics-daemon-n4p75 from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (2 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.147: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 16 15:29:47.147: INFO: network-check-source-7f6b75fdb6-lxqsg from openshift-network-diagnostics started at 2023-05-16 14:48:21 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container check-endpoints ready: true, restart count 0
May 16 15:29:47.147: INFO: network-check-target-bprb5 from openshift-network-diagnostics started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container network-check-target-container ready: true, restart count 0
May 16 15:29:47.147: INFO: collect-profiles-28070820-8vnf5 from openshift-operator-lifecycle-manager started at 2023-05-16 15:00:00 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container collect-profiles ready: false, restart count 0
May 16 15:29:47.147: INFO: collect-profiles-28070835-9g48l from openshift-operator-lifecycle-manager started at 2023-05-16 15:15:00 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container collect-profiles ready: false, restart count 0
May 16 15:29:47.147: INFO: ovnkube-node-w8qn8 from openshift-ovn-kubernetes started at 2023-05-16 13:44:25 +0000 UTC (5 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.147: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
May 16 15:29:47.147: INFO: 	Container ovn-acl-logging ready: true, restart count 0
May 16 15:29:47.147: INFO: 	Container ovn-controller ready: true, restart count 1
May 16 15:29:47.147: INFO: 	Container ovnkube-node ready: true, restart count 0
May 16 15:29:47.147: INFO: execpod99jrf from services-8534 started at 2023-05-16 15:29:35 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container agnhost-container ready: true, restart count 0
May 16 15:29:47.147: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-5r4tm from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 15:29:47.147: INFO: 	Container systemd-logs ready: true, restart count 0
May 16 15:29:47.147: INFO: webhook-to-be-mutated from webhook-1498 started at 2023-05-16 15:29:32 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.147: INFO: 	Container example ready: false, restart count 0
May 16 15:29:47.147: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-212-246.eu-central-1.compute.internal before test
May 16 15:29:47.169: INFO: aws-ebs-csi-driver-node-b4s28 from openshift-cluster-csi-drivers started at 2023-05-16 13:52:19 +0000 UTC (3 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container csi-driver ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container csi-liveness-probe ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
May 16 15:29:47.169: INFO: tuned-8x9rx from openshift-cluster-node-tuning-operator started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container tuned ready: true, restart count 0
May 16 15:29:47.169: INFO: dns-default-dbkcs from openshift-dns started at 2023-05-16 13:53:17 +0000 UTC (2 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container dns ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.169: INFO: node-resolver-gdzz2 from openshift-dns started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container dns-node-resolver ready: true, restart count 0
May 16 15:29:47.169: INFO: image-registry-b549d4f89-hs4qn from openshift-image-registry started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container registry ready: true, restart count 0
May 16 15:29:47.169: INFO: node-ca-nsxtf from openshift-image-registry started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container node-ca ready: true, restart count 0
May 16 15:29:47.169: INFO: ingress-canary-wphz5 from openshift-ingress-canary started at 2023-05-16 13:53:17 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
May 16 15:29:47.169: INFO: router-default-8f95c5-bbmm5 from openshift-ingress started at 2023-05-16 14:48:21 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container router ready: true, restart count 0
May 16 15:29:47.169: INFO: machine-config-daemon-z7sw6 from openshift-machine-config-operator started at 2023-05-16 13:52:19 +0000 UTC (2 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container machine-config-daemon ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 15:29:47.169: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (6 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container alertmanager ready: true, restart count 1
May 16 15:29:47.169: INFO: 	Container alertmanager-proxy ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container config-reloader ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 16 15:29:47.169: INFO: kube-state-metrics-6ccfb58dc4-wgwts from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (3 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container kube-state-metrics ready: true, restart count 0
May 16 15:29:47.169: INFO: node-exporter-jrknj from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container node-exporter ready: true, restart count 0
May 16 15:29:47.169: INFO: openshift-state-metrics-7d7f8b4cf8-9lbg2 from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (3 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container openshift-state-metrics ready: true, restart count 0
May 16 15:29:47.169: INFO: prometheus-adapter-7767c6bd47-bqlnh from openshift-monitoring started at 2023-05-16 13:53:50 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container prometheus-adapter ready: true, restart count 0
May 16 15:29:47.169: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-05-16 13:53:57 +0000 UTC (6 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container config-reloader ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container prometheus ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container prometheus-proxy ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container thanos-sidecar ready: true, restart count 0
May 16 15:29:47.169: INFO: prometheus-operator-admission-webhook-5d679565bb-96qsx from openshift-monitoring started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
May 16 15:29:47.169: INFO: thanos-querier-5c9d876b5c-wwd7q from openshift-monitoring started at 2023-05-16 13:53:54 +0000 UTC (6 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container oauth-proxy ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container prom-label-proxy ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container thanos-query ready: true, restart count 0
May 16 15:29:47.169: INFO: multus-additional-cni-plugins-wxcnn from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
May 16 15:29:47.169: INFO: multus-rpn9n from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container kube-multus ready: true, restart count 0
May 16 15:29:47.169: INFO: network-metrics-daemon-7sqls from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (2 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container network-metrics-daemon ready: true, restart count 0
May 16 15:29:47.169: INFO: network-check-target-8pcqz from openshift-network-diagnostics started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container network-check-target-container ready: true, restart count 0
May 16 15:29:47.169: INFO: ovnkube-node-kqvdm from openshift-ovn-kubernetes started at 2023-05-16 13:52:19 +0000 UTC (5 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container ovn-acl-logging ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container ovn-controller ready: true, restart count 1
May 16 15:29:47.169: INFO: 	Container ovnkube-node ready: true, restart count 0
May 16 15:29:47.169: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-gsqlr from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
May 16 15:29:47.169: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 16 15:29:47.169: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 05/16/23 15:29:47.169
May 16 15:29:47.177: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8350" to be "running"
May 16 15:29:47.180: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.690794ms
May 16 15:29:49.185: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008560518s
May 16 15:29:49.185: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 05/16/23 15:29:49.188
STEP: Trying to apply a random label on the found node. 05/16/23 15:29:49.209
STEP: verifying the node has the label kubernetes.io/e2e-462ec3a9-038d-45d7-a422-13fe2895bc24 95 05/16/23 15:29:49.225
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 05/16/23 15:29:49.233
May 16 15:29:49.249: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-8350" to be "not pending"
May 16 15:29:49.255: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.613215ms
May 16 15:29:51.258: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008549368s
May 16 15:29:51.258: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.161.164 on the node which pod4 resides and expect not scheduled 05/16/23 15:29:51.258
May 16 15:29:51.265: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-8350" to be "not pending"
May 16 15:29:51.268: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.036527ms
May 16 15:29:53.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006720298s
May 16 15:29:55.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006674558s
May 16 15:29:57.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006935405s
May 16 15:29:59.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006710718s
May 16 15:30:01.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007197461s
May 16 15:30:03.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006110456s
May 16 15:30:05.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007835039s
May 16 15:30:07.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006558546s
May 16 15:30:09.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007675865s
May 16 15:30:11.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.00718061s
May 16 15:30:13.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007090196s
May 16 15:30:15.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.008007878s
May 16 15:30:17.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.006673251s
May 16 15:30:19.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.007172096s
May 16 15:30:21.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007318383s
May 16 15:30:23.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00685675s
May 16 15:30:25.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006370714s
May 16 15:30:27.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006781729s
May 16 15:30:29.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006063852s
May 16 15:30:31.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.005962596s
May 16 15:30:33.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008357179s
May 16 15:30:35.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.007269368s
May 16 15:30:37.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.005901776s
May 16 15:30:39.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.007656808s
May 16 15:30:41.276: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.010619401s
May 16 15:30:43.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006130604s
May 16 15:30:45.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.00822521s
May 16 15:30:47.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007040539s
May 16 15:30:49.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008202666s
May 16 15:30:51.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.006016656s
May 16 15:30:53.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.006132205s
May 16 15:30:55.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.006797088s
May 16 15:30:57.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006755871s
May 16 15:30:59.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006423199s
May 16 15:31:01.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006153131s
May 16 15:31:03.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007078675s
May 16 15:31:05.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007578159s
May 16 15:31:07.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007886316s
May 16 15:31:09.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007433092s
May 16 15:31:11.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.006725828s
May 16 15:31:13.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006985221s
May 16 15:31:15.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006637834s
May 16 15:31:17.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.006532609s
May 16 15:31:19.274: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008450137s
May 16 15:31:21.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.005993958s
May 16 15:31:23.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006815155s
May 16 15:31:25.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.006317252s
May 16 15:31:27.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006738647s
May 16 15:31:29.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.005761284s
May 16 15:31:31.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007052085s
May 16 15:31:33.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005843858s
May 16 15:31:35.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.0058699s
May 16 15:31:37.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.00751881s
May 16 15:31:39.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007379485s
May 16 15:31:41.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.00611373s
May 16 15:31:43.293: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.027791525s
May 16 15:31:45.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.008043969s
May 16 15:31:47.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006272925s
May 16 15:31:49.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007307456s
May 16 15:31:51.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006125716s
May 16 15:31:53.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.007458286s
May 16 15:31:55.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.006592502s
May 16 15:31:57.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.00738487s
May 16 15:31:59.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.005872518s
May 16 15:32:01.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.007194606s
May 16 15:32:03.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.006205801s
May 16 15:32:05.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.007439792s
May 16 15:32:07.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.007321139s
May 16 15:32:09.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.007353875s
May 16 15:32:11.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.006116342s
May 16 15:32:13.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.006578824s
May 16 15:32:15.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.007685733s
May 16 15:32:17.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.006967527s
May 16 15:32:19.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.007919629s
May 16 15:32:21.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.005835847s
May 16 15:32:23.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.006394457s
May 16 15:32:25.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.006222734s
May 16 15:32:27.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.006842679s
May 16 15:32:29.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.006654042s
May 16 15:32:31.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.006767007s
May 16 15:32:33.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.00605864s
May 16 15:32:35.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.006661269s
May 16 15:32:37.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.007134636s
May 16 15:32:39.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.006002584s
May 16 15:32:41.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.006473292s
May 16 15:32:43.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.007598608s
May 16 15:32:45.276: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.011366499s
May 16 15:32:47.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.006274235s
May 16 15:32:49.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.006172837s
May 16 15:32:51.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.007547395s
May 16 15:32:53.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.006458033s
May 16 15:32:55.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.006190266s
May 16 15:32:57.279: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.013567222s
May 16 15:32:59.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.006720224s
May 16 15:33:01.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.006675386s
May 16 15:33:03.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.007174379s
May 16 15:33:05.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.006875488s
May 16 15:33:07.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.007590244s
May 16 15:33:09.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007028257s
May 16 15:33:11.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.006072575s
May 16 15:33:13.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.00742546s
May 16 15:33:15.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.007500633s
May 16 15:33:17.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.007240468s
May 16 15:33:19.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.00704492s
May 16 15:33:21.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.00673479s
May 16 15:33:23.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.006773419s
May 16 15:33:25.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.007691898s
May 16 15:33:27.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.007759289s
May 16 15:33:29.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.00639292s
May 16 15:33:31.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.006836961s
May 16 15:33:33.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.006635845s
May 16 15:33:35.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.007070527s
May 16 15:33:37.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.007184701s
May 16 15:33:39.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.00733579s
May 16 15:33:41.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.005977291s
May 16 15:33:43.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.007670701s
May 16 15:33:45.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.007401183s
May 16 15:33:47.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.006104917s
May 16 15:33:49.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.006648503s
May 16 15:33:51.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.007427725s
May 16 15:33:53.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.006692985s
May 16 15:33:55.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.007264422s
May 16 15:33:57.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.006671014s
May 16 15:33:59.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.006771059s
May 16 15:34:01.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.007116264s
May 16 15:34:03.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.006640274s
May 16 15:34:05.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.007452691s
May 16 15:34:07.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.007337869s
May 16 15:34:09.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.007503164s
May 16 15:34:11.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.006431313s
May 16 15:34:13.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.007584662s
May 16 15:34:15.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.007356807s
May 16 15:34:17.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.007178221s
May 16 15:34:19.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.007160307s
May 16 15:34:21.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.006215867s
May 16 15:34:23.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.006457403s
May 16 15:34:25.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.007977922s
May 16 15:34:27.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.007464986s
May 16 15:34:29.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.007018271s
May 16 15:34:31.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.005946014s
May 16 15:34:33.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.006473161s
May 16 15:34:35.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.007252184s
May 16 15:34:37.285: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.019808038s
May 16 15:34:39.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.007468196s
May 16 15:34:41.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.006400415s
May 16 15:34:43.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.00715108s
May 16 15:34:45.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008205973s
May 16 15:34:47.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.007092538s
May 16 15:34:49.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.007587912s
May 16 15:34:51.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.006432983s
May 16 15:34:51.274: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.009307439s
STEP: removing the label kubernetes.io/e2e-462ec3a9-038d-45d7-a422-13fe2895bc24 off the node ip-10-0-161-164.eu-central-1.compute.internal 05/16/23 15:34:51.274
STEP: verifying the node doesn't have the label kubernetes.io/e2e-462ec3a9-038d-45d7-a422-13fe2895bc24 05/16/23 15:34:51.285
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 15:34:51.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8350" for this suite. 05/16/23 15:34:51.294
------------------------------
â€¢ [SLOW TEST] [304.249 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:29:47.052
    May 16 15:29:47.052: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename sched-pred 05/16/23 15:29:47.052
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:29:47.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:29:47.071
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    May 16 15:29:47.072: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    May 16 15:29:47.085: INFO: Waiting for terminating namespaces to be deleted...
    May 16 15:29:47.093: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-132-142.eu-central-1.compute.internal before test
    May 16 15:29:47.125: INFO: aws-ebs-csi-driver-node-27d5z from openshift-cluster-csi-drivers started at 2023-05-16 13:52:17 +0000 UTC (3 container statuses recorded)
    May 16 15:29:47.125: INFO: 	Container csi-driver ready: true, restart count 0
    May 16 15:29:47.125: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    May 16 15:29:47.125: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    May 16 15:29:47.125: INFO: tuned-j9bjl from openshift-cluster-node-tuning-operator started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.125: INFO: 	Container tuned ready: true, restart count 0
    May 16 15:29:47.125: INFO: dns-default-9lcb7 from openshift-dns started at 2023-05-16 13:53:17 +0000 UTC (2 container statuses recorded)
    May 16 15:29:47.125: INFO: 	Container dns ready: true, restart count 0
    May 16 15:29:47.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.125: INFO: node-resolver-b47gh from openshift-dns started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.125: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May 16 15:29:47.125: INFO: image-registry-b549d4f89-4ck4v from openshift-image-registry started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.125: INFO: 	Container registry ready: true, restart count 0
    May 16 15:29:47.125: INFO: node-ca-px455 from openshift-image-registry started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.125: INFO: 	Container node-ca ready: true, restart count 0
    May 16 15:29:47.125: INFO: ingress-canary-4hv85 from openshift-ingress-canary started at 2023-05-16 13:53:17 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.125: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May 16 15:29:47.125: INFO: router-default-8f95c5-srcbl from openshift-ingress started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.125: INFO: 	Container router ready: true, restart count 0
    May 16 15:29:47.125: INFO: machine-config-daemon-l6bpj from openshift-machine-config-operator started at 2023-05-16 13:52:17 +0000 UTC (2 container statuses recorded)
    May 16 15:29:47.125: INFO: 	Container machine-config-daemon ready: true, restart count 0
    May 16 15:29:47.125: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 15:29:47.125: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (6 container statuses recorded)
    May 16 15:29:47.125: INFO: 	Container alertmanager ready: true, restart count 1
    May 16 15:29:47.125: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    May 16 15:29:47.125: INFO: 	Container config-reloader ready: true, restart count 0
    May 16 15:29:47.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.125: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    May 16 15:29:47.125: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May 16 15:29:47.125: INFO: node-exporter-pltn9 from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
    May 16 15:29:47.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.125: INFO: 	Container node-exporter ready: true, restart count 0
    May 16 15:29:47.125: INFO: prometheus-adapter-7767c6bd47-bfq4d from openshift-monitoring started at 2023-05-16 13:53:50 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.125: INFO: 	Container prometheus-adapter ready: true, restart count 0
    May 16 15:29:47.125: INFO: prometheus-operator-admission-webhook-5d679565bb-8lkws from openshift-monitoring started at 2023-05-16 14:48:21 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.125: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    May 16 15:29:47.125: INFO: telemeter-client-859dbffbb4-mt92v from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (3 container statuses recorded)
    May 16 15:29:47.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.125: INFO: 	Container reload ready: true, restart count 0
    May 16 15:29:47.125: INFO: 	Container telemeter-client ready: true, restart count 0
    May 16 15:29:47.125: INFO: thanos-querier-5c9d876b5c-xdt6m from openshift-monitoring started at 2023-05-16 13:53:54 +0000 UTC (6 container statuses recorded)
    May 16 15:29:47.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.125: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    May 16 15:29:47.125: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    May 16 15:29:47.125: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 15:29:47.125: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May 16 15:29:47.125: INFO: 	Container thanos-query ready: true, restart count 0
    May 16 15:29:47.125: INFO: multus-2wjqr from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.125: INFO: 	Container kube-multus ready: true, restart count 0
    May 16 15:29:47.125: INFO: multus-additional-cni-plugins-z2h4l from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.126: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May 16 15:29:47.126: INFO: network-metrics-daemon-rqdtf from openshift-multus started at 2023-05-16 13:52:17 +0000 UTC (2 container statuses recorded)
    May 16 15:29:47.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.126: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May 16 15:29:47.126: INFO: network-check-target-28wts from openshift-network-diagnostics started at 2023-05-16 13:52:17 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.126: INFO: 	Container network-check-target-container ready: true, restart count 0
    May 16 15:29:47.126: INFO: ovnkube-node-npwgq from openshift-ovn-kubernetes started at 2023-05-16 13:52:17 +0000 UTC (5 container statuses recorded)
    May 16 15:29:47.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.126: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    May 16 15:29:47.126: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    May 16 15:29:47.126: INFO: 	Container ovn-controller ready: true, restart count 1
    May 16 15:29:47.126: INFO: 	Container ovnkube-node ready: true, restart count 0
    May 16 15:29:47.126: INFO: sonobuoy from sonobuoy started at 2023-05-16 14:11:07 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.126: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    May 16 15:29:47.126: INFO: sonobuoy-e2e-job-840dd79630d44353 from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
    May 16 15:29:47.126: INFO: 	Container e2e ready: true, restart count 0
    May 16 15:29:47.126: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 16 15:29:47.126: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-7zsf6 from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
    May 16 15:29:47.126: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 16 15:29:47.126: INFO: 	Container systemd-logs ready: true, restart count 0
    May 16 15:29:47.126: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-161-164.eu-central-1.compute.internal before test
    May 16 15:29:47.147: INFO: aws-ebs-csi-driver-node-9m6vn from openshift-cluster-csi-drivers started at 2023-05-16 13:44:25 +0000 UTC (3 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container csi-driver ready: true, restart count 0
    May 16 15:29:47.147: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    May 16 15:29:47.147: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    May 16 15:29:47.147: INFO: tuned-6n5b7 from openshift-cluster-node-tuning-operator started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container tuned ready: true, restart count 0
    May 16 15:29:47.147: INFO: dns-default-pwtdq from openshift-dns started at 2023-05-16 14:48:42 +0000 UTC (2 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container dns ready: true, restart count 0
    May 16 15:29:47.147: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.147: INFO: node-resolver-jbmqq from openshift-dns started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May 16 15:29:47.147: INFO: node-ca-gw7rz from openshift-image-registry started at 2023-05-16 13:50:21 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container node-ca ready: true, restart count 0
    May 16 15:29:47.147: INFO: ingress-canary-mvqst from openshift-ingress-canary started at 2023-05-16 14:48:23 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May 16 15:29:47.147: INFO: machine-config-daemon-qhdsv from openshift-machine-config-operator started at 2023-05-16 13:44:25 +0000 UTC (2 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container machine-config-daemon ready: true, restart count 0
    May 16 15:29:47.147: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 15:29:47.147: INFO: node-exporter-l2zvs from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.147: INFO: 	Container node-exporter ready: true, restart count 0
    May 16 15:29:47.147: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-05-16 14:48:24 +0000 UTC (6 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container config-reloader ready: true, restart count 0
    May 16 15:29:47.147: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.147: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    May 16 15:29:47.147: INFO: 	Container prometheus ready: true, restart count 0
    May 16 15:29:47.147: INFO: 	Container prometheus-proxy ready: true, restart count 0
    May 16 15:29:47.147: INFO: 	Container thanos-sidecar ready: true, restart count 0
    May 16 15:29:47.147: INFO: multus-additional-cni-plugins-jkn7l from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May 16 15:29:47.147: INFO: multus-mc6qb from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container kube-multus ready: true, restart count 0
    May 16 15:29:47.147: INFO: network-metrics-daemon-n4p75 from openshift-multus started at 2023-05-16 13:44:25 +0000 UTC (2 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.147: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May 16 15:29:47.147: INFO: network-check-source-7f6b75fdb6-lxqsg from openshift-network-diagnostics started at 2023-05-16 14:48:21 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container check-endpoints ready: true, restart count 0
    May 16 15:29:47.147: INFO: network-check-target-bprb5 from openshift-network-diagnostics started at 2023-05-16 13:44:25 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container network-check-target-container ready: true, restart count 0
    May 16 15:29:47.147: INFO: collect-profiles-28070820-8vnf5 from openshift-operator-lifecycle-manager started at 2023-05-16 15:00:00 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container collect-profiles ready: false, restart count 0
    May 16 15:29:47.147: INFO: collect-profiles-28070835-9g48l from openshift-operator-lifecycle-manager started at 2023-05-16 15:15:00 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container collect-profiles ready: false, restart count 0
    May 16 15:29:47.147: INFO: ovnkube-node-w8qn8 from openshift-ovn-kubernetes started at 2023-05-16 13:44:25 +0000 UTC (5 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.147: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    May 16 15:29:47.147: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    May 16 15:29:47.147: INFO: 	Container ovn-controller ready: true, restart count 1
    May 16 15:29:47.147: INFO: 	Container ovnkube-node ready: true, restart count 0
    May 16 15:29:47.147: INFO: execpod99jrf from services-8534 started at 2023-05-16 15:29:35 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container agnhost-container ready: true, restart count 0
    May 16 15:29:47.147: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-5r4tm from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 16 15:29:47.147: INFO: 	Container systemd-logs ready: true, restart count 0
    May 16 15:29:47.147: INFO: webhook-to-be-mutated from webhook-1498 started at 2023-05-16 15:29:32 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.147: INFO: 	Container example ready: false, restart count 0
    May 16 15:29:47.147: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-212-246.eu-central-1.compute.internal before test
    May 16 15:29:47.169: INFO: aws-ebs-csi-driver-node-b4s28 from openshift-cluster-csi-drivers started at 2023-05-16 13:52:19 +0000 UTC (3 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container csi-driver ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container csi-liveness-probe ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    May 16 15:29:47.169: INFO: tuned-8x9rx from openshift-cluster-node-tuning-operator started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container tuned ready: true, restart count 0
    May 16 15:29:47.169: INFO: dns-default-dbkcs from openshift-dns started at 2023-05-16 13:53:17 +0000 UTC (2 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container dns ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.169: INFO: node-resolver-gdzz2 from openshift-dns started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container dns-node-resolver ready: true, restart count 0
    May 16 15:29:47.169: INFO: image-registry-b549d4f89-hs4qn from openshift-image-registry started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container registry ready: true, restart count 0
    May 16 15:29:47.169: INFO: node-ca-nsxtf from openshift-image-registry started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container node-ca ready: true, restart count 0
    May 16 15:29:47.169: INFO: ingress-canary-wphz5 from openshift-ingress-canary started at 2023-05-16 13:53:17 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    May 16 15:29:47.169: INFO: router-default-8f95c5-bbmm5 from openshift-ingress started at 2023-05-16 14:48:21 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container router ready: true, restart count 0
    May 16 15:29:47.169: INFO: machine-config-daemon-z7sw6 from openshift-machine-config-operator started at 2023-05-16 13:52:19 +0000 UTC (2 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container machine-config-daemon ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 15:29:47.169: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-05-16 13:53:53 +0000 UTC (6 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container alertmanager ready: true, restart count 1
    May 16 15:29:47.169: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container config-reloader ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May 16 15:29:47.169: INFO: kube-state-metrics-6ccfb58dc4-wgwts from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (3 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container kube-state-metrics ready: true, restart count 0
    May 16 15:29:47.169: INFO: node-exporter-jrknj from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (2 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container node-exporter ready: true, restart count 0
    May 16 15:29:47.169: INFO: openshift-state-metrics-7d7f8b4cf8-9lbg2 from openshift-monitoring started at 2023-05-16 13:53:47 +0000 UTC (3 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    May 16 15:29:47.169: INFO: prometheus-adapter-7767c6bd47-bqlnh from openshift-monitoring started at 2023-05-16 13:53:50 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container prometheus-adapter ready: true, restart count 0
    May 16 15:29:47.169: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-05-16 13:53:57 +0000 UTC (6 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container config-reloader ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container prometheus ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container prometheus-proxy ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container thanos-sidecar ready: true, restart count 0
    May 16 15:29:47.169: INFO: prometheus-operator-admission-webhook-5d679565bb-96qsx from openshift-monitoring started at 2023-05-16 13:53:34 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    May 16 15:29:47.169: INFO: thanos-querier-5c9d876b5c-wwd7q from openshift-monitoring started at 2023-05-16 13:53:54 +0000 UTC (6 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container oauth-proxy ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container prom-label-proxy ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container thanos-query ready: true, restart count 0
    May 16 15:29:47.169: INFO: multus-additional-cni-plugins-wxcnn from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    May 16 15:29:47.169: INFO: multus-rpn9n from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container kube-multus ready: true, restart count 0
    May 16 15:29:47.169: INFO: network-metrics-daemon-7sqls from openshift-multus started at 2023-05-16 13:52:19 +0000 UTC (2 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    May 16 15:29:47.169: INFO: network-check-target-8pcqz from openshift-network-diagnostics started at 2023-05-16 13:52:19 +0000 UTC (1 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container network-check-target-container ready: true, restart count 0
    May 16 15:29:47.169: INFO: ovnkube-node-kqvdm from openshift-ovn-kubernetes started at 2023-05-16 13:52:19 +0000 UTC (5 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container ovn-acl-logging ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container ovn-controller ready: true, restart count 1
    May 16 15:29:47.169: INFO: 	Container ovnkube-node ready: true, restart count 0
    May 16 15:29:47.169: INFO: sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-gsqlr from sonobuoy started at 2023-05-16 14:11:12 +0000 UTC (2 container statuses recorded)
    May 16 15:29:47.169: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    May 16 15:29:47.169: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 05/16/23 15:29:47.169
    May 16 15:29:47.177: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8350" to be "running"
    May 16 15:29:47.180: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.690794ms
    May 16 15:29:49.185: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008560518s
    May 16 15:29:49.185: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 05/16/23 15:29:49.188
    STEP: Trying to apply a random label on the found node. 05/16/23 15:29:49.209
    STEP: verifying the node has the label kubernetes.io/e2e-462ec3a9-038d-45d7-a422-13fe2895bc24 95 05/16/23 15:29:49.225
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 05/16/23 15:29:49.233
    May 16 15:29:49.249: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-8350" to be "not pending"
    May 16 15:29:49.255: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.613215ms
    May 16 15:29:51.258: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008549368s
    May 16 15:29:51.258: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.161.164 on the node which pod4 resides and expect not scheduled 05/16/23 15:29:51.258
    May 16 15:29:51.265: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-8350" to be "not pending"
    May 16 15:29:51.268: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.036527ms
    May 16 15:29:53.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006720298s
    May 16 15:29:55.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006674558s
    May 16 15:29:57.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006935405s
    May 16 15:29:59.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006710718s
    May 16 15:30:01.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007197461s
    May 16 15:30:03.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006110456s
    May 16 15:30:05.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007835039s
    May 16 15:30:07.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006558546s
    May 16 15:30:09.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007675865s
    May 16 15:30:11.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.00718061s
    May 16 15:30:13.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007090196s
    May 16 15:30:15.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.008007878s
    May 16 15:30:17.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.006673251s
    May 16 15:30:19.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.007172096s
    May 16 15:30:21.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007318383s
    May 16 15:30:23.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00685675s
    May 16 15:30:25.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006370714s
    May 16 15:30:27.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006781729s
    May 16 15:30:29.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.006063852s
    May 16 15:30:31.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.005962596s
    May 16 15:30:33.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008357179s
    May 16 15:30:35.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.007269368s
    May 16 15:30:37.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.005901776s
    May 16 15:30:39.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.007656808s
    May 16 15:30:41.276: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.010619401s
    May 16 15:30:43.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006130604s
    May 16 15:30:45.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.00822521s
    May 16 15:30:47.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007040539s
    May 16 15:30:49.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008202666s
    May 16 15:30:51.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.006016656s
    May 16 15:30:53.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.006132205s
    May 16 15:30:55.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.006797088s
    May 16 15:30:57.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006755871s
    May 16 15:30:59.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006423199s
    May 16 15:31:01.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006153131s
    May 16 15:31:03.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007078675s
    May 16 15:31:05.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007578159s
    May 16 15:31:07.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007886316s
    May 16 15:31:09.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007433092s
    May 16 15:31:11.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.006725828s
    May 16 15:31:13.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006985221s
    May 16 15:31:15.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006637834s
    May 16 15:31:17.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.006532609s
    May 16 15:31:19.274: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008450137s
    May 16 15:31:21.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.005993958s
    May 16 15:31:23.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006815155s
    May 16 15:31:25.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.006317252s
    May 16 15:31:27.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.006738647s
    May 16 15:31:29.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.005761284s
    May 16 15:31:31.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007052085s
    May 16 15:31:33.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005843858s
    May 16 15:31:35.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.0058699s
    May 16 15:31:37.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.00751881s
    May 16 15:31:39.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007379485s
    May 16 15:31:41.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.00611373s
    May 16 15:31:43.293: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.027791525s
    May 16 15:31:45.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.008043969s
    May 16 15:31:47.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006272925s
    May 16 15:31:49.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007307456s
    May 16 15:31:51.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006125716s
    May 16 15:31:53.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.007458286s
    May 16 15:31:55.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.006592502s
    May 16 15:31:57.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.00738487s
    May 16 15:31:59.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.005872518s
    May 16 15:32:01.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.007194606s
    May 16 15:32:03.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.006205801s
    May 16 15:32:05.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.007439792s
    May 16 15:32:07.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.007321139s
    May 16 15:32:09.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.007353875s
    May 16 15:32:11.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.006116342s
    May 16 15:32:13.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.006578824s
    May 16 15:32:15.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.007685733s
    May 16 15:32:17.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.006967527s
    May 16 15:32:19.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.007919629s
    May 16 15:32:21.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.005835847s
    May 16 15:32:23.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.006394457s
    May 16 15:32:25.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.006222734s
    May 16 15:32:27.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.006842679s
    May 16 15:32:29.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.006654042s
    May 16 15:32:31.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.006767007s
    May 16 15:32:33.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.00605864s
    May 16 15:32:35.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.006661269s
    May 16 15:32:37.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.007134636s
    May 16 15:32:39.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.006002584s
    May 16 15:32:41.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.006473292s
    May 16 15:32:43.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.007598608s
    May 16 15:32:45.276: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.011366499s
    May 16 15:32:47.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.006274235s
    May 16 15:32:49.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.006172837s
    May 16 15:32:51.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.007547395s
    May 16 15:32:53.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.006458033s
    May 16 15:32:55.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.006190266s
    May 16 15:32:57.279: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.013567222s
    May 16 15:32:59.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.006720224s
    May 16 15:33:01.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.006675386s
    May 16 15:33:03.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.007174379s
    May 16 15:33:05.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.006875488s
    May 16 15:33:07.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.007590244s
    May 16 15:33:09.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007028257s
    May 16 15:33:11.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.006072575s
    May 16 15:33:13.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.00742546s
    May 16 15:33:15.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.007500633s
    May 16 15:33:17.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.007240468s
    May 16 15:33:19.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.00704492s
    May 16 15:33:21.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.00673479s
    May 16 15:33:23.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.006773419s
    May 16 15:33:25.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.007691898s
    May 16 15:33:27.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.007759289s
    May 16 15:33:29.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.00639292s
    May 16 15:33:31.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.006836961s
    May 16 15:33:33.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.006635845s
    May 16 15:33:35.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.007070527s
    May 16 15:33:37.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.007184701s
    May 16 15:33:39.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.00733579s
    May 16 15:33:41.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.005977291s
    May 16 15:33:43.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.007670701s
    May 16 15:33:45.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.007401183s
    May 16 15:33:47.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.006104917s
    May 16 15:33:49.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.006648503s
    May 16 15:33:51.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.007427725s
    May 16 15:33:53.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.006692985s
    May 16 15:33:55.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.007264422s
    May 16 15:33:57.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.006671014s
    May 16 15:33:59.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.006771059s
    May 16 15:34:01.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.007116264s
    May 16 15:34:03.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.006640274s
    May 16 15:34:05.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.007452691s
    May 16 15:34:07.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.007337869s
    May 16 15:34:09.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.007503164s
    May 16 15:34:11.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.006431313s
    May 16 15:34:13.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.007584662s
    May 16 15:34:15.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.007356807s
    May 16 15:34:17.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.007178221s
    May 16 15:34:19.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.007160307s
    May 16 15:34:21.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.006215867s
    May 16 15:34:23.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.006457403s
    May 16 15:34:25.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.007977922s
    May 16 15:34:27.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.007464986s
    May 16 15:34:29.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.007018271s
    May 16 15:34:31.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.005946014s
    May 16 15:34:33.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.006473161s
    May 16 15:34:35.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.007252184s
    May 16 15:34:37.285: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.019808038s
    May 16 15:34:39.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.007468196s
    May 16 15:34:41.271: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.006400415s
    May 16 15:34:43.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.00715108s
    May 16 15:34:45.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008205973s
    May 16 15:34:47.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.007092538s
    May 16 15:34:49.273: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.007587912s
    May 16 15:34:51.272: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.006432983s
    May 16 15:34:51.274: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.009307439s
    STEP: removing the label kubernetes.io/e2e-462ec3a9-038d-45d7-a422-13fe2895bc24 off the node ip-10-0-161-164.eu-central-1.compute.internal 05/16/23 15:34:51.274
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-462ec3a9-038d-45d7-a422-13fe2895bc24 05/16/23 15:34:51.285
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 15:34:51.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8350" for this suite. 05/16/23 15:34:51.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:34:51.301
May 16 15:34:51.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename ingress 05/16/23 15:34:51.302
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:34:51.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:34:51.317
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 05/16/23 15:34:51.321
STEP: getting /apis/networking.k8s.io 05/16/23 15:34:51.323
STEP: getting /apis/networking.k8s.iov1 05/16/23 15:34:51.324
STEP: creating 05/16/23 15:34:51.324
STEP: getting 05/16/23 15:34:51.342
STEP: listing 05/16/23 15:34:51.345
STEP: watching 05/16/23 15:34:51.348
May 16 15:34:51.348: INFO: starting watch
STEP: cluster-wide listing 05/16/23 15:34:51.348
STEP: cluster-wide watching 05/16/23 15:34:51.351
May 16 15:34:51.351: INFO: starting watch
STEP: patching 05/16/23 15:34:51.352
STEP: updating 05/16/23 15:34:51.358
May 16 15:34:51.367: INFO: waiting for watch events with expected annotations
May 16 15:34:51.367: INFO: saw patched and updated annotations
STEP: patching /status 05/16/23 15:34:51.367
STEP: updating /status 05/16/23 15:34:51.372
STEP: get /status 05/16/23 15:34:51.382
STEP: deleting 05/16/23 15:34:51.39
STEP: deleting a collection 05/16/23 15:34:51.415
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
May 16 15:34:51.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-1889" for this suite. 05/16/23 15:34:51.436
------------------------------
â€¢ [0.140 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:34:51.301
    May 16 15:34:51.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename ingress 05/16/23 15:34:51.302
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:34:51.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:34:51.317
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 05/16/23 15:34:51.321
    STEP: getting /apis/networking.k8s.io 05/16/23 15:34:51.323
    STEP: getting /apis/networking.k8s.iov1 05/16/23 15:34:51.324
    STEP: creating 05/16/23 15:34:51.324
    STEP: getting 05/16/23 15:34:51.342
    STEP: listing 05/16/23 15:34:51.345
    STEP: watching 05/16/23 15:34:51.348
    May 16 15:34:51.348: INFO: starting watch
    STEP: cluster-wide listing 05/16/23 15:34:51.348
    STEP: cluster-wide watching 05/16/23 15:34:51.351
    May 16 15:34:51.351: INFO: starting watch
    STEP: patching 05/16/23 15:34:51.352
    STEP: updating 05/16/23 15:34:51.358
    May 16 15:34:51.367: INFO: waiting for watch events with expected annotations
    May 16 15:34:51.367: INFO: saw patched and updated annotations
    STEP: patching /status 05/16/23 15:34:51.367
    STEP: updating /status 05/16/23 15:34:51.372
    STEP: get /status 05/16/23 15:34:51.382
    STEP: deleting 05/16/23 15:34:51.39
    STEP: deleting a collection 05/16/23 15:34:51.415
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    May 16 15:34:51.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-1889" for this suite. 05/16/23 15:34:51.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:34:51.441
May 16 15:34:51.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename emptydir 05/16/23 15:34:51.442
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:34:51.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:34:51.455
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 05/16/23 15:34:51.459
May 16 15:34:51.478: INFO: Waiting up to 5m0s for pod "pod-51c06226-d012-418c-83e4-81d658cb4e4e" in namespace "emptydir-4506" to be "Succeeded or Failed"
May 16 15:34:51.482: INFO: Pod "pod-51c06226-d012-418c-83e4-81d658cb4e4e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.729369ms
May 16 15:34:53.485: INFO: Pod "pod-51c06226-d012-418c-83e4-81d658cb4e4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006808389s
May 16 15:34:55.495: INFO: Pod "pod-51c06226-d012-418c-83e4-81d658cb4e4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017021046s
STEP: Saw pod success 05/16/23 15:34:55.495
May 16 15:34:55.495: INFO: Pod "pod-51c06226-d012-418c-83e4-81d658cb4e4e" satisfied condition "Succeeded or Failed"
May 16 15:34:55.501: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-51c06226-d012-418c-83e4-81d658cb4e4e container test-container: <nil>
STEP: delete the pod 05/16/23 15:34:55.522
May 16 15:34:55.538: INFO: Waiting for pod pod-51c06226-d012-418c-83e4-81d658cb4e4e to disappear
May 16 15:34:55.541: INFO: Pod pod-51c06226-d012-418c-83e4-81d658cb4e4e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 16 15:34:55.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4506" for this suite. 05/16/23 15:34:55.545
------------------------------
â€¢ [4.121 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:34:51.441
    May 16 15:34:51.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename emptydir 05/16/23 15:34:51.442
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:34:51.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:34:51.455
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 05/16/23 15:34:51.459
    May 16 15:34:51.478: INFO: Waiting up to 5m0s for pod "pod-51c06226-d012-418c-83e4-81d658cb4e4e" in namespace "emptydir-4506" to be "Succeeded or Failed"
    May 16 15:34:51.482: INFO: Pod "pod-51c06226-d012-418c-83e4-81d658cb4e4e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.729369ms
    May 16 15:34:53.485: INFO: Pod "pod-51c06226-d012-418c-83e4-81d658cb4e4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006808389s
    May 16 15:34:55.495: INFO: Pod "pod-51c06226-d012-418c-83e4-81d658cb4e4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017021046s
    STEP: Saw pod success 05/16/23 15:34:55.495
    May 16 15:34:55.495: INFO: Pod "pod-51c06226-d012-418c-83e4-81d658cb4e4e" satisfied condition "Succeeded or Failed"
    May 16 15:34:55.501: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-51c06226-d012-418c-83e4-81d658cb4e4e container test-container: <nil>
    STEP: delete the pod 05/16/23 15:34:55.522
    May 16 15:34:55.538: INFO: Waiting for pod pod-51c06226-d012-418c-83e4-81d658cb4e4e to disappear
    May 16 15:34:55.541: INFO: Pod pod-51c06226-d012-418c-83e4-81d658cb4e4e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 16 15:34:55.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4506" for this suite. 05/16/23 15:34:55.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:34:55.563
May 16 15:34:55.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename daemonsets 05/16/23 15:34:55.563
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:34:55.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:34:55.584
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
May 16 15:34:55.634: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 05/16/23 15:34:55.64
May 16 15:34:55.647: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:34:55.647: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:34:55.647: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:34:55.651: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 15:34:55.651: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 15:34:56.656: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:34:56.657: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:34:56.657: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:34:56.660: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 16 15:34:56.660: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 15:34:57.655: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:34:57.655: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:34:57.655: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:34:57.658: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 16 15:34:57.658: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 05/16/23 15:34:57.669
STEP: Check that daemon pods images are updated. 05/16/23 15:34:57.679
May 16 15:34:57.682: INFO: Wrong image for pod: daemon-set-85rvc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May 16 15:34:57.682: INFO: Wrong image for pod: daemon-set-bb57w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May 16 15:34:57.682: INFO: Wrong image for pod: daemon-set-kjsl5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May 16 15:34:57.686: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:34:57.686: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:34:57.686: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:34:58.690: INFO: Wrong image for pod: daemon-set-85rvc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May 16 15:34:58.690: INFO: Wrong image for pod: daemon-set-bb57w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May 16 15:34:58.694: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:34:58.694: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:34:58.694: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:34:59.690: INFO: Wrong image for pod: daemon-set-85rvc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May 16 15:34:59.690: INFO: Wrong image for pod: daemon-set-bb57w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May 16 15:34:59.690: INFO: Pod daemon-set-hzj7f is not available
May 16 15:34:59.693: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:34:59.693: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:34:59.693: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:00.689: INFO: Wrong image for pod: daemon-set-85rvc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May 16 15:35:00.693: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:00.693: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:00.693: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:01.690: INFO: Wrong image for pod: daemon-set-85rvc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
May 16 15:35:01.690: INFO: Pod daemon-set-hrrdc is not available
May 16 15:35:01.694: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:01.694: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:01.694: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:02.693: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:02.693: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:02.693: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:03.689: INFO: Pod daemon-set-429qp is not available
May 16 15:35:03.693: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:03.693: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:03.693: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 05/16/23 15:35:03.693
May 16 15:35:03.697: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:03.697: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:03.697: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:03.699: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 16 15:35:03.699: INFO: Node ip-10-0-161-164.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 15:35:04.704: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:04.704: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:04.704: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:04.707: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 16 15:35:04.707: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 05/16/23 15:35:04.719
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3789, will wait for the garbage collector to delete the pods 05/16/23 15:35:04.719
May 16 15:35:04.778: INFO: Deleting DaemonSet.extensions daemon-set took: 5.692306ms
May 16 15:35:04.878: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.425998ms
May 16 15:35:07.481: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 15:35:07.481: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 16 15:35:07.484: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"98273"},"items":null}

May 16 15:35:07.486: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"98273"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 15:35:07.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3789" for this suite. 05/16/23 15:35:07.503
------------------------------
â€¢ [SLOW TEST] [11.946 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:34:55.563
    May 16 15:34:55.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename daemonsets 05/16/23 15:34:55.563
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:34:55.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:34:55.584
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    May 16 15:34:55.634: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 05/16/23 15:34:55.64
    May 16 15:34:55.647: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:34:55.647: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:34:55.647: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:34:55.651: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 15:34:55.651: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 15:34:56.656: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:34:56.657: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:34:56.657: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:34:56.660: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 16 15:34:56.660: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 15:34:57.655: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:34:57.655: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:34:57.655: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:34:57.658: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May 16 15:34:57.658: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 05/16/23 15:34:57.669
    STEP: Check that daemon pods images are updated. 05/16/23 15:34:57.679
    May 16 15:34:57.682: INFO: Wrong image for pod: daemon-set-85rvc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May 16 15:34:57.682: INFO: Wrong image for pod: daemon-set-bb57w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May 16 15:34:57.682: INFO: Wrong image for pod: daemon-set-kjsl5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May 16 15:34:57.686: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:34:57.686: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:34:57.686: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:34:58.690: INFO: Wrong image for pod: daemon-set-85rvc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May 16 15:34:58.690: INFO: Wrong image for pod: daemon-set-bb57w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May 16 15:34:58.694: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:34:58.694: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:34:58.694: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:34:59.690: INFO: Wrong image for pod: daemon-set-85rvc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May 16 15:34:59.690: INFO: Wrong image for pod: daemon-set-bb57w. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May 16 15:34:59.690: INFO: Pod daemon-set-hzj7f is not available
    May 16 15:34:59.693: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:34:59.693: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:34:59.693: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:00.689: INFO: Wrong image for pod: daemon-set-85rvc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May 16 15:35:00.693: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:00.693: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:00.693: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:01.690: INFO: Wrong image for pod: daemon-set-85rvc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    May 16 15:35:01.690: INFO: Pod daemon-set-hrrdc is not available
    May 16 15:35:01.694: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:01.694: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:01.694: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:02.693: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:02.693: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:02.693: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:03.689: INFO: Pod daemon-set-429qp is not available
    May 16 15:35:03.693: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:03.693: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:03.693: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 05/16/23 15:35:03.693
    May 16 15:35:03.697: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:03.697: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:03.697: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:03.699: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    May 16 15:35:03.699: INFO: Node ip-10-0-161-164.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 15:35:04.704: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:04.704: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:04.704: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:04.707: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May 16 15:35:04.707: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 05/16/23 15:35:04.719
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3789, will wait for the garbage collector to delete the pods 05/16/23 15:35:04.719
    May 16 15:35:04.778: INFO: Deleting DaemonSet.extensions daemon-set took: 5.692306ms
    May 16 15:35:04.878: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.425998ms
    May 16 15:35:07.481: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 15:35:07.481: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    May 16 15:35:07.484: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"98273"},"items":null}

    May 16 15:35:07.486: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"98273"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 15:35:07.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3789" for this suite. 05/16/23 15:35:07.503
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:35:07.509
May 16 15:35:07.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubectl 05/16/23 15:35:07.509
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:35:07.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:35:07.526
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 05/16/23 15:35:07.527
May 16 15:35:07.527: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6861 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 05/16/23 15:35:07.558
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 16 15:35:07.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6861" for this suite. 05/16/23 15:35:07.57
------------------------------
â€¢ [0.069 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:35:07.509
    May 16 15:35:07.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubectl 05/16/23 15:35:07.509
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:35:07.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:35:07.526
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 05/16/23 15:35:07.527
    May 16 15:35:07.527: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-6861 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 05/16/23 15:35:07.558
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 16 15:35:07.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6861" for this suite. 05/16/23 15:35:07.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:35:07.578
May 16 15:35:07.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename configmap 05/16/23 15:35:07.579
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:35:07.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:35:07.596
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 16 15:35:07.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8000" for this suite. 05/16/23 15:35:07.648
------------------------------
â€¢ [0.075 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:35:07.578
    May 16 15:35:07.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename configmap 05/16/23 15:35:07.579
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:35:07.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:35:07.596
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 16 15:35:07.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8000" for this suite. 05/16/23 15:35:07.648
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:35:07.654
May 16 15:35:07.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename kubectl 05/16/23 15:35:07.655
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:35:07.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:35:07.671
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
May 16 15:35:07.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3839 create -f -'
May 16 15:35:09.192: INFO: stderr: ""
May 16 15:35:09.192: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May 16 15:35:09.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3839 create -f -'
May 16 15:35:09.450: INFO: stderr: ""
May 16 15:35:09.450: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 05/16/23 15:35:09.45
May 16 15:35:10.454: INFO: Selector matched 1 pods for map[app:agnhost]
May 16 15:35:10.454: INFO: Found 0 / 1
May 16 15:35:11.454: INFO: Selector matched 1 pods for map[app:agnhost]
May 16 15:35:11.454: INFO: Found 1 / 1
May 16 15:35:11.454: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 16 15:35:11.456: INFO: Selector matched 1 pods for map[app:agnhost]
May 16 15:35:11.456: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 16 15:35:11.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3839 describe pod agnhost-primary-5fqnv'
May 16 15:35:11.512: INFO: stderr: ""
May 16 15:35:11.512: INFO: stdout: "Name:             agnhost-primary-5fqnv\nNamespace:        kubectl-3839\nPriority:         0\nService Account:  default\nNode:             ip-10-0-161-164.eu-central-1.compute.internal/10.0.161.164\nStart Time:       Tue, 16 May 2023 15:35:09 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      k8s.ovn.org/pod-networks:\n                    {\"default\":{\"ip_addresses\":[\"10.131.1.119/23\"],\"mac_address\":\"0a:58:0a:83:01:77\",\"gateway_ips\":[\"10.131.0.1\"],\"ip_address\":\"10.131.1.119/2...\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"ovn-kubernetes\",\n                        \"interface\": \"eth0\",\n                        \"ips\": [\n                            \"10.131.1.119\"\n                        ],\n                        \"mac\": \"0a:58:0a:83:01:77\",\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               10.131.1.119\nIPs:\n  IP:           10.131.1.119\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://73ac5539936f989d18dbaf61353f5d687a7780eaf4ad5ffc14b9b276fe59b000\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 16 May 2023 15:35:09 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bvks2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-bvks2:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       2s    default-scheduler  Successfully assigned kubectl-3839/agnhost-primary-5fqnv to ip-10-0-161-164.eu-central-1.compute.internal\n  Normal  AddedInterface  2s    multus             Add eth0 [10.131.1.119/23] from ovn-kubernetes\n  Normal  Pulled          2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created         2s    kubelet            Created container agnhost-primary\n  Normal  Started         2s    kubelet            Started container agnhost-primary\n"
May 16 15:35:11.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3839 describe rc agnhost-primary'
May 16 15:35:11.571: INFO: stderr: ""
May 16 15:35:11.571: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3839\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-5fqnv\n"
May 16 15:35:11.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3839 describe service agnhost-primary'
May 16 15:35:11.625: INFO: stderr: ""
May 16 15:35:11.625: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3839\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.30.58.247\nIPs:               172.30.58.247\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.131.1.119:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 16 15:35:11.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3839 describe node ip-10-0-130-191.eu-central-1.compute.internal'
May 16 15:35:11.864: INFO: stderr: ""
May 16 15:35:11.864: INFO: stdout: "Name:               ip-10-0-130-191.eu-central-1.compute.internal\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m6i.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-central-1\n                    failure-domain.beta.kubernetes.io/zone=eu-central-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-130-191.eu-central-1.compute.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=m6i.xlarge\n                    node.openshift.io/os_id=rhcos\n                    topology.ebs.csi.aws.com/zone=eu-central-1a\n                    topology.kubernetes.io/region=eu-central-1\n                    topology.kubernetes.io/zone=eu-central-1a\nAnnotations:        cloud.network.openshift.io/egress-ipconfig:\n                      [{\"interface\":\"eni-0d1f6f16393e6ade8\",\"ifaddr\":{\"ipv4\":\"10.0.128.0/19\"},\"capacity\":{\"ipv4\":14,\"ipv6\":15}}]\n                    csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-0019ccbe880f73e89\"}\n                    k8s.ovn.org/host-addresses: [\"10.0.130.191\"]\n                    k8s.ovn.org/l3-gateway-config:\n                      {\"default\":{\"mode\":\"shared\",\"interface-id\":\"br-ex_ip-10-0-130-191.eu-central-1.compute.internal\",\"mac-address\":\"02:bc:45:9b:dd:e2\",\"ip-add...\n                    k8s.ovn.org/node-chassis-id: 0fef31fa-cbba-49ae-a42b-a643056dae7e\n                    k8s.ovn.org/node-gateway-router-lrp-ifaddr: {\"ipv4\":\"100.64.0.4/16\"}\n                    k8s.ovn.org/node-mgmt-port-mac-address: 06:0e:f2:e4:18:4f\n                    k8s.ovn.org/node-primary-ifaddr: {\"ipv4\":\"10.0.130.191/19\"}\n                    k8s.ovn.org/node-subnets: {\"default\":[\"10.128.0.0/23\"]}\n                    machine.openshift.io/machine: openshift-machine-api/maszulik-q2l4r-master-0\n                    machineconfiguration.openshift.io/controlPlaneTopology: HighlyAvailable\n                    machineconfiguration.openshift.io/currentConfig: rendered-master-457ddbf8eb6a8f091bbf9ce502c97f7c\n                    machineconfiguration.openshift.io/desiredConfig: rendered-master-457ddbf8eb6a8f091bbf9ce502c97f7c\n                    machineconfiguration.openshift.io/desiredDrain: uncordon-rendered-master-457ddbf8eb6a8f091bbf9ce502c97f7c\n                    machineconfiguration.openshift.io/lastAppliedDrain: uncordon-rendered-master-457ddbf8eb6a8f091bbf9ce502c97f7c\n                    machineconfiguration.openshift.io/lastSyncedControllerConfigResourceVersion: 18593\n                    machineconfiguration.openshift.io/reason: \n                    machineconfiguration.openshift.io/state: Done\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 16 May 2023 13:36:25 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-130-191.eu-central-1.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 16 May 2023 15:35:02 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 16 May 2023 15:30:57 +0000   Tue, 16 May 2023 13:36:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 16 May 2023 15:30:57 +0000   Tue, 16 May 2023 13:36:25 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 16 May 2023 15:30:57 +0000   Tue, 16 May 2023 13:36:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 16 May 2023 15:30:57 +0000   Tue, 16 May 2023 13:39:18 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.130.191\n  Hostname:     ip-10-0-130-191.eu-central-1.compute.internal\n  InternalDNS:  ip-10-0-130-191.eu-central-1.compute.internal\nCapacity:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         4\n  ephemeral-storage:           125238252Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      16105260Ki\n  pods:                        250\nAllocatable:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         3500m\n  ephemeral-storage:           114345831029\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      14954284Ki\n  pods:                        250\nSystem Info:\n  Machine ID:                                  ec278b5b79cf358634ed00af18306b4d\n  System UUID:                                 ec278b5b-79cf-3586-34ed-00af18306b4d\n  Boot ID:                                     287c19a1-d907-4be4-8bf0-cb369fa2a1f5\n  Kernel Version:                              5.14.0-284.13.1.el9_2.x86_64\n  OS Image:                                    Red Hat Enterprise Linux CoreOS 413.92.202305041429-0 (Plow)\n  Operating System:                            linux\n  Architecture:                                amd64\n  Container Runtime Version:                   cri-o://1.26.3-3.rhaos4.13.git641290e.el9\n  Kubelet Version:                             v1.26.3+b404935\n  Kube-Proxy Version:                          v1.26.3+b404935\nProviderID:                                    aws:///eu-central-1a/i-0019ccbe880f73e89\nNon-terminated Pods:                           (48 in total)\n  Namespace                                    Name                                                                            CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                                    ----                                                                            ------------  ----------  ---------------  -------------  ---\n  openshift-apiserver                          apiserver-755b9bd647-gmf98                                                      110m (3%)     0 (0%)      250Mi (1%)       0 (0%)         103m\n  openshift-authentication                     oauth-openshift-7d8578cbdd-mpchp                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         103m\n  openshift-cloud-controller-manager-operator  cluster-cloud-controller-manager-operator-5f64d86dc5-2csvl                      20m (0%)      0 (0%)      75Mi (0%)        0 (0%)         118m\n  openshift-cloud-credential-operator          pod-identity-webhook-84b6dfbf4-7w75m                                            10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         106m\n  openshift-cluster-csi-drivers                aws-ebs-csi-driver-controller-5445d6dd8d-qstvn                                  110m (3%)     0 (0%)      400Mi (2%)       0 (0%)         114m\n  openshift-cluster-csi-drivers                aws-ebs-csi-driver-node-w2zhj                                                   30m (0%)      0 (0%)      150Mi (1%)       0 (0%)         115m\n  openshift-cluster-csi-drivers                aws-ebs-csi-driver-operator-5f4f4f9c7-gmkdf                                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         115m\n  openshift-cluster-node-tuning-operator       tuned-4wvs6                                                                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         114m\n  openshift-cluster-samples-operator           cluster-samples-operator-5cf4fd8895-tx96x                                       20m (0%)      0 (0%)      100Mi (0%)       0 (0%)         113m\n  openshift-cluster-storage-operator           csi-snapshot-controller-757544fbf6-xsmk8                                        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         115m\n  openshift-cluster-storage-operator           csi-snapshot-webhook-68d6b9b485-bwf5b                                           10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         115m\n  openshift-console-operator                   console-operator-5ffb45d94f-cwgb5                                               20m (0%)      0 (0%)      200Mi (1%)       0 (0%)         107m\n  openshift-console                            console-59dc7954f-f76f5                                                         10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         99m\n  openshift-console                            downloads-8b57f44bb-pfvl7                                                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         107m\n  openshift-controller-manager                 controller-manager-8674b974c5-7qm6x                                             100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         102m\n  openshift-dns                                dns-default-bgz89                                                               60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         114m\n  openshift-dns                                node-resolver-gdg2w                                                             5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         114m\n  openshift-etcd                               etcd-guard-ip-10-0-130-191.eu-central-1.compute.internal                        10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         111m\n  openshift-etcd                               etcd-ip-10-0-130-191.eu-central-1.compute.internal                              360m (10%)    0 (0%)      910Mi (6%)       0 (0%)         95m\n  openshift-image-registry                     node-ca-fck2q                                                                   10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         104m\n  openshift-kube-apiserver                     kube-apiserver-guard-ip-10-0-130-191.eu-central-1.compute.internal              10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         100m\n  openshift-kube-apiserver                     kube-apiserver-ip-10-0-130-191.eu-central-1.compute.internal                    290m (8%)     0 (0%)      1224Mi (8%)      0 (0%)         100m\n  openshift-kube-controller-manager            kube-controller-manager-guard-ip-10-0-130-191.eu-central-1.compute.internal     10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         112m\n  openshift-kube-controller-manager            kube-controller-manager-ip-10-0-130-191.eu-central-1.compute.internal           80m (2%)      0 (0%)      500Mi (3%)       0 (0%)         98m\n  openshift-kube-scheduler                     openshift-kube-scheduler-guard-ip-10-0-130-191.eu-central-1.compute.internal    10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         110m\n  openshift-kube-scheduler                     openshift-kube-scheduler-ip-10-0-130-191.eu-central-1.compute.internal          25m (0%)      0 (0%)      150Mi (1%)       0 (0%)         99m\n  openshift-machine-config-operator            machine-config-controller-ff46499b5-nb9zd                                       40m (1%)      0 (0%)      100Mi (0%)       0 (0%)         114m\n  openshift-machine-config-operator            machine-config-daemon-bgjwz                                                     40m (1%)      0 (0%)      100Mi (0%)       0 (0%)         115m\n  openshift-machine-config-operator            machine-config-server-2f4km                                                     20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         114m\n  openshift-marketplace                        certified-operators-wr82c                                                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         114m\n  openshift-marketplace                        community-operators-82hfp                                                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         114m\n  openshift-marketplace                        redhat-marketplace-ngjnw                                                        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         114m\n  openshift-marketplace                        redhat-operators-2vzvl                                                          10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         114m\n  openshift-monitoring                         node-exporter-6m7l9                                                             9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         101m\n  openshift-monitoring                         prometheus-operator-b87854fcc-pbbmf                                             6m (0%)       0 (0%)      165Mi (1%)       0 (0%)         101m\n  openshift-multus                             multus-457lh                                                                    10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         117m\n  openshift-multus                             multus-additional-cni-plugins-4zclc                                             10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         117m\n  openshift-multus                             multus-admission-controller-6d4ff7c98c-rlfwv                                    20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         114m\n  openshift-multus                             network-metrics-daemon-ch4tw                                                    20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         117m\n  openshift-network-diagnostics                network-check-target-7lnvz                                                      10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         117m\n  openshift-network-operator                   network-operator-85d54f8d5-b9hfn                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         118m\n  openshift-oauth-apiserver                    apiserver-5695b84654-zgn4p                                                      150m (4%)     0 (0%)      200Mi (1%)       0 (0%)         111m\n  openshift-operator-lifecycle-manager         packageserver-57446f76-xcf25                                                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         115m\n  openshift-ovn-kubernetes                     ovnkube-master-q2pq9                                                            60m (1%)      0 (0%)      1520Mi (10%)     0 (0%)         117m\n  openshift-ovn-kubernetes                     ovnkube-node-4qc9n                                                              50m (1%)      0 (0%)      660Mi (4%)       0 (0%)         117m\n  openshift-route-controller-manager           route-controller-manager-7c5ccc4b76-fpcvf                                       100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         103m\n  openshift-service-ca                         service-ca-7f49b8c7f5-9sk7n                                                     10m (0%)      0 (0%)      120Mi (0%)       0 (0%)         115m\n  sonobuoy                                     sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-jtkqf                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         83m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests      Limits\n  --------                    --------      ------\n  cpu                         1975m (56%)   0 (0%)\n  memory                      8242Mi (56%)  0 (0%)\n  ephemeral-storage           0 (0%)        0 (0%)\n  hugepages-1Gi               0 (0%)        0 (0%)\n  hugepages-2Mi               0 (0%)        0 (0%)\n  attachable-volumes-aws-ebs  0             0\nEvents:\n  Type     Reason                     Age                  From                 Message\n  ----     ------                     ----                 ----                 -------\n  Normal   RegisteredNode             118m                 node-controller      Node ip-10-0-130-191.eu-central-1.compute.internal event: Registered Node ip-10-0-130-191.eu-central-1.compute.internal in Controller\n  Warning  ErrorReconcilingNode       116m (x2 over 116m)  controlplane         [k8s.ovn.org/node-chassis-id annotation not found for node ip-10-0-130-191.eu-central-1.compute.internal, macAddress annotation not found for node \"ip-10-0-130-191.eu-central-1.compute.internal\" , k8s.ovn.org/l3-gateway-config annotation not found for node \"ip-10-0-130-191.eu-central-1.compute.internal\"]\n  Warning  ErrorReconcilingNode       116m                 controlplane         error creating gateway for node ip-10-0-130-191.eu-central-1.compute.internal: failed to init shared interface gateway: failed to create MAC Binding for dummy nexthop ip-10-0-130-191.eu-central-1.compute.internal: error getting datapath GR_ip-10-0-130-191.eu-central-1.compute.internal: object not found\n  Normal   Uncordon                   113m                 machineconfigdaemon  Update completed for config rendered-master-a3bff413c5dbb6f6c620c1f844f7cbe5 and node has been uncordoned\n  Normal   NodeDone                   113m                 machineconfigdaemon  Setting node ip-10-0-130-191.eu-central-1.compute.internal, currentConfig rendered-master-a3bff413c5dbb6f6c620c1f844f7cbe5 to Done\n  Normal   ConfigDriftMonitorStarted  113m                 machineconfigdaemon  Config Drift Monitor started, watching against rendered-master-a3bff413c5dbb6f6c620c1f844f7cbe5\n  Normal   RegisteredNode             108m                 node-controller      Node ip-10-0-130-191.eu-central-1.compute.internal event: Registered Node ip-10-0-130-191.eu-central-1.compute.internal in Controller\n  Normal   RegisteredNode             107m                 node-controller      Node ip-10-0-130-191.eu-central-1.compute.internal event: Registered Node ip-10-0-130-191.eu-central-1.compute.internal in Controller\n  Normal   ConfigDriftMonitorStopped  103m                 machineconfigdaemon  Config Drift Monitor stopped\n  Normal   OSUpdateStarted            103m                 machineconfigdaemon  \n  Normal   OSUpdateStaged             103m                 machineconfigdaemon  Changes to OS staged\n  Normal   PendingConfig              103m                 machineconfigdaemon  Written pending config rendered-master-457ddbf8eb6a8f091bbf9ce502c97f7c\n  Normal   SkipReboot                 103m                 machineconfigdaemon  Config changes do not require reboot.\n  Normal   Uncordon                   103m                 machineconfigdaemon  Update completed for config rendered-master-457ddbf8eb6a8f091bbf9ce502c97f7c and node has been uncordoned\n  Normal   NodeDone                   103m                 machineconfigdaemon  Setting node ip-10-0-130-191.eu-central-1.compute.internal, currentConfig rendered-master-457ddbf8eb6a8f091bbf9ce502c97f7c to Done\n  Normal   ConfigDriftMonitorStarted  103m                 machineconfigdaemon  Config Drift Monitor started, watching against rendered-master-457ddbf8eb6a8f091bbf9ce502c97f7c\n  Normal   RegisteredNode             101m                 node-controller      Node ip-10-0-130-191.eu-central-1.compute.internal event: Registered Node ip-10-0-130-191.eu-central-1.compute.internal in Controller\n  Normal   RegisteredNode             99m                  node-controller      Node ip-10-0-130-191.eu-central-1.compute.internal event: Registered Node ip-10-0-130-191.eu-central-1.compute.internal in Controller\n  Normal   RegisteredNode             95m                  node-controller      Node ip-10-0-130-191.eu-central-1.compute.internal event: Registered Node ip-10-0-130-191.eu-central-1.compute.internal in Controller\n"
May 16 15:35:11.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3839 describe namespace kubectl-3839'
May 16 15:35:11.919: INFO: stderr: ""
May 16 15:35:11.919: INFO: stdout: "Name:         kubectl-3839\nLabels:       e2e-framework=kubectl\n              e2e-run=38fb2494-031f-4052-9538-25d04cdaece2\n              kubernetes.io/metadata.name=kubectl-3839\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c65,c55\n              openshift.io/sa.scc.supplemental-groups: 1004270000/10000\n              openshift.io/sa.scc.uid-range: 1004270000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
May 16 15:35:11.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3839" for this suite. 05/16/23 15:35:11.923
------------------------------
â€¢ [4.275 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:35:07.654
    May 16 15:35:07.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename kubectl 05/16/23 15:35:07.655
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:35:07.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:35:07.671
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    May 16 15:35:07.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3839 create -f -'
    May 16 15:35:09.192: INFO: stderr: ""
    May 16 15:35:09.192: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    May 16 15:35:09.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3839 create -f -'
    May 16 15:35:09.450: INFO: stderr: ""
    May 16 15:35:09.450: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 05/16/23 15:35:09.45
    May 16 15:35:10.454: INFO: Selector matched 1 pods for map[app:agnhost]
    May 16 15:35:10.454: INFO: Found 0 / 1
    May 16 15:35:11.454: INFO: Selector matched 1 pods for map[app:agnhost]
    May 16 15:35:11.454: INFO: Found 1 / 1
    May 16 15:35:11.454: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    May 16 15:35:11.456: INFO: Selector matched 1 pods for map[app:agnhost]
    May 16 15:35:11.456: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    May 16 15:35:11.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3839 describe pod agnhost-primary-5fqnv'
    May 16 15:35:11.512: INFO: stderr: ""
    May 16 15:35:11.512: INFO: stdout: "Name:             agnhost-primary-5fqnv\nNamespace:        kubectl-3839\nPriority:         0\nService Account:  default\nNode:             ip-10-0-161-164.eu-central-1.compute.internal/10.0.161.164\nStart Time:       Tue, 16 May 2023 15:35:09 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      k8s.ovn.org/pod-networks:\n                    {\"default\":{\"ip_addresses\":[\"10.131.1.119/23\"],\"mac_address\":\"0a:58:0a:83:01:77\",\"gateway_ips\":[\"10.131.0.1\"],\"ip_address\":\"10.131.1.119/2...\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"ovn-kubernetes\",\n                        \"interface\": \"eth0\",\n                        \"ips\": [\n                            \"10.131.1.119\"\n                        ],\n                        \"mac\": \"0a:58:0a:83:01:77\",\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               10.131.1.119\nIPs:\n  IP:           10.131.1.119\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://73ac5539936f989d18dbaf61353f5d687a7780eaf4ad5ffc14b9b276fe59b000\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 16 May 2023 15:35:09 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bvks2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-bvks2:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       2s    default-scheduler  Successfully assigned kubectl-3839/agnhost-primary-5fqnv to ip-10-0-161-164.eu-central-1.compute.internal\n  Normal  AddedInterface  2s    multus             Add eth0 [10.131.1.119/23] from ovn-kubernetes\n  Normal  Pulled          2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created         2s    kubelet            Created container agnhost-primary\n  Normal  Started         2s    kubelet            Started container agnhost-primary\n"
    May 16 15:35:11.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3839 describe rc agnhost-primary'
    May 16 15:35:11.571: INFO: stderr: ""
    May 16 15:35:11.571: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3839\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-5fqnv\n"
    May 16 15:35:11.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3839 describe service agnhost-primary'
    May 16 15:35:11.625: INFO: stderr: ""
    May 16 15:35:11.625: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3839\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.30.58.247\nIPs:               172.30.58.247\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.131.1.119:6379\nSession Affinity:  None\nEvents:            <none>\n"
    May 16 15:35:11.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3839 describe node ip-10-0-130-191.eu-central-1.compute.internal'
    May 16 15:35:11.864: INFO: stderr: ""
    May 16 15:35:11.864: INFO: stdout: "Name:               ip-10-0-130-191.eu-central-1.compute.internal\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m6i.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-central-1\n                    failure-domain.beta.kubernetes.io/zone=eu-central-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-130-191.eu-central-1.compute.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=m6i.xlarge\n                    node.openshift.io/os_id=rhcos\n                    topology.ebs.csi.aws.com/zone=eu-central-1a\n                    topology.kubernetes.io/region=eu-central-1\n                    topology.kubernetes.io/zone=eu-central-1a\nAnnotations:        cloud.network.openshift.io/egress-ipconfig:\n                      [{\"interface\":\"eni-0d1f6f16393e6ade8\",\"ifaddr\":{\"ipv4\":\"10.0.128.0/19\"},\"capacity\":{\"ipv4\":14,\"ipv6\":15}}]\n                    csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-0019ccbe880f73e89\"}\n                    k8s.ovn.org/host-addresses: [\"10.0.130.191\"]\n                    k8s.ovn.org/l3-gateway-config:\n                      {\"default\":{\"mode\":\"shared\",\"interface-id\":\"br-ex_ip-10-0-130-191.eu-central-1.compute.internal\",\"mac-address\":\"02:bc:45:9b:dd:e2\",\"ip-add...\n                    k8s.ovn.org/node-chassis-id: 0fef31fa-cbba-49ae-a42b-a643056dae7e\n                    k8s.ovn.org/node-gateway-router-lrp-ifaddr: {\"ipv4\":\"100.64.0.4/16\"}\n                    k8s.ovn.org/node-mgmt-port-mac-address: 06:0e:f2:e4:18:4f\n                    k8s.ovn.org/node-primary-ifaddr: {\"ipv4\":\"10.0.130.191/19\"}\n                    k8s.ovn.org/node-subnets: {\"default\":[\"10.128.0.0/23\"]}\n                    machine.openshift.io/machine: openshift-machine-api/maszulik-q2l4r-master-0\n                    machineconfiguration.openshift.io/controlPlaneTopology: HighlyAvailable\n                    machineconfiguration.openshift.io/currentConfig: rendered-master-457ddbf8eb6a8f091bbf9ce502c97f7c\n                    machineconfiguration.openshift.io/desiredConfig: rendered-master-457ddbf8eb6a8f091bbf9ce502c97f7c\n                    machineconfiguration.openshift.io/desiredDrain: uncordon-rendered-master-457ddbf8eb6a8f091bbf9ce502c97f7c\n                    machineconfiguration.openshift.io/lastAppliedDrain: uncordon-rendered-master-457ddbf8eb6a8f091bbf9ce502c97f7c\n                    machineconfiguration.openshift.io/lastSyncedControllerConfigResourceVersion: 18593\n                    machineconfiguration.openshift.io/reason: \n                    machineconfiguration.openshift.io/state: Done\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 16 May 2023 13:36:25 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-130-191.eu-central-1.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 16 May 2023 15:35:02 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 16 May 2023 15:30:57 +0000   Tue, 16 May 2023 13:36:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 16 May 2023 15:30:57 +0000   Tue, 16 May 2023 13:36:25 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 16 May 2023 15:30:57 +0000   Tue, 16 May 2023 13:36:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 16 May 2023 15:30:57 +0000   Tue, 16 May 2023 13:39:18 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.130.191\n  Hostname:     ip-10-0-130-191.eu-central-1.compute.internal\n  InternalDNS:  ip-10-0-130-191.eu-central-1.compute.internal\nCapacity:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         4\n  ephemeral-storage:           125238252Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      16105260Ki\n  pods:                        250\nAllocatable:\n  attachable-volumes-aws-ebs:  39\n  cpu:                         3500m\n  ephemeral-storage:           114345831029\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      14954284Ki\n  pods:                        250\nSystem Info:\n  Machine ID:                                  ec278b5b79cf358634ed00af18306b4d\n  System UUID:                                 ec278b5b-79cf-3586-34ed-00af18306b4d\n  Boot ID:                                     287c19a1-d907-4be4-8bf0-cb369fa2a1f5\n  Kernel Version:                              5.14.0-284.13.1.el9_2.x86_64\n  OS Image:                                    Red Hat Enterprise Linux CoreOS 413.92.202305041429-0 (Plow)\n  Operating System:                            linux\n  Architecture:                                amd64\n  Container Runtime Version:                   cri-o://1.26.3-3.rhaos4.13.git641290e.el9\n  Kubelet Version:                             v1.26.3+b404935\n  Kube-Proxy Version:                          v1.26.3+b404935\nProviderID:                                    aws:///eu-central-1a/i-0019ccbe880f73e89\nNon-terminated Pods:                           (48 in total)\n  Namespace                                    Name                                                                            CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                                    ----                                                                            ------------  ----------  ---------------  -------------  ---\n  openshift-apiserver                          apiserver-755b9bd647-gmf98                                                      110m (3%)     0 (0%)      250Mi (1%)       0 (0%)         103m\n  openshift-authentication                     oauth-openshift-7d8578cbdd-mpchp                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         103m\n  openshift-cloud-controller-manager-operator  cluster-cloud-controller-manager-operator-5f64d86dc5-2csvl                      20m (0%)      0 (0%)      75Mi (0%)        0 (0%)         118m\n  openshift-cloud-credential-operator          pod-identity-webhook-84b6dfbf4-7w75m                                            10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         106m\n  openshift-cluster-csi-drivers                aws-ebs-csi-driver-controller-5445d6dd8d-qstvn                                  110m (3%)     0 (0%)      400Mi (2%)       0 (0%)         114m\n  openshift-cluster-csi-drivers                aws-ebs-csi-driver-node-w2zhj                                                   30m (0%)      0 (0%)      150Mi (1%)       0 (0%)         115m\n  openshift-cluster-csi-drivers                aws-ebs-csi-driver-operator-5f4f4f9c7-gmkdf                                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         115m\n  openshift-cluster-node-tuning-operator       tuned-4wvs6                                                                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         114m\n  openshift-cluster-samples-operator           cluster-samples-operator-5cf4fd8895-tx96x                                       20m (0%)      0 (0%)      100Mi (0%)       0 (0%)         113m\n  openshift-cluster-storage-operator           csi-snapshot-controller-757544fbf6-xsmk8                                        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         115m\n  openshift-cluster-storage-operator           csi-snapshot-webhook-68d6b9b485-bwf5b                                           10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         115m\n  openshift-console-operator                   console-operator-5ffb45d94f-cwgb5                                               20m (0%)      0 (0%)      200Mi (1%)       0 (0%)         107m\n  openshift-console                            console-59dc7954f-f76f5                                                         10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         99m\n  openshift-console                            downloads-8b57f44bb-pfvl7                                                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         107m\n  openshift-controller-manager                 controller-manager-8674b974c5-7qm6x                                             100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         102m\n  openshift-dns                                dns-default-bgz89                                                               60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         114m\n  openshift-dns                                node-resolver-gdg2w                                                             5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         114m\n  openshift-etcd                               etcd-guard-ip-10-0-130-191.eu-central-1.compute.internal                        10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         111m\n  openshift-etcd                               etcd-ip-10-0-130-191.eu-central-1.compute.internal                              360m (10%)    0 (0%)      910Mi (6%)       0 (0%)         95m\n  openshift-image-registry                     node-ca-fck2q                                                                   10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         104m\n  openshift-kube-apiserver                     kube-apiserver-guard-ip-10-0-130-191.eu-central-1.compute.internal              10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         100m\n  openshift-kube-apiserver                     kube-apiserver-ip-10-0-130-191.eu-central-1.compute.internal                    290m (8%)     0 (0%)      1224Mi (8%)      0 (0%)         100m\n  openshift-kube-controller-manager            kube-controller-manager-guard-ip-10-0-130-191.eu-central-1.compute.internal     10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         112m\n  openshift-kube-controller-manager            kube-controller-manager-ip-10-0-130-191.eu-central-1.compute.internal           80m (2%)      0 (0%)      500Mi (3%)       0 (0%)         98m\n  openshift-kube-scheduler                     openshift-kube-scheduler-guard-ip-10-0-130-191.eu-central-1.compute.internal    10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         110m\n  openshift-kube-scheduler                     openshift-kube-scheduler-ip-10-0-130-191.eu-central-1.compute.internal          25m (0%)      0 (0%)      150Mi (1%)       0 (0%)         99m\n  openshift-machine-config-operator            machine-config-controller-ff46499b5-nb9zd                                       40m (1%)      0 (0%)      100Mi (0%)       0 (0%)         114m\n  openshift-machine-config-operator            machine-config-daemon-bgjwz                                                     40m (1%)      0 (0%)      100Mi (0%)       0 (0%)         115m\n  openshift-machine-config-operator            machine-config-server-2f4km                                                     20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         114m\n  openshift-marketplace                        certified-operators-wr82c                                                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         114m\n  openshift-marketplace                        community-operators-82hfp                                                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         114m\n  openshift-marketplace                        redhat-marketplace-ngjnw                                                        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         114m\n  openshift-marketplace                        redhat-operators-2vzvl                                                          10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         114m\n  openshift-monitoring                         node-exporter-6m7l9                                                             9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         101m\n  openshift-monitoring                         prometheus-operator-b87854fcc-pbbmf                                             6m (0%)       0 (0%)      165Mi (1%)       0 (0%)         101m\n  openshift-multus                             multus-457lh                                                                    10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         117m\n  openshift-multus                             multus-additional-cni-plugins-4zclc                                             10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         117m\n  openshift-multus                             multus-admission-controller-6d4ff7c98c-rlfwv                                    20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         114m\n  openshift-multus                             network-metrics-daemon-ch4tw                                                    20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         117m\n  openshift-network-diagnostics                network-check-target-7lnvz                                                      10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         117m\n  openshift-network-operator                   network-operator-85d54f8d5-b9hfn                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         118m\n  openshift-oauth-apiserver                    apiserver-5695b84654-zgn4p                                                      150m (4%)     0 (0%)      200Mi (1%)       0 (0%)         111m\n  openshift-operator-lifecycle-manager         packageserver-57446f76-xcf25                                                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         115m\n  openshift-ovn-kubernetes                     ovnkube-master-q2pq9                                                            60m (1%)      0 (0%)      1520Mi (10%)     0 (0%)         117m\n  openshift-ovn-kubernetes                     ovnkube-node-4qc9n                                                              50m (1%)      0 (0%)      660Mi (4%)       0 (0%)         117m\n  openshift-route-controller-manager           route-controller-manager-7c5ccc4b76-fpcvf                                       100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         103m\n  openshift-service-ca                         service-ca-7f49b8c7f5-9sk7n                                                     10m (0%)      0 (0%)      120Mi (0%)       0 (0%)         115m\n  sonobuoy                                     sonobuoy-systemd-logs-daemon-set-060f8d14b42446ac-jtkqf                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         83m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests      Limits\n  --------                    --------      ------\n  cpu                         1975m (56%)   0 (0%)\n  memory                      8242Mi (56%)  0 (0%)\n  ephemeral-storage           0 (0%)        0 (0%)\n  hugepages-1Gi               0 (0%)        0 (0%)\n  hugepages-2Mi               0 (0%)        0 (0%)\n  attachable-volumes-aws-ebs  0             0\nEvents:\n  Type     Reason                     Age                  From                 Message\n  ----     ------                     ----                 ----                 -------\n  Normal   RegisteredNode             118m                 node-controller      Node ip-10-0-130-191.eu-central-1.compute.internal event: Registered Node ip-10-0-130-191.eu-central-1.compute.internal in Controller\n  Warning  ErrorReconcilingNode       116m (x2 over 116m)  controlplane         [k8s.ovn.org/node-chassis-id annotation not found for node ip-10-0-130-191.eu-central-1.compute.internal, macAddress annotation not found for node \"ip-10-0-130-191.eu-central-1.compute.internal\" , k8s.ovn.org/l3-gateway-config annotation not found for node \"ip-10-0-130-191.eu-central-1.compute.internal\"]\n  Warning  ErrorReconcilingNode       116m                 controlplane         error creating gateway for node ip-10-0-130-191.eu-central-1.compute.internal: failed to init shared interface gateway: failed to create MAC Binding for dummy nexthop ip-10-0-130-191.eu-central-1.compute.internal: error getting datapath GR_ip-10-0-130-191.eu-central-1.compute.internal: object not found\n  Normal   Uncordon                   113m                 machineconfigdaemon  Update completed for config rendered-master-a3bff413c5dbb6f6c620c1f844f7cbe5 and node has been uncordoned\n  Normal   NodeDone                   113m                 machineconfigdaemon  Setting node ip-10-0-130-191.eu-central-1.compute.internal, currentConfig rendered-master-a3bff413c5dbb6f6c620c1f844f7cbe5 to Done\n  Normal   ConfigDriftMonitorStarted  113m                 machineconfigdaemon  Config Drift Monitor started, watching against rendered-master-a3bff413c5dbb6f6c620c1f844f7cbe5\n  Normal   RegisteredNode             108m                 node-controller      Node ip-10-0-130-191.eu-central-1.compute.internal event: Registered Node ip-10-0-130-191.eu-central-1.compute.internal in Controller\n  Normal   RegisteredNode             107m                 node-controller      Node ip-10-0-130-191.eu-central-1.compute.internal event: Registered Node ip-10-0-130-191.eu-central-1.compute.internal in Controller\n  Normal   ConfigDriftMonitorStopped  103m                 machineconfigdaemon  Config Drift Monitor stopped\n  Normal   OSUpdateStarted            103m                 machineconfigdaemon  \n  Normal   OSUpdateStaged             103m                 machineconfigdaemon  Changes to OS staged\n  Normal   PendingConfig              103m                 machineconfigdaemon  Written pending config rendered-master-457ddbf8eb6a8f091bbf9ce502c97f7c\n  Normal   SkipReboot                 103m                 machineconfigdaemon  Config changes do not require reboot.\n  Normal   Uncordon                   103m                 machineconfigdaemon  Update completed for config rendered-master-457ddbf8eb6a8f091bbf9ce502c97f7c and node has been uncordoned\n  Normal   NodeDone                   103m                 machineconfigdaemon  Setting node ip-10-0-130-191.eu-central-1.compute.internal, currentConfig rendered-master-457ddbf8eb6a8f091bbf9ce502c97f7c to Done\n  Normal   ConfigDriftMonitorStarted  103m                 machineconfigdaemon  Config Drift Monitor started, watching against rendered-master-457ddbf8eb6a8f091bbf9ce502c97f7c\n  Normal   RegisteredNode             101m                 node-controller      Node ip-10-0-130-191.eu-central-1.compute.internal event: Registered Node ip-10-0-130-191.eu-central-1.compute.internal in Controller\n  Normal   RegisteredNode             99m                  node-controller      Node ip-10-0-130-191.eu-central-1.compute.internal event: Registered Node ip-10-0-130-191.eu-central-1.compute.internal in Controller\n  Normal   RegisteredNode             95m                  node-controller      Node ip-10-0-130-191.eu-central-1.compute.internal event: Registered Node ip-10-0-130-191.eu-central-1.compute.internal in Controller\n"
    May 16 15:35:11.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=kubectl-3839 describe namespace kubectl-3839'
    May 16 15:35:11.919: INFO: stderr: ""
    May 16 15:35:11.919: INFO: stdout: "Name:         kubectl-3839\nLabels:       e2e-framework=kubectl\n              e2e-run=38fb2494-031f-4052-9538-25d04cdaece2\n              kubernetes.io/metadata.name=kubectl-3839\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c65,c55\n              openshift.io/sa.scc.supplemental-groups: 1004270000/10000\n              openshift.io/sa.scc.uid-range: 1004270000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    May 16 15:35:11.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3839" for this suite. 05/16/23 15:35:11.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:35:11.93
May 16 15:35:11.930: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename dns 05/16/23 15:35:11.931
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:35:11.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:35:11.945
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 05/16/23 15:35:11.947
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-169.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-169.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 05/16/23 15:35:11.953
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-169.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-169.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 05/16/23 15:35:11.953
STEP: creating a pod to probe DNS 05/16/23 15:35:11.953
STEP: submitting the pod to kubernetes 05/16/23 15:35:11.954
W0516 15:35:11.967598      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:35:11.967: INFO: Waiting up to 15m0s for pod "dns-test-6aae15d5-0306-4030-aba9-44edd44530be" in namespace "dns-169" to be "running"
May 16 15:35:11.972: INFO: Pod "dns-test-6aae15d5-0306-4030-aba9-44edd44530be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.797346ms
May 16 15:35:13.975: INFO: Pod "dns-test-6aae15d5-0306-4030-aba9-44edd44530be": Phase="Running", Reason="", readiness=true. Elapsed: 2.007866039s
May 16 15:35:13.975: INFO: Pod "dns-test-6aae15d5-0306-4030-aba9-44edd44530be" satisfied condition "running"
STEP: retrieving the pod 05/16/23 15:35:13.975
STEP: looking for the results for each expected name from probers 05/16/23 15:35:13.978
May 16 15:35:13.992: INFO: DNS probes using dns-169/dns-test-6aae15d5-0306-4030-aba9-44edd44530be succeeded

STEP: deleting the pod 05/16/23 15:35:13.992
STEP: deleting the test headless service 05/16/23 15:35:14.005
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
May 16 15:35:14.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-169" for this suite. 05/16/23 15:35:14.02
------------------------------
â€¢ [2.096 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:35:11.93
    May 16 15:35:11.930: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename dns 05/16/23 15:35:11.931
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:35:11.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:35:11.945
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 05/16/23 15:35:11.947
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-169.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-169.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     05/16/23 15:35:11.953
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-169.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-169.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     05/16/23 15:35:11.953
    STEP: creating a pod to probe DNS 05/16/23 15:35:11.953
    STEP: submitting the pod to kubernetes 05/16/23 15:35:11.954
    W0516 15:35:11.967598      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:35:11.967: INFO: Waiting up to 15m0s for pod "dns-test-6aae15d5-0306-4030-aba9-44edd44530be" in namespace "dns-169" to be "running"
    May 16 15:35:11.972: INFO: Pod "dns-test-6aae15d5-0306-4030-aba9-44edd44530be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.797346ms
    May 16 15:35:13.975: INFO: Pod "dns-test-6aae15d5-0306-4030-aba9-44edd44530be": Phase="Running", Reason="", readiness=true. Elapsed: 2.007866039s
    May 16 15:35:13.975: INFO: Pod "dns-test-6aae15d5-0306-4030-aba9-44edd44530be" satisfied condition "running"
    STEP: retrieving the pod 05/16/23 15:35:13.975
    STEP: looking for the results for each expected name from probers 05/16/23 15:35:13.978
    May 16 15:35:13.992: INFO: DNS probes using dns-169/dns-test-6aae15d5-0306-4030-aba9-44edd44530be succeeded

    STEP: deleting the pod 05/16/23 15:35:13.992
    STEP: deleting the test headless service 05/16/23 15:35:14.005
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    May 16 15:35:14.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-169" for this suite. 05/16/23 15:35:14.02
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:35:14.026
May 16 15:35:14.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename container-probe 05/16/23 15:35:14.027
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:35:14.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:35:14.046
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
May 16 15:35:15.061: INFO: Waiting up to 5m0s for pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e" in namespace "container-probe-8595" to be "running and ready"
May 16 15:35:15.063: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.751894ms
May 16 15:35:15.063: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Pending, waiting for it to be Running (with Ready = true)
May 16 15:35:17.067: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 2.006609542s
May 16 15:35:17.067: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
May 16 15:35:19.068: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 4.007439633s
May 16 15:35:19.068: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
May 16 15:35:21.068: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 6.006988813s
May 16 15:35:21.068: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
May 16 15:35:23.067: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 8.005892554s
May 16 15:35:23.067: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
May 16 15:35:25.078: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 10.017263414s
May 16 15:35:25.078: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
May 16 15:35:27.068: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 12.007020018s
May 16 15:35:27.068: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
May 16 15:35:29.066: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 14.005714956s
May 16 15:35:29.066: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
May 16 15:35:31.067: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 16.006585286s
May 16 15:35:31.067: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
May 16 15:35:33.069: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 18.00845203s
May 16 15:35:33.069: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
May 16 15:35:35.067: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 20.006800735s
May 16 15:35:35.067: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
May 16 15:35:37.067: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=true. Elapsed: 22.006682165s
May 16 15:35:37.067: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = true)
May 16 15:35:37.067: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e" satisfied condition "running and ready"
May 16 15:35:37.070: INFO: Container started at 2023-05-16 15:35:15 +0000 UTC, pod became ready at 2023-05-16 15:35:35 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
May 16 15:35:37.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8595" for this suite. 05/16/23 15:35:37.074
------------------------------
â€¢ [SLOW TEST] [23.053 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:35:14.026
    May 16 15:35:14.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename container-probe 05/16/23 15:35:14.027
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:35:14.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:35:14.046
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    May 16 15:35:15.061: INFO: Waiting up to 5m0s for pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e" in namespace "container-probe-8595" to be "running and ready"
    May 16 15:35:15.063: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.751894ms
    May 16 15:35:15.063: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:35:17.067: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 2.006609542s
    May 16 15:35:17.067: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
    May 16 15:35:19.068: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 4.007439633s
    May 16 15:35:19.068: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
    May 16 15:35:21.068: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 6.006988813s
    May 16 15:35:21.068: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
    May 16 15:35:23.067: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 8.005892554s
    May 16 15:35:23.067: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
    May 16 15:35:25.078: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 10.017263414s
    May 16 15:35:25.078: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
    May 16 15:35:27.068: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 12.007020018s
    May 16 15:35:27.068: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
    May 16 15:35:29.066: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 14.005714956s
    May 16 15:35:29.066: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
    May 16 15:35:31.067: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 16.006585286s
    May 16 15:35:31.067: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
    May 16 15:35:33.069: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 18.00845203s
    May 16 15:35:33.069: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
    May 16 15:35:35.067: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=false. Elapsed: 20.006800735s
    May 16 15:35:35.067: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = false)
    May 16 15:35:37.067: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e": Phase="Running", Reason="", readiness=true. Elapsed: 22.006682165s
    May 16 15:35:37.067: INFO: The phase of Pod test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e is Running (Ready = true)
    May 16 15:35:37.067: INFO: Pod "test-webserver-d93b1df4-9894-48bf-b3be-be276d4e6b3e" satisfied condition "running and ready"
    May 16 15:35:37.070: INFO: Container started at 2023-05-16 15:35:15 +0000 UTC, pod became ready at 2023-05-16 15:35:35 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    May 16 15:35:37.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8595" for this suite. 05/16/23 15:35:37.074
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:35:37.08
May 16 15:35:37.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename secrets 05/16/23 15:35:37.08
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:35:37.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:35:37.098
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May 16 15:35:37.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2039" for this suite. 05/16/23 15:35:37.149
------------------------------
â€¢ [0.080 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:35:37.08
    May 16 15:35:37.080: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename secrets 05/16/23 15:35:37.08
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:35:37.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:35:37.098
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May 16 15:35:37.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2039" for this suite. 05/16/23 15:35:37.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:35:37.16
May 16 15:35:37.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename daemonsets 05/16/23 15:35:37.161
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:35:37.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:35:37.177
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 05/16/23 15:35:37.209
STEP: Check that daemon pods launch on every node of the cluster. 05/16/23 15:35:37.217
May 16 15:35:37.224: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:37.224: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:37.224: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:37.230: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 16 15:35:37.230: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 15:35:38.235: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:38.235: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:38.235: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:38.238: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 16 15:35:38.238: INFO: Node ip-10-0-161-164.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 15:35:39.234: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:39.234: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:39.235: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:35:39.237: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 16 15:35:39.237: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 05/16/23 15:35:39.24
STEP: DeleteCollection of the DaemonSets 05/16/23 15:35:39.245
STEP: Verify that ReplicaSets have been deleted 05/16/23 15:35:39.252
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
May 16 15:35:39.267: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"98957"},"items":null}

May 16 15:35:39.270: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"98959"},"items":[{"metadata":{"name":"daemon-set-7snw9","generateName":"daemon-set-","namespace":"daemonsets-7762","uid":"3a6f4cdf-ef96-4c6d-b312-af2f47e4d494","resourceVersion":"98956","creationTimestamp":"2023-05-16T15:35:37Z","deletionTimestamp":"2023-05-16T15:36:09Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.131.1.122/23\"],\"mac_address\":\"0a:58:0a:83:01:7a\",\"gateway_ips\":[\"10.131.0.1\"],\"ip_address\":\"10.131.1.122/23\",\"gateway_ip\":\"10.131.0.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.1.122\"\n    ],\n    \"mac\": \"0a:58:0a:83:01:7a\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6ef47846-b0c4-4921-88b6-2c278bc6da5e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-130-191","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ef47846-b0c4-4921-88b6-2c278bc6da5e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-v6hfz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-v6hfz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-161-164.eu-central-1.compute.internal","securityContext":{"seLinuxOptions":{"level":"s0:c66,c10"}},"imagePullSecrets":[{"name":"default-dockercfg-plhmn"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-161-164.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:37Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:38Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:38Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:37Z"}],"hostIP":"10.0.161.164","podIP":"10.131.1.122","podIPs":[{"ip":"10.131.1.122"}],"startTime":"2023-05-16T15:35:37Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-16T15:35:37Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://037f2d10c419449a714cb90d85d9312ffcdaca36ebed87f43ffbdbc117ee3399","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-88t64","generateName":"daemon-set-","namespace":"daemonsets-7762","uid":"310ae335-0ebf-4428-a74d-eb7df21e2fcd","resourceVersion":"98959","creationTimestamp":"2023-05-16T15:35:37Z","deletionTimestamp":"2023-05-16T15:36:09Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.2.180/23\"],\"mac_address\":\"0a:58:0a:80:02:b4\",\"gateway_ips\":[\"10.128.2.1\"],\"ip_address\":\"10.128.2.180/23\",\"gateway_ip\":\"10.128.2.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.2.180\"\n    ],\n    \"mac\": \"0a:58:0a:80:02:b4\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6ef47846-b0c4-4921-88b6-2c278bc6da5e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-130-191","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ef47846-b0c4-4921-88b6-2c278bc6da5e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-72wqs","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-72wqs","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-132-142.eu-central-1.compute.internal","securityContext":{"seLinuxOptions":{"level":"s0:c66,c10"}},"imagePullSecrets":[{"name":"default-dockercfg-plhmn"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-132-142.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:37Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:38Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:38Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:37Z"}],"hostIP":"10.0.132.142","podIP":"10.128.2.180","podIPs":[{"ip":"10.128.2.180"}],"startTime":"2023-05-16T15:35:37Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-16T15:35:37Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://4dacfc42b0df8960f0cb2d49e6a46732cf96a6cbde3d28d19f4330b8ee48e94f","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hlbzn","generateName":"daemon-set-","namespace":"daemonsets-7762","uid":"82f7af3b-576d-4301-ab2f-498802dd4da7","resourceVersion":"98957","creationTimestamp":"2023-05-16T15:35:37Z","deletionTimestamp":"2023-05-16T15:36:09Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.129.2.162/23\"],\"mac_address\":\"0a:58:0a:81:02:a2\",\"gateway_ips\":[\"10.129.2.1\"],\"ip_address\":\"10.129.2.162/23\",\"gateway_ip\":\"10.129.2.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.129.2.162\"\n    ],\n    \"mac\": \"0a:58:0a:81:02:a2\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6ef47846-b0c4-4921-88b6-2c278bc6da5e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-130-191","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ef47846-b0c4-4921-88b6-2c278bc6da5e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-2gskh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-2gskh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-212-246.eu-central-1.compute.internal","securityContext":{"seLinuxOptions":{"level":"s0:c66,c10"}},"imagePullSecrets":[{"name":"default-dockercfg-plhmn"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-212-246.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:37Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:38Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:38Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:37Z"}],"hostIP":"10.0.212.246","podIP":"10.129.2.162","podIPs":[{"ip":"10.129.2.162"}],"startTime":"2023-05-16T15:35:37Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-16T15:35:38Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://166124ed473b60c2998aa4c4ef4542cfc2d91023fb47e1cabab69a4ff79dd5ab","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 15:35:39.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7762" for this suite. 05/16/23 15:35:39.284
------------------------------
â€¢ [2.130 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:35:37.16
    May 16 15:35:37.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename daemonsets 05/16/23 15:35:37.161
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:35:37.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:35:37.177
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 05/16/23 15:35:37.209
    STEP: Check that daemon pods launch on every node of the cluster. 05/16/23 15:35:37.217
    May 16 15:35:37.224: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:37.224: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:37.224: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:37.230: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    May 16 15:35:37.230: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 15:35:38.235: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:38.235: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:38.235: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:38.238: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    May 16 15:35:38.238: INFO: Node ip-10-0-161-164.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 15:35:39.234: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:39.234: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:39.235: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:35:39.237: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    May 16 15:35:39.237: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 05/16/23 15:35:39.24
    STEP: DeleteCollection of the DaemonSets 05/16/23 15:35:39.245
    STEP: Verify that ReplicaSets have been deleted 05/16/23 15:35:39.252
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    May 16 15:35:39.267: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"98957"},"items":null}

    May 16 15:35:39.270: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"98959"},"items":[{"metadata":{"name":"daemon-set-7snw9","generateName":"daemon-set-","namespace":"daemonsets-7762","uid":"3a6f4cdf-ef96-4c6d-b312-af2f47e4d494","resourceVersion":"98956","creationTimestamp":"2023-05-16T15:35:37Z","deletionTimestamp":"2023-05-16T15:36:09Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.131.1.122/23\"],\"mac_address\":\"0a:58:0a:83:01:7a\",\"gateway_ips\":[\"10.131.0.1\"],\"ip_address\":\"10.131.1.122/23\",\"gateway_ip\":\"10.131.0.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.131.1.122\"\n    ],\n    \"mac\": \"0a:58:0a:83:01:7a\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6ef47846-b0c4-4921-88b6-2c278bc6da5e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-130-191","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ef47846-b0c4-4921-88b6-2c278bc6da5e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-v6hfz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-v6hfz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-161-164.eu-central-1.compute.internal","securityContext":{"seLinuxOptions":{"level":"s0:c66,c10"}},"imagePullSecrets":[{"name":"default-dockercfg-plhmn"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-161-164.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:37Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:38Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:38Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:37Z"}],"hostIP":"10.0.161.164","podIP":"10.131.1.122","podIPs":[{"ip":"10.131.1.122"}],"startTime":"2023-05-16T15:35:37Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-16T15:35:37Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://037f2d10c419449a714cb90d85d9312ffcdaca36ebed87f43ffbdbc117ee3399","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-88t64","generateName":"daemon-set-","namespace":"daemonsets-7762","uid":"310ae335-0ebf-4428-a74d-eb7df21e2fcd","resourceVersion":"98959","creationTimestamp":"2023-05-16T15:35:37Z","deletionTimestamp":"2023-05-16T15:36:09Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.2.180/23\"],\"mac_address\":\"0a:58:0a:80:02:b4\",\"gateway_ips\":[\"10.128.2.1\"],\"ip_address\":\"10.128.2.180/23\",\"gateway_ip\":\"10.128.2.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.2.180\"\n    ],\n    \"mac\": \"0a:58:0a:80:02:b4\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6ef47846-b0c4-4921-88b6-2c278bc6da5e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-130-191","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ef47846-b0c4-4921-88b6-2c278bc6da5e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.2.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-72wqs","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-72wqs","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-132-142.eu-central-1.compute.internal","securityContext":{"seLinuxOptions":{"level":"s0:c66,c10"}},"imagePullSecrets":[{"name":"default-dockercfg-plhmn"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-132-142.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:37Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:38Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:38Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:37Z"}],"hostIP":"10.0.132.142","podIP":"10.128.2.180","podIPs":[{"ip":"10.128.2.180"}],"startTime":"2023-05-16T15:35:37Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-16T15:35:37Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://4dacfc42b0df8960f0cb2d49e6a46732cf96a6cbde3d28d19f4330b8ee48e94f","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hlbzn","generateName":"daemon-set-","namespace":"daemonsets-7762","uid":"82f7af3b-576d-4301-ab2f-498802dd4da7","resourceVersion":"98957","creationTimestamp":"2023-05-16T15:35:37Z","deletionTimestamp":"2023-05-16T15:36:09Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.129.2.162/23\"],\"mac_address\":\"0a:58:0a:81:02:a2\",\"gateway_ips\":[\"10.129.2.1\"],\"ip_address\":\"10.129.2.162/23\",\"gateway_ip\":\"10.129.2.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.129.2.162\"\n    ],\n    \"mac\": \"0a:58:0a:81:02:a2\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"6ef47846-b0c4-4921-88b6-2c278bc6da5e","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-130-191","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ef47846-b0c4-4921-88b6-2c278bc6da5e\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:37Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-16T15:35:38Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.129.2.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-2gskh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-2gskh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-212-246.eu-central-1.compute.internal","securityContext":{"seLinuxOptions":{"level":"s0:c66,c10"}},"imagePullSecrets":[{"name":"default-dockercfg-plhmn"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-212-246.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:37Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:38Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:38Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-16T15:35:37Z"}],"hostIP":"10.0.212.246","podIP":"10.129.2.162","podIPs":[{"ip":"10.129.2.162"}],"startTime":"2023-05-16T15:35:37Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-16T15:35:38Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://166124ed473b60c2998aa4c4ef4542cfc2d91023fb47e1cabab69a4ff79dd5ab","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 15:35:39.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7762" for this suite. 05/16/23 15:35:39.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:35:39.291
May 16 15:35:39.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename secrets 05/16/23 15:35:39.291
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:35:39.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:35:39.315
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-cacee9d7-88a9-449a-9d7f-aa1b2fed75dd 05/16/23 15:35:39.365
STEP: Creating a pod to test consume secrets 05/16/23 15:35:39.372
May 16 15:35:39.417: INFO: Waiting up to 5m0s for pod "pod-secrets-90671913-3e5d-4216-8c8b-d6c7c7d6c154" in namespace "secrets-3938" to be "Succeeded or Failed"
May 16 15:35:39.419: INFO: Pod "pod-secrets-90671913-3e5d-4216-8c8b-d6c7c7d6c154": Phase="Pending", Reason="", readiness=false. Elapsed: 2.191362ms
May 16 15:35:41.423: INFO: Pod "pod-secrets-90671913-3e5d-4216-8c8b-d6c7c7d6c154": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005855873s
May 16 15:35:43.424: INFO: Pod "pod-secrets-90671913-3e5d-4216-8c8b-d6c7c7d6c154": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0072068s
STEP: Saw pod success 05/16/23 15:35:43.424
May 16 15:35:43.424: INFO: Pod "pod-secrets-90671913-3e5d-4216-8c8b-d6c7c7d6c154" satisfied condition "Succeeded or Failed"
May 16 15:35:43.429: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-secrets-90671913-3e5d-4216-8c8b-d6c7c7d6c154 container secret-volume-test: <nil>
STEP: delete the pod 05/16/23 15:35:43.435
May 16 15:35:43.448: INFO: Waiting for pod pod-secrets-90671913-3e5d-4216-8c8b-d6c7c7d6c154 to disappear
May 16 15:35:43.450: INFO: Pod pod-secrets-90671913-3e5d-4216-8c8b-d6c7c7d6c154 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May 16 15:35:43.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3938" for this suite. 05/16/23 15:35:43.456
STEP: Destroying namespace "secret-namespace-1978" for this suite. 05/16/23 15:35:43.464
------------------------------
â€¢ [4.179 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:35:39.291
    May 16 15:35:39.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename secrets 05/16/23 15:35:39.291
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:35:39.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:35:39.315
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-cacee9d7-88a9-449a-9d7f-aa1b2fed75dd 05/16/23 15:35:39.365
    STEP: Creating a pod to test consume secrets 05/16/23 15:35:39.372
    May 16 15:35:39.417: INFO: Waiting up to 5m0s for pod "pod-secrets-90671913-3e5d-4216-8c8b-d6c7c7d6c154" in namespace "secrets-3938" to be "Succeeded or Failed"
    May 16 15:35:39.419: INFO: Pod "pod-secrets-90671913-3e5d-4216-8c8b-d6c7c7d6c154": Phase="Pending", Reason="", readiness=false. Elapsed: 2.191362ms
    May 16 15:35:41.423: INFO: Pod "pod-secrets-90671913-3e5d-4216-8c8b-d6c7c7d6c154": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005855873s
    May 16 15:35:43.424: INFO: Pod "pod-secrets-90671913-3e5d-4216-8c8b-d6c7c7d6c154": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0072068s
    STEP: Saw pod success 05/16/23 15:35:43.424
    May 16 15:35:43.424: INFO: Pod "pod-secrets-90671913-3e5d-4216-8c8b-d6c7c7d6c154" satisfied condition "Succeeded or Failed"
    May 16 15:35:43.429: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-secrets-90671913-3e5d-4216-8c8b-d6c7c7d6c154 container secret-volume-test: <nil>
    STEP: delete the pod 05/16/23 15:35:43.435
    May 16 15:35:43.448: INFO: Waiting for pod pod-secrets-90671913-3e5d-4216-8c8b-d6c7c7d6c154 to disappear
    May 16 15:35:43.450: INFO: Pod pod-secrets-90671913-3e5d-4216-8c8b-d6c7c7d6c154 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May 16 15:35:43.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3938" for this suite. 05/16/23 15:35:43.456
    STEP: Destroying namespace "secret-namespace-1978" for this suite. 05/16/23 15:35:43.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:35:43.47
May 16 15:35:43.470: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename deployment 05/16/23 15:35:43.471
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:35:43.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:35:43.487
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
W0516 15:35:44.495008      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:35:44.497: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 16 15:35:49.501: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 05/16/23 15:35:49.501
May 16 15:35:49.501: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 16 15:35:51.504: INFO: Creating deployment "test-rollover-deployment"
May 16 15:35:51.513: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 16 15:35:53.519: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 16 15:35:53.524: INFO: Ensure that both replica sets have 1 created replica
May 16 15:35:53.529: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 16 15:35:53.536: INFO: Updating deployment test-rollover-deployment
May 16 15:35:53.536: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 16 15:35:55.547: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 16 15:35:55.573: INFO: Make sure deployment "test-rollover-deployment" is complete
May 16 15:35:55.605: INFO: all replica sets need to contain the pod-template-hash label
May 16 15:35:55.605: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 15:35:57.611: INFO: all replica sets need to contain the pod-template-hash label
May 16 15:35:57.611: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 15:35:59.612: INFO: all replica sets need to contain the pod-template-hash label
May 16 15:35:59.612: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 15:36:01.610: INFO: all replica sets need to contain the pod-template-hash label
May 16 15:36:01.610: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 15:36:03.612: INFO: all replica sets need to contain the pod-template-hash label
May 16 15:36:03.612: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 16 15:36:05.612: INFO: 
May 16 15:36:05.612: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
May 16 15:36:05.620: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3467  cf52103d-aec3-4e6a-bf9e-8b58e33cd9f6 99448 2 2023-05-16 15:35:51 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-16 15:35:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:36:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008ccb678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-16 15:35:51 +0000 UTC,LastTransitionTime:2023-05-16 15:35:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-05-16 15:36:04 +0000 UTC,LastTransitionTime:2023-05-16 15:35:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 16 15:36:05.623: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-3467  2f3e415f-61b3-4539-9101-bfef67e357eb 99438 2 2023-05-16 15:35:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment cf52103d-aec3-4e6a-bf9e-8b58e33cd9f6 0xc009e0c657 0xc009e0c658}] [] [{kube-controller-manager Update apps/v1 2023-05-16 15:35:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf52103d-aec3-4e6a-bf9e-8b58e33cd9f6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:36:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009e0c708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 16 15:36:05.623: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 16 15:36:05.623: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3467  6b8c2210-c6a3-4798-ab7a-57ab5affd569 99447 2 2023-05-16 15:35:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment cf52103d-aec3-4e6a-bf9e-8b58e33cd9f6 0xc009e0c527 0xc009e0c528}] [] [{e2e.test Update apps/v1 2023-05-16 15:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:36:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf52103d-aec3-4e6a-bf9e-8b58e33cd9f6\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:36:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc009e0c5e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 16 15:36:05.623: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-3467  116c825a-0093-4d1e-80db-50d923f5d80e 99350 2 2023-05-16 15:35:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment cf52103d-aec3-4e6a-bf9e-8b58e33cd9f6 0xc009e0c777 0xc009e0c778}] [] [{kube-controller-manager Update apps/v1 2023-05-16 15:35:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf52103d-aec3-4e6a-bf9e-8b58e33cd9f6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:35:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009e0c828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 16 15:36:05.626: INFO: Pod "test-rollover-deployment-6c6df9974f-glw2k" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-glw2k test-rollover-deployment-6c6df9974f- deployment-3467  d271e8fe-905d-47b1-9028-cdaefc7a84f1 99367 0 2023-05-16 15:35:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.1.126/23"],"mac_address":"0a:58:0a:83:01:7e","gateway_ips":["10.131.0.1"],"ip_address":"10.131.1.126/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.131.1.126"
    ],
    "mac": "0a:58:0a:83:01:7e",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 2f3e415f-61b3-4539-9101-bfef67e357eb 0xc009e0ce37 0xc009e0ce38}] [] [{ip-10-0-130-191 Update v1 2023-05-16 15:35:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 15:35:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f3e415f-61b3-4539-9101-bfef67e357eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 15:35:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-05-16 15:35:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k8hlc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k8hlc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c66,c25,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gfc8g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:35:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:35:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:35:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:35:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:10.131.1.126,StartTime:2023-05-16 15:35:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 15:35:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://425005e3b0828dafbf89a2adec4807e2f832a991a1c1f9f3b8db4202d8c7c9b5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.1.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
May 16 15:36:05.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3467" for this suite. 05/16/23 15:36:05.629
------------------------------
â€¢ [SLOW TEST] [22.164 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:35:43.47
    May 16 15:35:43.470: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename deployment 05/16/23 15:35:43.471
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:35:43.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:35:43.487
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    W0516 15:35:44.495008      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:35:44.497: INFO: Pod name rollover-pod: Found 0 pods out of 1
    May 16 15:35:49.501: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 05/16/23 15:35:49.501
    May 16 15:35:49.501: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    May 16 15:35:51.504: INFO: Creating deployment "test-rollover-deployment"
    May 16 15:35:51.513: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    May 16 15:35:53.519: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    May 16 15:35:53.524: INFO: Ensure that both replica sets have 1 created replica
    May 16 15:35:53.529: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    May 16 15:35:53.536: INFO: Updating deployment test-rollover-deployment
    May 16 15:35:53.536: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    May 16 15:35:55.547: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    May 16 15:35:55.573: INFO: Make sure deployment "test-rollover-deployment" is complete
    May 16 15:35:55.605: INFO: all replica sets need to contain the pod-template-hash label
    May 16 15:35:55.605: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 16 15:35:57.611: INFO: all replica sets need to contain the pod-template-hash label
    May 16 15:35:57.611: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 16 15:35:59.612: INFO: all replica sets need to contain the pod-template-hash label
    May 16 15:35:59.612: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 16 15:36:01.610: INFO: all replica sets need to contain the pod-template-hash label
    May 16 15:36:01.610: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 16 15:36:03.612: INFO: all replica sets need to contain the pod-template-hash label
    May 16 15:36:03.612: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 16, 15, 35, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 16, 15, 35, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    May 16 15:36:05.612: INFO: 
    May 16 15:36:05.612: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    May 16 15:36:05.620: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-3467  cf52103d-aec3-4e6a-bf9e-8b58e33cd9f6 99448 2 2023-05-16 15:35:51 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-16 15:35:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:36:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc008ccb678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-16 15:35:51 +0000 UTC,LastTransitionTime:2023-05-16 15:35:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-05-16 15:36:04 +0000 UTC,LastTransitionTime:2023-05-16 15:35:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    May 16 15:36:05.623: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-3467  2f3e415f-61b3-4539-9101-bfef67e357eb 99438 2 2023-05-16 15:35:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment cf52103d-aec3-4e6a-bf9e-8b58e33cd9f6 0xc009e0c657 0xc009e0c658}] [] [{kube-controller-manager Update apps/v1 2023-05-16 15:35:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf52103d-aec3-4e6a-bf9e-8b58e33cd9f6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:36:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009e0c708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    May 16 15:36:05.623: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    May 16 15:36:05.623: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3467  6b8c2210-c6a3-4798-ab7a-57ab5affd569 99447 2 2023-05-16 15:35:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment cf52103d-aec3-4e6a-bf9e-8b58e33cd9f6 0xc009e0c527 0xc009e0c528}] [] [{e2e.test Update apps/v1 2023-05-16 15:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:36:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf52103d-aec3-4e6a-bf9e-8b58e33cd9f6\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:36:04 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc009e0c5e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May 16 15:36:05.623: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-3467  116c825a-0093-4d1e-80db-50d923f5d80e 99350 2 2023-05-16 15:35:51 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment cf52103d-aec3-4e6a-bf9e-8b58e33cd9f6 0xc009e0c777 0xc009e0c778}] [] [{kube-controller-manager Update apps/v1 2023-05-16 15:35:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf52103d-aec3-4e6a-bf9e-8b58e33cd9f6\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-16 15:35:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009e0c828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    May 16 15:36:05.626: INFO: Pod "test-rollover-deployment-6c6df9974f-glw2k" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-glw2k test-rollover-deployment-6c6df9974f- deployment-3467  d271e8fe-905d-47b1-9028-cdaefc7a84f1 99367 0 2023-05-16 15:35:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.131.1.126/23"],"mac_address":"0a:58:0a:83:01:7e","gateway_ips":["10.131.0.1"],"ip_address":"10.131.1.126/23","gateway_ip":"10.131.0.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.131.1.126"
        ],
        "mac": "0a:58:0a:83:01:7e",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 2f3e415f-61b3-4539-9101-bfef67e357eb 0xc009e0ce37 0xc009e0ce38}] [] [{ip-10-0-130-191 Update v1 2023-05-16 15:35:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-05-16 15:35:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f3e415f-61b3-4539-9101-bfef67e357eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-16 15:35:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.131.1.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-05-16 15:35:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k8hlc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k8hlc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-161-164.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c66,c25,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gfc8g,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:35:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:35:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:35:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-16 15:35:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.161.164,PodIP:10.131.1.126,StartTime:2023-05-16 15:35:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-16 15:35:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://425005e3b0828dafbf89a2adec4807e2f832a991a1c1f9f3b8db4202d8c7c9b5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.131.1.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    May 16 15:36:05.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3467" for this suite. 05/16/23 15:36:05.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:36:05.635
May 16 15:36:05.635: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename containers 05/16/23 15:36:05.636
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:36:05.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:36:05.65
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
May 16 15:36:06.668: INFO: Waiting up to 5m0s for pod "client-containers-372fb5d4-873c-4d17-9a79-4fb8a98c191e" in namespace "containers-4167" to be "running"
May 16 15:36:06.671: INFO: Pod "client-containers-372fb5d4-873c-4d17-9a79-4fb8a98c191e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.740498ms
May 16 15:36:08.675: INFO: Pod "client-containers-372fb5d4-873c-4d17-9a79-4fb8a98c191e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006825794s
May 16 15:36:08.675: INFO: Pod "client-containers-372fb5d4-873c-4d17-9a79-4fb8a98c191e" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
May 16 15:36:08.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4167" for this suite. 05/16/23 15:36:08.686
------------------------------
â€¢ [3.057 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:36:05.635
    May 16 15:36:05.635: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename containers 05/16/23 15:36:05.636
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:36:05.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:36:05.65
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    May 16 15:36:06.668: INFO: Waiting up to 5m0s for pod "client-containers-372fb5d4-873c-4d17-9a79-4fb8a98c191e" in namespace "containers-4167" to be "running"
    May 16 15:36:06.671: INFO: Pod "client-containers-372fb5d4-873c-4d17-9a79-4fb8a98c191e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.740498ms
    May 16 15:36:08.675: INFO: Pod "client-containers-372fb5d4-873c-4d17-9a79-4fb8a98c191e": Phase="Running", Reason="", readiness=true. Elapsed: 2.006825794s
    May 16 15:36:08.675: INFO: Pod "client-containers-372fb5d4-873c-4d17-9a79-4fb8a98c191e" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    May 16 15:36:08.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4167" for this suite. 05/16/23 15:36:08.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:36:08.693
May 16 15:36:08.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename emptydir 05/16/23 15:36:08.694
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:36:08.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:36:08.71
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 05/16/23 15:36:08.712
May 16 15:36:09.724: INFO: Waiting up to 5m0s for pod "pod-c060010b-aeb3-4cae-9dc1-b4b80855edfb" in namespace "emptydir-4311" to be "Succeeded or Failed"
May 16 15:36:09.727: INFO: Pod "pod-c060010b-aeb3-4cae-9dc1-b4b80855edfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.98371ms
May 16 15:36:11.731: INFO: Pod "pod-c060010b-aeb3-4cae-9dc1-b4b80855edfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006694099s
May 16 15:36:13.733: INFO: Pod "pod-c060010b-aeb3-4cae-9dc1-b4b80855edfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008716808s
STEP: Saw pod success 05/16/23 15:36:13.733
May 16 15:36:13.733: INFO: Pod "pod-c060010b-aeb3-4cae-9dc1-b4b80855edfb" satisfied condition "Succeeded or Failed"
May 16 15:36:13.735: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-c060010b-aeb3-4cae-9dc1-b4b80855edfb container test-container: <nil>
STEP: delete the pod 05/16/23 15:36:13.741
May 16 15:36:13.752: INFO: Waiting for pod pod-c060010b-aeb3-4cae-9dc1-b4b80855edfb to disappear
May 16 15:36:13.755: INFO: Pod pod-c060010b-aeb3-4cae-9dc1-b4b80855edfb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 16 15:36:13.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4311" for this suite. 05/16/23 15:36:13.758
------------------------------
â€¢ [SLOW TEST] [5.071 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:36:08.693
    May 16 15:36:08.693: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename emptydir 05/16/23 15:36:08.694
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:36:08.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:36:08.71
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 05/16/23 15:36:08.712
    May 16 15:36:09.724: INFO: Waiting up to 5m0s for pod "pod-c060010b-aeb3-4cae-9dc1-b4b80855edfb" in namespace "emptydir-4311" to be "Succeeded or Failed"
    May 16 15:36:09.727: INFO: Pod "pod-c060010b-aeb3-4cae-9dc1-b4b80855edfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.98371ms
    May 16 15:36:11.731: INFO: Pod "pod-c060010b-aeb3-4cae-9dc1-b4b80855edfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006694099s
    May 16 15:36:13.733: INFO: Pod "pod-c060010b-aeb3-4cae-9dc1-b4b80855edfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008716808s
    STEP: Saw pod success 05/16/23 15:36:13.733
    May 16 15:36:13.733: INFO: Pod "pod-c060010b-aeb3-4cae-9dc1-b4b80855edfb" satisfied condition "Succeeded or Failed"
    May 16 15:36:13.735: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-c060010b-aeb3-4cae-9dc1-b4b80855edfb container test-container: <nil>
    STEP: delete the pod 05/16/23 15:36:13.741
    May 16 15:36:13.752: INFO: Waiting for pod pod-c060010b-aeb3-4cae-9dc1-b4b80855edfb to disappear
    May 16 15:36:13.755: INFO: Pod pod-c060010b-aeb3-4cae-9dc1-b4b80855edfb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 16 15:36:13.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4311" for this suite. 05/16/23 15:36:13.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:36:13.764
May 16 15:36:13.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename downward-api 05/16/23 15:36:13.765
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:36:13.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:36:13.782
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 05/16/23 15:36:13.784
May 16 15:36:14.794: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3da82c8e-22ac-44a5-84c4-87d4d837dbf7" in namespace "downward-api-745" to be "Succeeded or Failed"
May 16 15:36:14.796: INFO: Pod "downwardapi-volume-3da82c8e-22ac-44a5-84c4-87d4d837dbf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.497975ms
May 16 15:36:16.799: INFO: Pod "downwardapi-volume-3da82c8e-22ac-44a5-84c4-87d4d837dbf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005576225s
May 16 15:36:18.807: INFO: Pod "downwardapi-volume-3da82c8e-22ac-44a5-84c4-87d4d837dbf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013299214s
STEP: Saw pod success 05/16/23 15:36:18.807
May 16 15:36:18.807: INFO: Pod "downwardapi-volume-3da82c8e-22ac-44a5-84c4-87d4d837dbf7" satisfied condition "Succeeded or Failed"
May 16 15:36:18.819: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-3da82c8e-22ac-44a5-84c4-87d4d837dbf7 container client-container: <nil>
STEP: delete the pod 05/16/23 15:36:18.825
May 16 15:36:18.856: INFO: Waiting for pod downwardapi-volume-3da82c8e-22ac-44a5-84c4-87d4d837dbf7 to disappear
May 16 15:36:18.859: INFO: Pod downwardapi-volume-3da82c8e-22ac-44a5-84c4-87d4d837dbf7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 16 15:36:18.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-745" for this suite. 05/16/23 15:36:18.863
------------------------------
â€¢ [SLOW TEST] [5.114 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:36:13.764
    May 16 15:36:13.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename downward-api 05/16/23 15:36:13.765
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:36:13.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:36:13.782
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 05/16/23 15:36:13.784
    May 16 15:36:14.794: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3da82c8e-22ac-44a5-84c4-87d4d837dbf7" in namespace "downward-api-745" to be "Succeeded or Failed"
    May 16 15:36:14.796: INFO: Pod "downwardapi-volume-3da82c8e-22ac-44a5-84c4-87d4d837dbf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.497975ms
    May 16 15:36:16.799: INFO: Pod "downwardapi-volume-3da82c8e-22ac-44a5-84c4-87d4d837dbf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005576225s
    May 16 15:36:18.807: INFO: Pod "downwardapi-volume-3da82c8e-22ac-44a5-84c4-87d4d837dbf7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013299214s
    STEP: Saw pod success 05/16/23 15:36:18.807
    May 16 15:36:18.807: INFO: Pod "downwardapi-volume-3da82c8e-22ac-44a5-84c4-87d4d837dbf7" satisfied condition "Succeeded or Failed"
    May 16 15:36:18.819: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-3da82c8e-22ac-44a5-84c4-87d4d837dbf7 container client-container: <nil>
    STEP: delete the pod 05/16/23 15:36:18.825
    May 16 15:36:18.856: INFO: Waiting for pod downwardapi-volume-3da82c8e-22ac-44a5-84c4-87d4d837dbf7 to disappear
    May 16 15:36:18.859: INFO: Pod downwardapi-volume-3da82c8e-22ac-44a5-84c4-87d4d837dbf7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 16 15:36:18.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-745" for this suite. 05/16/23 15:36:18.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:36:18.878
May 16 15:36:18.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename crd-publish-openapi 05/16/23 15:36:18.879
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:36:18.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:36:18.948
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
May 16 15:36:18.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/16/23 15:36:22.309
May 16 15:36:22.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-3244 --namespace=crd-publish-openapi-3244 create -f -'
May 16 15:36:23.455: INFO: stderr: ""
May 16 15:36:23.455: INFO: stdout: "e2e-test-crd-publish-openapi-3761-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 16 15:36:23.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-3244 --namespace=crd-publish-openapi-3244 delete e2e-test-crd-publish-openapi-3761-crds test-cr'
May 16 15:36:23.525: INFO: stderr: ""
May 16 15:36:23.525: INFO: stdout: "e2e-test-crd-publish-openapi-3761-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 16 15:36:23.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-3244 --namespace=crd-publish-openapi-3244 apply -f -'
May 16 15:36:24.469: INFO: stderr: ""
May 16 15:36:24.469: INFO: stdout: "e2e-test-crd-publish-openapi-3761-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 16 15:36:24.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-3244 --namespace=crd-publish-openapi-3244 delete e2e-test-crd-publish-openapi-3761-crds test-cr'
May 16 15:36:24.521: INFO: stderr: ""
May 16 15:36:24.521: INFO: stdout: "e2e-test-crd-publish-openapi-3761-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 05/16/23 15:36:24.521
May 16 15:36:24.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-3244 explain e2e-test-crd-publish-openapi-3761-crds'
May 16 15:36:25.428: INFO: stderr: ""
May 16 15:36:25.428: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3761-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 15:36:28.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3244" for this suite. 05/16/23 15:36:28.408
------------------------------
â€¢ [SLOW TEST] [9.534 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:36:18.878
    May 16 15:36:18.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename crd-publish-openapi 05/16/23 15:36:18.879
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:36:18.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:36:18.948
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    May 16 15:36:18.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 05/16/23 15:36:22.309
    May 16 15:36:22.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-3244 --namespace=crd-publish-openapi-3244 create -f -'
    May 16 15:36:23.455: INFO: stderr: ""
    May 16 15:36:23.455: INFO: stdout: "e2e-test-crd-publish-openapi-3761-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    May 16 15:36:23.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-3244 --namespace=crd-publish-openapi-3244 delete e2e-test-crd-publish-openapi-3761-crds test-cr'
    May 16 15:36:23.525: INFO: stderr: ""
    May 16 15:36:23.525: INFO: stdout: "e2e-test-crd-publish-openapi-3761-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    May 16 15:36:23.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-3244 --namespace=crd-publish-openapi-3244 apply -f -'
    May 16 15:36:24.469: INFO: stderr: ""
    May 16 15:36:24.469: INFO: stdout: "e2e-test-crd-publish-openapi-3761-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    May 16 15:36:24.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-3244 --namespace=crd-publish-openapi-3244 delete e2e-test-crd-publish-openapi-3761-crds test-cr'
    May 16 15:36:24.521: INFO: stderr: ""
    May 16 15:36:24.521: INFO: stdout: "e2e-test-crd-publish-openapi-3761-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 05/16/23 15:36:24.521
    May 16 15:36:24.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-3244 explain e2e-test-crd-publish-openapi-3761-crds'
    May 16 15:36:25.428: INFO: stderr: ""
    May 16 15:36:25.428: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3761-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 15:36:28.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3244" for this suite. 05/16/23 15:36:28.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:36:28.413
May 16 15:36:28.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename services 05/16/23 15:36:28.414
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:36:28.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:36:28.429
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 05/16/23 15:36:28.441
STEP: watching for the Service to be added 05/16/23 15:36:28.454
May 16 15:36:28.456: INFO: Found Service test-service-jbrdz in namespace services-7431 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
May 16 15:36:28.456: INFO: Service test-service-jbrdz created
STEP: Getting /status 05/16/23 15:36:28.456
May 16 15:36:28.461: INFO: Service test-service-jbrdz has LoadBalancer: {[]}
STEP: patching the ServiceStatus 05/16/23 15:36:28.461
STEP: watching for the Service to be patched 05/16/23 15:36:28.47
May 16 15:36:28.471: INFO: observed Service test-service-jbrdz in namespace services-7431 with annotations: map[] & LoadBalancer: {[]}
May 16 15:36:28.471: INFO: Found Service test-service-jbrdz in namespace services-7431 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
May 16 15:36:28.471: INFO: Service test-service-jbrdz has service status patched
STEP: updating the ServiceStatus 05/16/23 15:36:28.471
May 16 15:36:28.489: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 05/16/23 15:36:28.489
May 16 15:36:28.490: INFO: Observed Service test-service-jbrdz in namespace services-7431 with annotations: map[] & Conditions: {[]}
May 16 15:36:28.490: INFO: Observed event: &Service{ObjectMeta:{test-service-jbrdz  services-7431  18284aa5-beb6-41ab-836a-757b426b4a18 99919 0 2023-05-16 15:36:28 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-16 15:36:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-16 15:36:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.30.180.33,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.30.180.33],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
May 16 15:36:28.490: INFO: Found Service test-service-jbrdz in namespace services-7431 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 16 15:36:28.490: INFO: Service test-service-jbrdz has service status updated
STEP: patching the service 05/16/23 15:36:28.49
STEP: watching for the Service to be patched 05/16/23 15:36:28.504
May 16 15:36:28.506: INFO: observed Service test-service-jbrdz in namespace services-7431 with labels: map[test-service-static:true]
May 16 15:36:28.506: INFO: observed Service test-service-jbrdz in namespace services-7431 with labels: map[test-service-static:true]
May 16 15:36:28.506: INFO: observed Service test-service-jbrdz in namespace services-7431 with labels: map[test-service-static:true]
May 16 15:36:28.506: INFO: Found Service test-service-jbrdz in namespace services-7431 with labels: map[test-service:patched test-service-static:true]
May 16 15:36:28.506: INFO: Service test-service-jbrdz patched
STEP: deleting the service 05/16/23 15:36:28.506
STEP: watching for the Service to be deleted 05/16/23 15:36:28.521
May 16 15:36:28.521: INFO: Observed event: ADDED
May 16 15:36:28.521: INFO: Observed event: MODIFIED
May 16 15:36:28.521: INFO: Observed event: MODIFIED
May 16 15:36:28.522: INFO: Observed event: MODIFIED
May 16 15:36:28.522: INFO: Found Service test-service-jbrdz in namespace services-7431 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
May 16 15:36:28.522: INFO: Service test-service-jbrdz deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 16 15:36:28.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7431" for this suite. 05/16/23 15:36:28.527
------------------------------
â€¢ [0.121 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:36:28.413
    May 16 15:36:28.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename services 05/16/23 15:36:28.414
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:36:28.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:36:28.429
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 05/16/23 15:36:28.441
    STEP: watching for the Service to be added 05/16/23 15:36:28.454
    May 16 15:36:28.456: INFO: Found Service test-service-jbrdz in namespace services-7431 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    May 16 15:36:28.456: INFO: Service test-service-jbrdz created
    STEP: Getting /status 05/16/23 15:36:28.456
    May 16 15:36:28.461: INFO: Service test-service-jbrdz has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 05/16/23 15:36:28.461
    STEP: watching for the Service to be patched 05/16/23 15:36:28.47
    May 16 15:36:28.471: INFO: observed Service test-service-jbrdz in namespace services-7431 with annotations: map[] & LoadBalancer: {[]}
    May 16 15:36:28.471: INFO: Found Service test-service-jbrdz in namespace services-7431 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    May 16 15:36:28.471: INFO: Service test-service-jbrdz has service status patched
    STEP: updating the ServiceStatus 05/16/23 15:36:28.471
    May 16 15:36:28.489: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 05/16/23 15:36:28.489
    May 16 15:36:28.490: INFO: Observed Service test-service-jbrdz in namespace services-7431 with annotations: map[] & Conditions: {[]}
    May 16 15:36:28.490: INFO: Observed event: &Service{ObjectMeta:{test-service-jbrdz  services-7431  18284aa5-beb6-41ab-836a-757b426b4a18 99919 0 2023-05-16 15:36:28 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-16 15:36:28 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-16 15:36:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.30.180.33,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.30.180.33],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    May 16 15:36:28.490: INFO: Found Service test-service-jbrdz in namespace services-7431 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    May 16 15:36:28.490: INFO: Service test-service-jbrdz has service status updated
    STEP: patching the service 05/16/23 15:36:28.49
    STEP: watching for the Service to be patched 05/16/23 15:36:28.504
    May 16 15:36:28.506: INFO: observed Service test-service-jbrdz in namespace services-7431 with labels: map[test-service-static:true]
    May 16 15:36:28.506: INFO: observed Service test-service-jbrdz in namespace services-7431 with labels: map[test-service-static:true]
    May 16 15:36:28.506: INFO: observed Service test-service-jbrdz in namespace services-7431 with labels: map[test-service-static:true]
    May 16 15:36:28.506: INFO: Found Service test-service-jbrdz in namespace services-7431 with labels: map[test-service:patched test-service-static:true]
    May 16 15:36:28.506: INFO: Service test-service-jbrdz patched
    STEP: deleting the service 05/16/23 15:36:28.506
    STEP: watching for the Service to be deleted 05/16/23 15:36:28.521
    May 16 15:36:28.521: INFO: Observed event: ADDED
    May 16 15:36:28.521: INFO: Observed event: MODIFIED
    May 16 15:36:28.521: INFO: Observed event: MODIFIED
    May 16 15:36:28.522: INFO: Observed event: MODIFIED
    May 16 15:36:28.522: INFO: Found Service test-service-jbrdz in namespace services-7431 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    May 16 15:36:28.522: INFO: Service test-service-jbrdz deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 16 15:36:28.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7431" for this suite. 05/16/23 15:36:28.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:36:28.535
May 16 15:36:28.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename replication-controller 05/16/23 15:36:28.541
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:36:28.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:36:28.567
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
May 16 15:36:28.570: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 05/16/23 15:36:29.594
STEP: Checking rc "condition-test" has the desired failure condition set 05/16/23 15:36:29.598
STEP: Scaling down rc "condition-test" to satisfy pod quota 05/16/23 15:36:30.604
May 16 15:36:30.613: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 05/16/23 15:36:30.613
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
May 16 15:36:31.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-574" for this suite. 05/16/23 15:36:31.622
------------------------------
â€¢ [3.093 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:36:28.535
    May 16 15:36:28.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename replication-controller 05/16/23 15:36:28.541
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:36:28.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:36:28.567
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    May 16 15:36:28.570: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 05/16/23 15:36:29.594
    STEP: Checking rc "condition-test" has the desired failure condition set 05/16/23 15:36:29.598
    STEP: Scaling down rc "condition-test" to satisfy pod quota 05/16/23 15:36:30.604
    May 16 15:36:30.613: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 05/16/23 15:36:30.613
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    May 16 15:36:31.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-574" for this suite. 05/16/23 15:36:31.622
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:36:31.628
May 16 15:36:31.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename sched-preemption 05/16/23 15:36:31.628
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:36:31.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:36:31.644
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
May 16 15:36:31.663: INFO: Waiting up to 1m0s for all nodes to be ready
May 16 15:37:31.768: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 05/16/23 15:37:31.773
May 16 15:37:31.803: INFO: Created pod: pod0-0-sched-preemption-low-priority
May 16 15:37:31.810: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May 16 15:37:31.837: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May 16 15:37:31.865: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May 16 15:37:31.892: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May 16 15:37:31.906: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 05/16/23 15:37:31.906
May 16 15:37:31.906: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3906" to be "running"
May 16 15:37:31.912: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.686408ms
May 16 15:37:33.916: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009649996s
May 16 15:37:33.916: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
May 16 15:37:33.916: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3906" to be "running"
May 16 15:37:33.918: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.599836ms
May 16 15:37:33.918: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
May 16 15:37:33.918: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3906" to be "running"
May 16 15:37:33.921: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.477768ms
May 16 15:37:33.921: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
May 16 15:37:33.921: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3906" to be "running"
May 16 15:37:33.923: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.765643ms
May 16 15:37:33.923: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
May 16 15:37:33.923: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3906" to be "running"
May 16 15:37:33.926: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.222214ms
May 16 15:37:33.926: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
May 16 15:37:33.926: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3906" to be "running"
May 16 15:37:33.928: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.155322ms
May 16 15:37:33.928: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 05/16/23 15:37:33.928
May 16 15:37:33.937: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
May 16 15:37:33.940: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.079675ms
May 16 15:37:35.944: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00697169s
May 16 15:37:37.946: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008421329s
May 16 15:37:39.943: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006159286s
May 16 15:37:39.944: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 15:37:39.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3906" for this suite. 05/16/23 15:37:40.016
------------------------------
â€¢ [SLOW TEST] [68.394 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:36:31.628
    May 16 15:36:31.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename sched-preemption 05/16/23 15:36:31.628
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:36:31.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:36:31.644
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    May 16 15:36:31.663: INFO: Waiting up to 1m0s for all nodes to be ready
    May 16 15:37:31.768: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 05/16/23 15:37:31.773
    May 16 15:37:31.803: INFO: Created pod: pod0-0-sched-preemption-low-priority
    May 16 15:37:31.810: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    May 16 15:37:31.837: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    May 16 15:37:31.865: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    May 16 15:37:31.892: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    May 16 15:37:31.906: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 05/16/23 15:37:31.906
    May 16 15:37:31.906: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3906" to be "running"
    May 16 15:37:31.912: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.686408ms
    May 16 15:37:33.916: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009649996s
    May 16 15:37:33.916: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    May 16 15:37:33.916: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3906" to be "running"
    May 16 15:37:33.918: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.599836ms
    May 16 15:37:33.918: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    May 16 15:37:33.918: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3906" to be "running"
    May 16 15:37:33.921: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.477768ms
    May 16 15:37:33.921: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    May 16 15:37:33.921: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3906" to be "running"
    May 16 15:37:33.923: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.765643ms
    May 16 15:37:33.923: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    May 16 15:37:33.923: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3906" to be "running"
    May 16 15:37:33.926: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.222214ms
    May 16 15:37:33.926: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    May 16 15:37:33.926: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3906" to be "running"
    May 16 15:37:33.928: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.155322ms
    May 16 15:37:33.928: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 05/16/23 15:37:33.928
    May 16 15:37:33.937: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    May 16 15:37:33.940: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.079675ms
    May 16 15:37:35.944: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00697169s
    May 16 15:37:37.946: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008421329s
    May 16 15:37:39.943: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006159286s
    May 16 15:37:39.944: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 15:37:39.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3906" for this suite. 05/16/23 15:37:40.016
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:37:40.022
May 16 15:37:40.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename configmap 05/16/23 15:37:40.023
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:37:40.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:37:40.041
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 05/16/23 15:37:40.043
STEP: fetching the ConfigMap 05/16/23 15:37:40.05
STEP: patching the ConfigMap 05/16/23 15:37:40.054
STEP: listing all ConfigMaps in all namespaces with a label selector 05/16/23 15:37:40.061
STEP: deleting the ConfigMap by collection with a label selector 05/16/23 15:37:40.112
STEP: listing all ConfigMaps in test namespace 05/16/23 15:37:40.119
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
May 16 15:37:40.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3564" for this suite. 05/16/23 15:37:40.124
------------------------------
â€¢ [0.108 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:37:40.022
    May 16 15:37:40.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename configmap 05/16/23 15:37:40.023
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:37:40.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:37:40.041
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 05/16/23 15:37:40.043
    STEP: fetching the ConfigMap 05/16/23 15:37:40.05
    STEP: patching the ConfigMap 05/16/23 15:37:40.054
    STEP: listing all ConfigMaps in all namespaces with a label selector 05/16/23 15:37:40.061
    STEP: deleting the ConfigMap by collection with a label selector 05/16/23 15:37:40.112
    STEP: listing all ConfigMaps in test namespace 05/16/23 15:37:40.119
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    May 16 15:37:40.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3564" for this suite. 05/16/23 15:37:40.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:37:40.131
May 16 15:37:40.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename controllerrevisions 05/16/23 15:37:40.132
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:37:40.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:37:40.149
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-hzw97-daemon-set" 05/16/23 15:37:40.181
STEP: Check that daemon pods launch on every node of the cluster. 05/16/23 15:37:40.187
May 16 15:37:40.196: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:37:40.196: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:37:40.197: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:37:40.202: INFO: Number of nodes with available pods controlled by daemonset e2e-hzw97-daemon-set: 0
May 16 15:37:40.202: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 15:37:41.208: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:37:41.208: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:37:41.208: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:37:41.214: INFO: Number of nodes with available pods controlled by daemonset e2e-hzw97-daemon-set: 0
May 16 15:37:41.214: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 16 15:37:42.206: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:37:42.207: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:37:42.207: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 16 15:37:42.210: INFO: Number of nodes with available pods controlled by daemonset e2e-hzw97-daemon-set: 3
May 16 15:37:42.210: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-hzw97-daemon-set
STEP: Confirm DaemonSet "e2e-hzw97-daemon-set" successfully created with "daemonset-name=e2e-hzw97-daemon-set" label 05/16/23 15:37:42.212
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-hzw97-daemon-set" 05/16/23 15:37:42.218
May 16 15:37:42.221: INFO: Located ControllerRevision: "e2e-hzw97-daemon-set-c59b989b"
STEP: Patching ControllerRevision "e2e-hzw97-daemon-set-c59b989b" 05/16/23 15:37:42.224
May 16 15:37:42.229: INFO: e2e-hzw97-daemon-set-c59b989b has been patched
STEP: Create a new ControllerRevision 05/16/23 15:37:42.229
May 16 15:37:42.233: INFO: Created ControllerRevision: e2e-hzw97-daemon-set-b4f5bbf8
STEP: Confirm that there are two ControllerRevisions 05/16/23 15:37:42.233
May 16 15:37:42.233: INFO: Requesting list of ControllerRevisions to confirm quantity
May 16 15:37:42.236: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-hzw97-daemon-set-c59b989b" 05/16/23 15:37:42.236
STEP: Confirm that there is only one ControllerRevision 05/16/23 15:37:42.241
May 16 15:37:42.241: INFO: Requesting list of ControllerRevisions to confirm quantity
May 16 15:37:42.243: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-hzw97-daemon-set-b4f5bbf8" 05/16/23 15:37:42.245
May 16 15:37:42.252: INFO: e2e-hzw97-daemon-set-b4f5bbf8 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 05/16/23 15:37:42.252
W0516 15:37:42.257055      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 05/16/23 15:37:42.257
May 16 15:37:42.257: INFO: Requesting list of ControllerRevisions to confirm quantity
May 16 15:37:43.260: INFO: Requesting list of ControllerRevisions to confirm quantity
May 16 15:37:43.262: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-hzw97-daemon-set-b4f5bbf8=updated" 05/16/23 15:37:43.262
STEP: Confirm that there is only one ControllerRevision 05/16/23 15:37:43.269
May 16 15:37:43.269: INFO: Requesting list of ControllerRevisions to confirm quantity
May 16 15:37:43.272: INFO: Found 1 ControllerRevisions
May 16 15:37:43.274: INFO: ControllerRevision "e2e-hzw97-daemon-set-644445767f" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-hzw97-daemon-set" 05/16/23 15:37:43.277
STEP: deleting DaemonSet.extensions e2e-hzw97-daemon-set in namespace controllerrevisions-9958, will wait for the garbage collector to delete the pods 05/16/23 15:37:43.277
May 16 15:37:43.335: INFO: Deleting DaemonSet.extensions e2e-hzw97-daemon-set took: 5.461073ms
May 16 15:37:43.436: INFO: Terminating DaemonSet.extensions e2e-hzw97-daemon-set pods took: 100.92172ms
May 16 15:37:44.939: INFO: Number of nodes with available pods controlled by daemonset e2e-hzw97-daemon-set: 0
May 16 15:37:44.940: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-hzw97-daemon-set
May 16 15:37:44.942: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"100784"},"items":null}

May 16 15:37:44.944: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"100784"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 15:37:44.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-9958" for this suite. 05/16/23 15:37:44.958
------------------------------
â€¢ [4.832 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:37:40.131
    May 16 15:37:40.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename controllerrevisions 05/16/23 15:37:40.132
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:37:40.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:37:40.149
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-hzw97-daemon-set" 05/16/23 15:37:40.181
    STEP: Check that daemon pods launch on every node of the cluster. 05/16/23 15:37:40.187
    May 16 15:37:40.196: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:37:40.196: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:37:40.197: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:37:40.202: INFO: Number of nodes with available pods controlled by daemonset e2e-hzw97-daemon-set: 0
    May 16 15:37:40.202: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 15:37:41.208: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:37:41.208: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:37:41.208: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:37:41.214: INFO: Number of nodes with available pods controlled by daemonset e2e-hzw97-daemon-set: 0
    May 16 15:37:41.214: INFO: Node ip-10-0-132-142.eu-central-1.compute.internal is running 0 daemon pod, expected 1
    May 16 15:37:42.206: INFO: DaemonSet pods can't tolerate node ip-10-0-130-191.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:37:42.207: INFO: DaemonSet pods can't tolerate node ip-10-0-175-210.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:37:42.207: INFO: DaemonSet pods can't tolerate node ip-10-0-203-52.eu-central-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    May 16 15:37:42.210: INFO: Number of nodes with available pods controlled by daemonset e2e-hzw97-daemon-set: 3
    May 16 15:37:42.210: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-hzw97-daemon-set
    STEP: Confirm DaemonSet "e2e-hzw97-daemon-set" successfully created with "daemonset-name=e2e-hzw97-daemon-set" label 05/16/23 15:37:42.212
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-hzw97-daemon-set" 05/16/23 15:37:42.218
    May 16 15:37:42.221: INFO: Located ControllerRevision: "e2e-hzw97-daemon-set-c59b989b"
    STEP: Patching ControllerRevision "e2e-hzw97-daemon-set-c59b989b" 05/16/23 15:37:42.224
    May 16 15:37:42.229: INFO: e2e-hzw97-daemon-set-c59b989b has been patched
    STEP: Create a new ControllerRevision 05/16/23 15:37:42.229
    May 16 15:37:42.233: INFO: Created ControllerRevision: e2e-hzw97-daemon-set-b4f5bbf8
    STEP: Confirm that there are two ControllerRevisions 05/16/23 15:37:42.233
    May 16 15:37:42.233: INFO: Requesting list of ControllerRevisions to confirm quantity
    May 16 15:37:42.236: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-hzw97-daemon-set-c59b989b" 05/16/23 15:37:42.236
    STEP: Confirm that there is only one ControllerRevision 05/16/23 15:37:42.241
    May 16 15:37:42.241: INFO: Requesting list of ControllerRevisions to confirm quantity
    May 16 15:37:42.243: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-hzw97-daemon-set-b4f5bbf8" 05/16/23 15:37:42.245
    May 16 15:37:42.252: INFO: e2e-hzw97-daemon-set-b4f5bbf8 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 05/16/23 15:37:42.252
    W0516 15:37:42.257055      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 05/16/23 15:37:42.257
    May 16 15:37:42.257: INFO: Requesting list of ControllerRevisions to confirm quantity
    May 16 15:37:43.260: INFO: Requesting list of ControllerRevisions to confirm quantity
    May 16 15:37:43.262: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-hzw97-daemon-set-b4f5bbf8=updated" 05/16/23 15:37:43.262
    STEP: Confirm that there is only one ControllerRevision 05/16/23 15:37:43.269
    May 16 15:37:43.269: INFO: Requesting list of ControllerRevisions to confirm quantity
    May 16 15:37:43.272: INFO: Found 1 ControllerRevisions
    May 16 15:37:43.274: INFO: ControllerRevision "e2e-hzw97-daemon-set-644445767f" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-hzw97-daemon-set" 05/16/23 15:37:43.277
    STEP: deleting DaemonSet.extensions e2e-hzw97-daemon-set in namespace controllerrevisions-9958, will wait for the garbage collector to delete the pods 05/16/23 15:37:43.277
    May 16 15:37:43.335: INFO: Deleting DaemonSet.extensions e2e-hzw97-daemon-set took: 5.461073ms
    May 16 15:37:43.436: INFO: Terminating DaemonSet.extensions e2e-hzw97-daemon-set pods took: 100.92172ms
    May 16 15:37:44.939: INFO: Number of nodes with available pods controlled by daemonset e2e-hzw97-daemon-set: 0
    May 16 15:37:44.940: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-hzw97-daemon-set
    May 16 15:37:44.942: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"100784"},"items":null}

    May 16 15:37:44.944: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"100784"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 15:37:44.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-9958" for this suite. 05/16/23 15:37:44.958
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:37:44.964
May 16 15:37:44.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 15:37:44.964
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:37:44.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:37:44.978
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-38d88ff5-1eee-47dc-b9fc-f2eb1b7bf9ff 05/16/23 15:37:44.98
STEP: Creating a pod to test consume secrets 05/16/23 15:37:44.987
May 16 15:37:45.010: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e7f082c3-891f-4a28-a05c-cf49f79a77a0" in namespace "projected-2503" to be "Succeeded or Failed"
May 16 15:37:45.014: INFO: Pod "pod-projected-secrets-e7f082c3-891f-4a28-a05c-cf49f79a77a0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.712627ms
May 16 15:37:47.017: INFO: Pod "pod-projected-secrets-e7f082c3-891f-4a28-a05c-cf49f79a77a0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006491165s
May 16 15:37:49.019: INFO: Pod "pod-projected-secrets-e7f082c3-891f-4a28-a05c-cf49f79a77a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008739103s
STEP: Saw pod success 05/16/23 15:37:49.019
May 16 15:37:49.019: INFO: Pod "pod-projected-secrets-e7f082c3-891f-4a28-a05c-cf49f79a77a0" satisfied condition "Succeeded or Failed"
May 16 15:37:49.021: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-secrets-e7f082c3-891f-4a28-a05c-cf49f79a77a0 container projected-secret-volume-test: <nil>
STEP: delete the pod 05/16/23 15:37:49.031
May 16 15:37:49.042: INFO: Waiting for pod pod-projected-secrets-e7f082c3-891f-4a28-a05c-cf49f79a77a0 to disappear
May 16 15:37:49.044: INFO: Pod pod-projected-secrets-e7f082c3-891f-4a28-a05c-cf49f79a77a0 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May 16 15:37:49.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2503" for this suite. 05/16/23 15:37:49.047
------------------------------
â€¢ [4.092 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:37:44.964
    May 16 15:37:44.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 15:37:44.964
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:37:44.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:37:44.978
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-38d88ff5-1eee-47dc-b9fc-f2eb1b7bf9ff 05/16/23 15:37:44.98
    STEP: Creating a pod to test consume secrets 05/16/23 15:37:44.987
    May 16 15:37:45.010: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e7f082c3-891f-4a28-a05c-cf49f79a77a0" in namespace "projected-2503" to be "Succeeded or Failed"
    May 16 15:37:45.014: INFO: Pod "pod-projected-secrets-e7f082c3-891f-4a28-a05c-cf49f79a77a0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.712627ms
    May 16 15:37:47.017: INFO: Pod "pod-projected-secrets-e7f082c3-891f-4a28-a05c-cf49f79a77a0": Phase="Running", Reason="", readiness=false. Elapsed: 2.006491165s
    May 16 15:37:49.019: INFO: Pod "pod-projected-secrets-e7f082c3-891f-4a28-a05c-cf49f79a77a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008739103s
    STEP: Saw pod success 05/16/23 15:37:49.019
    May 16 15:37:49.019: INFO: Pod "pod-projected-secrets-e7f082c3-891f-4a28-a05c-cf49f79a77a0" satisfied condition "Succeeded or Failed"
    May 16 15:37:49.021: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-secrets-e7f082c3-891f-4a28-a05c-cf49f79a77a0 container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/16/23 15:37:49.031
    May 16 15:37:49.042: INFO: Waiting for pod pod-projected-secrets-e7f082c3-891f-4a28-a05c-cf49f79a77a0 to disappear
    May 16 15:37:49.044: INFO: Pod pod-projected-secrets-e7f082c3-891f-4a28-a05c-cf49f79a77a0 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May 16 15:37:49.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2503" for this suite. 05/16/23 15:37:49.047
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:37:49.056
May 16 15:37:49.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename namespaces 05/16/23 15:37:49.057
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:37:49.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:37:49.076
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 05/16/23 15:37:49.077
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:37:49.1
STEP: Creating a pod in the namespace 05/16/23 15:37:49.102
STEP: Waiting for the pod to have running status 05/16/23 15:37:50.113
May 16 15:37:50.113: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6759" to be "running"
May 16 15:37:50.115: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.379209ms
May 16 15:37:52.118: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005428581s
May 16 15:37:52.118: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 05/16/23 15:37:52.118
STEP: Waiting for the namespace to be removed. 05/16/23 15:37:52.124
STEP: Recreating the namespace 05/16/23 15:38:05.127
STEP: Verifying there are no pods in the namespace 05/16/23 15:38:05.14
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 15:38:05.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3531" for this suite. 05/16/23 15:38:05.154
STEP: Destroying namespace "nsdeletetest-6759" for this suite. 05/16/23 15:38:05.167
May 16 15:38:05.169: INFO: Namespace nsdeletetest-6759 was already deleted
STEP: Destroying namespace "nsdeletetest-2627" for this suite. 05/16/23 15:38:05.169
------------------------------
â€¢ [SLOW TEST] [16.120 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:37:49.056
    May 16 15:37:49.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename namespaces 05/16/23 15:37:49.057
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:37:49.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:37:49.076
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 05/16/23 15:37:49.077
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:37:49.1
    STEP: Creating a pod in the namespace 05/16/23 15:37:49.102
    STEP: Waiting for the pod to have running status 05/16/23 15:37:50.113
    May 16 15:37:50.113: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-6759" to be "running"
    May 16 15:37:50.115: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.379209ms
    May 16 15:37:52.118: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005428581s
    May 16 15:37:52.118: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 05/16/23 15:37:52.118
    STEP: Waiting for the namespace to be removed. 05/16/23 15:37:52.124
    STEP: Recreating the namespace 05/16/23 15:38:05.127
    STEP: Verifying there are no pods in the namespace 05/16/23 15:38:05.14
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 15:38:05.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3531" for this suite. 05/16/23 15:38:05.154
    STEP: Destroying namespace "nsdeletetest-6759" for this suite. 05/16/23 15:38:05.167
    May 16 15:38:05.169: INFO: Namespace nsdeletetest-6759 was already deleted
    STEP: Destroying namespace "nsdeletetest-2627" for this suite. 05/16/23 15:38:05.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:38:05.177
May 16 15:38:05.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename crd-publish-openapi 05/16/23 15:38:05.177
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:38:05.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:38:05.194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
May 16 15:38:05.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 05/16/23 15:38:08.891
May 16 15:38:08.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 --namespace=crd-publish-openapi-980 create -f -'
May 16 15:38:09.893: INFO: stderr: ""
May 16 15:38:09.893: INFO: stdout: "e2e-test-crd-publish-openapi-4560-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 16 15:38:09.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 --namespace=crd-publish-openapi-980 delete e2e-test-crd-publish-openapi-4560-crds test-foo'
May 16 15:38:09.965: INFO: stderr: ""
May 16 15:38:09.965: INFO: stdout: "e2e-test-crd-publish-openapi-4560-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 16 15:38:09.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 --namespace=crd-publish-openapi-980 apply -f -'
May 16 15:38:10.224: INFO: stderr: ""
May 16 15:38:10.224: INFO: stdout: "e2e-test-crd-publish-openapi-4560-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 16 15:38:10.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 --namespace=crd-publish-openapi-980 delete e2e-test-crd-publish-openapi-4560-crds test-foo'
May 16 15:38:10.278: INFO: stderr: ""
May 16 15:38:10.278: INFO: stdout: "e2e-test-crd-publish-openapi-4560-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 05/16/23 15:38:10.278
May 16 15:38:10.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 --namespace=crd-publish-openapi-980 create -f -'
May 16 15:38:10.535: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 05/16/23 15:38:10.535
May 16 15:38:10.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 --namespace=crd-publish-openapi-980 create -f -'
May 16 15:38:11.464: INFO: rc: 1
May 16 15:38:11.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 --namespace=crd-publish-openapi-980 apply -f -'
May 16 15:38:11.722: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 05/16/23 15:38:11.722
May 16 15:38:11.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 --namespace=crd-publish-openapi-980 create -f -'
May 16 15:38:11.973: INFO: rc: 1
May 16 15:38:11.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 --namespace=crd-publish-openapi-980 apply -f -'
May 16 15:38:12.862: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 05/16/23 15:38:12.862
May 16 15:38:12.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 explain e2e-test-crd-publish-openapi-4560-crds'
May 16 15:38:13.109: INFO: stderr: ""
May 16 15:38:13.109: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4560-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 05/16/23 15:38:13.109
May 16 15:38:13.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 explain e2e-test-crd-publish-openapi-4560-crds.metadata'
May 16 15:38:13.365: INFO: stderr: ""
May 16 15:38:13.365: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4560-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 16 15:38:13.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 explain e2e-test-crd-publish-openapi-4560-crds.spec'
May 16 15:38:13.619: INFO: stderr: ""
May 16 15:38:13.619: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4560-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 16 15:38:13.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 explain e2e-test-crd-publish-openapi-4560-crds.spec.bars'
May 16 15:38:13.874: INFO: stderr: ""
May 16 15:38:13.874: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4560-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 05/16/23 15:38:13.874
May 16 15:38:13.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 explain e2e-test-crd-publish-openapi-4560-crds.spec.bars2'
May 16 15:38:14.132: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
May 16 15:38:16.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-980" for this suite. 05/16/23 15:38:16.915
------------------------------
â€¢ [SLOW TEST] [11.744 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:38:05.177
    May 16 15:38:05.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename crd-publish-openapi 05/16/23 15:38:05.177
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:38:05.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:38:05.194
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    May 16 15:38:05.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 05/16/23 15:38:08.891
    May 16 15:38:08.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 --namespace=crd-publish-openapi-980 create -f -'
    May 16 15:38:09.893: INFO: stderr: ""
    May 16 15:38:09.893: INFO: stdout: "e2e-test-crd-publish-openapi-4560-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    May 16 15:38:09.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 --namespace=crd-publish-openapi-980 delete e2e-test-crd-publish-openapi-4560-crds test-foo'
    May 16 15:38:09.965: INFO: stderr: ""
    May 16 15:38:09.965: INFO: stdout: "e2e-test-crd-publish-openapi-4560-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    May 16 15:38:09.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 --namespace=crd-publish-openapi-980 apply -f -'
    May 16 15:38:10.224: INFO: stderr: ""
    May 16 15:38:10.224: INFO: stdout: "e2e-test-crd-publish-openapi-4560-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    May 16 15:38:10.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 --namespace=crd-publish-openapi-980 delete e2e-test-crd-publish-openapi-4560-crds test-foo'
    May 16 15:38:10.278: INFO: stderr: ""
    May 16 15:38:10.278: INFO: stdout: "e2e-test-crd-publish-openapi-4560-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 05/16/23 15:38:10.278
    May 16 15:38:10.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 --namespace=crd-publish-openapi-980 create -f -'
    May 16 15:38:10.535: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 05/16/23 15:38:10.535
    May 16 15:38:10.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 --namespace=crd-publish-openapi-980 create -f -'
    May 16 15:38:11.464: INFO: rc: 1
    May 16 15:38:11.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 --namespace=crd-publish-openapi-980 apply -f -'
    May 16 15:38:11.722: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 05/16/23 15:38:11.722
    May 16 15:38:11.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 --namespace=crd-publish-openapi-980 create -f -'
    May 16 15:38:11.973: INFO: rc: 1
    May 16 15:38:11.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 --namespace=crd-publish-openapi-980 apply -f -'
    May 16 15:38:12.862: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 05/16/23 15:38:12.862
    May 16 15:38:12.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 explain e2e-test-crd-publish-openapi-4560-crds'
    May 16 15:38:13.109: INFO: stderr: ""
    May 16 15:38:13.109: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4560-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 05/16/23 15:38:13.109
    May 16 15:38:13.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 explain e2e-test-crd-publish-openapi-4560-crds.metadata'
    May 16 15:38:13.365: INFO: stderr: ""
    May 16 15:38:13.365: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4560-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    May 16 15:38:13.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 explain e2e-test-crd-publish-openapi-4560-crds.spec'
    May 16 15:38:13.619: INFO: stderr: ""
    May 16 15:38:13.619: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4560-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    May 16 15:38:13.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 explain e2e-test-crd-publish-openapi-4560-crds.spec.bars'
    May 16 15:38:13.874: INFO: stderr: ""
    May 16 15:38:13.874: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4560-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 05/16/23 15:38:13.874
    May 16 15:38:13.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=crd-publish-openapi-980 explain e2e-test-crd-publish-openapi-4560-crds.spec.bars2'
    May 16 15:38:14.132: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    May 16 15:38:16.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-980" for this suite. 05/16/23 15:38:16.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:38:16.922
May 16 15:38:16.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename emptydir 05/16/23 15:38:16.922
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:38:16.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:38:16.945
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 05/16/23 15:38:16.947
May 16 15:38:16.964: INFO: Waiting up to 5m0s for pod "pod-20cb5fe1-c91c-4819-8101-cfed8188f534" in namespace "emptydir-6074" to be "Succeeded or Failed"
May 16 15:38:16.967: INFO: Pod "pod-20cb5fe1-c91c-4819-8101-cfed8188f534": Phase="Pending", Reason="", readiness=false. Elapsed: 2.68874ms
May 16 15:38:18.972: INFO: Pod "pod-20cb5fe1-c91c-4819-8101-cfed8188f534": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007611189s
May 16 15:38:20.971: INFO: Pod "pod-20cb5fe1-c91c-4819-8101-cfed8188f534": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00663022s
STEP: Saw pod success 05/16/23 15:38:20.971
May 16 15:38:20.971: INFO: Pod "pod-20cb5fe1-c91c-4819-8101-cfed8188f534" satisfied condition "Succeeded or Failed"
May 16 15:38:20.974: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-20cb5fe1-c91c-4819-8101-cfed8188f534 container test-container: <nil>
STEP: delete the pod 05/16/23 15:38:20.982
May 16 15:38:20.993: INFO: Waiting for pod pod-20cb5fe1-c91c-4819-8101-cfed8188f534 to disappear
May 16 15:38:20.996: INFO: Pod pod-20cb5fe1-c91c-4819-8101-cfed8188f534 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
May 16 15:38:20.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6074" for this suite. 05/16/23 15:38:21
------------------------------
â€¢ [4.085 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:38:16.922
    May 16 15:38:16.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename emptydir 05/16/23 15:38:16.922
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:38:16.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:38:16.945
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 05/16/23 15:38:16.947
    May 16 15:38:16.964: INFO: Waiting up to 5m0s for pod "pod-20cb5fe1-c91c-4819-8101-cfed8188f534" in namespace "emptydir-6074" to be "Succeeded or Failed"
    May 16 15:38:16.967: INFO: Pod "pod-20cb5fe1-c91c-4819-8101-cfed8188f534": Phase="Pending", Reason="", readiness=false. Elapsed: 2.68874ms
    May 16 15:38:18.972: INFO: Pod "pod-20cb5fe1-c91c-4819-8101-cfed8188f534": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007611189s
    May 16 15:38:20.971: INFO: Pod "pod-20cb5fe1-c91c-4819-8101-cfed8188f534": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00663022s
    STEP: Saw pod success 05/16/23 15:38:20.971
    May 16 15:38:20.971: INFO: Pod "pod-20cb5fe1-c91c-4819-8101-cfed8188f534" satisfied condition "Succeeded or Failed"
    May 16 15:38:20.974: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-20cb5fe1-c91c-4819-8101-cfed8188f534 container test-container: <nil>
    STEP: delete the pod 05/16/23 15:38:20.982
    May 16 15:38:20.993: INFO: Waiting for pod pod-20cb5fe1-c91c-4819-8101-cfed8188f534 to disappear
    May 16 15:38:20.996: INFO: Pod pod-20cb5fe1-c91c-4819-8101-cfed8188f534 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    May 16 15:38:20.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6074" for this suite. 05/16/23 15:38:21
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:38:21.006
May 16 15:38:21.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename resourcequota 05/16/23 15:38:21.007
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:38:21.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:38:21.026
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 05/16/23 15:38:21.028
STEP: Creating a ResourceQuota 05/16/23 15:38:26.039
STEP: Ensuring resource quota status is calculated 05/16/23 15:38:26.045
STEP: Creating a Pod that fits quota 05/16/23 15:38:28.049
STEP: Ensuring ResourceQuota status captures the pod usage 05/16/23 15:38:28.065
STEP: Not allowing a pod to be created that exceeds remaining quota 05/16/23 15:38:30.069
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 05/16/23 15:38:30.073
STEP: Ensuring a pod cannot update its resource requirements 05/16/23 15:38:30.077
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 05/16/23 15:38:30.081
STEP: Deleting the pod 05/16/23 15:38:32.086
STEP: Ensuring resource quota status released the pod usage 05/16/23 15:38:32.098
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
May 16 15:38:34.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3247" for this suite. 05/16/23 15:38:34.107
------------------------------
â€¢ [SLOW TEST] [13.107 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:38:21.006
    May 16 15:38:21.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename resourcequota 05/16/23 15:38:21.007
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:38:21.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:38:21.026
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 05/16/23 15:38:21.028
    STEP: Creating a ResourceQuota 05/16/23 15:38:26.039
    STEP: Ensuring resource quota status is calculated 05/16/23 15:38:26.045
    STEP: Creating a Pod that fits quota 05/16/23 15:38:28.049
    STEP: Ensuring ResourceQuota status captures the pod usage 05/16/23 15:38:28.065
    STEP: Not allowing a pod to be created that exceeds remaining quota 05/16/23 15:38:30.069
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 05/16/23 15:38:30.073
    STEP: Ensuring a pod cannot update its resource requirements 05/16/23 15:38:30.077
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 05/16/23 15:38:30.081
    STEP: Deleting the pod 05/16/23 15:38:32.086
    STEP: Ensuring resource quota status released the pod usage 05/16/23 15:38:32.098
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    May 16 15:38:34.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3247" for this suite. 05/16/23 15:38:34.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:38:34.113
May 16 15:38:34.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 15:38:34.114
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:38:34.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:38:34.134
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 05/16/23 15:38:34.137
May 16 15:38:34.153: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc15254d-c269-4139-b2ba-999f7d2b3d15" in namespace "projected-6206" to be "Succeeded or Failed"
May 16 15:38:34.156: INFO: Pod "downwardapi-volume-bc15254d-c269-4139-b2ba-999f7d2b3d15": Phase="Pending", Reason="", readiness=false. Elapsed: 3.096752ms
May 16 15:38:36.161: INFO: Pod "downwardapi-volume-bc15254d-c269-4139-b2ba-999f7d2b3d15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007670932s
May 16 15:38:38.161: INFO: Pod "downwardapi-volume-bc15254d-c269-4139-b2ba-999f7d2b3d15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007896007s
STEP: Saw pod success 05/16/23 15:38:38.161
May 16 15:38:38.161: INFO: Pod "downwardapi-volume-bc15254d-c269-4139-b2ba-999f7d2b3d15" satisfied condition "Succeeded or Failed"
May 16 15:38:38.164: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-bc15254d-c269-4139-b2ba-999f7d2b3d15 container client-container: <nil>
STEP: delete the pod 05/16/23 15:38:38.169
May 16 15:38:38.178: INFO: Waiting for pod downwardapi-volume-bc15254d-c269-4139-b2ba-999f7d2b3d15 to disappear
May 16 15:38:38.181: INFO: Pod downwardapi-volume-bc15254d-c269-4139-b2ba-999f7d2b3d15 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
May 16 15:38:38.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6206" for this suite. 05/16/23 15:38:38.185
------------------------------
â€¢ [4.077 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:38:34.113
    May 16 15:38:34.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 15:38:34.114
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:38:34.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:38:34.134
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 05/16/23 15:38:34.137
    May 16 15:38:34.153: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc15254d-c269-4139-b2ba-999f7d2b3d15" in namespace "projected-6206" to be "Succeeded or Failed"
    May 16 15:38:34.156: INFO: Pod "downwardapi-volume-bc15254d-c269-4139-b2ba-999f7d2b3d15": Phase="Pending", Reason="", readiness=false. Elapsed: 3.096752ms
    May 16 15:38:36.161: INFO: Pod "downwardapi-volume-bc15254d-c269-4139-b2ba-999f7d2b3d15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007670932s
    May 16 15:38:38.161: INFO: Pod "downwardapi-volume-bc15254d-c269-4139-b2ba-999f7d2b3d15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007896007s
    STEP: Saw pod success 05/16/23 15:38:38.161
    May 16 15:38:38.161: INFO: Pod "downwardapi-volume-bc15254d-c269-4139-b2ba-999f7d2b3d15" satisfied condition "Succeeded or Failed"
    May 16 15:38:38.164: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-bc15254d-c269-4139-b2ba-999f7d2b3d15 container client-container: <nil>
    STEP: delete the pod 05/16/23 15:38:38.169
    May 16 15:38:38.178: INFO: Waiting for pod downwardapi-volume-bc15254d-c269-4139-b2ba-999f7d2b3d15 to disappear
    May 16 15:38:38.181: INFO: Pod downwardapi-volume-bc15254d-c269-4139-b2ba-999f7d2b3d15 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    May 16 15:38:38.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6206" for this suite. 05/16/23 15:38:38.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:38:38.191
May 16 15:38:38.191: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename downward-api 05/16/23 15:38:38.191
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:38:38.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:38:38.208
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 05/16/23 15:38:38.212
W0516 15:38:38.226675      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
May 16 15:38:38.226: INFO: Waiting up to 5m0s for pod "downward-api-5138f5e4-76a2-4749-aae7-2d0f2ac9964a" in namespace "downward-api-8945" to be "Succeeded or Failed"
May 16 15:38:38.229: INFO: Pod "downward-api-5138f5e4-76a2-4749-aae7-2d0f2ac9964a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.731215ms
May 16 15:38:40.233: INFO: Pod "downward-api-5138f5e4-76a2-4749-aae7-2d0f2ac9964a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007100226s
May 16 15:38:42.233: INFO: Pod "downward-api-5138f5e4-76a2-4749-aae7-2d0f2ac9964a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006780735s
STEP: Saw pod success 05/16/23 15:38:42.233
May 16 15:38:42.233: INFO: Pod "downward-api-5138f5e4-76a2-4749-aae7-2d0f2ac9964a" satisfied condition "Succeeded or Failed"
May 16 15:38:42.236: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downward-api-5138f5e4-76a2-4749-aae7-2d0f2ac9964a container dapi-container: <nil>
STEP: delete the pod 05/16/23 15:38:42.242
May 16 15:38:42.263: INFO: Waiting for pod downward-api-5138f5e4-76a2-4749-aae7-2d0f2ac9964a to disappear
May 16 15:38:42.266: INFO: Pod downward-api-5138f5e4-76a2-4749-aae7-2d0f2ac9964a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
May 16 15:38:42.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8945" for this suite. 05/16/23 15:38:42.269
------------------------------
â€¢ [4.084 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:38:38.191
    May 16 15:38:38.191: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename downward-api 05/16/23 15:38:38.191
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:38:38.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:38:38.208
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 05/16/23 15:38:38.212
    W0516 15:38:38.226675      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    May 16 15:38:38.226: INFO: Waiting up to 5m0s for pod "downward-api-5138f5e4-76a2-4749-aae7-2d0f2ac9964a" in namespace "downward-api-8945" to be "Succeeded or Failed"
    May 16 15:38:38.229: INFO: Pod "downward-api-5138f5e4-76a2-4749-aae7-2d0f2ac9964a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.731215ms
    May 16 15:38:40.233: INFO: Pod "downward-api-5138f5e4-76a2-4749-aae7-2d0f2ac9964a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007100226s
    May 16 15:38:42.233: INFO: Pod "downward-api-5138f5e4-76a2-4749-aae7-2d0f2ac9964a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006780735s
    STEP: Saw pod success 05/16/23 15:38:42.233
    May 16 15:38:42.233: INFO: Pod "downward-api-5138f5e4-76a2-4749-aae7-2d0f2ac9964a" satisfied condition "Succeeded or Failed"
    May 16 15:38:42.236: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downward-api-5138f5e4-76a2-4749-aae7-2d0f2ac9964a container dapi-container: <nil>
    STEP: delete the pod 05/16/23 15:38:42.242
    May 16 15:38:42.263: INFO: Waiting for pod downward-api-5138f5e4-76a2-4749-aae7-2d0f2ac9964a to disappear
    May 16 15:38:42.266: INFO: Pod downward-api-5138f5e4-76a2-4749-aae7-2d0f2ac9964a no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    May 16 15:38:42.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8945" for this suite. 05/16/23 15:38:42.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:38:42.275
May 16 15:38:42.275: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename secrets 05/16/23 15:38:42.275
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:38:42.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:38:42.297
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-8aff4151-9710-4585-b7b2-fbd15716e85d 05/16/23 15:38:42.3
STEP: Creating a pod to test consume secrets 05/16/23 15:38:42.308
May 16 15:38:42.322: INFO: Waiting up to 5m0s for pod "pod-secrets-ff8b2100-a659-4338-ad8e-e5a4715887c4" in namespace "secrets-9004" to be "Succeeded or Failed"
May 16 15:38:42.326: INFO: Pod "pod-secrets-ff8b2100-a659-4338-ad8e-e5a4715887c4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.8038ms
May 16 15:38:44.330: INFO: Pod "pod-secrets-ff8b2100-a659-4338-ad8e-e5a4715887c4": Phase="Running", Reason="", readiness=false. Elapsed: 2.008153024s
May 16 15:38:46.329: INFO: Pod "pod-secrets-ff8b2100-a659-4338-ad8e-e5a4715887c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007286122s
STEP: Saw pod success 05/16/23 15:38:46.329
May 16 15:38:46.329: INFO: Pod "pod-secrets-ff8b2100-a659-4338-ad8e-e5a4715887c4" satisfied condition "Succeeded or Failed"
May 16 15:38:46.335: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-secrets-ff8b2100-a659-4338-ad8e-e5a4715887c4 container secret-volume-test: <nil>
STEP: delete the pod 05/16/23 15:38:46.34
May 16 15:38:46.354: INFO: Waiting for pod pod-secrets-ff8b2100-a659-4338-ad8e-e5a4715887c4 to disappear
May 16 15:38:46.357: INFO: Pod pod-secrets-ff8b2100-a659-4338-ad8e-e5a4715887c4 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
May 16 15:38:46.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9004" for this suite. 05/16/23 15:38:46.364
------------------------------
â€¢ [4.100 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:38:42.275
    May 16 15:38:42.275: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename secrets 05/16/23 15:38:42.275
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:38:42.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:38:42.297
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-8aff4151-9710-4585-b7b2-fbd15716e85d 05/16/23 15:38:42.3
    STEP: Creating a pod to test consume secrets 05/16/23 15:38:42.308
    May 16 15:38:42.322: INFO: Waiting up to 5m0s for pod "pod-secrets-ff8b2100-a659-4338-ad8e-e5a4715887c4" in namespace "secrets-9004" to be "Succeeded or Failed"
    May 16 15:38:42.326: INFO: Pod "pod-secrets-ff8b2100-a659-4338-ad8e-e5a4715887c4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.8038ms
    May 16 15:38:44.330: INFO: Pod "pod-secrets-ff8b2100-a659-4338-ad8e-e5a4715887c4": Phase="Running", Reason="", readiness=false. Elapsed: 2.008153024s
    May 16 15:38:46.329: INFO: Pod "pod-secrets-ff8b2100-a659-4338-ad8e-e5a4715887c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007286122s
    STEP: Saw pod success 05/16/23 15:38:46.329
    May 16 15:38:46.329: INFO: Pod "pod-secrets-ff8b2100-a659-4338-ad8e-e5a4715887c4" satisfied condition "Succeeded or Failed"
    May 16 15:38:46.335: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-secrets-ff8b2100-a659-4338-ad8e-e5a4715887c4 container secret-volume-test: <nil>
    STEP: delete the pod 05/16/23 15:38:46.34
    May 16 15:38:46.354: INFO: Waiting for pod pod-secrets-ff8b2100-a659-4338-ad8e-e5a4715887c4 to disappear
    May 16 15:38:46.357: INFO: Pod pod-secrets-ff8b2100-a659-4338-ad8e-e5a4715887c4 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    May 16 15:38:46.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9004" for this suite. 05/16/23 15:38:46.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:38:46.376
May 16 15:38:46.376: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename pod-network-test 05/16/23 15:38:46.376
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:38:46.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:38:46.407
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-1472 05/16/23 15:38:46.409
STEP: creating a selector 05/16/23 15:38:46.409
STEP: Creating the service pods in kubernetes 05/16/23 15:38:46.409
May 16 15:38:46.410: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 16 15:38:46.457: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1472" to be "running and ready"
May 16 15:38:46.461: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.654978ms
May 16 15:38:46.461: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 16 15:38:48.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008241269s
May 16 15:38:48.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 15:38:50.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008251217s
May 16 15:38:50.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 15:38:52.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.00849637s
May 16 15:38:52.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 15:38:54.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008391277s
May 16 15:38:54.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 15:38:56.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008862035s
May 16 15:38:56.466: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 15:38:58.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008725454s
May 16 15:38:58.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 15:39:00.466: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.009453923s
May 16 15:39:00.466: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 15:39:02.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.008405829s
May 16 15:39:02.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 15:39:04.466: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009678876s
May 16 15:39:04.466: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 15:39:06.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.007907828s
May 16 15:39:06.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
May 16 15:39:08.466: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.009527508s
May 16 15:39:08.466: INFO: The phase of Pod netserver-0 is Running (Ready = true)
May 16 15:39:08.466: INFO: Pod "netserver-0" satisfied condition "running and ready"
May 16 15:39:08.469: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1472" to be "running and ready"
May 16 15:39:08.472: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.228232ms
May 16 15:39:08.472: INFO: The phase of Pod netserver-1 is Running (Ready = true)
May 16 15:39:08.472: INFO: Pod "netserver-1" satisfied condition "running and ready"
May 16 15:39:08.475: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1472" to be "running and ready"
May 16 15:39:08.478: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.02912ms
May 16 15:39:08.478: INFO: The phase of Pod netserver-2 is Running (Ready = true)
May 16 15:39:08.478: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 05/16/23 15:39:08.481
May 16 15:39:08.495: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1472" to be "running"
May 16 15:39:08.498: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.862542ms
May 16 15:39:10.504: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009185204s
May 16 15:39:10.504: INFO: Pod "test-container-pod" satisfied condition "running"
May 16 15:39:10.514: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1472" to be "running"
May 16 15:39:10.516: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.524964ms
May 16 15:39:10.516: INFO: Pod "host-test-container-pod" satisfied condition "running"
May 16 15:39:10.519: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 16 15:39:10.519: INFO: Going to poll 10.128.2.186 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May 16 15:39:10.522: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.2.186:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1472 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:39:10.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:39:10.522: INFO: ExecWithOptions: Clientset creation
May 16 15:39:10.522: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1472/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.2.186%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 16 15:39:10.622: INFO: Found all 1 expected endpoints: [netserver-0]
May 16 15:39:10.622: INFO: Going to poll 10.131.1.140 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May 16 15:39:10.625: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.131.1.140:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1472 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:39:10.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:39:10.626: INFO: ExecWithOptions: Clientset creation
May 16 15:39:10.626: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1472/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.131.1.140%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 16 15:39:10.709: INFO: Found all 1 expected endpoints: [netserver-1]
May 16 15:39:10.709: INFO: Going to poll 10.129.2.166 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May 16 15:39:10.712: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.129.2.166:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1472 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 16 15:39:10.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
May 16 15:39:10.712: INFO: ExecWithOptions: Clientset creation
May 16 15:39:10.712: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1472/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.129.2.166%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
May 16 15:39:10.798: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
May 16 15:39:10.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1472" for this suite. 05/16/23 15:39:10.803
------------------------------
â€¢ [SLOW TEST] [24.433 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:38:46.376
    May 16 15:38:46.376: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename pod-network-test 05/16/23 15:38:46.376
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:38:46.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:38:46.407
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-1472 05/16/23 15:38:46.409
    STEP: creating a selector 05/16/23 15:38:46.409
    STEP: Creating the service pods in kubernetes 05/16/23 15:38:46.409
    May 16 15:38:46.410: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    May 16 15:38:46.457: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1472" to be "running and ready"
    May 16 15:38:46.461: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.654978ms
    May 16 15:38:46.461: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:38:48.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008241269s
    May 16 15:38:48.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 15:38:50.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008251217s
    May 16 15:38:50.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 15:38:52.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.00849637s
    May 16 15:38:52.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 15:38:54.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008391277s
    May 16 15:38:54.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 15:38:56.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008862035s
    May 16 15:38:56.466: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 15:38:58.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.008725454s
    May 16 15:38:58.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 15:39:00.466: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.009453923s
    May 16 15:39:00.466: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 15:39:02.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.008405829s
    May 16 15:39:02.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 15:39:04.466: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009678876s
    May 16 15:39:04.466: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 15:39:06.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.007907828s
    May 16 15:39:06.465: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    May 16 15:39:08.466: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.009527508s
    May 16 15:39:08.466: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    May 16 15:39:08.466: INFO: Pod "netserver-0" satisfied condition "running and ready"
    May 16 15:39:08.469: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1472" to be "running and ready"
    May 16 15:39:08.472: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.228232ms
    May 16 15:39:08.472: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    May 16 15:39:08.472: INFO: Pod "netserver-1" satisfied condition "running and ready"
    May 16 15:39:08.475: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1472" to be "running and ready"
    May 16 15:39:08.478: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.02912ms
    May 16 15:39:08.478: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    May 16 15:39:08.478: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 05/16/23 15:39:08.481
    May 16 15:39:08.495: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1472" to be "running"
    May 16 15:39:08.498: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.862542ms
    May 16 15:39:10.504: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009185204s
    May 16 15:39:10.504: INFO: Pod "test-container-pod" satisfied condition "running"
    May 16 15:39:10.514: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1472" to be "running"
    May 16 15:39:10.516: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.524964ms
    May 16 15:39:10.516: INFO: Pod "host-test-container-pod" satisfied condition "running"
    May 16 15:39:10.519: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    May 16 15:39:10.519: INFO: Going to poll 10.128.2.186 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    May 16 15:39:10.522: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.2.186:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1472 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:39:10.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:39:10.522: INFO: ExecWithOptions: Clientset creation
    May 16 15:39:10.522: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1472/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.2.186%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 16 15:39:10.622: INFO: Found all 1 expected endpoints: [netserver-0]
    May 16 15:39:10.622: INFO: Going to poll 10.131.1.140 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    May 16 15:39:10.625: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.131.1.140:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1472 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:39:10.625: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:39:10.626: INFO: ExecWithOptions: Clientset creation
    May 16 15:39:10.626: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1472/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.131.1.140%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 16 15:39:10.709: INFO: Found all 1 expected endpoints: [netserver-1]
    May 16 15:39:10.709: INFO: Going to poll 10.129.2.166 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    May 16 15:39:10.712: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.129.2.166:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1472 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    May 16 15:39:10.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    May 16 15:39:10.712: INFO: ExecWithOptions: Clientset creation
    May 16 15:39:10.712: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-1472/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.129.2.166%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    May 16 15:39:10.798: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    May 16 15:39:10.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1472" for this suite. 05/16/23 15:39:10.803
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:39:10.81
May 16 15:39:10.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename downward-api 05/16/23 15:39:10.81
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:39:10.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:39:10.834
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 05/16/23 15:39:10.837
May 16 15:39:10.852: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02a40874-2c8c-4791-a922-8b37ae68060a" in namespace "downward-api-4674" to be "Succeeded or Failed"
May 16 15:39:10.855: INFO: Pod "downwardapi-volume-02a40874-2c8c-4791-a922-8b37ae68060a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.248251ms
May 16 15:39:12.859: INFO: Pod "downwardapi-volume-02a40874-2c8c-4791-a922-8b37ae68060a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007605875s
May 16 15:39:14.859: INFO: Pod "downwardapi-volume-02a40874-2c8c-4791-a922-8b37ae68060a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007184736s
STEP: Saw pod success 05/16/23 15:39:14.859
May 16 15:39:14.859: INFO: Pod "downwardapi-volume-02a40874-2c8c-4791-a922-8b37ae68060a" satisfied condition "Succeeded or Failed"
May 16 15:39:14.862: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-02a40874-2c8c-4791-a922-8b37ae68060a container client-container: <nil>
STEP: delete the pod 05/16/23 15:39:14.867
May 16 15:39:14.879: INFO: Waiting for pod downwardapi-volume-02a40874-2c8c-4791-a922-8b37ae68060a to disappear
May 16 15:39:14.881: INFO: Pod downwardapi-volume-02a40874-2c8c-4791-a922-8b37ae68060a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 16 15:39:14.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4674" for this suite. 05/16/23 15:39:14.885
------------------------------
â€¢ [4.081 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:39:10.81
    May 16 15:39:10.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename downward-api 05/16/23 15:39:10.81
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:39:10.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:39:10.834
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 05/16/23 15:39:10.837
    May 16 15:39:10.852: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02a40874-2c8c-4791-a922-8b37ae68060a" in namespace "downward-api-4674" to be "Succeeded or Failed"
    May 16 15:39:10.855: INFO: Pod "downwardapi-volume-02a40874-2c8c-4791-a922-8b37ae68060a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.248251ms
    May 16 15:39:12.859: INFO: Pod "downwardapi-volume-02a40874-2c8c-4791-a922-8b37ae68060a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007605875s
    May 16 15:39:14.859: INFO: Pod "downwardapi-volume-02a40874-2c8c-4791-a922-8b37ae68060a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007184736s
    STEP: Saw pod success 05/16/23 15:39:14.859
    May 16 15:39:14.859: INFO: Pod "downwardapi-volume-02a40874-2c8c-4791-a922-8b37ae68060a" satisfied condition "Succeeded or Failed"
    May 16 15:39:14.862: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-02a40874-2c8c-4791-a922-8b37ae68060a container client-container: <nil>
    STEP: delete the pod 05/16/23 15:39:14.867
    May 16 15:39:14.879: INFO: Waiting for pod downwardapi-volume-02a40874-2c8c-4791-a922-8b37ae68060a to disappear
    May 16 15:39:14.881: INFO: Pod downwardapi-volume-02a40874-2c8c-4791-a922-8b37ae68060a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 16 15:39:14.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4674" for this suite. 05/16/23 15:39:14.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:39:14.892
May 16 15:39:14.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename job 05/16/23 15:39:14.892
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:39:14.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:39:14.915
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 05/16/23 15:39:14.918
W0516 15:39:14.925840      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism 05/16/23 15:39:14.925
STEP: delete a job 05/16/23 15:39:16.93
STEP: deleting Job.batch foo in namespace job-8779, will wait for the garbage collector to delete the pods 05/16/23 15:39:16.93
May 16 15:39:16.989: INFO: Deleting Job.batch foo took: 5.959678ms
May 16 15:39:17.090: INFO: Terminating Job.batch foo pods took: 100.290871ms
STEP: Ensuring job was deleted 05/16/23 15:39:49.19
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
May 16 15:39:49.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8779" for this suite. 05/16/23 15:39:49.202
------------------------------
â€¢ [SLOW TEST] [34.316 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:39:14.892
    May 16 15:39:14.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename job 05/16/23 15:39:14.892
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:39:14.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:39:14.915
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 05/16/23 15:39:14.918
    W0516 15:39:14.925840      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring active pods == parallelism 05/16/23 15:39:14.925
    STEP: delete a job 05/16/23 15:39:16.93
    STEP: deleting Job.batch foo in namespace job-8779, will wait for the garbage collector to delete the pods 05/16/23 15:39:16.93
    May 16 15:39:16.989: INFO: Deleting Job.batch foo took: 5.959678ms
    May 16 15:39:17.090: INFO: Terminating Job.batch foo pods took: 100.290871ms
    STEP: Ensuring job was deleted 05/16/23 15:39:49.19
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    May 16 15:39:49.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8779" for this suite. 05/16/23 15:39:49.202
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:39:49.208
May 16 15:39:49.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 15:39:49.209
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:39:49.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:39:49.234
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-d88332f3-660e-4d3a-927f-1ef19116bfbb 05/16/23 15:39:49.236
STEP: Creating a pod to test consume secrets 05/16/23 15:39:49.244
May 16 15:39:49.254: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e7642e2d-b4df-440b-afe7-ab07101ed7bb" in namespace "projected-305" to be "Succeeded or Failed"
May 16 15:39:49.257: INFO: Pod "pod-projected-secrets-e7642e2d-b4df-440b-afe7-ab07101ed7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.315277ms
May 16 15:39:51.261: INFO: Pod "pod-projected-secrets-e7642e2d-b4df-440b-afe7-ab07101ed7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007169787s
May 16 15:39:53.264: INFO: Pod "pod-projected-secrets-e7642e2d-b4df-440b-afe7-ab07101ed7bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010653753s
STEP: Saw pod success 05/16/23 15:39:53.264
May 16 15:39:53.265: INFO: Pod "pod-projected-secrets-e7642e2d-b4df-440b-afe7-ab07101ed7bb" satisfied condition "Succeeded or Failed"
May 16 15:39:53.274: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-secrets-e7642e2d-b4df-440b-afe7-ab07101ed7bb container projected-secret-volume-test: <nil>
STEP: delete the pod 05/16/23 15:39:53.284
May 16 15:39:53.298: INFO: Waiting for pod pod-projected-secrets-e7642e2d-b4df-440b-afe7-ab07101ed7bb to disappear
May 16 15:39:53.302: INFO: Pod pod-projected-secrets-e7642e2d-b4df-440b-afe7-ab07101ed7bb no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
May 16 15:39:53.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-305" for this suite. 05/16/23 15:39:53.308
------------------------------
â€¢ [4.106 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:39:49.208
    May 16 15:39:49.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 15:39:49.209
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:39:49.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:39:49.234
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-d88332f3-660e-4d3a-927f-1ef19116bfbb 05/16/23 15:39:49.236
    STEP: Creating a pod to test consume secrets 05/16/23 15:39:49.244
    May 16 15:39:49.254: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e7642e2d-b4df-440b-afe7-ab07101ed7bb" in namespace "projected-305" to be "Succeeded or Failed"
    May 16 15:39:49.257: INFO: Pod "pod-projected-secrets-e7642e2d-b4df-440b-afe7-ab07101ed7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.315277ms
    May 16 15:39:51.261: INFO: Pod "pod-projected-secrets-e7642e2d-b4df-440b-afe7-ab07101ed7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007169787s
    May 16 15:39:53.264: INFO: Pod "pod-projected-secrets-e7642e2d-b4df-440b-afe7-ab07101ed7bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010653753s
    STEP: Saw pod success 05/16/23 15:39:53.264
    May 16 15:39:53.265: INFO: Pod "pod-projected-secrets-e7642e2d-b4df-440b-afe7-ab07101ed7bb" satisfied condition "Succeeded or Failed"
    May 16 15:39:53.274: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-secrets-e7642e2d-b4df-440b-afe7-ab07101ed7bb container projected-secret-volume-test: <nil>
    STEP: delete the pod 05/16/23 15:39:53.284
    May 16 15:39:53.298: INFO: Waiting for pod pod-projected-secrets-e7642e2d-b4df-440b-afe7-ab07101ed7bb to disappear
    May 16 15:39:53.302: INFO: Pod pod-projected-secrets-e7642e2d-b4df-440b-afe7-ab07101ed7bb no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    May 16 15:39:53.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-305" for this suite. 05/16/23 15:39:53.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:39:53.315
May 16 15:39:53.315: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename projected 05/16/23 15:39:53.315
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:39:53.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:39:53.338
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-c738da0d-67c0-471b-bf31-8876ecf3881d 05/16/23 15:39:53.341
STEP: Creating a pod to test consume configMaps 05/16/23 15:39:53.346
May 16 15:39:53.362: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-212f1fc4-6575-4bbe-8366-92404806fc5d" in namespace "projected-5776" to be "Succeeded or Failed"
May 16 15:39:53.365: INFO: Pod "pod-projected-configmaps-212f1fc4-6575-4bbe-8366-92404806fc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.86767ms
May 16 15:39:55.368: INFO: Pod "pod-projected-configmaps-212f1fc4-6575-4bbe-8366-92404806fc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006052078s
May 16 15:39:57.368: INFO: Pod "pod-projected-configmaps-212f1fc4-6575-4bbe-8366-92404806fc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006764956s
STEP: Saw pod success 05/16/23 15:39:57.368
May 16 15:39:57.369: INFO: Pod "pod-projected-configmaps-212f1fc4-6575-4bbe-8366-92404806fc5d" satisfied condition "Succeeded or Failed"
May 16 15:39:57.372: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-configmaps-212f1fc4-6575-4bbe-8366-92404806fc5d container agnhost-container: <nil>
STEP: delete the pod 05/16/23 15:39:57.38
May 16 15:39:57.389: INFO: Waiting for pod pod-projected-configmaps-212f1fc4-6575-4bbe-8366-92404806fc5d to disappear
May 16 15:39:57.392: INFO: Pod pod-projected-configmaps-212f1fc4-6575-4bbe-8366-92404806fc5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
May 16 15:39:57.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5776" for this suite. 05/16/23 15:39:57.396
------------------------------
â€¢ [4.087 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:39:53.315
    May 16 15:39:53.315: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename projected 05/16/23 15:39:53.315
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:39:53.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:39:53.338
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-c738da0d-67c0-471b-bf31-8876ecf3881d 05/16/23 15:39:53.341
    STEP: Creating a pod to test consume configMaps 05/16/23 15:39:53.346
    May 16 15:39:53.362: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-212f1fc4-6575-4bbe-8366-92404806fc5d" in namespace "projected-5776" to be "Succeeded or Failed"
    May 16 15:39:53.365: INFO: Pod "pod-projected-configmaps-212f1fc4-6575-4bbe-8366-92404806fc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.86767ms
    May 16 15:39:55.368: INFO: Pod "pod-projected-configmaps-212f1fc4-6575-4bbe-8366-92404806fc5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006052078s
    May 16 15:39:57.368: INFO: Pod "pod-projected-configmaps-212f1fc4-6575-4bbe-8366-92404806fc5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006764956s
    STEP: Saw pod success 05/16/23 15:39:57.368
    May 16 15:39:57.369: INFO: Pod "pod-projected-configmaps-212f1fc4-6575-4bbe-8366-92404806fc5d" satisfied condition "Succeeded or Failed"
    May 16 15:39:57.372: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod pod-projected-configmaps-212f1fc4-6575-4bbe-8366-92404806fc5d container agnhost-container: <nil>
    STEP: delete the pod 05/16/23 15:39:57.38
    May 16 15:39:57.389: INFO: Waiting for pod pod-projected-configmaps-212f1fc4-6575-4bbe-8366-92404806fc5d to disappear
    May 16 15:39:57.392: INFO: Pod pod-projected-configmaps-212f1fc4-6575-4bbe-8366-92404806fc5d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    May 16 15:39:57.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5776" for this suite. 05/16/23 15:39:57.396
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:39:57.402
May 16 15:39:57.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename container-runtime 05/16/23 15:39:57.403
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:39:57.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:39:57.424
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 05/16/23 15:39:57.426
W0516 15:39:57.439783      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: wait for the container to reach Failed 05/16/23 15:39:57.439
STEP: get the container status 05/16/23 15:40:00.454
STEP: the container should be terminated 05/16/23 15:40:00.457
STEP: the termination message should be set 05/16/23 15:40:00.457
May 16 15:40:00.457: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 05/16/23 15:40:00.457
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
May 16 15:40:00.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3138" for this suite. 05/16/23 15:40:00.476
------------------------------
â€¢ [3.078 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:39:57.402
    May 16 15:39:57.402: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename container-runtime 05/16/23 15:39:57.403
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:39:57.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:39:57.424
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 05/16/23 15:39:57.426
    W0516 15:39:57.439783      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: wait for the container to reach Failed 05/16/23 15:39:57.439
    STEP: get the container status 05/16/23 15:40:00.454
    STEP: the container should be terminated 05/16/23 15:40:00.457
    STEP: the termination message should be set 05/16/23 15:40:00.457
    May 16 15:40:00.457: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 05/16/23 15:40:00.457
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    May 16 15:40:00.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3138" for this suite. 05/16/23 15:40:00.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:40:00.481
May 16 15:40:00.481: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename pods 05/16/23 15:40:00.482
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:40:00.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:40:00.502
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
May 16 15:40:00.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: creating the pod 05/16/23 15:40:00.505
STEP: submitting the pod to kubernetes 05/16/23 15:40:00.505
May 16 15:40:00.524: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-0280e836-871c-48b2-bd64-be12e29a70b2" in namespace "pods-4142" to be "running and ready"
May 16 15:40:00.533: INFO: Pod "pod-logs-websocket-0280e836-871c-48b2-bd64-be12e29a70b2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.664683ms
May 16 15:40:00.533: INFO: The phase of Pod pod-logs-websocket-0280e836-871c-48b2-bd64-be12e29a70b2 is Pending, waiting for it to be Running (with Ready = true)
May 16 15:40:02.542: INFO: Pod "pod-logs-websocket-0280e836-871c-48b2-bd64-be12e29a70b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017641121s
May 16 15:40:02.542: INFO: The phase of Pod pod-logs-websocket-0280e836-871c-48b2-bd64-be12e29a70b2 is Running (Ready = true)
May 16 15:40:02.542: INFO: Pod "pod-logs-websocket-0280e836-871c-48b2-bd64-be12e29a70b2" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
May 16 15:40:02.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4142" for this suite. 05/16/23 15:40:02.566
------------------------------
â€¢ [2.091 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:40:00.481
    May 16 15:40:00.481: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename pods 05/16/23 15:40:00.482
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:40:00.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:40:00.502
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    May 16 15:40:00.504: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: creating the pod 05/16/23 15:40:00.505
    STEP: submitting the pod to kubernetes 05/16/23 15:40:00.505
    May 16 15:40:00.524: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-0280e836-871c-48b2-bd64-be12e29a70b2" in namespace "pods-4142" to be "running and ready"
    May 16 15:40:00.533: INFO: Pod "pod-logs-websocket-0280e836-871c-48b2-bd64-be12e29a70b2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.664683ms
    May 16 15:40:00.533: INFO: The phase of Pod pod-logs-websocket-0280e836-871c-48b2-bd64-be12e29a70b2 is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:40:02.542: INFO: Pod "pod-logs-websocket-0280e836-871c-48b2-bd64-be12e29a70b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017641121s
    May 16 15:40:02.542: INFO: The phase of Pod pod-logs-websocket-0280e836-871c-48b2-bd64-be12e29a70b2 is Running (Ready = true)
    May 16 15:40:02.542: INFO: Pod "pod-logs-websocket-0280e836-871c-48b2-bd64-be12e29a70b2" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    May 16 15:40:02.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4142" for this suite. 05/16/23 15:40:02.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:40:02.574
May 16 15:40:02.574: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename disruption 05/16/23 15:40:02.575
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:40:02.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:40:02.592
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 05/16/23 15:40:02.594
STEP: Waiting for the pdb to be processed 05/16/23 15:40:02.603
STEP: updating the pdb 05/16/23 15:40:04.611
STEP: Waiting for the pdb to be processed 05/16/23 15:40:04.62
STEP: patching the pdb 05/16/23 15:40:06.625
STEP: Waiting for the pdb to be processed 05/16/23 15:40:06.634
STEP: Waiting for the pdb to be deleted 05/16/23 15:40:08.645
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
May 16 15:40:08.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6882" for this suite. 05/16/23 15:40:08.652
------------------------------
â€¢ [SLOW TEST] [6.085 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:40:02.574
    May 16 15:40:02.574: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename disruption 05/16/23 15:40:02.575
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:40:02.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:40:02.592
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 05/16/23 15:40:02.594
    STEP: Waiting for the pdb to be processed 05/16/23 15:40:02.603
    STEP: updating the pdb 05/16/23 15:40:04.611
    STEP: Waiting for the pdb to be processed 05/16/23 15:40:04.62
    STEP: patching the pdb 05/16/23 15:40:06.625
    STEP: Waiting for the pdb to be processed 05/16/23 15:40:06.634
    STEP: Waiting for the pdb to be deleted 05/16/23 15:40:08.645
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    May 16 15:40:08.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6882" for this suite. 05/16/23 15:40:08.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:40:08.67
May 16 15:40:08.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename var-expansion 05/16/23 15:40:08.671
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:40:08.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:40:08.695
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 05/16/23 15:40:08.697
May 16 15:40:08.712: INFO: Waiting up to 5m0s for pod "var-expansion-0cea3108-51c6-44c7-ba26-c6f7e26170e9" in namespace "var-expansion-5198" to be "Succeeded or Failed"
May 16 15:40:08.714: INFO: Pod "var-expansion-0cea3108-51c6-44c7-ba26-c6f7e26170e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.625329ms
May 16 15:40:10.719: INFO: Pod "var-expansion-0cea3108-51c6-44c7-ba26-c6f7e26170e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007256318s
May 16 15:40:12.719: INFO: Pod "var-expansion-0cea3108-51c6-44c7-ba26-c6f7e26170e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007275633s
STEP: Saw pod success 05/16/23 15:40:12.719
May 16 15:40:12.719: INFO: Pod "var-expansion-0cea3108-51c6-44c7-ba26-c6f7e26170e9" satisfied condition "Succeeded or Failed"
May 16 15:40:12.722: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod var-expansion-0cea3108-51c6-44c7-ba26-c6f7e26170e9 container dapi-container: <nil>
STEP: delete the pod 05/16/23 15:40:12.727
May 16 15:40:12.737: INFO: Waiting for pod var-expansion-0cea3108-51c6-44c7-ba26-c6f7e26170e9 to disappear
May 16 15:40:12.739: INFO: Pod var-expansion-0cea3108-51c6-44c7-ba26-c6f7e26170e9 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
May 16 15:40:12.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5198" for this suite. 05/16/23 15:40:12.743
------------------------------
â€¢ [4.080 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:40:08.67
    May 16 15:40:08.670: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename var-expansion 05/16/23 15:40:08.671
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:40:08.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:40:08.695
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 05/16/23 15:40:08.697
    May 16 15:40:08.712: INFO: Waiting up to 5m0s for pod "var-expansion-0cea3108-51c6-44c7-ba26-c6f7e26170e9" in namespace "var-expansion-5198" to be "Succeeded or Failed"
    May 16 15:40:08.714: INFO: Pod "var-expansion-0cea3108-51c6-44c7-ba26-c6f7e26170e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.625329ms
    May 16 15:40:10.719: INFO: Pod "var-expansion-0cea3108-51c6-44c7-ba26-c6f7e26170e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007256318s
    May 16 15:40:12.719: INFO: Pod "var-expansion-0cea3108-51c6-44c7-ba26-c6f7e26170e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007275633s
    STEP: Saw pod success 05/16/23 15:40:12.719
    May 16 15:40:12.719: INFO: Pod "var-expansion-0cea3108-51c6-44c7-ba26-c6f7e26170e9" satisfied condition "Succeeded or Failed"
    May 16 15:40:12.722: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod var-expansion-0cea3108-51c6-44c7-ba26-c6f7e26170e9 container dapi-container: <nil>
    STEP: delete the pod 05/16/23 15:40:12.727
    May 16 15:40:12.737: INFO: Waiting for pod var-expansion-0cea3108-51c6-44c7-ba26-c6f7e26170e9 to disappear
    May 16 15:40:12.739: INFO: Pod var-expansion-0cea3108-51c6-44c7-ba26-c6f7e26170e9 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    May 16 15:40:12.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5198" for this suite. 05/16/23 15:40:12.743
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:40:12.75
May 16 15:40:12.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename gc 05/16/23 15:40:12.751
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:40:12.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:40:12.769
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 05/16/23 15:40:12.784
STEP: create the rc2 05/16/23 15:40:12.789
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 05/16/23 15:40:17.798
STEP: delete the rc simpletest-rc-to-be-deleted 05/16/23 15:40:18.252
STEP: wait for the rc to be deleted 05/16/23 15:40:18.263
STEP: Gathering metrics 05/16/23 15:40:23.273
W0516 15:40:23.275707      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0516 15:40:23.275723      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May 16 15:40:23.275: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 16 15:40:23.275: INFO: Deleting pod "simpletest-rc-to-be-deleted-25ttb" in namespace "gc-9189"
May 16 15:40:23.285: INFO: Deleting pod "simpletest-rc-to-be-deleted-287jf" in namespace "gc-9189"
May 16 15:40:23.294: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qhtw" in namespace "gc-9189"
May 16 15:40:23.305: INFO: Deleting pod "simpletest-rc-to-be-deleted-49kql" in namespace "gc-9189"
May 16 15:40:23.318: INFO: Deleting pod "simpletest-rc-to-be-deleted-4l4hq" in namespace "gc-9189"
May 16 15:40:23.329: INFO: Deleting pod "simpletest-rc-to-be-deleted-4pzc5" in namespace "gc-9189"
May 16 15:40:23.343: INFO: Deleting pod "simpletest-rc-to-be-deleted-5b9qv" in namespace "gc-9189"
May 16 15:40:23.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-5frbb" in namespace "gc-9189"
May 16 15:40:23.367: INFO: Deleting pod "simpletest-rc-to-be-deleted-5m9v4" in namespace "gc-9189"
May 16 15:40:23.378: INFO: Deleting pod "simpletest-rc-to-be-deleted-5n6px" in namespace "gc-9189"
May 16 15:40:23.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qn5p" in namespace "gc-9189"
May 16 15:40:23.401: INFO: Deleting pod "simpletest-rc-to-be-deleted-625w4" in namespace "gc-9189"
May 16 15:40:23.413: INFO: Deleting pod "simpletest-rc-to-be-deleted-69g8j" in namespace "gc-9189"
May 16 15:40:23.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lfkr" in namespace "gc-9189"
May 16 15:40:23.448: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vdhx" in namespace "gc-9189"
May 16 15:40:23.459: INFO: Deleting pod "simpletest-rc-to-be-deleted-6xvj6" in namespace "gc-9189"
May 16 15:40:23.471: INFO: Deleting pod "simpletest-rc-to-be-deleted-7p2pl" in namespace "gc-9189"
May 16 15:40:23.481: INFO: Deleting pod "simpletest-rc-to-be-deleted-7sxqw" in namespace "gc-9189"
May 16 15:40:23.492: INFO: Deleting pod "simpletest-rc-to-be-deleted-88tx7" in namespace "gc-9189"
May 16 15:40:23.510: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bjv8" in namespace "gc-9189"
May 16 15:40:23.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-94nz4" in namespace "gc-9189"
May 16 15:40:23.542: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bvf8" in namespace "gc-9189"
May 16 15:40:23.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fhlp" in namespace "gc-9189"
May 16 15:40:23.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-9hxl5" in namespace "gc-9189"
May 16 15:40:23.579: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mns4" in namespace "gc-9189"
May 16 15:40:23.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-9r4vp" in namespace "gc-9189"
May 16 15:40:23.607: INFO: Deleting pod "simpletest-rc-to-be-deleted-9t8gc" in namespace "gc-9189"
May 16 15:40:23.616: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9xln" in namespace "gc-9189"
May 16 15:40:23.629: INFO: Deleting pod "simpletest-rc-to-be-deleted-bd4q8" in namespace "gc-9189"
May 16 15:40:23.643: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdx79" in namespace "gc-9189"
May 16 15:40:23.660: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmlwq" in namespace "gc-9189"
May 16 15:40:23.681: INFO: Deleting pod "simpletest-rc-to-be-deleted-cb52l" in namespace "gc-9189"
May 16 15:40:23.703: INFO: Deleting pod "simpletest-rc-to-be-deleted-cccgw" in namespace "gc-9189"
May 16 15:40:23.716: INFO: Deleting pod "simpletest-rc-to-be-deleted-cf742" in namespace "gc-9189"
May 16 15:40:23.736: INFO: Deleting pod "simpletest-rc-to-be-deleted-cp2kn" in namespace "gc-9189"
May 16 15:40:23.752: INFO: Deleting pod "simpletest-rc-to-be-deleted-dbdxd" in namespace "gc-9189"
May 16 15:40:23.763: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpdw4" in namespace "gc-9189"
May 16 15:40:23.778: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdm2w" in namespace "gc-9189"
May 16 15:40:23.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-fh2dw" in namespace "gc-9189"
May 16 15:40:23.802: INFO: Deleting pod "simpletest-rc-to-be-deleted-gq7kb" in namespace "gc-9189"
May 16 15:40:23.817: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzb8g" in namespace "gc-9189"
May 16 15:40:23.829: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmnqh" in namespace "gc-9189"
May 16 15:40:23.841: INFO: Deleting pod "simpletest-rc-to-be-deleted-jjmcm" in namespace "gc-9189"
May 16 15:40:23.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-jqgr8" in namespace "gc-9189"
May 16 15:40:23.867: INFO: Deleting pod "simpletest-rc-to-be-deleted-k72pm" in namespace "gc-9189"
May 16 15:40:23.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-k8ln2" in namespace "gc-9189"
May 16 15:40:23.890: INFO: Deleting pod "simpletest-rc-to-be-deleted-lq2z4" in namespace "gc-9189"
May 16 15:40:23.902: INFO: Deleting pod "simpletest-rc-to-be-deleted-m74xj" in namespace "gc-9189"
May 16 15:40:23.913: INFO: Deleting pod "simpletest-rc-to-be-deleted-m7chd" in namespace "gc-9189"
May 16 15:40:23.924: INFO: Deleting pod "simpletest-rc-to-be-deleted-mszdt" in namespace "gc-9189"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
May 16 15:40:23.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9189" for this suite. 05/16/23 15:40:23.969
------------------------------
â€¢ [SLOW TEST] [11.231 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:40:12.75
    May 16 15:40:12.750: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename gc 05/16/23 15:40:12.751
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:40:12.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:40:12.769
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 05/16/23 15:40:12.784
    STEP: create the rc2 05/16/23 15:40:12.789
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 05/16/23 15:40:17.798
    STEP: delete the rc simpletest-rc-to-be-deleted 05/16/23 15:40:18.252
    STEP: wait for the rc to be deleted 05/16/23 15:40:18.263
    STEP: Gathering metrics 05/16/23 15:40:23.273
    W0516 15:40:23.275707      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
    W0516 15:40:23.275723      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    May 16 15:40:23.275: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    May 16 15:40:23.275: INFO: Deleting pod "simpletest-rc-to-be-deleted-25ttb" in namespace "gc-9189"
    May 16 15:40:23.285: INFO: Deleting pod "simpletest-rc-to-be-deleted-287jf" in namespace "gc-9189"
    May 16 15:40:23.294: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qhtw" in namespace "gc-9189"
    May 16 15:40:23.305: INFO: Deleting pod "simpletest-rc-to-be-deleted-49kql" in namespace "gc-9189"
    May 16 15:40:23.318: INFO: Deleting pod "simpletest-rc-to-be-deleted-4l4hq" in namespace "gc-9189"
    May 16 15:40:23.329: INFO: Deleting pod "simpletest-rc-to-be-deleted-4pzc5" in namespace "gc-9189"
    May 16 15:40:23.343: INFO: Deleting pod "simpletest-rc-to-be-deleted-5b9qv" in namespace "gc-9189"
    May 16 15:40:23.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-5frbb" in namespace "gc-9189"
    May 16 15:40:23.367: INFO: Deleting pod "simpletest-rc-to-be-deleted-5m9v4" in namespace "gc-9189"
    May 16 15:40:23.378: INFO: Deleting pod "simpletest-rc-to-be-deleted-5n6px" in namespace "gc-9189"
    May 16 15:40:23.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qn5p" in namespace "gc-9189"
    May 16 15:40:23.401: INFO: Deleting pod "simpletest-rc-to-be-deleted-625w4" in namespace "gc-9189"
    May 16 15:40:23.413: INFO: Deleting pod "simpletest-rc-to-be-deleted-69g8j" in namespace "gc-9189"
    May 16 15:40:23.429: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lfkr" in namespace "gc-9189"
    May 16 15:40:23.448: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vdhx" in namespace "gc-9189"
    May 16 15:40:23.459: INFO: Deleting pod "simpletest-rc-to-be-deleted-6xvj6" in namespace "gc-9189"
    May 16 15:40:23.471: INFO: Deleting pod "simpletest-rc-to-be-deleted-7p2pl" in namespace "gc-9189"
    May 16 15:40:23.481: INFO: Deleting pod "simpletest-rc-to-be-deleted-7sxqw" in namespace "gc-9189"
    May 16 15:40:23.492: INFO: Deleting pod "simpletest-rc-to-be-deleted-88tx7" in namespace "gc-9189"
    May 16 15:40:23.510: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bjv8" in namespace "gc-9189"
    May 16 15:40:23.525: INFO: Deleting pod "simpletest-rc-to-be-deleted-94nz4" in namespace "gc-9189"
    May 16 15:40:23.542: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bvf8" in namespace "gc-9189"
    May 16 15:40:23.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fhlp" in namespace "gc-9189"
    May 16 15:40:23.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-9hxl5" in namespace "gc-9189"
    May 16 15:40:23.579: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mns4" in namespace "gc-9189"
    May 16 15:40:23.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-9r4vp" in namespace "gc-9189"
    May 16 15:40:23.607: INFO: Deleting pod "simpletest-rc-to-be-deleted-9t8gc" in namespace "gc-9189"
    May 16 15:40:23.616: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9xln" in namespace "gc-9189"
    May 16 15:40:23.629: INFO: Deleting pod "simpletest-rc-to-be-deleted-bd4q8" in namespace "gc-9189"
    May 16 15:40:23.643: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdx79" in namespace "gc-9189"
    May 16 15:40:23.660: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmlwq" in namespace "gc-9189"
    May 16 15:40:23.681: INFO: Deleting pod "simpletest-rc-to-be-deleted-cb52l" in namespace "gc-9189"
    May 16 15:40:23.703: INFO: Deleting pod "simpletest-rc-to-be-deleted-cccgw" in namespace "gc-9189"
    May 16 15:40:23.716: INFO: Deleting pod "simpletest-rc-to-be-deleted-cf742" in namespace "gc-9189"
    May 16 15:40:23.736: INFO: Deleting pod "simpletest-rc-to-be-deleted-cp2kn" in namespace "gc-9189"
    May 16 15:40:23.752: INFO: Deleting pod "simpletest-rc-to-be-deleted-dbdxd" in namespace "gc-9189"
    May 16 15:40:23.763: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpdw4" in namespace "gc-9189"
    May 16 15:40:23.778: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdm2w" in namespace "gc-9189"
    May 16 15:40:23.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-fh2dw" in namespace "gc-9189"
    May 16 15:40:23.802: INFO: Deleting pod "simpletest-rc-to-be-deleted-gq7kb" in namespace "gc-9189"
    May 16 15:40:23.817: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzb8g" in namespace "gc-9189"
    May 16 15:40:23.829: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmnqh" in namespace "gc-9189"
    May 16 15:40:23.841: INFO: Deleting pod "simpletest-rc-to-be-deleted-jjmcm" in namespace "gc-9189"
    May 16 15:40:23.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-jqgr8" in namespace "gc-9189"
    May 16 15:40:23.867: INFO: Deleting pod "simpletest-rc-to-be-deleted-k72pm" in namespace "gc-9189"
    May 16 15:40:23.878: INFO: Deleting pod "simpletest-rc-to-be-deleted-k8ln2" in namespace "gc-9189"
    May 16 15:40:23.890: INFO: Deleting pod "simpletest-rc-to-be-deleted-lq2z4" in namespace "gc-9189"
    May 16 15:40:23.902: INFO: Deleting pod "simpletest-rc-to-be-deleted-m74xj" in namespace "gc-9189"
    May 16 15:40:23.913: INFO: Deleting pod "simpletest-rc-to-be-deleted-m7chd" in namespace "gc-9189"
    May 16 15:40:23.924: INFO: Deleting pod "simpletest-rc-to-be-deleted-mszdt" in namespace "gc-9189"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    May 16 15:40:23.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9189" for this suite. 05/16/23 15:40:23.969
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:40:23.982
May 16 15:40:23.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename taint-multiple-pods 05/16/23 15:40:23.982
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:40:24.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:40:24.014
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
May 16 15:40:24.029: INFO: Waiting up to 1m0s for all nodes to be ready
May 16 15:41:24.195: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
May 16 15:41:24.200: INFO: Starting informer...
STEP: Starting pods... 05/16/23 15:41:24.2
May 16 15:41:24.427: INFO: Pod1 is running on ip-10-0-161-164.eu-central-1.compute.internal. Tainting Node
May 16 15:41:24.637: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-987" to be "running"
May 16 15:41:24.640: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.75881ms
May 16 15:41:26.644: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007316901s
May 16 15:41:26.644: INFO: Pod "taint-eviction-b1" satisfied condition "running"
May 16 15:41:26.644: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-987" to be "running"
May 16 15:41:26.647: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.898716ms
May 16 15:41:26.647: INFO: Pod "taint-eviction-b2" satisfied condition "running"
May 16 15:41:26.647: INFO: Pod2 is running on ip-10-0-161-164.eu-central-1.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node 05/16/23 15:41:26.647
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/16/23 15:41:26.657
STEP: Waiting for Pod1 and Pod2 to be deleted 05/16/23 15:41:26.668
May 16 15:41:32.180: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May 16 15:41:52.217: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/16/23 15:41:52.235
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 15:41:52.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-987" for this suite. 05/16/23 15:41:52.25
------------------------------
â€¢ [SLOW TEST] [88.288 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:40:23.982
    May 16 15:40:23.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename taint-multiple-pods 05/16/23 15:40:23.982
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:40:24.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:40:24.014
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    May 16 15:40:24.029: INFO: Waiting up to 1m0s for all nodes to be ready
    May 16 15:41:24.195: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    May 16 15:41:24.200: INFO: Starting informer...
    STEP: Starting pods... 05/16/23 15:41:24.2
    May 16 15:41:24.427: INFO: Pod1 is running on ip-10-0-161-164.eu-central-1.compute.internal. Tainting Node
    May 16 15:41:24.637: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-987" to be "running"
    May 16 15:41:24.640: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.75881ms
    May 16 15:41:26.644: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007316901s
    May 16 15:41:26.644: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    May 16 15:41:26.644: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-987" to be "running"
    May 16 15:41:26.647: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.898716ms
    May 16 15:41:26.647: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    May 16 15:41:26.647: INFO: Pod2 is running on ip-10-0-161-164.eu-central-1.compute.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 05/16/23 15:41:26.647
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/16/23 15:41:26.657
    STEP: Waiting for Pod1 and Pod2 to be deleted 05/16/23 15:41:26.668
    May 16 15:41:32.180: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    May 16 15:41:52.217: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 05/16/23 15:41:52.235
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 15:41:52.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-987" for this suite. 05/16/23 15:41:52.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:41:52.27
May 16 15:41:52.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename namespaces 05/16/23 15:41:52.27
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:41:52.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:41:52.333
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-4175" 05/16/23 15:41:52.336
May 16 15:41:52.364: INFO: Namespace "namespaces-4175" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"38fb2494-031f-4052-9538-25d04cdaece2", "kubernetes.io/metadata.name":"namespaces-4175", "namespaces-4175":"updated", "pod-security.kubernetes.io/audit":"privileged", "pod-security.kubernetes.io/audit-version":"v1.24", "pod-security.kubernetes.io/enforce":"baseline", "pod-security.kubernetes.io/warn":"privileged", "pod-security.kubernetes.io/warn-version":"v1.24"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
May 16 15:41:52.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4175" for this suite. 05/16/23 15:41:52.374
------------------------------
â€¢ [0.118 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:41:52.27
    May 16 15:41:52.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename namespaces 05/16/23 15:41:52.27
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:41:52.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:41:52.333
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-4175" 05/16/23 15:41:52.336
    May 16 15:41:52.364: INFO: Namespace "namespaces-4175" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"38fb2494-031f-4052-9538-25d04cdaece2", "kubernetes.io/metadata.name":"namespaces-4175", "namespaces-4175":"updated", "pod-security.kubernetes.io/audit":"privileged", "pod-security.kubernetes.io/audit-version":"v1.24", "pod-security.kubernetes.io/enforce":"baseline", "pod-security.kubernetes.io/warn":"privileged", "pod-security.kubernetes.io/warn-version":"v1.24"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    May 16 15:41:52.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4175" for this suite. 05/16/23 15:41:52.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:41:52.388
May 16 15:41:52.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename services 05/16/23 15:41:52.389
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:41:52.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:41:52.411
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-5505 05/16/23 15:41:52.418
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5505 to expose endpoints map[] 05/16/23 15:41:52.434
May 16 15:41:52.442: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
May 16 15:41:53.449: INFO: successfully validated that service multi-endpoint-test in namespace services-5505 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5505 05/16/23 15:41:53.449
May 16 15:41:53.460: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5505" to be "running and ready"
May 16 15:41:53.462: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.712824ms
May 16 15:41:53.462: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 16 15:41:55.466: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006766275s
May 16 15:41:55.466: INFO: The phase of Pod pod1 is Running (Ready = true)
May 16 15:41:55.466: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5505 to expose endpoints map[pod1:[100]] 05/16/23 15:41:55.469
May 16 15:41:55.478: INFO: successfully validated that service multi-endpoint-test in namespace services-5505 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5505 05/16/23 15:41:55.478
May 16 15:41:55.487: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5505" to be "running and ready"
May 16 15:41:55.491: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.314641ms
May 16 15:41:55.491: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 16 15:41:57.495: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008696189s
May 16 15:41:57.495: INFO: The phase of Pod pod2 is Running (Ready = true)
May 16 15:41:57.495: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5505 to expose endpoints map[pod1:[100] pod2:[101]] 05/16/23 15:41:57.5
May 16 15:41:57.514: INFO: successfully validated that service multi-endpoint-test in namespace services-5505 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 05/16/23 15:41:57.514
May 16 15:41:57.514: INFO: Creating new exec pod
May 16 15:41:57.520: INFO: Waiting up to 5m0s for pod "execpodx26ks" in namespace "services-5505" to be "running"
May 16 15:41:57.523: INFO: Pod "execpodx26ks": Phase="Pending", Reason="", readiness=false. Elapsed: 2.940988ms
May 16 15:41:59.526: INFO: Pod "execpodx26ks": Phase="Running", Reason="", readiness=true. Elapsed: 2.006075779s
May 16 15:41:59.526: INFO: Pod "execpodx26ks" satisfied condition "running"
May 16 15:42:00.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5505 exec execpodx26ks -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
May 16 15:42:00.651: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
May 16 15:42:00.651: INFO: stdout: ""
May 16 15:42:00.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5505 exec execpodx26ks -- /bin/sh -x -c nc -v -z -w 2 172.30.180.213 80'
May 16 15:42:00.768: INFO: stderr: "+ nc -v -z -w 2 172.30.180.213 80\nConnection to 172.30.180.213 80 port [tcp/http] succeeded!\n"
May 16 15:42:00.768: INFO: stdout: ""
May 16 15:42:00.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5505 exec execpodx26ks -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
May 16 15:42:00.867: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
May 16 15:42:00.867: INFO: stdout: ""
May 16 15:42:00.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5505 exec execpodx26ks -- /bin/sh -x -c nc -v -z -w 2 172.30.180.213 81'
May 16 15:42:00.982: INFO: stderr: "+ nc -v -z -w 2 172.30.180.213 81\nConnection to 172.30.180.213 81 port [tcp/*] succeeded!\n"
May 16 15:42:00.982: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-5505 05/16/23 15:42:00.982
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5505 to expose endpoints map[pod2:[101]] 05/16/23 15:42:00.995
May 16 15:42:01.004: INFO: successfully validated that service multi-endpoint-test in namespace services-5505 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5505 05/16/23 15:42:01.004
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5505 to expose endpoints map[] 05/16/23 15:42:01.016
May 16 15:42:02.047: INFO: successfully validated that service multi-endpoint-test in namespace services-5505 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
May 16 15:42:02.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5505" for this suite. 05/16/23 15:42:02.099
------------------------------
â€¢ [SLOW TEST] [9.728 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:41:52.388
    May 16 15:41:52.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename services 05/16/23 15:41:52.389
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:41:52.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:41:52.411
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-5505 05/16/23 15:41:52.418
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5505 to expose endpoints map[] 05/16/23 15:41:52.434
    May 16 15:41:52.442: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    May 16 15:41:53.449: INFO: successfully validated that service multi-endpoint-test in namespace services-5505 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5505 05/16/23 15:41:53.449
    May 16 15:41:53.460: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5505" to be "running and ready"
    May 16 15:41:53.462: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.712824ms
    May 16 15:41:53.462: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:41:55.466: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006766275s
    May 16 15:41:55.466: INFO: The phase of Pod pod1 is Running (Ready = true)
    May 16 15:41:55.466: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5505 to expose endpoints map[pod1:[100]] 05/16/23 15:41:55.469
    May 16 15:41:55.478: INFO: successfully validated that service multi-endpoint-test in namespace services-5505 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-5505 05/16/23 15:41:55.478
    May 16 15:41:55.487: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5505" to be "running and ready"
    May 16 15:41:55.491: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.314641ms
    May 16 15:41:55.491: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    May 16 15:41:57.495: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008696189s
    May 16 15:41:57.495: INFO: The phase of Pod pod2 is Running (Ready = true)
    May 16 15:41:57.495: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5505 to expose endpoints map[pod1:[100] pod2:[101]] 05/16/23 15:41:57.5
    May 16 15:41:57.514: INFO: successfully validated that service multi-endpoint-test in namespace services-5505 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 05/16/23 15:41:57.514
    May 16 15:41:57.514: INFO: Creating new exec pod
    May 16 15:41:57.520: INFO: Waiting up to 5m0s for pod "execpodx26ks" in namespace "services-5505" to be "running"
    May 16 15:41:57.523: INFO: Pod "execpodx26ks": Phase="Pending", Reason="", readiness=false. Elapsed: 2.940988ms
    May 16 15:41:59.526: INFO: Pod "execpodx26ks": Phase="Running", Reason="", readiness=true. Elapsed: 2.006075779s
    May 16 15:41:59.526: INFO: Pod "execpodx26ks" satisfied condition "running"
    May 16 15:42:00.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5505 exec execpodx26ks -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    May 16 15:42:00.651: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    May 16 15:42:00.651: INFO: stdout: ""
    May 16 15:42:00.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5505 exec execpodx26ks -- /bin/sh -x -c nc -v -z -w 2 172.30.180.213 80'
    May 16 15:42:00.768: INFO: stderr: "+ nc -v -z -w 2 172.30.180.213 80\nConnection to 172.30.180.213 80 port [tcp/http] succeeded!\n"
    May 16 15:42:00.768: INFO: stdout: ""
    May 16 15:42:00.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5505 exec execpodx26ks -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    May 16 15:42:00.867: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    May 16 15:42:00.867: INFO: stdout: ""
    May 16 15:42:00.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1791468767 --namespace=services-5505 exec execpodx26ks -- /bin/sh -x -c nc -v -z -w 2 172.30.180.213 81'
    May 16 15:42:00.982: INFO: stderr: "+ nc -v -z -w 2 172.30.180.213 81\nConnection to 172.30.180.213 81 port [tcp/*] succeeded!\n"
    May 16 15:42:00.982: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-5505 05/16/23 15:42:00.982
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5505 to expose endpoints map[pod2:[101]] 05/16/23 15:42:00.995
    May 16 15:42:01.004: INFO: successfully validated that service multi-endpoint-test in namespace services-5505 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-5505 05/16/23 15:42:01.004
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5505 to expose endpoints map[] 05/16/23 15:42:01.016
    May 16 15:42:02.047: INFO: successfully validated that service multi-endpoint-test in namespace services-5505 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    May 16 15:42:02.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5505" for this suite. 05/16/23 15:42:02.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 05/16/23 15:42:02.116
May 16 15:42:02.116: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
STEP: Building a namespace api object, basename downward-api 05/16/23 15:42:02.117
STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:42:02.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:42:02.143
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 05/16/23 15:42:02.151
May 16 15:42:02.173: INFO: Waiting up to 5m0s for pod "downwardapi-volume-41a7362c-074b-4a54-8274-df565ba980bb" in namespace "downward-api-8484" to be "Succeeded or Failed"
May 16 15:42:02.180: INFO: Pod "downwardapi-volume-41a7362c-074b-4a54-8274-df565ba980bb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.026516ms
May 16 15:42:04.217: INFO: Pod "downwardapi-volume-41a7362c-074b-4a54-8274-df565ba980bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043819911s
May 16 15:42:06.184: INFO: Pod "downwardapi-volume-41a7362c-074b-4a54-8274-df565ba980bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010872647s
STEP: Saw pod success 05/16/23 15:42:06.184
May 16 15:42:06.184: INFO: Pod "downwardapi-volume-41a7362c-074b-4a54-8274-df565ba980bb" satisfied condition "Succeeded or Failed"
May 16 15:42:06.187: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-41a7362c-074b-4a54-8274-df565ba980bb container client-container: <nil>
STEP: delete the pod 05/16/23 15:42:06.195
May 16 15:42:06.204: INFO: Waiting for pod downwardapi-volume-41a7362c-074b-4a54-8274-df565ba980bb to disappear
May 16 15:42:06.207: INFO: Pod downwardapi-volume-41a7362c-074b-4a54-8274-df565ba980bb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
May 16 15:42:06.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8484" for this suite. 05/16/23 15:42:06.211
------------------------------
â€¢ [4.101 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 05/16/23 15:42:02.116
    May 16 15:42:02.116: INFO: >>> kubeConfig: /tmp/kubeconfig-1791468767
    STEP: Building a namespace api object, basename downward-api 05/16/23 15:42:02.117
    STEP: Waiting for a default service account to be provisioned in namespace 05/16/23 15:42:02.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 05/16/23 15:42:02.143
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 05/16/23 15:42:02.151
    May 16 15:42:02.173: INFO: Waiting up to 5m0s for pod "downwardapi-volume-41a7362c-074b-4a54-8274-df565ba980bb" in namespace "downward-api-8484" to be "Succeeded or Failed"
    May 16 15:42:02.180: INFO: Pod "downwardapi-volume-41a7362c-074b-4a54-8274-df565ba980bb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.026516ms
    May 16 15:42:04.217: INFO: Pod "downwardapi-volume-41a7362c-074b-4a54-8274-df565ba980bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043819911s
    May 16 15:42:06.184: INFO: Pod "downwardapi-volume-41a7362c-074b-4a54-8274-df565ba980bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010872647s
    STEP: Saw pod success 05/16/23 15:42:06.184
    May 16 15:42:06.184: INFO: Pod "downwardapi-volume-41a7362c-074b-4a54-8274-df565ba980bb" satisfied condition "Succeeded or Failed"
    May 16 15:42:06.187: INFO: Trying to get logs from node ip-10-0-161-164.eu-central-1.compute.internal pod downwardapi-volume-41a7362c-074b-4a54-8274-df565ba980bb container client-container: <nil>
    STEP: delete the pod 05/16/23 15:42:06.195
    May 16 15:42:06.204: INFO: Waiting for pod downwardapi-volume-41a7362c-074b-4a54-8274-df565ba980bb to disappear
    May 16 15:42:06.207: INFO: Pod downwardapi-volume-41a7362c-074b-4a54-8274-df565ba980bb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    May 16 15:42:06.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8484" for this suite. 05/16/23 15:42:06.211
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
May 16 15:42:06.217: INFO: Running AfterSuite actions on node 1
May 16 15:42:06.217: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    May 16 15:42:06.217: INFO: Running AfterSuite actions on node 1
    May 16 15:42:06.217: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.053 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5445.369 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h30m45.629759713s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

