I0622 10:19:44.019310      23 e2e.go:126] Starting e2e run "e5d41edb-1c5f-49df-95d4-4a50c9c0e5cf" on Ginkgo node 1
Jun 22 10:19:44.066: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1687429183 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jun 22 10:19:44.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:19:44.366: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 22 10:19:44.403: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 22 10:19:44.458: INFO: 34 / 34 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 22 10:19:44.458: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Jun 22 10:19:44.458: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 22 10:19:44.471: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'antrea-agent' (0 seconds elapsed)
Jun 22 10:19:44.471: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jun 22 10:19:44.471: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'vsphere-cloud-controller-manager' (0 seconds elapsed)
Jun 22 10:19:44.471: INFO: e2e test version: v1.26.5
Jun 22 10:19:44.472: INFO: kube-apiserver version: v1.26.5+vmware.1
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jun 22 10:19:44.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:19:44.479: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.118 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jun 22 10:19:44.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:19:44.366: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jun 22 10:19:44.403: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jun 22 10:19:44.458: INFO: 34 / 34 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jun 22 10:19:44.458: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
    Jun 22 10:19:44.458: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jun 22 10:19:44.471: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'antrea-agent' (0 seconds elapsed)
    Jun 22 10:19:44.471: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jun 22 10:19:44.471: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'vsphere-cloud-controller-manager' (0 seconds elapsed)
    Jun 22 10:19:44.471: INFO: e2e test version: v1.26.5
    Jun 22 10:19:44.472: INFO: kube-apiserver version: v1.26.5+vmware.1
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jun 22 10:19:44.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:19:44.479: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:19:44.516
Jun 22 10:19:44.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename sched-pred 06/22/23 10:19:44.517
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:19:44.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:19:44.542
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jun 22 10:19:44.547: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 10:19:44.568: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 10:19:44.576: INFO: 
Logging pods the apiserver thinks is on node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k before test
Jun 22 10:19:44.605: INFO: corgi-test-8c78878b-6r5tv from corgi-test-privileged started at 2023-06-22 06:12:10 +0000 UTC (1 container statuses recorded)
Jun 22 10:19:44.605: INFO: 	Container nginx ready: true, restart count 0
Jun 22 10:19:44.605: INFO: antrea-agent-dxs2v from kube-system started at 2023-06-22 05:45:55 +0000 UTC (2 container statuses recorded)
Jun 22 10:19:44.605: INFO: 	Container antrea-agent ready: true, restart count 0
Jun 22 10:19:44.605: INFO: 	Container antrea-ovs ready: true, restart count 0
Jun 22 10:19:44.605: INFO: kube-proxy-czx75 from kube-system started at 2023-06-22 05:45:55 +0000 UTC (1 container statuses recorded)
Jun 22 10:19:44.605: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 10:19:44.605: INFO: sonobuoy-e2e-job-38d4831e07f74ae5 from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
Jun 22 10:19:44.605: INFO: 	Container e2e ready: false, restart count 0
Jun 22 10:19:44.605: INFO: 	Container sonobuoy-worker ready: false, restart count 0
Jun 22 10:19:44.605: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-5d74n from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
Jun 22 10:19:44.605: INFO: 	Container sonobuoy-worker ready: false, restart count 0
Jun 22 10:19:44.605: INFO: 	Container systemd-logs ready: false, restart count 0
Jun 22 10:19:44.605: INFO: register-placeholder-rklvd from vmware-system-antrea started at 2023-06-22 10:17:16 +0000 UTC (1 container statuses recorded)
Jun 22 10:19:44.605: INFO: 	Container register ready: false, restart count 0
Jun 22 10:19:44.605: INFO: vsphere-csi-node-mp5v2 from vmware-system-csi started at 2023-06-22 05:45:55 +0000 UTC (3 container statuses recorded)
Jun 22 10:19:44.605: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 22 10:19:44.605: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 22 10:19:44.605: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jun 22 10:19:44.605: INFO: 
Logging pods the apiserver thinks is on node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw before test
Jun 22 10:19:44.636: INFO: antrea-agent-n9smp from kube-system started at 2023-06-22 05:45:26 +0000 UTC (2 container statuses recorded)
Jun 22 10:19:44.636: INFO: 	Container antrea-agent ready: true, restart count 0
Jun 22 10:19:44.636: INFO: 	Container antrea-ovs ready: true, restart count 0
Jun 22 10:19:44.636: INFO: kube-proxy-rlz8k from kube-system started at 2023-06-22 05:45:26 +0000 UTC (1 container statuses recorded)
Jun 22 10:19:44.636: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 10:19:44.636: INFO: sonobuoy from sonobuoy started at 2023-06-22 10:19:41 +0000 UTC (1 container statuses recorded)
Jun 22 10:19:44.636: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 10:19:44.636: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-f4lqb from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
Jun 22 10:19:44.637: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 10:19:44.637: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 10:19:44.637: INFO: vsphere-csi-node-v6pm5 from vmware-system-csi started at 2023-06-22 05:45:26 +0000 UTC (3 container statuses recorded)
Jun 22 10:19:44.637: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 22 10:19:44.637: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 22 10:19:44.637: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jun 22 10:19:44.637: INFO: 
Logging pods the apiserver thinks is on node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 before test
Jun 22 10:19:44.663: INFO: antrea-agent-94vtc from kube-system started at 2023-06-22 05:45:16 +0000 UTC (2 container statuses recorded)
Jun 22 10:19:44.664: INFO: 	Container antrea-agent ready: true, restart count 1
Jun 22 10:19:44.664: INFO: 	Container antrea-ovs ready: true, restart count 0
Jun 22 10:19:44.664: INFO: kube-proxy-8rvhg from kube-system started at 2023-06-22 05:45:16 +0000 UTC (1 container statuses recorded)
Jun 22 10:19:44.664: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 10:19:44.664: INFO: metrics-server-64f66d5b9-8j569 from kube-system started at 2023-06-22 05:45:49 +0000 UTC (1 container statuses recorded)
Jun 22 10:19:44.664: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 10:19:44.664: INFO: secretgen-controller-7bdd5787fd-h9nfx from secretgen-controller started at 2023-06-22 05:45:49 +0000 UTC (1 container statuses recorded)
Jun 22 10:19:44.664: INFO: 	Container secretgen-controller ready: true, restart count 0
Jun 22 10:19:44.664: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-7x88w from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
Jun 22 10:19:44.664: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 10:19:44.664: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 10:19:44.664: INFO: vsphere-csi-node-wsgg4 from vmware-system-csi started at 2023-06-22 05:45:17 +0000 UTC (3 container statuses recorded)
Jun 22 10:19:44.664: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 22 10:19:44.664: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 22 10:19:44.664: INFO: 	Container vsphere-csi-node ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 06/22/23 10:19:44.664
Jun 22 10:19:44.679: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-433" to be "running"
Jun 22 10:19:44.688: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.869489ms
Jun 22 10:19:46.696: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.01631661s
Jun 22 10:19:46.696: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 06/22/23 10:19:46.705
STEP: Trying to apply a random label on the found node. 06/22/23 10:19:46.729
STEP: verifying the node has the label kubernetes.io/e2e-4bd7deae-7f2f-4546-800b-afa56831188c 42 06/22/23 10:19:46.745
STEP: Trying to relaunch the pod, now with labels. 06/22/23 10:19:46.751
Jun 22 10:19:46.759: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-433" to be "not pending"
Jun 22 10:19:46.770: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 11.177754ms
Jun 22 10:19:48.919: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.159660763s
Jun 22 10:19:48.919: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-4bd7deae-7f2f-4546-800b-afa56831188c off the node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw 06/22/23 10:19:48.923
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4bd7deae-7f2f-4546-800b-afa56831188c 06/22/23 10:19:48.95
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:19:48.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-433" for this suite. 06/22/23 10:19:48.965
------------------------------
• [4.459 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:19:44.516
    Jun 22 10:19:44.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename sched-pred 06/22/23 10:19:44.517
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:19:44.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:19:44.542
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jun 22 10:19:44.547: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun 22 10:19:44.568: INFO: Waiting for terminating namespaces to be deleted...
    Jun 22 10:19:44.576: INFO: 
    Logging pods the apiserver thinks is on node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k before test
    Jun 22 10:19:44.605: INFO: corgi-test-8c78878b-6r5tv from corgi-test-privileged started at 2023-06-22 06:12:10 +0000 UTC (1 container statuses recorded)
    Jun 22 10:19:44.605: INFO: 	Container nginx ready: true, restart count 0
    Jun 22 10:19:44.605: INFO: antrea-agent-dxs2v from kube-system started at 2023-06-22 05:45:55 +0000 UTC (2 container statuses recorded)
    Jun 22 10:19:44.605: INFO: 	Container antrea-agent ready: true, restart count 0
    Jun 22 10:19:44.605: INFO: 	Container antrea-ovs ready: true, restart count 0
    Jun 22 10:19:44.605: INFO: kube-proxy-czx75 from kube-system started at 2023-06-22 05:45:55 +0000 UTC (1 container statuses recorded)
    Jun 22 10:19:44.605: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 22 10:19:44.605: INFO: sonobuoy-e2e-job-38d4831e07f74ae5 from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
    Jun 22 10:19:44.605: INFO: 	Container e2e ready: false, restart count 0
    Jun 22 10:19:44.605: INFO: 	Container sonobuoy-worker ready: false, restart count 0
    Jun 22 10:19:44.605: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-5d74n from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
    Jun 22 10:19:44.605: INFO: 	Container sonobuoy-worker ready: false, restart count 0
    Jun 22 10:19:44.605: INFO: 	Container systemd-logs ready: false, restart count 0
    Jun 22 10:19:44.605: INFO: register-placeholder-rklvd from vmware-system-antrea started at 2023-06-22 10:17:16 +0000 UTC (1 container statuses recorded)
    Jun 22 10:19:44.605: INFO: 	Container register ready: false, restart count 0
    Jun 22 10:19:44.605: INFO: vsphere-csi-node-mp5v2 from vmware-system-csi started at 2023-06-22 05:45:55 +0000 UTC (3 container statuses recorded)
    Jun 22 10:19:44.605: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 22 10:19:44.605: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 22 10:19:44.605: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Jun 22 10:19:44.605: INFO: 
    Logging pods the apiserver thinks is on node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw before test
    Jun 22 10:19:44.636: INFO: antrea-agent-n9smp from kube-system started at 2023-06-22 05:45:26 +0000 UTC (2 container statuses recorded)
    Jun 22 10:19:44.636: INFO: 	Container antrea-agent ready: true, restart count 0
    Jun 22 10:19:44.636: INFO: 	Container antrea-ovs ready: true, restart count 0
    Jun 22 10:19:44.636: INFO: kube-proxy-rlz8k from kube-system started at 2023-06-22 05:45:26 +0000 UTC (1 container statuses recorded)
    Jun 22 10:19:44.636: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 22 10:19:44.636: INFO: sonobuoy from sonobuoy started at 2023-06-22 10:19:41 +0000 UTC (1 container statuses recorded)
    Jun 22 10:19:44.636: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun 22 10:19:44.636: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-f4lqb from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
    Jun 22 10:19:44.637: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 22 10:19:44.637: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 22 10:19:44.637: INFO: vsphere-csi-node-v6pm5 from vmware-system-csi started at 2023-06-22 05:45:26 +0000 UTC (3 container statuses recorded)
    Jun 22 10:19:44.637: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 22 10:19:44.637: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 22 10:19:44.637: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Jun 22 10:19:44.637: INFO: 
    Logging pods the apiserver thinks is on node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 before test
    Jun 22 10:19:44.663: INFO: antrea-agent-94vtc from kube-system started at 2023-06-22 05:45:16 +0000 UTC (2 container statuses recorded)
    Jun 22 10:19:44.664: INFO: 	Container antrea-agent ready: true, restart count 1
    Jun 22 10:19:44.664: INFO: 	Container antrea-ovs ready: true, restart count 0
    Jun 22 10:19:44.664: INFO: kube-proxy-8rvhg from kube-system started at 2023-06-22 05:45:16 +0000 UTC (1 container statuses recorded)
    Jun 22 10:19:44.664: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 22 10:19:44.664: INFO: metrics-server-64f66d5b9-8j569 from kube-system started at 2023-06-22 05:45:49 +0000 UTC (1 container statuses recorded)
    Jun 22 10:19:44.664: INFO: 	Container metrics-server ready: true, restart count 0
    Jun 22 10:19:44.664: INFO: secretgen-controller-7bdd5787fd-h9nfx from secretgen-controller started at 2023-06-22 05:45:49 +0000 UTC (1 container statuses recorded)
    Jun 22 10:19:44.664: INFO: 	Container secretgen-controller ready: true, restart count 0
    Jun 22 10:19:44.664: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-7x88w from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
    Jun 22 10:19:44.664: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 22 10:19:44.664: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 22 10:19:44.664: INFO: vsphere-csi-node-wsgg4 from vmware-system-csi started at 2023-06-22 05:45:17 +0000 UTC (3 container statuses recorded)
    Jun 22 10:19:44.664: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 22 10:19:44.664: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 22 10:19:44.664: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 06/22/23 10:19:44.664
    Jun 22 10:19:44.679: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-433" to be "running"
    Jun 22 10:19:44.688: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.869489ms
    Jun 22 10:19:46.696: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.01631661s
    Jun 22 10:19:46.696: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 06/22/23 10:19:46.705
    STEP: Trying to apply a random label on the found node. 06/22/23 10:19:46.729
    STEP: verifying the node has the label kubernetes.io/e2e-4bd7deae-7f2f-4546-800b-afa56831188c 42 06/22/23 10:19:46.745
    STEP: Trying to relaunch the pod, now with labels. 06/22/23 10:19:46.751
    Jun 22 10:19:46.759: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-433" to be "not pending"
    Jun 22 10:19:46.770: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 11.177754ms
    Jun 22 10:19:48.919: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.159660763s
    Jun 22 10:19:48.919: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-4bd7deae-7f2f-4546-800b-afa56831188c off the node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw 06/22/23 10:19:48.923
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-4bd7deae-7f2f-4546-800b-afa56831188c 06/22/23 10:19:48.95
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:19:48.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-433" for this suite. 06/22/23 10:19:48.965
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:19:48.976
Jun 22 10:19:48.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename secrets 06/22/23 10:19:48.977
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:19:49.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:19:49.006
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-03e5ce43-cf73-4825-a256-57a33ed281a1 06/22/23 10:19:49.01
STEP: Creating a pod to test consume secrets 06/22/23 10:19:49.017
W0622 10:19:49.030686      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:19:49.030: INFO: Waiting up to 5m0s for pod "pod-secrets-a1565ec0-5891-42f5-9e8a-cf18e1f59087" in namespace "secrets-1282" to be "Succeeded or Failed"
Jun 22 10:19:49.035: INFO: Pod "pod-secrets-a1565ec0-5891-42f5-9e8a-cf18e1f59087": Phase="Pending", Reason="", readiness=false. Elapsed: 4.83854ms
Jun 22 10:19:51.042: INFO: Pod "pod-secrets-a1565ec0-5891-42f5-9e8a-cf18e1f59087": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011536906s
Jun 22 10:19:53.042: INFO: Pod "pod-secrets-a1565ec0-5891-42f5-9e8a-cf18e1f59087": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011484791s
STEP: Saw pod success 06/22/23 10:19:53.042
Jun 22 10:19:53.042: INFO: Pod "pod-secrets-a1565ec0-5891-42f5-9e8a-cf18e1f59087" satisfied condition "Succeeded or Failed"
Jun 22 10:19:53.047: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-secrets-a1565ec0-5891-42f5-9e8a-cf18e1f59087 container secret-volume-test: <nil>
STEP: delete the pod 06/22/23 10:19:53.069
Jun 22 10:19:53.089: INFO: Waiting for pod pod-secrets-a1565ec0-5891-42f5-9e8a-cf18e1f59087 to disappear
Jun 22 10:19:53.094: INFO: Pod pod-secrets-a1565ec0-5891-42f5-9e8a-cf18e1f59087 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun 22 10:19:53.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1282" for this suite. 06/22/23 10:19:53.105
------------------------------
• [4.138 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:19:48.976
    Jun 22 10:19:48.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename secrets 06/22/23 10:19:48.977
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:19:49.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:19:49.006
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-03e5ce43-cf73-4825-a256-57a33ed281a1 06/22/23 10:19:49.01
    STEP: Creating a pod to test consume secrets 06/22/23 10:19:49.017
    W0622 10:19:49.030686      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:19:49.030: INFO: Waiting up to 5m0s for pod "pod-secrets-a1565ec0-5891-42f5-9e8a-cf18e1f59087" in namespace "secrets-1282" to be "Succeeded or Failed"
    Jun 22 10:19:49.035: INFO: Pod "pod-secrets-a1565ec0-5891-42f5-9e8a-cf18e1f59087": Phase="Pending", Reason="", readiness=false. Elapsed: 4.83854ms
    Jun 22 10:19:51.042: INFO: Pod "pod-secrets-a1565ec0-5891-42f5-9e8a-cf18e1f59087": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011536906s
    Jun 22 10:19:53.042: INFO: Pod "pod-secrets-a1565ec0-5891-42f5-9e8a-cf18e1f59087": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011484791s
    STEP: Saw pod success 06/22/23 10:19:53.042
    Jun 22 10:19:53.042: INFO: Pod "pod-secrets-a1565ec0-5891-42f5-9e8a-cf18e1f59087" satisfied condition "Succeeded or Failed"
    Jun 22 10:19:53.047: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-secrets-a1565ec0-5891-42f5-9e8a-cf18e1f59087 container secret-volume-test: <nil>
    STEP: delete the pod 06/22/23 10:19:53.069
    Jun 22 10:19:53.089: INFO: Waiting for pod pod-secrets-a1565ec0-5891-42f5-9e8a-cf18e1f59087 to disappear
    Jun 22 10:19:53.094: INFO: Pod pod-secrets-a1565ec0-5891-42f5-9e8a-cf18e1f59087 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:19:53.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1282" for this suite. 06/22/23 10:19:53.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:19:53.115
Jun 22 10:19:53.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename emptydir 06/22/23 10:19:53.117
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:19:53.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:19:53.143
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 06/22/23 10:19:53.148
W0622 10:19:53.160147      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "busybox-main-container", "busybox-sub-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "busybox-main-container", "busybox-sub-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "busybox-main-container", "busybox-sub-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "busybox-main-container", "busybox-sub-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:19:53.160: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-ef1f9ac9-c59f-449b-b481-034eacd1ad1f" in namespace "emptydir-2028" to be "running"
Jun 22 10:19:53.169: INFO: Pod "pod-sharedvolume-ef1f9ac9-c59f-449b-b481-034eacd1ad1f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.156046ms
Jun 22 10:19:55.175: INFO: Pod "pod-sharedvolume-ef1f9ac9-c59f-449b-b481-034eacd1ad1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015319949s
Jun 22 10:19:57.177: INFO: Pod "pod-sharedvolume-ef1f9ac9-c59f-449b-b481-034eacd1ad1f": Phase="Running", Reason="", readiness=false. Elapsed: 4.01708484s
Jun 22 10:19:57.177: INFO: Pod "pod-sharedvolume-ef1f9ac9-c59f-449b-b481-034eacd1ad1f" satisfied condition "running"
STEP: Reading file content from the nginx-container 06/22/23 10:19:57.177
Jun 22 10:19:57.177: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2028 PodName:pod-sharedvolume-ef1f9ac9-c59f-449b-b481-034eacd1ad1f ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:19:57.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:19:57.178: INFO: ExecWithOptions: Clientset creation
Jun 22 10:19:57.178: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/emptydir-2028/pods/pod-sharedvolume-ef1f9ac9-c59f-449b-b481-034eacd1ad1f/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jun 22 10:19:57.289: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun 22 10:19:57.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2028" for this suite. 06/22/23 10:19:57.298
------------------------------
• [4.193 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:19:53.115
    Jun 22 10:19:53.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename emptydir 06/22/23 10:19:53.117
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:19:53.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:19:53.143
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 06/22/23 10:19:53.148
    W0622 10:19:53.160147      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "busybox-main-container", "busybox-sub-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "busybox-main-container", "busybox-sub-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "busybox-main-container", "busybox-sub-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "busybox-main-container", "busybox-sub-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:19:53.160: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-ef1f9ac9-c59f-449b-b481-034eacd1ad1f" in namespace "emptydir-2028" to be "running"
    Jun 22 10:19:53.169: INFO: Pod "pod-sharedvolume-ef1f9ac9-c59f-449b-b481-034eacd1ad1f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.156046ms
    Jun 22 10:19:55.175: INFO: Pod "pod-sharedvolume-ef1f9ac9-c59f-449b-b481-034eacd1ad1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015319949s
    Jun 22 10:19:57.177: INFO: Pod "pod-sharedvolume-ef1f9ac9-c59f-449b-b481-034eacd1ad1f": Phase="Running", Reason="", readiness=false. Elapsed: 4.01708484s
    Jun 22 10:19:57.177: INFO: Pod "pod-sharedvolume-ef1f9ac9-c59f-449b-b481-034eacd1ad1f" satisfied condition "running"
    STEP: Reading file content from the nginx-container 06/22/23 10:19:57.177
    Jun 22 10:19:57.177: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2028 PodName:pod-sharedvolume-ef1f9ac9-c59f-449b-b481-034eacd1ad1f ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:19:57.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:19:57.178: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:19:57.178: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/emptydir-2028/pods/pod-sharedvolume-ef1f9ac9-c59f-449b-b481-034eacd1ad1f/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jun 22 10:19:57.289: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:19:57.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2028" for this suite. 06/22/23 10:19:57.298
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:19:57.31
Jun 22 10:19:57.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubelet-test 06/22/23 10:19:57.311
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:19:57.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:19:57.335
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
W0622 10:19:57.351888      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "bin-falseb2d8f49e-5a40-4134-b90f-4ff74576bc4a" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "bin-falseb2d8f49e-5a40-4134-b90f-4ff74576bc4a" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "bin-falseb2d8f49e-5a40-4134-b90f-4ff74576bc4a" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "bin-falseb2d8f49e-5a40-4134-b90f-4ff74576bc4a" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jun 22 10:20:01.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8604" for this suite. 06/22/23 10:20:01.377
------------------------------
• [4.081 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:19:57.31
    Jun 22 10:19:57.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubelet-test 06/22/23 10:19:57.311
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:19:57.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:19:57.335
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    W0622 10:19:57.351888      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "bin-falseb2d8f49e-5a40-4134-b90f-4ff74576bc4a" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "bin-falseb2d8f49e-5a40-4134-b90f-4ff74576bc4a" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "bin-falseb2d8f49e-5a40-4134-b90f-4ff74576bc4a" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "bin-falseb2d8f49e-5a40-4134-b90f-4ff74576bc4a" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:20:01.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8604" for this suite. 06/22/23 10:20:01.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:20:01.394
Jun 22 10:20:01.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubectl 06/22/23 10:20:01.395
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:20:01.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:20:01.423
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Jun 22 10:20:01.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-2560 version'
Jun 22 10:20:01.589: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jun 22 10:20:01.589: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.5\", GitCommit:\"890a139214b4de1f01543d15003b5bda71aae9c7\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:14:46Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.5+vmware.1\", GitCommit:\"09fccb1858504b18691f5daab5ac48eb92fbacab\", GitTreeState:\"clean\", BuildDate:\"2023-05-23T16:57:21Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun 22 10:20:01.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2560" for this suite. 06/22/23 10:20:01.598
------------------------------
• [0.218 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:20:01.394
    Jun 22 10:20:01.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubectl 06/22/23 10:20:01.395
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:20:01.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:20:01.423
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Jun 22 10:20:01.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-2560 version'
    Jun 22 10:20:01.589: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jun 22 10:20:01.589: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.5\", GitCommit:\"890a139214b4de1f01543d15003b5bda71aae9c7\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:14:46Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.5+vmware.1\", GitCommit:\"09fccb1858504b18691f5daab5ac48eb92fbacab\", GitTreeState:\"clean\", BuildDate:\"2023-05-23T16:57:21Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:20:01.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2560" for this suite. 06/22/23 10:20:01.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:20:01.612
Jun 22 10:20:01.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename namespaces 06/22/23 10:20:01.613
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:20:01.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:20:01.643
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 06/22/23 10:20:01.646
STEP: patching the Namespace 06/22/23 10:20:01.679
STEP: get the Namespace and ensuring it has the label 06/22/23 10:20:01.689
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:20:01.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7549" for this suite. 06/22/23 10:20:01.708
STEP: Destroying namespace "nspatchtest-56f4baff-f0fd-4a37-b853-b22a1ec6b6d3-3924" for this suite. 06/22/23 10:20:01.723
------------------------------
• [0.123 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:20:01.612
    Jun 22 10:20:01.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename namespaces 06/22/23 10:20:01.613
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:20:01.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:20:01.643
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 06/22/23 10:20:01.646
    STEP: patching the Namespace 06/22/23 10:20:01.679
    STEP: get the Namespace and ensuring it has the label 06/22/23 10:20:01.689
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:20:01.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7549" for this suite. 06/22/23 10:20:01.708
    STEP: Destroying namespace "nspatchtest-56f4baff-f0fd-4a37-b853-b22a1ec6b6d3-3924" for this suite. 06/22/23 10:20:01.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:20:01.739
Jun 22 10:20:01.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename configmap 06/22/23 10:20:01.74
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:20:01.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:20:01.798
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-27f132c9-7efb-4633-a19b-fa616116ccb4 06/22/23 10:20:01.802
STEP: Creating a pod to test consume configMaps 06/22/23 10:20:01.812
W0622 10:20:01.826482      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:20:01.826: INFO: Waiting up to 5m0s for pod "pod-configmaps-830713a1-e4b7-4fa8-812d-14d122404f6a" in namespace "configmap-4631" to be "Succeeded or Failed"
Jun 22 10:20:01.838: INFO: Pod "pod-configmaps-830713a1-e4b7-4fa8-812d-14d122404f6a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.028747ms
Jun 22 10:20:03.933: INFO: Pod "pod-configmaps-830713a1-e4b7-4fa8-812d-14d122404f6a": Phase="Running", Reason="", readiness=true. Elapsed: 2.106824506s
Jun 22 10:20:05.846: INFO: Pod "pod-configmaps-830713a1-e4b7-4fa8-812d-14d122404f6a": Phase="Running", Reason="", readiness=false. Elapsed: 4.019398626s
Jun 22 10:20:07.846: INFO: Pod "pod-configmaps-830713a1-e4b7-4fa8-812d-14d122404f6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02013482s
STEP: Saw pod success 06/22/23 10:20:07.846
Jun 22 10:20:07.847: INFO: Pod "pod-configmaps-830713a1-e4b7-4fa8-812d-14d122404f6a" satisfied condition "Succeeded or Failed"
Jun 22 10:20:07.854: INFO: Trying to get logs from node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 pod pod-configmaps-830713a1-e4b7-4fa8-812d-14d122404f6a container agnhost-container: <nil>
STEP: delete the pod 06/22/23 10:20:07.877
Jun 22 10:20:07.897: INFO: Waiting for pod pod-configmaps-830713a1-e4b7-4fa8-812d-14d122404f6a to disappear
Jun 22 10:20:07.902: INFO: Pod pod-configmaps-830713a1-e4b7-4fa8-812d-14d122404f6a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun 22 10:20:07.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4631" for this suite. 06/22/23 10:20:07.91
------------------------------
• [SLOW TEST] [6.180 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:20:01.739
    Jun 22 10:20:01.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename configmap 06/22/23 10:20:01.74
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:20:01.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:20:01.798
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-27f132c9-7efb-4633-a19b-fa616116ccb4 06/22/23 10:20:01.802
    STEP: Creating a pod to test consume configMaps 06/22/23 10:20:01.812
    W0622 10:20:01.826482      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:20:01.826: INFO: Waiting up to 5m0s for pod "pod-configmaps-830713a1-e4b7-4fa8-812d-14d122404f6a" in namespace "configmap-4631" to be "Succeeded or Failed"
    Jun 22 10:20:01.838: INFO: Pod "pod-configmaps-830713a1-e4b7-4fa8-812d-14d122404f6a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.028747ms
    Jun 22 10:20:03.933: INFO: Pod "pod-configmaps-830713a1-e4b7-4fa8-812d-14d122404f6a": Phase="Running", Reason="", readiness=true. Elapsed: 2.106824506s
    Jun 22 10:20:05.846: INFO: Pod "pod-configmaps-830713a1-e4b7-4fa8-812d-14d122404f6a": Phase="Running", Reason="", readiness=false. Elapsed: 4.019398626s
    Jun 22 10:20:07.846: INFO: Pod "pod-configmaps-830713a1-e4b7-4fa8-812d-14d122404f6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02013482s
    STEP: Saw pod success 06/22/23 10:20:07.846
    Jun 22 10:20:07.847: INFO: Pod "pod-configmaps-830713a1-e4b7-4fa8-812d-14d122404f6a" satisfied condition "Succeeded or Failed"
    Jun 22 10:20:07.854: INFO: Trying to get logs from node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 pod pod-configmaps-830713a1-e4b7-4fa8-812d-14d122404f6a container agnhost-container: <nil>
    STEP: delete the pod 06/22/23 10:20:07.877
    Jun 22 10:20:07.897: INFO: Waiting for pod pod-configmaps-830713a1-e4b7-4fa8-812d-14d122404f6a to disappear
    Jun 22 10:20:07.902: INFO: Pod pod-configmaps-830713a1-e4b7-4fa8-812d-14d122404f6a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:20:07.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4631" for this suite. 06/22/23 10:20:07.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:20:07.921
Jun 22 10:20:07.921: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename configmap 06/22/23 10:20:07.922
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:20:07.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:20:07.95
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-303aee51-cd5c-4f8b-90b7-da867ea3869a 06/22/23 10:20:07.956
STEP: Creating a pod to test consume configMaps 06/22/23 10:20:07.966
W0622 10:20:07.979370      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:20:07.979: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd21bc25-6274-4e37-b820-541d24108460" in namespace "configmap-8144" to be "Succeeded or Failed"
Jun 22 10:20:07.988: INFO: Pod "pod-configmaps-cd21bc25-6274-4e37-b820-541d24108460": Phase="Pending", Reason="", readiness=false. Elapsed: 8.579209ms
Jun 22 10:20:09.995: INFO: Pod "pod-configmaps-cd21bc25-6274-4e37-b820-541d24108460": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015673466s
Jun 22 10:20:11.995: INFO: Pod "pod-configmaps-cd21bc25-6274-4e37-b820-541d24108460": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01595902s
STEP: Saw pod success 06/22/23 10:20:11.995
Jun 22 10:20:11.995: INFO: Pod "pod-configmaps-cd21bc25-6274-4e37-b820-541d24108460" satisfied condition "Succeeded or Failed"
Jun 22 10:20:12.000: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-configmaps-cd21bc25-6274-4e37-b820-541d24108460 container agnhost-container: <nil>
STEP: delete the pod 06/22/23 10:20:12.013
Jun 22 10:20:12.036: INFO: Waiting for pod pod-configmaps-cd21bc25-6274-4e37-b820-541d24108460 to disappear
Jun 22 10:20:12.043: INFO: Pod pod-configmaps-cd21bc25-6274-4e37-b820-541d24108460 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun 22 10:20:12.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8144" for this suite. 06/22/23 10:20:12.052
------------------------------
• [4.141 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:20:07.921
    Jun 22 10:20:07.921: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename configmap 06/22/23 10:20:07.922
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:20:07.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:20:07.95
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-303aee51-cd5c-4f8b-90b7-da867ea3869a 06/22/23 10:20:07.956
    STEP: Creating a pod to test consume configMaps 06/22/23 10:20:07.966
    W0622 10:20:07.979370      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:20:07.979: INFO: Waiting up to 5m0s for pod "pod-configmaps-cd21bc25-6274-4e37-b820-541d24108460" in namespace "configmap-8144" to be "Succeeded or Failed"
    Jun 22 10:20:07.988: INFO: Pod "pod-configmaps-cd21bc25-6274-4e37-b820-541d24108460": Phase="Pending", Reason="", readiness=false. Elapsed: 8.579209ms
    Jun 22 10:20:09.995: INFO: Pod "pod-configmaps-cd21bc25-6274-4e37-b820-541d24108460": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015673466s
    Jun 22 10:20:11.995: INFO: Pod "pod-configmaps-cd21bc25-6274-4e37-b820-541d24108460": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01595902s
    STEP: Saw pod success 06/22/23 10:20:11.995
    Jun 22 10:20:11.995: INFO: Pod "pod-configmaps-cd21bc25-6274-4e37-b820-541d24108460" satisfied condition "Succeeded or Failed"
    Jun 22 10:20:12.000: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-configmaps-cd21bc25-6274-4e37-b820-541d24108460 container agnhost-container: <nil>
    STEP: delete the pod 06/22/23 10:20:12.013
    Jun 22 10:20:12.036: INFO: Waiting for pod pod-configmaps-cd21bc25-6274-4e37-b820-541d24108460 to disappear
    Jun 22 10:20:12.043: INFO: Pod pod-configmaps-cd21bc25-6274-4e37-b820-541d24108460 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:20:12.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8144" for this suite. 06/22/23 10:20:12.052
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:20:12.063
Jun 22 10:20:12.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename containers 06/22/23 10:20:12.064
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:20:12.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:20:12.087
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
W0622 10:20:12.104374      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:20:12.104: INFO: Waiting up to 5m0s for pod "client-containers-235cc4e2-547f-48a9-928b-730a31788d68" in namespace "containers-1351" to be "running"
Jun 22 10:20:12.111: INFO: Pod "client-containers-235cc4e2-547f-48a9-928b-730a31788d68": Phase="Pending", Reason="", readiness=false. Elapsed: 7.368845ms
Jun 22 10:20:14.118: INFO: Pod "client-containers-235cc4e2-547f-48a9-928b-730a31788d68": Phase="Running", Reason="", readiness=true. Elapsed: 2.01403362s
Jun 22 10:20:14.118: INFO: Pod "client-containers-235cc4e2-547f-48a9-928b-730a31788d68" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jun 22 10:20:14.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1351" for this suite. 06/22/23 10:20:14.137
------------------------------
• [2.085 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:20:12.063
    Jun 22 10:20:12.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename containers 06/22/23 10:20:12.064
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:20:12.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:20:12.087
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    W0622 10:20:12.104374      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:20:12.104: INFO: Waiting up to 5m0s for pod "client-containers-235cc4e2-547f-48a9-928b-730a31788d68" in namespace "containers-1351" to be "running"
    Jun 22 10:20:12.111: INFO: Pod "client-containers-235cc4e2-547f-48a9-928b-730a31788d68": Phase="Pending", Reason="", readiness=false. Elapsed: 7.368845ms
    Jun 22 10:20:14.118: INFO: Pod "client-containers-235cc4e2-547f-48a9-928b-730a31788d68": Phase="Running", Reason="", readiness=true. Elapsed: 2.01403362s
    Jun 22 10:20:14.118: INFO: Pod "client-containers-235cc4e2-547f-48a9-928b-730a31788d68" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:20:14.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1351" for this suite. 06/22/23 10:20:14.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:20:14.149
Jun 22 10:20:14.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename resourcequota 06/22/23 10:20:14.151
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:20:14.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:20:14.174
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 06/22/23 10:20:58.183
STEP: Creating a ResourceQuota 06/22/23 10:21:03.19
STEP: Ensuring resource quota status is calculated 06/22/23 10:21:03.204
STEP: Creating a ConfigMap 06/22/23 10:21:09.212
STEP: Ensuring resource quota status captures configMap creation 06/22/23 10:21:09.245
STEP: Deleting a ConfigMap 06/22/23 10:21:11.252
STEP: Ensuring resource quota status released usage 06/22/23 10:21:11.262
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun 22 10:21:13.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3314" for this suite. 06/22/23 10:21:13.278
------------------------------
• [SLOW TEST] [59.139 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:20:14.149
    Jun 22 10:20:14.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename resourcequota 06/22/23 10:20:14.151
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:20:14.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:20:14.174
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 06/22/23 10:20:58.183
    STEP: Creating a ResourceQuota 06/22/23 10:21:03.19
    STEP: Ensuring resource quota status is calculated 06/22/23 10:21:03.204
    STEP: Creating a ConfigMap 06/22/23 10:21:09.212
    STEP: Ensuring resource quota status captures configMap creation 06/22/23 10:21:09.245
    STEP: Deleting a ConfigMap 06/22/23 10:21:11.252
    STEP: Ensuring resource quota status released usage 06/22/23 10:21:11.262
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:21:13.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3314" for this suite. 06/22/23 10:21:13.278
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:21:13.29
Jun 22 10:21:13.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 10:21:13.291
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:21:13.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:21:13.324
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-4523fbf1-fcf9-4f43-9552-25537352d698 06/22/23 10:21:13.328
STEP: Creating a pod to test consume configMaps 06/22/23 10:21:13.34
W0622 10:21:13.352046      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:21:13.352: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b83daa6e-e83d-430d-9d5f-268667a13f50" in namespace "projected-1398" to be "Succeeded or Failed"
Jun 22 10:21:13.357: INFO: Pod "pod-projected-configmaps-b83daa6e-e83d-430d-9d5f-268667a13f50": Phase="Pending", Reason="", readiness=false. Elapsed: 5.101219ms
Jun 22 10:21:15.363: INFO: Pod "pod-projected-configmaps-b83daa6e-e83d-430d-9d5f-268667a13f50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011463292s
Jun 22 10:21:17.364: INFO: Pod "pod-projected-configmaps-b83daa6e-e83d-430d-9d5f-268667a13f50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012320104s
STEP: Saw pod success 06/22/23 10:21:17.364
Jun 22 10:21:17.364: INFO: Pod "pod-projected-configmaps-b83daa6e-e83d-430d-9d5f-268667a13f50" satisfied condition "Succeeded or Failed"
Jun 22 10:21:17.370: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-configmaps-b83daa6e-e83d-430d-9d5f-268667a13f50 container agnhost-container: <nil>
STEP: delete the pod 06/22/23 10:21:17.38
Jun 22 10:21:17.400: INFO: Waiting for pod pod-projected-configmaps-b83daa6e-e83d-430d-9d5f-268667a13f50 to disappear
Jun 22 10:21:17.405: INFO: Pod pod-projected-configmaps-b83daa6e-e83d-430d-9d5f-268667a13f50 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun 22 10:21:17.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1398" for this suite. 06/22/23 10:21:17.412
------------------------------
• [4.131 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:21:13.29
    Jun 22 10:21:13.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 10:21:13.291
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:21:13.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:21:13.324
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-4523fbf1-fcf9-4f43-9552-25537352d698 06/22/23 10:21:13.328
    STEP: Creating a pod to test consume configMaps 06/22/23 10:21:13.34
    W0622 10:21:13.352046      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:21:13.352: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b83daa6e-e83d-430d-9d5f-268667a13f50" in namespace "projected-1398" to be "Succeeded or Failed"
    Jun 22 10:21:13.357: INFO: Pod "pod-projected-configmaps-b83daa6e-e83d-430d-9d5f-268667a13f50": Phase="Pending", Reason="", readiness=false. Elapsed: 5.101219ms
    Jun 22 10:21:15.363: INFO: Pod "pod-projected-configmaps-b83daa6e-e83d-430d-9d5f-268667a13f50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011463292s
    Jun 22 10:21:17.364: INFO: Pod "pod-projected-configmaps-b83daa6e-e83d-430d-9d5f-268667a13f50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012320104s
    STEP: Saw pod success 06/22/23 10:21:17.364
    Jun 22 10:21:17.364: INFO: Pod "pod-projected-configmaps-b83daa6e-e83d-430d-9d5f-268667a13f50" satisfied condition "Succeeded or Failed"
    Jun 22 10:21:17.370: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-configmaps-b83daa6e-e83d-430d-9d5f-268667a13f50 container agnhost-container: <nil>
    STEP: delete the pod 06/22/23 10:21:17.38
    Jun 22 10:21:17.400: INFO: Waiting for pod pod-projected-configmaps-b83daa6e-e83d-430d-9d5f-268667a13f50 to disappear
    Jun 22 10:21:17.405: INFO: Pod pod-projected-configmaps-b83daa6e-e83d-430d-9d5f-268667a13f50 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:21:17.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1398" for this suite. 06/22/23 10:21:17.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:21:17.422
Jun 22 10:21:17.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename emptydir 06/22/23 10:21:17.423
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:21:17.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:21:17.447
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 06/22/23 10:21:17.45
W0622 10:21:17.462352      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:21:17.462: INFO: Waiting up to 5m0s for pod "pod-e371095e-0a2b-4e80-8bb8-60353af5d4ee" in namespace "emptydir-6802" to be "Succeeded or Failed"
Jun 22 10:21:17.466: INFO: Pod "pod-e371095e-0a2b-4e80-8bb8-60353af5d4ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.284026ms
Jun 22 10:21:19.476: INFO: Pod "pod-e371095e-0a2b-4e80-8bb8-60353af5d4ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013922224s
Jun 22 10:21:21.474: INFO: Pod "pod-e371095e-0a2b-4e80-8bb8-60353af5d4ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011854048s
STEP: Saw pod success 06/22/23 10:21:21.474
Jun 22 10:21:21.474: INFO: Pod "pod-e371095e-0a2b-4e80-8bb8-60353af5d4ee" satisfied condition "Succeeded or Failed"
Jun 22 10:21:21.479: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-e371095e-0a2b-4e80-8bb8-60353af5d4ee container test-container: <nil>
STEP: delete the pod 06/22/23 10:21:21.491
Jun 22 10:21:21.511: INFO: Waiting for pod pod-e371095e-0a2b-4e80-8bb8-60353af5d4ee to disappear
Jun 22 10:21:21.516: INFO: Pod pod-e371095e-0a2b-4e80-8bb8-60353af5d4ee no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun 22 10:21:21.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6802" for this suite. 06/22/23 10:21:21.523
------------------------------
• [4.112 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:21:17.422
    Jun 22 10:21:17.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename emptydir 06/22/23 10:21:17.423
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:21:17.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:21:17.447
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 06/22/23 10:21:17.45
    W0622 10:21:17.462352      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:21:17.462: INFO: Waiting up to 5m0s for pod "pod-e371095e-0a2b-4e80-8bb8-60353af5d4ee" in namespace "emptydir-6802" to be "Succeeded or Failed"
    Jun 22 10:21:17.466: INFO: Pod "pod-e371095e-0a2b-4e80-8bb8-60353af5d4ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.284026ms
    Jun 22 10:21:19.476: INFO: Pod "pod-e371095e-0a2b-4e80-8bb8-60353af5d4ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013922224s
    Jun 22 10:21:21.474: INFO: Pod "pod-e371095e-0a2b-4e80-8bb8-60353af5d4ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011854048s
    STEP: Saw pod success 06/22/23 10:21:21.474
    Jun 22 10:21:21.474: INFO: Pod "pod-e371095e-0a2b-4e80-8bb8-60353af5d4ee" satisfied condition "Succeeded or Failed"
    Jun 22 10:21:21.479: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-e371095e-0a2b-4e80-8bb8-60353af5d4ee container test-container: <nil>
    STEP: delete the pod 06/22/23 10:21:21.491
    Jun 22 10:21:21.511: INFO: Waiting for pod pod-e371095e-0a2b-4e80-8bb8-60353af5d4ee to disappear
    Jun 22 10:21:21.516: INFO: Pod pod-e371095e-0a2b-4e80-8bb8-60353af5d4ee no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:21:21.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6802" for this suite. 06/22/23 10:21:21.523
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:21:21.535
Jun 22 10:21:21.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename pods 06/22/23 10:21:21.536
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:21:21.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:21:21.562
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 06/22/23 10:21:21.566
Jun 22 10:21:21.579: INFO: Waiting up to 5m0s for pod "pod-hostip-bd62ad44-e2e1-4db2-bd5a-bc6d9ec76e54" in namespace "pods-2218" to be "running and ready"
Jun 22 10:21:21.583: INFO: Pod "pod-hostip-bd62ad44-e2e1-4db2-bd5a-bc6d9ec76e54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.322448ms
Jun 22 10:21:21.583: INFO: The phase of Pod pod-hostip-bd62ad44-e2e1-4db2-bd5a-bc6d9ec76e54 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:21:23.589: INFO: Pod "pod-hostip-bd62ad44-e2e1-4db2-bd5a-bc6d9ec76e54": Phase="Running", Reason="", readiness=true. Elapsed: 2.010869602s
Jun 22 10:21:23.589: INFO: The phase of Pod pod-hostip-bd62ad44-e2e1-4db2-bd5a-bc6d9ec76e54 is Running (Ready = true)
Jun 22 10:21:23.589: INFO: Pod "pod-hostip-bd62ad44-e2e1-4db2-bd5a-bc6d9ec76e54" satisfied condition "running and ready"
Jun 22 10:21:23.599: INFO: Pod pod-hostip-bd62ad44-e2e1-4db2-bd5a-bc6d9ec76e54 has hostIP: 10.92.226.162
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun 22 10:21:23.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2218" for this suite. 06/22/23 10:21:23.606
------------------------------
• [2.080 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:21:21.535
    Jun 22 10:21:21.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename pods 06/22/23 10:21:21.536
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:21:21.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:21:21.562
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 06/22/23 10:21:21.566
    Jun 22 10:21:21.579: INFO: Waiting up to 5m0s for pod "pod-hostip-bd62ad44-e2e1-4db2-bd5a-bc6d9ec76e54" in namespace "pods-2218" to be "running and ready"
    Jun 22 10:21:21.583: INFO: Pod "pod-hostip-bd62ad44-e2e1-4db2-bd5a-bc6d9ec76e54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.322448ms
    Jun 22 10:21:21.583: INFO: The phase of Pod pod-hostip-bd62ad44-e2e1-4db2-bd5a-bc6d9ec76e54 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:21:23.589: INFO: Pod "pod-hostip-bd62ad44-e2e1-4db2-bd5a-bc6d9ec76e54": Phase="Running", Reason="", readiness=true. Elapsed: 2.010869602s
    Jun 22 10:21:23.589: INFO: The phase of Pod pod-hostip-bd62ad44-e2e1-4db2-bd5a-bc6d9ec76e54 is Running (Ready = true)
    Jun 22 10:21:23.589: INFO: Pod "pod-hostip-bd62ad44-e2e1-4db2-bd5a-bc6d9ec76e54" satisfied condition "running and ready"
    Jun 22 10:21:23.599: INFO: Pod pod-hostip-bd62ad44-e2e1-4db2-bd5a-bc6d9ec76e54 has hostIP: 10.92.226.162
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:21:23.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2218" for this suite. 06/22/23 10:21:23.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:21:23.615
Jun 22 10:21:23.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename svcaccounts 06/22/23 10:21:23.616
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:21:23.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:21:23.64
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-cz7bl"  06/22/23 10:21:23.644
Jun 22 10:21:23.651: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-cz7bl"  06/22/23 10:21:23.651
Jun 22 10:21:23.661: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun 22 10:21:23.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8693" for this suite. 06/22/23 10:21:23.667
------------------------------
• [0.061 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:21:23.615
    Jun 22 10:21:23.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename svcaccounts 06/22/23 10:21:23.616
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:21:23.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:21:23.64
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-cz7bl"  06/22/23 10:21:23.644
    Jun 22 10:21:23.651: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-cz7bl"  06/22/23 10:21:23.651
    Jun 22 10:21:23.661: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:21:23.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8693" for this suite. 06/22/23 10:21:23.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:21:23.678
Jun 22 10:21:23.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename disruption 06/22/23 10:21:23.679
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:21:23.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:21:23.704
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 06/22/23 10:21:23.715
W0622 10:21:25.741684      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "donothing" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "donothing" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "donothing" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "donothing" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0622 10:21:25.751217      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "donothing" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "donothing" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "donothing" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "donothing" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0622 10:21:25.761746      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "donothing" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "donothing" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "donothing" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "donothing" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting for all pods to be running 06/22/23 10:21:25.761
Jun 22 10:21:25.772: INFO: running pods: 0 < 3
Jun 22 10:21:27.779: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jun 22 10:21:29.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2805" for this suite. 06/22/23 10:21:29.792
------------------------------
• [SLOW TEST] [6.124 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:21:23.678
    Jun 22 10:21:23.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename disruption 06/22/23 10:21:23.679
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:21:23.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:21:23.704
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 06/22/23 10:21:23.715
    W0622 10:21:25.741684      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "donothing" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "donothing" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "donothing" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "donothing" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0622 10:21:25.751217      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "donothing" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "donothing" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "donothing" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "donothing" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0622 10:21:25.761746      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "donothing" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "donothing" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "donothing" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "donothing" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Waiting for all pods to be running 06/22/23 10:21:25.761
    Jun 22 10:21:25.772: INFO: running pods: 0 < 3
    Jun 22 10:21:27.779: INFO: running pods: 2 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:21:29.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2805" for this suite. 06/22/23 10:21:29.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:21:29.803
Jun 22 10:21:29.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 10:21:29.804
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:21:29.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:21:29.834
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-09e294af-6581-451c-8a36-6191e466301d 06/22/23 10:21:29.837
STEP: Creating a pod to test consume secrets 06/22/23 10:21:29.844
W0622 10:21:29.859843      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:21:29.860: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a698e83a-b412-4e02-8d13-78db36995881" in namespace "projected-2657" to be "Succeeded or Failed"
Jun 22 10:21:29.867: INFO: Pod "pod-projected-secrets-a698e83a-b412-4e02-8d13-78db36995881": Phase="Pending", Reason="", readiness=false. Elapsed: 7.902814ms
Jun 22 10:21:31.874: INFO: Pod "pod-projected-secrets-a698e83a-b412-4e02-8d13-78db36995881": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014330511s
Jun 22 10:21:33.874: INFO: Pod "pod-projected-secrets-a698e83a-b412-4e02-8d13-78db36995881": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014799783s
STEP: Saw pod success 06/22/23 10:21:33.874
Jun 22 10:21:33.875: INFO: Pod "pod-projected-secrets-a698e83a-b412-4e02-8d13-78db36995881" satisfied condition "Succeeded or Failed"
Jun 22 10:21:33.879: INFO: Trying to get logs from node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k pod pod-projected-secrets-a698e83a-b412-4e02-8d13-78db36995881 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/22/23 10:21:33.9
Jun 22 10:21:33.919: INFO: Waiting for pod pod-projected-secrets-a698e83a-b412-4e02-8d13-78db36995881 to disappear
Jun 22 10:21:33.923: INFO: Pod pod-projected-secrets-a698e83a-b412-4e02-8d13-78db36995881 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun 22 10:21:33.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2657" for this suite. 06/22/23 10:21:33.931
------------------------------
• [4.137 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:21:29.803
    Jun 22 10:21:29.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 10:21:29.804
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:21:29.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:21:29.834
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-09e294af-6581-451c-8a36-6191e466301d 06/22/23 10:21:29.837
    STEP: Creating a pod to test consume secrets 06/22/23 10:21:29.844
    W0622 10:21:29.859843      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:21:29.860: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a698e83a-b412-4e02-8d13-78db36995881" in namespace "projected-2657" to be "Succeeded or Failed"
    Jun 22 10:21:29.867: INFO: Pod "pod-projected-secrets-a698e83a-b412-4e02-8d13-78db36995881": Phase="Pending", Reason="", readiness=false. Elapsed: 7.902814ms
    Jun 22 10:21:31.874: INFO: Pod "pod-projected-secrets-a698e83a-b412-4e02-8d13-78db36995881": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014330511s
    Jun 22 10:21:33.874: INFO: Pod "pod-projected-secrets-a698e83a-b412-4e02-8d13-78db36995881": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014799783s
    STEP: Saw pod success 06/22/23 10:21:33.874
    Jun 22 10:21:33.875: INFO: Pod "pod-projected-secrets-a698e83a-b412-4e02-8d13-78db36995881" satisfied condition "Succeeded or Failed"
    Jun 22 10:21:33.879: INFO: Trying to get logs from node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k pod pod-projected-secrets-a698e83a-b412-4e02-8d13-78db36995881 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/22/23 10:21:33.9
    Jun 22 10:21:33.919: INFO: Waiting for pod pod-projected-secrets-a698e83a-b412-4e02-8d13-78db36995881 to disappear
    Jun 22 10:21:33.923: INFO: Pod pod-projected-secrets-a698e83a-b412-4e02-8d13-78db36995881 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:21:33.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2657" for this suite. 06/22/23 10:21:33.931
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:21:33.94
Jun 22 10:21:33.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename resourcequota 06/22/23 10:21:33.941
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:21:33.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:21:33.968
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 06/22/23 10:21:33.972
STEP: Counting existing ResourceQuota 06/22/23 10:21:38.983
STEP: Creating a ResourceQuota 06/22/23 10:21:43.99
STEP: Ensuring resource quota status is calculated 06/22/23 10:21:43.999
STEP: Creating a Secret 06/22/23 10:21:46.006
STEP: Ensuring resource quota status captures secret creation 06/22/23 10:21:46.023
STEP: Deleting a secret 06/22/23 10:21:48.03
STEP: Ensuring resource quota status released usage 06/22/23 10:21:48.042
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun 22 10:21:50.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3972" for this suite. 06/22/23 10:21:50.058
------------------------------
• [SLOW TEST] [16.129 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:21:33.94
    Jun 22 10:21:33.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename resourcequota 06/22/23 10:21:33.941
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:21:33.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:21:33.968
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 06/22/23 10:21:33.972
    STEP: Counting existing ResourceQuota 06/22/23 10:21:38.983
    STEP: Creating a ResourceQuota 06/22/23 10:21:43.99
    STEP: Ensuring resource quota status is calculated 06/22/23 10:21:43.999
    STEP: Creating a Secret 06/22/23 10:21:46.006
    STEP: Ensuring resource quota status captures secret creation 06/22/23 10:21:46.023
    STEP: Deleting a secret 06/22/23 10:21:48.03
    STEP: Ensuring resource quota status released usage 06/22/23 10:21:48.042
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:21:50.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3972" for this suite. 06/22/23 10:21:50.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:21:50.07
Jun 22 10:21:50.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename job 06/22/23 10:21:50.072
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:21:50.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:21:50.098
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 06/22/23 10:21:50.105
W0622 10:21:50.114716      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Patching the Job 06/22/23 10:21:50.114
W0622 10:21:50.127595      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Watching for Job to be patched 06/22/23 10:21:50.127
Jun 22 10:21:50.130: INFO: Event ADDED observed for Job e2e-4tc8z in namespace job-4987 with labels: map[e2e-job-label:e2e-4tc8z] and annotations: map[batch.kubernetes.io/job-tracking:]
Jun 22 10:21:50.130: INFO: Event MODIFIED found for Job e2e-4tc8z in namespace job-4987 with labels: map[e2e-4tc8z:patched e2e-job-label:e2e-4tc8z] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 06/22/23 10:21:50.13
W0622 10:21:50.143419      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0622 10:21:50.169300      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Watching for Job to be updated 06/22/23 10:21:50.169
Jun 22 10:21:50.171: INFO: Event MODIFIED found for Job e2e-4tc8z in namespace job-4987 with labels: map[e2e-4tc8z:patched e2e-job-label:e2e-4tc8z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 22 10:21:50.171: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 06/22/23 10:21:50.171
Jun 22 10:21:50.176: INFO: Job: e2e-4tc8z as labels: map[e2e-4tc8z:patched e2e-job-label:e2e-4tc8z]
STEP: Waiting for job to complete 06/22/23 10:21:50.176
STEP: Delete a job collection with a labelselector 06/22/23 10:22:00.184
STEP: Watching for Job to be deleted 06/22/23 10:22:00.197
Jun 22 10:22:00.200: INFO: Event MODIFIED observed for Job e2e-4tc8z in namespace job-4987 with labels: map[e2e-4tc8z:patched e2e-job-label:e2e-4tc8z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 22 10:22:00.200: INFO: Event MODIFIED observed for Job e2e-4tc8z in namespace job-4987 with labels: map[e2e-4tc8z:patched e2e-job-label:e2e-4tc8z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 22 10:22:00.200: INFO: Event MODIFIED observed for Job e2e-4tc8z in namespace job-4987 with labels: map[e2e-4tc8z:patched e2e-job-label:e2e-4tc8z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 22 10:22:00.200: INFO: Event MODIFIED observed for Job e2e-4tc8z in namespace job-4987 with labels: map[e2e-4tc8z:patched e2e-job-label:e2e-4tc8z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 22 10:22:00.201: INFO: Event MODIFIED observed for Job e2e-4tc8z in namespace job-4987 with labels: map[e2e-4tc8z:patched e2e-job-label:e2e-4tc8z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 22 10:22:00.201: INFO: Event DELETED found for Job e2e-4tc8z in namespace job-4987 with labels: map[e2e-4tc8z:patched e2e-job-label:e2e-4tc8z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 06/22/23 10:22:00.201
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jun 22 10:22:00.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4987" for this suite. 06/22/23 10:22:00.214
------------------------------
• [SLOW TEST] [10.155 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:21:50.07
    Jun 22 10:21:50.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename job 06/22/23 10:21:50.072
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:21:50.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:21:50.098
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 06/22/23 10:21:50.105
    W0622 10:21:50.114716      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Patching the Job 06/22/23 10:21:50.114
    W0622 10:21:50.127595      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Watching for Job to be patched 06/22/23 10:21:50.127
    Jun 22 10:21:50.130: INFO: Event ADDED observed for Job e2e-4tc8z in namespace job-4987 with labels: map[e2e-job-label:e2e-4tc8z] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jun 22 10:21:50.130: INFO: Event MODIFIED found for Job e2e-4tc8z in namespace job-4987 with labels: map[e2e-4tc8z:patched e2e-job-label:e2e-4tc8z] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 06/22/23 10:21:50.13
    W0622 10:21:50.143419      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0622 10:21:50.169300      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Watching for Job to be updated 06/22/23 10:21:50.169
    Jun 22 10:21:50.171: INFO: Event MODIFIED found for Job e2e-4tc8z in namespace job-4987 with labels: map[e2e-4tc8z:patched e2e-job-label:e2e-4tc8z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 22 10:21:50.171: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 06/22/23 10:21:50.171
    Jun 22 10:21:50.176: INFO: Job: e2e-4tc8z as labels: map[e2e-4tc8z:patched e2e-job-label:e2e-4tc8z]
    STEP: Waiting for job to complete 06/22/23 10:21:50.176
    STEP: Delete a job collection with a labelselector 06/22/23 10:22:00.184
    STEP: Watching for Job to be deleted 06/22/23 10:22:00.197
    Jun 22 10:22:00.200: INFO: Event MODIFIED observed for Job e2e-4tc8z in namespace job-4987 with labels: map[e2e-4tc8z:patched e2e-job-label:e2e-4tc8z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 22 10:22:00.200: INFO: Event MODIFIED observed for Job e2e-4tc8z in namespace job-4987 with labels: map[e2e-4tc8z:patched e2e-job-label:e2e-4tc8z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 22 10:22:00.200: INFO: Event MODIFIED observed for Job e2e-4tc8z in namespace job-4987 with labels: map[e2e-4tc8z:patched e2e-job-label:e2e-4tc8z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 22 10:22:00.200: INFO: Event MODIFIED observed for Job e2e-4tc8z in namespace job-4987 with labels: map[e2e-4tc8z:patched e2e-job-label:e2e-4tc8z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 22 10:22:00.201: INFO: Event MODIFIED observed for Job e2e-4tc8z in namespace job-4987 with labels: map[e2e-4tc8z:patched e2e-job-label:e2e-4tc8z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 22 10:22:00.201: INFO: Event DELETED found for Job e2e-4tc8z in namespace job-4987 with labels: map[e2e-4tc8z:patched e2e-job-label:e2e-4tc8z] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 06/22/23 10:22:00.201
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:22:00.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4987" for this suite. 06/22/23 10:22:00.214
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:22:00.226
Jun 22 10:22:00.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename crd-publish-openapi 06/22/23 10:22:00.228
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:00.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:00.261
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 06/22/23 10:22:00.265
Jun 22 10:22:00.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:22:03.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:22:13.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2439" for this suite. 06/22/23 10:22:13.365
------------------------------
• [SLOW TEST] [13.150 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:22:00.226
    Jun 22 10:22:00.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename crd-publish-openapi 06/22/23 10:22:00.228
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:00.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:00.261
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 06/22/23 10:22:00.265
    Jun 22 10:22:00.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:22:03.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:22:13.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2439" for this suite. 06/22/23 10:22:13.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:22:13.381
Jun 22 10:22:13.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 10:22:13.384
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:13.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:13.406
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 06/22/23 10:22:13.41
W0622 10:22:13.423835      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:22:13.424: INFO: Waiting up to 5m0s for pod "labelsupdatebc8132a6-5fe6-4b08-ad67-5be3456905f4" in namespace "projected-9634" to be "running and ready"
Jun 22 10:22:13.430: INFO: Pod "labelsupdatebc8132a6-5fe6-4b08-ad67-5be3456905f4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.516176ms
Jun 22 10:22:13.430: INFO: The phase of Pod labelsupdatebc8132a6-5fe6-4b08-ad67-5be3456905f4 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:22:15.437: INFO: Pod "labelsupdatebc8132a6-5fe6-4b08-ad67-5be3456905f4": Phase="Running", Reason="", readiness=true. Elapsed: 2.0122731s
Jun 22 10:22:15.437: INFO: The phase of Pod labelsupdatebc8132a6-5fe6-4b08-ad67-5be3456905f4 is Running (Ready = true)
Jun 22 10:22:15.437: INFO: Pod "labelsupdatebc8132a6-5fe6-4b08-ad67-5be3456905f4" satisfied condition "running and ready"
Jun 22 10:22:15.987: INFO: Successfully updated pod "labelsupdatebc8132a6-5fe6-4b08-ad67-5be3456905f4"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun 22 10:22:20.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9634" for this suite. 06/22/23 10:22:20.028
------------------------------
• [SLOW TEST] [6.655 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:22:13.381
    Jun 22 10:22:13.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 10:22:13.384
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:13.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:13.406
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 06/22/23 10:22:13.41
    W0622 10:22:13.423835      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:22:13.424: INFO: Waiting up to 5m0s for pod "labelsupdatebc8132a6-5fe6-4b08-ad67-5be3456905f4" in namespace "projected-9634" to be "running and ready"
    Jun 22 10:22:13.430: INFO: Pod "labelsupdatebc8132a6-5fe6-4b08-ad67-5be3456905f4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.516176ms
    Jun 22 10:22:13.430: INFO: The phase of Pod labelsupdatebc8132a6-5fe6-4b08-ad67-5be3456905f4 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:22:15.437: INFO: Pod "labelsupdatebc8132a6-5fe6-4b08-ad67-5be3456905f4": Phase="Running", Reason="", readiness=true. Elapsed: 2.0122731s
    Jun 22 10:22:15.437: INFO: The phase of Pod labelsupdatebc8132a6-5fe6-4b08-ad67-5be3456905f4 is Running (Ready = true)
    Jun 22 10:22:15.437: INFO: Pod "labelsupdatebc8132a6-5fe6-4b08-ad67-5be3456905f4" satisfied condition "running and ready"
    Jun 22 10:22:15.987: INFO: Successfully updated pod "labelsupdatebc8132a6-5fe6-4b08-ad67-5be3456905f4"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:22:20.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9634" for this suite. 06/22/23 10:22:20.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:22:20.037
Jun 22 10:22:20.037: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 10:22:20.038
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:20.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:20.06
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-38dbefe5-5e08-4170-870c-baed363d67bd 06/22/23 10:22:20.063
STEP: Creating a pod to test consume secrets 06/22/23 10:22:20.07
W0622 10:22:20.082924      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:22:20.083: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0d9c5402-bc13-4613-84f2-09c80692a3d5" in namespace "projected-5676" to be "Succeeded or Failed"
Jun 22 10:22:20.087: INFO: Pod "pod-projected-secrets-0d9c5402-bc13-4613-84f2-09c80692a3d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.311305ms
Jun 22 10:22:22.094: INFO: Pod "pod-projected-secrets-0d9c5402-bc13-4613-84f2-09c80692a3d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011205414s
Jun 22 10:22:24.094: INFO: Pod "pod-projected-secrets-0d9c5402-bc13-4613-84f2-09c80692a3d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010872454s
STEP: Saw pod success 06/22/23 10:22:24.094
Jun 22 10:22:24.094: INFO: Pod "pod-projected-secrets-0d9c5402-bc13-4613-84f2-09c80692a3d5" satisfied condition "Succeeded or Failed"
Jun 22 10:22:24.099: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-secrets-0d9c5402-bc13-4613-84f2-09c80692a3d5 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/22/23 10:22:24.109
Jun 22 10:22:24.126: INFO: Waiting for pod pod-projected-secrets-0d9c5402-bc13-4613-84f2-09c80692a3d5 to disappear
Jun 22 10:22:24.130: INFO: Pod pod-projected-secrets-0d9c5402-bc13-4613-84f2-09c80692a3d5 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun 22 10:22:24.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5676" for this suite. 06/22/23 10:22:24.137
------------------------------
• [4.108 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:22:20.037
    Jun 22 10:22:20.037: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 10:22:20.038
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:20.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:20.06
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-38dbefe5-5e08-4170-870c-baed363d67bd 06/22/23 10:22:20.063
    STEP: Creating a pod to test consume secrets 06/22/23 10:22:20.07
    W0622 10:22:20.082924      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:22:20.083: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0d9c5402-bc13-4613-84f2-09c80692a3d5" in namespace "projected-5676" to be "Succeeded or Failed"
    Jun 22 10:22:20.087: INFO: Pod "pod-projected-secrets-0d9c5402-bc13-4613-84f2-09c80692a3d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.311305ms
    Jun 22 10:22:22.094: INFO: Pod "pod-projected-secrets-0d9c5402-bc13-4613-84f2-09c80692a3d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011205414s
    Jun 22 10:22:24.094: INFO: Pod "pod-projected-secrets-0d9c5402-bc13-4613-84f2-09c80692a3d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010872454s
    STEP: Saw pod success 06/22/23 10:22:24.094
    Jun 22 10:22:24.094: INFO: Pod "pod-projected-secrets-0d9c5402-bc13-4613-84f2-09c80692a3d5" satisfied condition "Succeeded or Failed"
    Jun 22 10:22:24.099: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-secrets-0d9c5402-bc13-4613-84f2-09c80692a3d5 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/22/23 10:22:24.109
    Jun 22 10:22:24.126: INFO: Waiting for pod pod-projected-secrets-0d9c5402-bc13-4613-84f2-09c80692a3d5 to disappear
    Jun 22 10:22:24.130: INFO: Pod pod-projected-secrets-0d9c5402-bc13-4613-84f2-09c80692a3d5 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:22:24.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5676" for this suite. 06/22/23 10:22:24.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:22:24.147
Jun 22 10:22:24.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename webhook 06/22/23 10:22:24.149
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:24.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:24.17
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/22/23 10:22:24.189
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 10:22:25.138
STEP: Deploying the webhook pod 06/22/23 10:22:25.15
W0622 10:22:25.169406      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 10:22:25.169
Jun 22 10:22:25.180: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 06/22/23 10:22:27.195
STEP: Verifying the service has paired with the endpoint 06/22/23 10:22:27.218
Jun 22 10:22:28.219: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 06/22/23 10:22:28.225
STEP: create a configmap that should be updated by the webhook 06/22/23 10:22:28.255
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:22:28.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2090" for this suite. 06/22/23 10:22:28.355
STEP: Destroying namespace "webhook-2090-markers" for this suite. 06/22/23 10:22:28.367
------------------------------
• [4.230 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:22:24.147
    Jun 22 10:22:24.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename webhook 06/22/23 10:22:24.149
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:24.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:24.17
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/22/23 10:22:24.189
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 10:22:25.138
    STEP: Deploying the webhook pod 06/22/23 10:22:25.15
    W0622 10:22:25.169406      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 10:22:25.169
    Jun 22 10:22:25.180: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 06/22/23 10:22:27.195
    STEP: Verifying the service has paired with the endpoint 06/22/23 10:22:27.218
    Jun 22 10:22:28.219: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 06/22/23 10:22:28.225
    STEP: create a configmap that should be updated by the webhook 06/22/23 10:22:28.255
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:22:28.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2090" for this suite. 06/22/23 10:22:28.355
    STEP: Destroying namespace "webhook-2090-markers" for this suite. 06/22/23 10:22:28.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:22:28.378
Jun 22 10:22:28.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename services 06/22/23 10:22:28.379
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:28.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:28.403
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3150 06/22/23 10:22:28.407
STEP: changing the ExternalName service to type=ClusterIP 06/22/23 10:22:28.414
STEP: creating replication controller externalname-service in namespace services-3150 06/22/23 10:22:28.441
W0622 10:22:28.451210      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "externalname-service" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "externalname-service" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "externalname-service" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "externalname-service" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0622 10:22:28.451589      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3150, replica count: 2
I0622 10:22:31.504180      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 10:22:31.504: INFO: Creating new exec pod
W0622 10:22:31.512886      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:22:31.513: INFO: Waiting up to 5m0s for pod "execpodqrgjg" in namespace "services-3150" to be "running"
Jun 22 10:22:31.517: INFO: Pod "execpodqrgjg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.672544ms
Jun 22 10:22:33.523: INFO: Pod "execpodqrgjg": Phase="Running", Reason="", readiness=true. Elapsed: 2.009992739s
Jun 22 10:22:33.523: INFO: Pod "execpodqrgjg" satisfied condition "running"
Jun 22 10:22:34.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-3150 exec execpodqrgjg -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jun 22 10:22:34.821: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 22 10:22:34.821: INFO: stdout: ""
Jun 22 10:22:34.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-3150 exec execpodqrgjg -- /bin/sh -x -c nc -v -z -w 2 100.69.21.122 80'
Jun 22 10:22:35.004: INFO: stderr: "+ nc -v -z -w 2 100.69.21.122 80\nConnection to 100.69.21.122 80 port [tcp/http] succeeded!\n"
Jun 22 10:22:35.004: INFO: stdout: ""
Jun 22 10:22:35.004: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun 22 10:22:35.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3150" for this suite. 06/22/23 10:22:35.043
------------------------------
• [SLOW TEST] [6.673 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:22:28.378
    Jun 22 10:22:28.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename services 06/22/23 10:22:28.379
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:28.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:28.403
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-3150 06/22/23 10:22:28.407
    STEP: changing the ExternalName service to type=ClusterIP 06/22/23 10:22:28.414
    STEP: creating replication controller externalname-service in namespace services-3150 06/22/23 10:22:28.441
    W0622 10:22:28.451210      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "externalname-service" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "externalname-service" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "externalname-service" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "externalname-service" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0622 10:22:28.451589      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3150, replica count: 2
    I0622 10:22:31.504180      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 22 10:22:31.504: INFO: Creating new exec pod
    W0622 10:22:31.512886      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:22:31.513: INFO: Waiting up to 5m0s for pod "execpodqrgjg" in namespace "services-3150" to be "running"
    Jun 22 10:22:31.517: INFO: Pod "execpodqrgjg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.672544ms
    Jun 22 10:22:33.523: INFO: Pod "execpodqrgjg": Phase="Running", Reason="", readiness=true. Elapsed: 2.009992739s
    Jun 22 10:22:33.523: INFO: Pod "execpodqrgjg" satisfied condition "running"
    Jun 22 10:22:34.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-3150 exec execpodqrgjg -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jun 22 10:22:34.821: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 22 10:22:34.821: INFO: stdout: ""
    Jun 22 10:22:34.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-3150 exec execpodqrgjg -- /bin/sh -x -c nc -v -z -w 2 100.69.21.122 80'
    Jun 22 10:22:35.004: INFO: stderr: "+ nc -v -z -w 2 100.69.21.122 80\nConnection to 100.69.21.122 80 port [tcp/http] succeeded!\n"
    Jun 22 10:22:35.004: INFO: stdout: ""
    Jun 22 10:22:35.004: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:22:35.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3150" for this suite. 06/22/23 10:22:35.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:22:35.054
Jun 22 10:22:35.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename services 06/22/23 10:22:35.055
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:35.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:35.08
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-6716 06/22/23 10:22:35.084
STEP: creating service affinity-nodeport in namespace services-6716 06/22/23 10:22:35.084
STEP: creating replication controller affinity-nodeport in namespace services-6716 06/22/23 10:22:35.11
W0622 10:22:35.120759      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "affinity-nodeport" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-nodeport" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-nodeport" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-nodeport" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0622 10:22:35.120888      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-6716, replica count: 3
I0622 10:22:38.171565      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 10:22:38.194: INFO: Creating new exec pod
W0622 10:22:38.203037      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:22:38.203: INFO: Waiting up to 5m0s for pod "execpod-affinitygstc9" in namespace "services-6716" to be "running"
Jun 22 10:22:38.207: INFO: Pod "execpod-affinitygstc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.891496ms
Jun 22 10:22:40.212: INFO: Pod "execpod-affinitygstc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009508229s
Jun 22 10:22:40.212: INFO: Pod "execpod-affinitygstc9" satisfied condition "running"
Jun 22 10:22:41.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-6716 exec execpod-affinitygstc9 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Jun 22 10:22:41.411: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jun 22 10:22:41.411: INFO: stdout: ""
Jun 22 10:22:41.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-6716 exec execpod-affinitygstc9 -- /bin/sh -x -c nc -v -z -w 2 100.66.23.56 80'
Jun 22 10:22:41.586: INFO: stderr: "+ nc -v -z -w 2 100.66.23.56 80\nConnection to 100.66.23.56 80 port [tcp/http] succeeded!\n"
Jun 22 10:22:41.586: INFO: stdout: ""
Jun 22 10:22:41.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-6716 exec execpod-affinitygstc9 -- /bin/sh -x -c nc -v -z -w 2 10.92.226.162 31337'
Jun 22 10:22:41.755: INFO: stderr: "+ nc -v -z -w 2 10.92.226.162 31337\nConnection to 10.92.226.162 31337 port [tcp/*] succeeded!\n"
Jun 22 10:22:41.755: INFO: stdout: ""
Jun 22 10:22:41.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-6716 exec execpod-affinitygstc9 -- /bin/sh -x -c nc -v -z -w 2 10.92.224.179 31337'
Jun 22 10:22:41.936: INFO: stderr: "+ nc -v -z -w 2 10.92.224.179 31337\nConnection to 10.92.224.179 31337 port [tcp/*] succeeded!\n"
Jun 22 10:22:41.936: INFO: stdout: ""
Jun 22 10:22:41.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-6716 exec execpod-affinitygstc9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.92.224.179:31337/ ; done'
Jun 22 10:22:42.210: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n"
Jun 22 10:22:42.210: INFO: stdout: "\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w"
Jun 22 10:22:42.210: INFO: Received response from host: affinity-nodeport-m4l4w
Jun 22 10:22:42.210: INFO: Received response from host: affinity-nodeport-m4l4w
Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
Jun 22 10:22:42.211: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-6716, will wait for the garbage collector to delete the pods 06/22/23 10:22:42.232
Jun 22 10:22:42.296: INFO: Deleting ReplicationController affinity-nodeport took: 8.510057ms
Jun 22 10:22:42.397: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.097197ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun 22 10:22:44.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6716" for this suite. 06/22/23 10:22:44.835
------------------------------
• [SLOW TEST] [9.789 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:22:35.054
    Jun 22 10:22:35.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename services 06/22/23 10:22:35.055
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:35.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:35.08
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-6716 06/22/23 10:22:35.084
    STEP: creating service affinity-nodeport in namespace services-6716 06/22/23 10:22:35.084
    STEP: creating replication controller affinity-nodeport in namespace services-6716 06/22/23 10:22:35.11
    W0622 10:22:35.120759      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "affinity-nodeport" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-nodeport" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-nodeport" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-nodeport" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0622 10:22:35.120888      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-6716, replica count: 3
    I0622 10:22:38.171565      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 22 10:22:38.194: INFO: Creating new exec pod
    W0622 10:22:38.203037      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:22:38.203: INFO: Waiting up to 5m0s for pod "execpod-affinitygstc9" in namespace "services-6716" to be "running"
    Jun 22 10:22:38.207: INFO: Pod "execpod-affinitygstc9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.891496ms
    Jun 22 10:22:40.212: INFO: Pod "execpod-affinitygstc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009508229s
    Jun 22 10:22:40.212: INFO: Pod "execpod-affinitygstc9" satisfied condition "running"
    Jun 22 10:22:41.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-6716 exec execpod-affinitygstc9 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Jun 22 10:22:41.411: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jun 22 10:22:41.411: INFO: stdout: ""
    Jun 22 10:22:41.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-6716 exec execpod-affinitygstc9 -- /bin/sh -x -c nc -v -z -w 2 100.66.23.56 80'
    Jun 22 10:22:41.586: INFO: stderr: "+ nc -v -z -w 2 100.66.23.56 80\nConnection to 100.66.23.56 80 port [tcp/http] succeeded!\n"
    Jun 22 10:22:41.586: INFO: stdout: ""
    Jun 22 10:22:41.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-6716 exec execpod-affinitygstc9 -- /bin/sh -x -c nc -v -z -w 2 10.92.226.162 31337'
    Jun 22 10:22:41.755: INFO: stderr: "+ nc -v -z -w 2 10.92.226.162 31337\nConnection to 10.92.226.162 31337 port [tcp/*] succeeded!\n"
    Jun 22 10:22:41.755: INFO: stdout: ""
    Jun 22 10:22:41.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-6716 exec execpod-affinitygstc9 -- /bin/sh -x -c nc -v -z -w 2 10.92.224.179 31337'
    Jun 22 10:22:41.936: INFO: stderr: "+ nc -v -z -w 2 10.92.224.179 31337\nConnection to 10.92.224.179 31337 port [tcp/*] succeeded!\n"
    Jun 22 10:22:41.936: INFO: stdout: ""
    Jun 22 10:22:41.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-6716 exec execpod-affinitygstc9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.92.224.179:31337/ ; done'
    Jun 22 10:22:42.210: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:31337/\n"
    Jun 22 10:22:42.210: INFO: stdout: "\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w\naffinity-nodeport-m4l4w"
    Jun 22 10:22:42.210: INFO: Received response from host: affinity-nodeport-m4l4w
    Jun 22 10:22:42.210: INFO: Received response from host: affinity-nodeport-m4l4w
    Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
    Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
    Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
    Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
    Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
    Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
    Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
    Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
    Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
    Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
    Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
    Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
    Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
    Jun 22 10:22:42.211: INFO: Received response from host: affinity-nodeport-m4l4w
    Jun 22 10:22:42.211: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-6716, will wait for the garbage collector to delete the pods 06/22/23 10:22:42.232
    Jun 22 10:22:42.296: INFO: Deleting ReplicationController affinity-nodeport took: 8.510057ms
    Jun 22 10:22:42.397: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.097197ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:22:44.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6716" for this suite. 06/22/23 10:22:44.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:22:44.845
Jun 22 10:22:44.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 10:22:44.846
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:44.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:44.867
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-be42308f-f0eb-4b6f-bf4e-2406a5fed5f6 06/22/23 10:22:44.87
STEP: Creating a pod to test consume secrets 06/22/23 10:22:44.876
W0622 10:22:44.890669      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:22:44.890: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c66844b6-f1ba-4efa-b324-d8d85d9eb736" in namespace "projected-1234" to be "Succeeded or Failed"
Jun 22 10:22:44.898: INFO: Pod "pod-projected-secrets-c66844b6-f1ba-4efa-b324-d8d85d9eb736": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013555ms
Jun 22 10:22:46.905: INFO: Pod "pod-projected-secrets-c66844b6-f1ba-4efa-b324-d8d85d9eb736": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014433842s
Jun 22 10:22:48.905: INFO: Pod "pod-projected-secrets-c66844b6-f1ba-4efa-b324-d8d85d9eb736": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014897385s
STEP: Saw pod success 06/22/23 10:22:48.905
Jun 22 10:22:48.905: INFO: Pod "pod-projected-secrets-c66844b6-f1ba-4efa-b324-d8d85d9eb736" satisfied condition "Succeeded or Failed"
Jun 22 10:22:48.910: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-secrets-c66844b6-f1ba-4efa-b324-d8d85d9eb736 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/22/23 10:22:48.922
Jun 22 10:22:48.940: INFO: Waiting for pod pod-projected-secrets-c66844b6-f1ba-4efa-b324-d8d85d9eb736 to disappear
Jun 22 10:22:48.946: INFO: Pod pod-projected-secrets-c66844b6-f1ba-4efa-b324-d8d85d9eb736 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun 22 10:22:48.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1234" for this suite. 06/22/23 10:22:48.952
------------------------------
• [4.115 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:22:44.845
    Jun 22 10:22:44.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 10:22:44.846
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:44.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:44.867
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-be42308f-f0eb-4b6f-bf4e-2406a5fed5f6 06/22/23 10:22:44.87
    STEP: Creating a pod to test consume secrets 06/22/23 10:22:44.876
    W0622 10:22:44.890669      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:22:44.890: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c66844b6-f1ba-4efa-b324-d8d85d9eb736" in namespace "projected-1234" to be "Succeeded or Failed"
    Jun 22 10:22:44.898: INFO: Pod "pod-projected-secrets-c66844b6-f1ba-4efa-b324-d8d85d9eb736": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013555ms
    Jun 22 10:22:46.905: INFO: Pod "pod-projected-secrets-c66844b6-f1ba-4efa-b324-d8d85d9eb736": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014433842s
    Jun 22 10:22:48.905: INFO: Pod "pod-projected-secrets-c66844b6-f1ba-4efa-b324-d8d85d9eb736": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014897385s
    STEP: Saw pod success 06/22/23 10:22:48.905
    Jun 22 10:22:48.905: INFO: Pod "pod-projected-secrets-c66844b6-f1ba-4efa-b324-d8d85d9eb736" satisfied condition "Succeeded or Failed"
    Jun 22 10:22:48.910: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-secrets-c66844b6-f1ba-4efa-b324-d8d85d9eb736 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/22/23 10:22:48.922
    Jun 22 10:22:48.940: INFO: Waiting for pod pod-projected-secrets-c66844b6-f1ba-4efa-b324-d8d85d9eb736 to disappear
    Jun 22 10:22:48.946: INFO: Pod pod-projected-secrets-c66844b6-f1ba-4efa-b324-d8d85d9eb736 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:22:48.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1234" for this suite. 06/22/23 10:22:48.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:22:48.961
Jun 22 10:22:48.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename container-lifecycle-hook 06/22/23 10:22:48.962
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:48.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:49.003
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 06/22/23 10:22:49.013
W0622 10:22:49.026151      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:22:49.026: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4422" to be "running and ready"
Jun 22 10:22:49.033: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.594025ms
Jun 22 10:22:49.033: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:22:51.039: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012913129s
Jun 22 10:22:51.039: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun 22 10:22:51.039: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 06/22/23 10:22:51.044
W0622 10:22:51.050793      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-with-poststart-http-hook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-with-poststart-http-hook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-with-poststart-http-hook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-with-poststart-http-hook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:22:51.050: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4422" to be "running and ready"
Jun 22 10:22:51.060: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.95785ms
Jun 22 10:22:51.060: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:22:53.067: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016663698s
Jun 22 10:22:53.067: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:22:55.067: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.016867734s
Jun 22 10:22:55.067: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jun 22 10:22:55.067: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 06/22/23 10:22:55.072
STEP: delete the pod with lifecycle hook 06/22/23 10:22:55.091
Jun 22 10:22:55.103: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 10:22:55.108: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 10:22:57.109: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 10:22:57.115: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jun 22 10:22:57.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4422" for this suite. 06/22/23 10:22:57.121
------------------------------
• [SLOW TEST] [8.169 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:22:48.961
    Jun 22 10:22:48.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/22/23 10:22:48.962
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:48.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:49.003
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 06/22/23 10:22:49.013
    W0622 10:22:49.026151      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:22:49.026: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4422" to be "running and ready"
    Jun 22 10:22:49.033: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.594025ms
    Jun 22 10:22:49.033: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:22:51.039: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012913129s
    Jun 22 10:22:51.039: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun 22 10:22:51.039: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 06/22/23 10:22:51.044
    W0622 10:22:51.050793      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-with-poststart-http-hook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-with-poststart-http-hook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-with-poststart-http-hook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-with-poststart-http-hook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:22:51.050: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-4422" to be "running and ready"
    Jun 22 10:22:51.060: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.95785ms
    Jun 22 10:22:51.060: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:22:53.067: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016663698s
    Jun 22 10:22:53.067: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:22:55.067: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.016867734s
    Jun 22 10:22:55.067: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jun 22 10:22:55.067: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 06/22/23 10:22:55.072
    STEP: delete the pod with lifecycle hook 06/22/23 10:22:55.091
    Jun 22 10:22:55.103: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jun 22 10:22:55.108: INFO: Pod pod-with-poststart-http-hook still exists
    Jun 22 10:22:57.109: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jun 22 10:22:57.115: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:22:57.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4422" for this suite. 06/22/23 10:22:57.121
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:22:57.131
Jun 22 10:22:57.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename replication-controller 06/22/23 10:22:57.132
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:57.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:57.155
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-5pgt5" 06/22/23 10:22:57.159
W0622 10:22:57.167075      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:22:57.167: INFO: Get Replication Controller "e2e-rc-5pgt5" to confirm replicas
Jun 22 10:22:58.171: INFO: Get Replication Controller "e2e-rc-5pgt5" to confirm replicas
Jun 22 10:22:58.177: INFO: Found 1 replicas for "e2e-rc-5pgt5" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-5pgt5" 06/22/23 10:22:58.177
STEP: Updating a scale subresource 06/22/23 10:22:58.182
STEP: Verifying replicas where modified for replication controller "e2e-rc-5pgt5" 06/22/23 10:22:58.192
Jun 22 10:22:58.192: INFO: Get Replication Controller "e2e-rc-5pgt5" to confirm replicas
Jun 22 10:22:59.201: INFO: Get Replication Controller "e2e-rc-5pgt5" to confirm replicas
Jun 22 10:22:59.207: INFO: Found 2 replicas for "e2e-rc-5pgt5" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jun 22 10:22:59.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5495" for this suite. 06/22/23 10:22:59.214
------------------------------
• [2.092 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:22:57.131
    Jun 22 10:22:57.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename replication-controller 06/22/23 10:22:57.132
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:57.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:57.155
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-5pgt5" 06/22/23 10:22:57.159
    W0622 10:22:57.167075      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:22:57.167: INFO: Get Replication Controller "e2e-rc-5pgt5" to confirm replicas
    Jun 22 10:22:58.171: INFO: Get Replication Controller "e2e-rc-5pgt5" to confirm replicas
    Jun 22 10:22:58.177: INFO: Found 1 replicas for "e2e-rc-5pgt5" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-5pgt5" 06/22/23 10:22:58.177
    STEP: Updating a scale subresource 06/22/23 10:22:58.182
    STEP: Verifying replicas where modified for replication controller "e2e-rc-5pgt5" 06/22/23 10:22:58.192
    Jun 22 10:22:58.192: INFO: Get Replication Controller "e2e-rc-5pgt5" to confirm replicas
    Jun 22 10:22:59.201: INFO: Get Replication Controller "e2e-rc-5pgt5" to confirm replicas
    Jun 22 10:22:59.207: INFO: Found 2 replicas for "e2e-rc-5pgt5" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:22:59.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5495" for this suite. 06/22/23 10:22:59.214
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:22:59.226
Jun 22 10:22:59.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename taint-multiple-pods 06/22/23 10:22:59.227
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:59.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:59.248
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Jun 22 10:22:59.252: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 22 10:23:59.315: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Jun 22 10:23:59.324: INFO: Starting informer...
STEP: Starting pods... 06/22/23 10:23:59.324
W0622 10:23:59.340065      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:23:59.551: INFO: Pod1 is running on wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw. Tainting Node
W0622 10:23:59.558511      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:23:59.764: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-66" to be "running"
Jun 22 10:23:59.770: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.837567ms
Jun 22 10:24:01.778: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013826272s
Jun 22 10:24:01.778: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jun 22 10:24:01.778: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-66" to be "running"
Jun 22 10:24:01.783: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.841655ms
Jun 22 10:24:01.783: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jun 22 10:24:01.783: INFO: Pod2 is running on wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw. Tainting Node
STEP: Trying to apply a taint on the Node 06/22/23 10:24:01.783
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/22/23 10:24:01.804
STEP: Waiting for Pod1 and Pod2 to be deleted 06/22/23 10:24:01.81
Jun 22 10:24:07.966: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun 22 10:24:28.037: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/22/23 10:24:28.056
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:24:28.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-66" for this suite. 06/22/23 10:24:28.068
------------------------------
• [SLOW TEST] [88.850 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:22:59.226
    Jun 22 10:22:59.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename taint-multiple-pods 06/22/23 10:22:59.227
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:22:59.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:22:59.248
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Jun 22 10:22:59.252: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 22 10:23:59.315: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Jun 22 10:23:59.324: INFO: Starting informer...
    STEP: Starting pods... 06/22/23 10:23:59.324
    W0622 10:23:59.340065      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:23:59.551: INFO: Pod1 is running on wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw. Tainting Node
    W0622 10:23:59.558511      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:23:59.764: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-66" to be "running"
    Jun 22 10:23:59.770: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.837567ms
    Jun 22 10:24:01.778: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013826272s
    Jun 22 10:24:01.778: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jun 22 10:24:01.778: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-66" to be "running"
    Jun 22 10:24:01.783: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.841655ms
    Jun 22 10:24:01.783: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jun 22 10:24:01.783: INFO: Pod2 is running on wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw. Tainting Node
    STEP: Trying to apply a taint on the Node 06/22/23 10:24:01.783
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/22/23 10:24:01.804
    STEP: Waiting for Pod1 and Pod2 to be deleted 06/22/23 10:24:01.81
    Jun 22 10:24:07.966: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jun 22 10:24:28.037: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/22/23 10:24:28.056
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:24:28.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-66" for this suite. 06/22/23 10:24:28.068
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:24:28.077
Jun 22 10:24:28.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename var-expansion 06/22/23 10:24:28.079
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:24:28.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:24:28.099
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 06/22/23 10:24:28.102
W0622 10:24:28.114890      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:24:28.115: INFO: Waiting up to 5m0s for pod "var-expansion-1dbe2489-249e-4880-b459-957468677f70" in namespace "var-expansion-4968" to be "Succeeded or Failed"
Jun 22 10:24:28.119: INFO: Pod "var-expansion-1dbe2489-249e-4880-b459-957468677f70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.450911ms
Jun 22 10:24:30.142: INFO: Pod "var-expansion-1dbe2489-249e-4880-b459-957468677f70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027366219s
Jun 22 10:24:32.130: INFO: Pod "var-expansion-1dbe2489-249e-4880-b459-957468677f70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015639471s
STEP: Saw pod success 06/22/23 10:24:32.13
Jun 22 10:24:32.133: INFO: Pod "var-expansion-1dbe2489-249e-4880-b459-957468677f70" satisfied condition "Succeeded or Failed"
Jun 22 10:24:32.141: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod var-expansion-1dbe2489-249e-4880-b459-957468677f70 container dapi-container: <nil>
STEP: delete the pod 06/22/23 10:24:32.166
Jun 22 10:24:32.196: INFO: Waiting for pod var-expansion-1dbe2489-249e-4880-b459-957468677f70 to disappear
Jun 22 10:24:32.203: INFO: Pod var-expansion-1dbe2489-249e-4880-b459-957468677f70 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun 22 10:24:32.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4968" for this suite. 06/22/23 10:24:32.228
------------------------------
• [4.165 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:24:28.077
    Jun 22 10:24:28.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename var-expansion 06/22/23 10:24:28.079
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:24:28.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:24:28.099
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 06/22/23 10:24:28.102
    W0622 10:24:28.114890      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:24:28.115: INFO: Waiting up to 5m0s for pod "var-expansion-1dbe2489-249e-4880-b459-957468677f70" in namespace "var-expansion-4968" to be "Succeeded or Failed"
    Jun 22 10:24:28.119: INFO: Pod "var-expansion-1dbe2489-249e-4880-b459-957468677f70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.450911ms
    Jun 22 10:24:30.142: INFO: Pod "var-expansion-1dbe2489-249e-4880-b459-957468677f70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027366219s
    Jun 22 10:24:32.130: INFO: Pod "var-expansion-1dbe2489-249e-4880-b459-957468677f70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015639471s
    STEP: Saw pod success 06/22/23 10:24:32.13
    Jun 22 10:24:32.133: INFO: Pod "var-expansion-1dbe2489-249e-4880-b459-957468677f70" satisfied condition "Succeeded or Failed"
    Jun 22 10:24:32.141: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod var-expansion-1dbe2489-249e-4880-b459-957468677f70 container dapi-container: <nil>
    STEP: delete the pod 06/22/23 10:24:32.166
    Jun 22 10:24:32.196: INFO: Waiting for pod var-expansion-1dbe2489-249e-4880-b459-957468677f70 to disappear
    Jun 22 10:24:32.203: INFO: Pod var-expansion-1dbe2489-249e-4880-b459-957468677f70 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:24:32.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4968" for this suite. 06/22/23 10:24:32.228
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:24:32.248
Jun 22 10:24:32.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename emptydir 06/22/23 10:24:32.258
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:24:32.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:24:32.289
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 06/22/23 10:24:32.296
W0622 10:24:32.319598      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:24:32.320: INFO: Waiting up to 5m0s for pod "pod-53d3979a-db17-4257-b852-8c8a222c8441" in namespace "emptydir-9856" to be "Succeeded or Failed"
Jun 22 10:24:32.329: INFO: Pod "pod-53d3979a-db17-4257-b852-8c8a222c8441": Phase="Pending", Reason="", readiness=false. Elapsed: 9.57047ms
Jun 22 10:24:34.337: INFO: Pod "pod-53d3979a-db17-4257-b852-8c8a222c8441": Phase="Running", Reason="", readiness=false. Elapsed: 2.017098248s
Jun 22 10:24:36.337: INFO: Pod "pod-53d3979a-db17-4257-b852-8c8a222c8441": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01779943s
STEP: Saw pod success 06/22/23 10:24:36.338
Jun 22 10:24:36.338: INFO: Pod "pod-53d3979a-db17-4257-b852-8c8a222c8441" satisfied condition "Succeeded or Failed"
Jun 22 10:24:36.343: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-53d3979a-db17-4257-b852-8c8a222c8441 container test-container: <nil>
STEP: delete the pod 06/22/23 10:24:36.353
Jun 22 10:24:36.372: INFO: Waiting for pod pod-53d3979a-db17-4257-b852-8c8a222c8441 to disappear
Jun 22 10:24:36.376: INFO: Pod pod-53d3979a-db17-4257-b852-8c8a222c8441 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun 22 10:24:36.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9856" for this suite. 06/22/23 10:24:36.388
------------------------------
• [4.150 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:24:32.248
    Jun 22 10:24:32.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename emptydir 06/22/23 10:24:32.258
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:24:32.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:24:32.289
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 06/22/23 10:24:32.296
    W0622 10:24:32.319598      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:24:32.320: INFO: Waiting up to 5m0s for pod "pod-53d3979a-db17-4257-b852-8c8a222c8441" in namespace "emptydir-9856" to be "Succeeded or Failed"
    Jun 22 10:24:32.329: INFO: Pod "pod-53d3979a-db17-4257-b852-8c8a222c8441": Phase="Pending", Reason="", readiness=false. Elapsed: 9.57047ms
    Jun 22 10:24:34.337: INFO: Pod "pod-53d3979a-db17-4257-b852-8c8a222c8441": Phase="Running", Reason="", readiness=false. Elapsed: 2.017098248s
    Jun 22 10:24:36.337: INFO: Pod "pod-53d3979a-db17-4257-b852-8c8a222c8441": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01779943s
    STEP: Saw pod success 06/22/23 10:24:36.338
    Jun 22 10:24:36.338: INFO: Pod "pod-53d3979a-db17-4257-b852-8c8a222c8441" satisfied condition "Succeeded or Failed"
    Jun 22 10:24:36.343: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-53d3979a-db17-4257-b852-8c8a222c8441 container test-container: <nil>
    STEP: delete the pod 06/22/23 10:24:36.353
    Jun 22 10:24:36.372: INFO: Waiting for pod pod-53d3979a-db17-4257-b852-8c8a222c8441 to disappear
    Jun 22 10:24:36.376: INFO: Pod pod-53d3979a-db17-4257-b852-8c8a222c8441 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:24:36.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9856" for this suite. 06/22/23 10:24:36.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:24:36.403
Jun 22 10:24:36.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename services 06/22/23 10:24:36.405
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:24:36.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:24:36.429
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-7416 06/22/23 10:24:36.434
STEP: creating service affinity-nodeport-transition in namespace services-7416 06/22/23 10:24:36.434
STEP: creating replication controller affinity-nodeport-transition in namespace services-7416 06/22/23 10:24:36.474
W0622 10:24:36.489384      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "affinity-nodeport-transition" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-nodeport-transition" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-nodeport-transition" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-nodeport-transition" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0622 10:24:36.491915      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-7416, replica count: 3
I0622 10:24:39.543860      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 10:24:39.566: INFO: Creating new exec pod
W0622 10:24:39.574550      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:24:39.574: INFO: Waiting up to 5m0s for pod "execpod-affinityqghsx" in namespace "services-7416" to be "running"
Jun 22 10:24:39.579: INFO: Pod "execpod-affinityqghsx": Phase="Pending", Reason="", readiness=false. Elapsed: 5.044551ms
Jun 22 10:24:41.586: INFO: Pod "execpod-affinityqghsx": Phase="Running", Reason="", readiness=true. Elapsed: 2.011940274s
Jun 22 10:24:41.586: INFO: Pod "execpod-affinityqghsx" satisfied condition "running"
Jun 22 10:24:42.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-7416 exec execpod-affinityqghsx -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jun 22 10:24:42.910: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jun 22 10:24:42.910: INFO: stdout: ""
Jun 22 10:24:42.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-7416 exec execpod-affinityqghsx -- /bin/sh -x -c nc -v -z -w 2 100.71.104.80 80'
Jun 22 10:24:43.118: INFO: stderr: "+ nc -v -z -w 2 100.71.104.80 80\nConnection to 100.71.104.80 80 port [tcp/http] succeeded!\n"
Jun 22 10:24:43.118: INFO: stdout: ""
Jun 22 10:24:43.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-7416 exec execpod-affinityqghsx -- /bin/sh -x -c nc -v -z -w 2 10.92.224.179 32596'
Jun 22 10:24:43.319: INFO: stderr: "+ nc -v -z -w 2 10.92.224.179 32596\nConnection to 10.92.224.179 32596 port [tcp/*] succeeded!\n"
Jun 22 10:24:43.319: INFO: stdout: ""
Jun 22 10:24:43.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-7416 exec execpod-affinityqghsx -- /bin/sh -x -c nc -v -z -w 2 10.92.226.162 32596'
Jun 22 10:24:43.516: INFO: stderr: "+ nc -v -z -w 2 10.92.226.162 32596\nConnection to 10.92.226.162 32596 port [tcp/*] succeeded!\n"
Jun 22 10:24:43.516: INFO: stdout: ""
Jun 22 10:24:43.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-7416 exec execpod-affinityqghsx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.92.224.179:32596/ ; done'
Jun 22 10:24:43.846: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n"
Jun 22 10:24:43.846: INFO: stdout: "\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-8p6tf\naffinity-nodeport-transition-zwsfd\naffinity-nodeport-transition-8p6tf\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-zwsfd\naffinity-nodeport-transition-zwsfd\naffinity-nodeport-transition-8p6tf\naffinity-nodeport-transition-zwsfd\naffinity-nodeport-transition-zwsfd\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-8p6tf\naffinity-nodeport-transition-zwsfd\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-zwsfd"
Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-8p6tf
Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-zwsfd
Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-8p6tf
Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-zwsfd
Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-zwsfd
Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-8p6tf
Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-zwsfd
Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-zwsfd
Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-8p6tf
Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-zwsfd
Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-zwsfd
Jun 22 10:24:43.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-7416 exec execpod-affinityqghsx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.92.224.179:32596/ ; done'
Jun 22 10:24:44.161: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n"
Jun 22 10:24:44.162: INFO: stdout: "\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t"
Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
Jun 22 10:24:44.162: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7416, will wait for the garbage collector to delete the pods 06/22/23 10:24:44.183
Jun 22 10:24:44.247: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.406714ms
Jun 22 10:24:44.448: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 201.019517ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun 22 10:24:46.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7416" for this suite. 06/22/23 10:24:47.003
------------------------------
• [SLOW TEST] [10.611 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:24:36.403
    Jun 22 10:24:36.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename services 06/22/23 10:24:36.405
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:24:36.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:24:36.429
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-7416 06/22/23 10:24:36.434
    STEP: creating service affinity-nodeport-transition in namespace services-7416 06/22/23 10:24:36.434
    STEP: creating replication controller affinity-nodeport-transition in namespace services-7416 06/22/23 10:24:36.474
    W0622 10:24:36.489384      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "affinity-nodeport-transition" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-nodeport-transition" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-nodeport-transition" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-nodeport-transition" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0622 10:24:36.491915      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-7416, replica count: 3
    I0622 10:24:39.543860      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 22 10:24:39.566: INFO: Creating new exec pod
    W0622 10:24:39.574550      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:24:39.574: INFO: Waiting up to 5m0s for pod "execpod-affinityqghsx" in namespace "services-7416" to be "running"
    Jun 22 10:24:39.579: INFO: Pod "execpod-affinityqghsx": Phase="Pending", Reason="", readiness=false. Elapsed: 5.044551ms
    Jun 22 10:24:41.586: INFO: Pod "execpod-affinityqghsx": Phase="Running", Reason="", readiness=true. Elapsed: 2.011940274s
    Jun 22 10:24:41.586: INFO: Pod "execpod-affinityqghsx" satisfied condition "running"
    Jun 22 10:24:42.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-7416 exec execpod-affinityqghsx -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jun 22 10:24:42.910: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jun 22 10:24:42.910: INFO: stdout: ""
    Jun 22 10:24:42.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-7416 exec execpod-affinityqghsx -- /bin/sh -x -c nc -v -z -w 2 100.71.104.80 80'
    Jun 22 10:24:43.118: INFO: stderr: "+ nc -v -z -w 2 100.71.104.80 80\nConnection to 100.71.104.80 80 port [tcp/http] succeeded!\n"
    Jun 22 10:24:43.118: INFO: stdout: ""
    Jun 22 10:24:43.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-7416 exec execpod-affinityqghsx -- /bin/sh -x -c nc -v -z -w 2 10.92.224.179 32596'
    Jun 22 10:24:43.319: INFO: stderr: "+ nc -v -z -w 2 10.92.224.179 32596\nConnection to 10.92.224.179 32596 port [tcp/*] succeeded!\n"
    Jun 22 10:24:43.319: INFO: stdout: ""
    Jun 22 10:24:43.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-7416 exec execpod-affinityqghsx -- /bin/sh -x -c nc -v -z -w 2 10.92.226.162 32596'
    Jun 22 10:24:43.516: INFO: stderr: "+ nc -v -z -w 2 10.92.226.162 32596\nConnection to 10.92.226.162 32596 port [tcp/*] succeeded!\n"
    Jun 22 10:24:43.516: INFO: stdout: ""
    Jun 22 10:24:43.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-7416 exec execpod-affinityqghsx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.92.224.179:32596/ ; done'
    Jun 22 10:24:43.846: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n"
    Jun 22 10:24:43.846: INFO: stdout: "\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-8p6tf\naffinity-nodeport-transition-zwsfd\naffinity-nodeport-transition-8p6tf\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-zwsfd\naffinity-nodeport-transition-zwsfd\naffinity-nodeport-transition-8p6tf\naffinity-nodeport-transition-zwsfd\naffinity-nodeport-transition-zwsfd\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-8p6tf\naffinity-nodeport-transition-zwsfd\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-zwsfd"
    Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-8p6tf
    Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-zwsfd
    Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-8p6tf
    Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-zwsfd
    Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-zwsfd
    Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-8p6tf
    Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-zwsfd
    Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-zwsfd
    Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-8p6tf
    Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-zwsfd
    Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:43.846: INFO: Received response from host: affinity-nodeport-transition-zwsfd
    Jun 22 10:24:43.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-7416 exec execpod-affinityqghsx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.92.224.179:32596/ ; done'
    Jun 22 10:24:44.161: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.92.224.179:32596/\n"
    Jun 22 10:24:44.162: INFO: stdout: "\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t\naffinity-nodeport-transition-7l29t"
    Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:44.162: INFO: Received response from host: affinity-nodeport-transition-7l29t
    Jun 22 10:24:44.162: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7416, will wait for the garbage collector to delete the pods 06/22/23 10:24:44.183
    Jun 22 10:24:44.247: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.406714ms
    Jun 22 10:24:44.448: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 201.019517ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:24:46.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7416" for this suite. 06/22/23 10:24:47.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:24:47.017
Jun 22 10:24:47.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubectl 06/22/23 10:24:47.019
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:24:47.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:24:47.042
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 06/22/23 10:24:47.047
Jun 22 10:24:47.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-3840 create -f -'
Jun 22 10:24:48.183: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"pause\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"pause\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"pause\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"pause\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Jun 22 10:24:48.183: INFO: stdout: "pod/pause created\n"
Jun 22 10:24:48.183: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 22 10:24:48.184: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3840" to be "running and ready"
Jun 22 10:24:48.191: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.332835ms
Jun 22 10:24:48.191: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw' to be 'Running' but was 'Pending'
Jun 22 10:24:50.200: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.016452754s
Jun 22 10:24:50.201: INFO: Pod "pause" satisfied condition "running and ready"
Jun 22 10:24:50.201: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 06/22/23 10:24:50.201
Jun 22 10:24:50.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-3840 label pods pause testing-label=testing-label-value'
Jun 22 10:24:50.315: INFO: stderr: ""
Jun 22 10:24:50.315: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 06/22/23 10:24:50.315
Jun 22 10:24:50.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-3840 get pod pause -L testing-label'
Jun 22 10:24:50.407: INFO: stderr: ""
Jun 22 10:24:50.407: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 06/22/23 10:24:50.407
Jun 22 10:24:50.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-3840 label pods pause testing-label-'
Jun 22 10:24:50.512: INFO: stderr: ""
Jun 22 10:24:50.512: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 06/22/23 10:24:50.512
Jun 22 10:24:50.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-3840 get pod pause -L testing-label'
Jun 22 10:24:50.608: INFO: stderr: ""
Jun 22 10:24:50.608: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 06/22/23 10:24:50.608
Jun 22 10:24:50.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-3840 delete --grace-period=0 --force -f -'
Jun 22 10:24:50.721: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 10:24:50.721: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 22 10:24:50.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-3840 get rc,svc -l name=pause --no-headers'
Jun 22 10:24:50.818: INFO: stderr: "No resources found in kubectl-3840 namespace.\n"
Jun 22 10:24:50.818: INFO: stdout: ""
Jun 22 10:24:50.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-3840 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 10:24:50.904: INFO: stderr: ""
Jun 22 10:24:50.904: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun 22 10:24:50.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3840" for this suite. 06/22/23 10:24:50.914
------------------------------
• [3.905 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:24:47.017
    Jun 22 10:24:47.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubectl 06/22/23 10:24:47.019
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:24:47.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:24:47.042
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 06/22/23 10:24:47.047
    Jun 22 10:24:47.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-3840 create -f -'
    Jun 22 10:24:48.183: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"pause\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"pause\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"pause\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"pause\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
    Jun 22 10:24:48.183: INFO: stdout: "pod/pause created\n"
    Jun 22 10:24:48.183: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jun 22 10:24:48.184: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3840" to be "running and ready"
    Jun 22 10:24:48.191: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.332835ms
    Jun 22 10:24:48.191: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw' to be 'Running' but was 'Pending'
    Jun 22 10:24:50.200: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.016452754s
    Jun 22 10:24:50.201: INFO: Pod "pause" satisfied condition "running and ready"
    Jun 22 10:24:50.201: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 06/22/23 10:24:50.201
    Jun 22 10:24:50.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-3840 label pods pause testing-label=testing-label-value'
    Jun 22 10:24:50.315: INFO: stderr: ""
    Jun 22 10:24:50.315: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 06/22/23 10:24:50.315
    Jun 22 10:24:50.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-3840 get pod pause -L testing-label'
    Jun 22 10:24:50.407: INFO: stderr: ""
    Jun 22 10:24:50.407: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 06/22/23 10:24:50.407
    Jun 22 10:24:50.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-3840 label pods pause testing-label-'
    Jun 22 10:24:50.512: INFO: stderr: ""
    Jun 22 10:24:50.512: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 06/22/23 10:24:50.512
    Jun 22 10:24:50.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-3840 get pod pause -L testing-label'
    Jun 22 10:24:50.608: INFO: stderr: ""
    Jun 22 10:24:50.608: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 06/22/23 10:24:50.608
    Jun 22 10:24:50.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-3840 delete --grace-period=0 --force -f -'
    Jun 22 10:24:50.721: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 22 10:24:50.721: INFO: stdout: "pod \"pause\" force deleted\n"
    Jun 22 10:24:50.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-3840 get rc,svc -l name=pause --no-headers'
    Jun 22 10:24:50.818: INFO: stderr: "No resources found in kubectl-3840 namespace.\n"
    Jun 22 10:24:50.818: INFO: stdout: ""
    Jun 22 10:24:50.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-3840 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jun 22 10:24:50.904: INFO: stderr: ""
    Jun 22 10:24:50.904: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:24:50.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3840" for this suite. 06/22/23 10:24:50.914
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:24:50.923
Jun 22 10:24:50.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename events 06/22/23 10:24:50.925
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:24:50.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:24:50.946
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 06/22/23 10:24:50.95
Jun 22 10:24:50.958: INFO: created test-event-1
Jun 22 10:24:50.964: INFO: created test-event-2
Jun 22 10:24:50.969: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 06/22/23 10:24:50.969
STEP: delete collection of events 06/22/23 10:24:50.973
Jun 22 10:24:50.973: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 06/22/23 10:24:51.015
Jun 22 10:24:51.015: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jun 22 10:24:51.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9185" for this suite. 06/22/23 10:24:51.033
------------------------------
• [0.119 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:24:50.923
    Jun 22 10:24:50.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename events 06/22/23 10:24:50.925
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:24:50.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:24:50.946
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 06/22/23 10:24:50.95
    Jun 22 10:24:50.958: INFO: created test-event-1
    Jun 22 10:24:50.964: INFO: created test-event-2
    Jun 22 10:24:50.969: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 06/22/23 10:24:50.969
    STEP: delete collection of events 06/22/23 10:24:50.973
    Jun 22 10:24:50.973: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 06/22/23 10:24:51.015
    Jun 22 10:24:51.015: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:24:51.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9185" for this suite. 06/22/23 10:24:51.033
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:24:51.054
Jun 22 10:24:51.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename job 06/22/23 10:24:51.058
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:24:51.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:24:51.083
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 06/22/23 10:24:51.088
W0622 10:24:51.099822      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions 06/22/23 10:24:51.1
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jun 22 10:25:03.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8327" for this suite. 06/22/23 10:25:03.116
------------------------------
• [SLOW TEST] [12.072 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:24:51.054
    Jun 22 10:24:51.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename job 06/22/23 10:24:51.058
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:24:51.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:24:51.083
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 06/22/23 10:24:51.088
    W0622 10:24:51.099822      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring job reaches completions 06/22/23 10:24:51.1
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:25:03.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8327" for this suite. 06/22/23 10:25:03.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:25:03.127
Jun 22 10:25:03.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename secrets 06/22/23 10:25:03.128
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:25:03.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:25:03.153
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-1abdc8e7-afeb-4917-aca1-5143e6a382c8 06/22/23 10:25:03.167
STEP: Creating secret with name s-test-opt-upd-5083f5a6-417f-4961-89b6-90e6faf145c7 06/22/23 10:25:03.173
STEP: Creating the pod 06/22/23 10:25:03.18
W0622 10:25:03.193942      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:25:03.194: INFO: Waiting up to 5m0s for pod "pod-secrets-ed942a51-e450-45ab-829b-4c6b566dd6bf" in namespace "secrets-5526" to be "running and ready"
Jun 22 10:25:03.199: INFO: Pod "pod-secrets-ed942a51-e450-45ab-829b-4c6b566dd6bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.278002ms
Jun 22 10:25:03.199: INFO: The phase of Pod pod-secrets-ed942a51-e450-45ab-829b-4c6b566dd6bf is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:25:05.207: INFO: Pod "pod-secrets-ed942a51-e450-45ab-829b-4c6b566dd6bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.013082303s
Jun 22 10:25:05.207: INFO: The phase of Pod pod-secrets-ed942a51-e450-45ab-829b-4c6b566dd6bf is Running (Ready = true)
Jun 22 10:25:05.207: INFO: Pod "pod-secrets-ed942a51-e450-45ab-829b-4c6b566dd6bf" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-1abdc8e7-afeb-4917-aca1-5143e6a382c8 06/22/23 10:25:05.244
STEP: Updating secret s-test-opt-upd-5083f5a6-417f-4961-89b6-90e6faf145c7 06/22/23 10:25:05.254
STEP: Creating secret with name s-test-opt-create-07c0b9d4-507f-4482-a765-625c5fbfce22 06/22/23 10:25:05.262
STEP: waiting to observe update in volume 06/22/23 10:25:05.268
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun 22 10:26:15.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5526" for this suite. 06/22/23 10:26:15.785
------------------------------
• [SLOW TEST] [72.671 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:25:03.127
    Jun 22 10:25:03.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename secrets 06/22/23 10:25:03.128
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:25:03.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:25:03.153
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-1abdc8e7-afeb-4917-aca1-5143e6a382c8 06/22/23 10:25:03.167
    STEP: Creating secret with name s-test-opt-upd-5083f5a6-417f-4961-89b6-90e6faf145c7 06/22/23 10:25:03.173
    STEP: Creating the pod 06/22/23 10:25:03.18
    W0622 10:25:03.193942      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:25:03.194: INFO: Waiting up to 5m0s for pod "pod-secrets-ed942a51-e450-45ab-829b-4c6b566dd6bf" in namespace "secrets-5526" to be "running and ready"
    Jun 22 10:25:03.199: INFO: Pod "pod-secrets-ed942a51-e450-45ab-829b-4c6b566dd6bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.278002ms
    Jun 22 10:25:03.199: INFO: The phase of Pod pod-secrets-ed942a51-e450-45ab-829b-4c6b566dd6bf is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:25:05.207: INFO: Pod "pod-secrets-ed942a51-e450-45ab-829b-4c6b566dd6bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.013082303s
    Jun 22 10:25:05.207: INFO: The phase of Pod pod-secrets-ed942a51-e450-45ab-829b-4c6b566dd6bf is Running (Ready = true)
    Jun 22 10:25:05.207: INFO: Pod "pod-secrets-ed942a51-e450-45ab-829b-4c6b566dd6bf" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-1abdc8e7-afeb-4917-aca1-5143e6a382c8 06/22/23 10:25:05.244
    STEP: Updating secret s-test-opt-upd-5083f5a6-417f-4961-89b6-90e6faf145c7 06/22/23 10:25:05.254
    STEP: Creating secret with name s-test-opt-create-07c0b9d4-507f-4482-a765-625c5fbfce22 06/22/23 10:25:05.262
    STEP: waiting to observe update in volume 06/22/23 10:25:05.268
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:26:15.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5526" for this suite. 06/22/23 10:26:15.785
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:26:15.8
Jun 22 10:26:15.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 10:26:15.803
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:26:15.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:26:15.826
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 06/22/23 10:26:15.831
W0622 10:26:15.843814      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:26:15.844: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7635b986-8896-46ec-9915-aece42fcb116" in namespace "projected-942" to be "Succeeded or Failed"
Jun 22 10:26:15.849: INFO: Pod "downwardapi-volume-7635b986-8896-46ec-9915-aece42fcb116": Phase="Pending", Reason="", readiness=false. Elapsed: 4.945819ms
Jun 22 10:26:17.856: INFO: Pod "downwardapi-volume-7635b986-8896-46ec-9915-aece42fcb116": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01271219s
Jun 22 10:26:19.856: INFO: Pod "downwardapi-volume-7635b986-8896-46ec-9915-aece42fcb116": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012281132s
STEP: Saw pod success 06/22/23 10:26:19.856
Jun 22 10:26:19.857: INFO: Pod "downwardapi-volume-7635b986-8896-46ec-9915-aece42fcb116" satisfied condition "Succeeded or Failed"
Jun 22 10:26:19.862: INFO: Trying to get logs from node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 pod downwardapi-volume-7635b986-8896-46ec-9915-aece42fcb116 container client-container: <nil>
STEP: delete the pod 06/22/23 10:26:19.889
Jun 22 10:26:19.910: INFO: Waiting for pod downwardapi-volume-7635b986-8896-46ec-9915-aece42fcb116 to disappear
Jun 22 10:26:19.914: INFO: Pod downwardapi-volume-7635b986-8896-46ec-9915-aece42fcb116 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun 22 10:26:19.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-942" for this suite. 06/22/23 10:26:19.924
------------------------------
• [4.134 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:26:15.8
    Jun 22 10:26:15.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 10:26:15.803
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:26:15.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:26:15.826
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 06/22/23 10:26:15.831
    W0622 10:26:15.843814      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:26:15.844: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7635b986-8896-46ec-9915-aece42fcb116" in namespace "projected-942" to be "Succeeded or Failed"
    Jun 22 10:26:15.849: INFO: Pod "downwardapi-volume-7635b986-8896-46ec-9915-aece42fcb116": Phase="Pending", Reason="", readiness=false. Elapsed: 4.945819ms
    Jun 22 10:26:17.856: INFO: Pod "downwardapi-volume-7635b986-8896-46ec-9915-aece42fcb116": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01271219s
    Jun 22 10:26:19.856: INFO: Pod "downwardapi-volume-7635b986-8896-46ec-9915-aece42fcb116": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012281132s
    STEP: Saw pod success 06/22/23 10:26:19.856
    Jun 22 10:26:19.857: INFO: Pod "downwardapi-volume-7635b986-8896-46ec-9915-aece42fcb116" satisfied condition "Succeeded or Failed"
    Jun 22 10:26:19.862: INFO: Trying to get logs from node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 pod downwardapi-volume-7635b986-8896-46ec-9915-aece42fcb116 container client-container: <nil>
    STEP: delete the pod 06/22/23 10:26:19.889
    Jun 22 10:26:19.910: INFO: Waiting for pod downwardapi-volume-7635b986-8896-46ec-9915-aece42fcb116 to disappear
    Jun 22 10:26:19.914: INFO: Pod downwardapi-volume-7635b986-8896-46ec-9915-aece42fcb116 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:26:19.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-942" for this suite. 06/22/23 10:26:19.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:26:19.94
Jun 22 10:26:19.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename disruption 06/22/23 10:26:19.942
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:26:19.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:26:19.964
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 06/22/23 10:26:19.969
STEP: Waiting for the pdb to be processed 06/22/23 10:26:19.979
W0622 10:26:21.998075      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "donothing" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "donothing" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "donothing" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "donothing" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: First trying to evict a pod which shouldn't be evictable 06/22/23 10:26:21.998
STEP: Waiting for all pods to be running 06/22/23 10:26:21.998
Jun 22 10:26:22.004: INFO: pods: 0 < 3
STEP: locating a running pod 06/22/23 10:26:24.011
STEP: Updating the pdb to allow a pod to be evicted 06/22/23 10:26:24.026
STEP: Waiting for the pdb to be processed 06/22/23 10:26:24.038
STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/22/23 10:26:26.048
STEP: Waiting for all pods to be running 06/22/23 10:26:26.049
STEP: Waiting for the pdb to observed all healthy pods 06/22/23 10:26:26.053
STEP: Patching the pdb to disallow a pod to be evicted 06/22/23 10:26:26.087
STEP: Waiting for the pdb to be processed 06/22/23 10:26:26.112
STEP: Waiting for all pods to be running 06/22/23 10:26:26.129
Jun 22 10:26:26.138: INFO: running pods: 2 < 3
STEP: locating a running pod 06/22/23 10:26:28.146
STEP: Deleting the pdb to allow a pod to be evicted 06/22/23 10:26:28.162
STEP: Waiting for the pdb to be deleted 06/22/23 10:26:28.171
STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/22/23 10:26:28.199
STEP: Waiting for all pods to be running 06/22/23 10:26:28.199
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jun 22 10:26:28.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5074" for this suite. 06/22/23 10:26:28.244
------------------------------
• [SLOW TEST] [8.313 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:26:19.94
    Jun 22 10:26:19.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename disruption 06/22/23 10:26:19.942
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:26:19.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:26:19.964
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 06/22/23 10:26:19.969
    STEP: Waiting for the pdb to be processed 06/22/23 10:26:19.979
    W0622 10:26:21.998075      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "donothing" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "donothing" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "donothing" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "donothing" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: First trying to evict a pod which shouldn't be evictable 06/22/23 10:26:21.998
    STEP: Waiting for all pods to be running 06/22/23 10:26:21.998
    Jun 22 10:26:22.004: INFO: pods: 0 < 3
    STEP: locating a running pod 06/22/23 10:26:24.011
    STEP: Updating the pdb to allow a pod to be evicted 06/22/23 10:26:24.026
    STEP: Waiting for the pdb to be processed 06/22/23 10:26:24.038
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/22/23 10:26:26.048
    STEP: Waiting for all pods to be running 06/22/23 10:26:26.049
    STEP: Waiting for the pdb to observed all healthy pods 06/22/23 10:26:26.053
    STEP: Patching the pdb to disallow a pod to be evicted 06/22/23 10:26:26.087
    STEP: Waiting for the pdb to be processed 06/22/23 10:26:26.112
    STEP: Waiting for all pods to be running 06/22/23 10:26:26.129
    Jun 22 10:26:26.138: INFO: running pods: 2 < 3
    STEP: locating a running pod 06/22/23 10:26:28.146
    STEP: Deleting the pdb to allow a pod to be evicted 06/22/23 10:26:28.162
    STEP: Waiting for the pdb to be deleted 06/22/23 10:26:28.171
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/22/23 10:26:28.199
    STEP: Waiting for all pods to be running 06/22/23 10:26:28.199
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:26:28.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5074" for this suite. 06/22/23 10:26:28.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:26:28.258
Jun 22 10:26:28.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename cronjob 06/22/23 10:26:28.259
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:26:28.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:26:28.283
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 06/22/23 10:26:28.286
W0622 10:26:28.296681      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring more than one job is running at a time 06/22/23 10:26:28.296
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 06/22/23 10:28:00.305
STEP: Removing cronjob 06/22/23 10:28:00.317
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jun 22 10:28:00.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-2835" for this suite. 06/22/23 10:28:00.355
------------------------------
• [SLOW TEST] [92.119 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:26:28.258
    Jun 22 10:26:28.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename cronjob 06/22/23 10:26:28.259
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:26:28.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:26:28.283
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 06/22/23 10:26:28.286
    W0622 10:26:28.296681      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring more than one job is running at a time 06/22/23 10:26:28.296
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 06/22/23 10:28:00.305
    STEP: Removing cronjob 06/22/23 10:28:00.317
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:28:00.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-2835" for this suite. 06/22/23 10:28:00.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:28:00.388
Jun 22 10:28:00.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename subpath 06/22/23 10:28:00.4
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:28:00.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:28:00.437
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/22/23 10:28:00.443
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-8vxh 06/22/23 10:28:00.458
STEP: Creating a pod to test atomic-volume-subpath 06/22/23 10:28:00.459
W0622 10:28:00.475124      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container-subpath-configmap-8vxh" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-subpath-configmap-8vxh" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-subpath-configmap-8vxh" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-subpath-configmap-8vxh" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:28:00.476: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8vxh" in namespace "subpath-6182" to be "Succeeded or Failed"
Jun 22 10:28:00.483: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Pending", Reason="", readiness=false. Elapsed: 7.447333ms
Jun 22 10:28:02.491: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 2.015483186s
Jun 22 10:28:04.491: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 4.014823841s
Jun 22 10:28:06.492: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 6.015973364s
Jun 22 10:28:08.491: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 8.015429327s
Jun 22 10:28:10.491: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 10.015493883s
Jun 22 10:28:12.489: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 12.01369422s
Jun 22 10:28:14.490: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 14.01410723s
Jun 22 10:28:16.491: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 16.015062842s
Jun 22 10:28:18.495: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 18.018930106s
Jun 22 10:28:20.492: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 20.016198656s
Jun 22 10:28:22.491: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=false. Elapsed: 22.015781959s
Jun 22 10:28:24.490: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014040367s
STEP: Saw pod success 06/22/23 10:28:24.49
Jun 22 10:28:24.491: INFO: Pod "pod-subpath-test-configmap-8vxh" satisfied condition "Succeeded or Failed"
Jun 22 10:28:24.496: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-subpath-test-configmap-8vxh container test-container-subpath-configmap-8vxh: <nil>
STEP: delete the pod 06/22/23 10:28:24.522
Jun 22 10:28:24.541: INFO: Waiting for pod pod-subpath-test-configmap-8vxh to disappear
Jun 22 10:28:24.545: INFO: Pod pod-subpath-test-configmap-8vxh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8vxh 06/22/23 10:28:24.545
Jun 22 10:28:24.546: INFO: Deleting pod "pod-subpath-test-configmap-8vxh" in namespace "subpath-6182"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jun 22 10:28:24.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6182" for this suite. 06/22/23 10:28:24.561
------------------------------
• [SLOW TEST] [24.185 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:28:00.388
    Jun 22 10:28:00.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename subpath 06/22/23 10:28:00.4
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:28:00.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:28:00.437
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/22/23 10:28:00.443
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-8vxh 06/22/23 10:28:00.458
    STEP: Creating a pod to test atomic-volume-subpath 06/22/23 10:28:00.459
    W0622 10:28:00.475124      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container-subpath-configmap-8vxh" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-subpath-configmap-8vxh" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-subpath-configmap-8vxh" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-subpath-configmap-8vxh" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:28:00.476: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8vxh" in namespace "subpath-6182" to be "Succeeded or Failed"
    Jun 22 10:28:00.483: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Pending", Reason="", readiness=false. Elapsed: 7.447333ms
    Jun 22 10:28:02.491: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 2.015483186s
    Jun 22 10:28:04.491: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 4.014823841s
    Jun 22 10:28:06.492: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 6.015973364s
    Jun 22 10:28:08.491: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 8.015429327s
    Jun 22 10:28:10.491: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 10.015493883s
    Jun 22 10:28:12.489: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 12.01369422s
    Jun 22 10:28:14.490: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 14.01410723s
    Jun 22 10:28:16.491: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 16.015062842s
    Jun 22 10:28:18.495: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 18.018930106s
    Jun 22 10:28:20.492: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=true. Elapsed: 20.016198656s
    Jun 22 10:28:22.491: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Running", Reason="", readiness=false. Elapsed: 22.015781959s
    Jun 22 10:28:24.490: INFO: Pod "pod-subpath-test-configmap-8vxh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014040367s
    STEP: Saw pod success 06/22/23 10:28:24.49
    Jun 22 10:28:24.491: INFO: Pod "pod-subpath-test-configmap-8vxh" satisfied condition "Succeeded or Failed"
    Jun 22 10:28:24.496: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-subpath-test-configmap-8vxh container test-container-subpath-configmap-8vxh: <nil>
    STEP: delete the pod 06/22/23 10:28:24.522
    Jun 22 10:28:24.541: INFO: Waiting for pod pod-subpath-test-configmap-8vxh to disappear
    Jun 22 10:28:24.545: INFO: Pod pod-subpath-test-configmap-8vxh no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-8vxh 06/22/23 10:28:24.545
    Jun 22 10:28:24.546: INFO: Deleting pod "pod-subpath-test-configmap-8vxh" in namespace "subpath-6182"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:28:24.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6182" for this suite. 06/22/23 10:28:24.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:28:24.576
Jun 22 10:28:24.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 10:28:24.579
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:28:24.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:28:24.599
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 06/22/23 10:28:24.605
W0622 10:28:24.618684      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:28:24.618: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eea8f523-1cf5-47f2-991a-1cf34bdee0da" in namespace "projected-9856" to be "Succeeded or Failed"
Jun 22 10:28:24.623: INFO: Pod "downwardapi-volume-eea8f523-1cf5-47f2-991a-1cf34bdee0da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.88576ms
Jun 22 10:28:26.631: INFO: Pod "downwardapi-volume-eea8f523-1cf5-47f2-991a-1cf34bdee0da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01201301s
Jun 22 10:28:28.630: INFO: Pod "downwardapi-volume-eea8f523-1cf5-47f2-991a-1cf34bdee0da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011903422s
STEP: Saw pod success 06/22/23 10:28:28.63
Jun 22 10:28:28.631: INFO: Pod "downwardapi-volume-eea8f523-1cf5-47f2-991a-1cf34bdee0da" satisfied condition "Succeeded or Failed"
Jun 22 10:28:28.636: INFO: Trying to get logs from node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 pod downwardapi-volume-eea8f523-1cf5-47f2-991a-1cf34bdee0da container client-container: <nil>
STEP: delete the pod 06/22/23 10:28:28.656
Jun 22 10:28:28.673: INFO: Waiting for pod downwardapi-volume-eea8f523-1cf5-47f2-991a-1cf34bdee0da to disappear
Jun 22 10:28:28.678: INFO: Pod downwardapi-volume-eea8f523-1cf5-47f2-991a-1cf34bdee0da no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun 22 10:28:28.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9856" for this suite. 06/22/23 10:28:28.688
------------------------------
• [4.121 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:28:24.576
    Jun 22 10:28:24.576: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 10:28:24.579
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:28:24.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:28:24.599
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 06/22/23 10:28:24.605
    W0622 10:28:24.618684      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:28:24.618: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eea8f523-1cf5-47f2-991a-1cf34bdee0da" in namespace "projected-9856" to be "Succeeded or Failed"
    Jun 22 10:28:24.623: INFO: Pod "downwardapi-volume-eea8f523-1cf5-47f2-991a-1cf34bdee0da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.88576ms
    Jun 22 10:28:26.631: INFO: Pod "downwardapi-volume-eea8f523-1cf5-47f2-991a-1cf34bdee0da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01201301s
    Jun 22 10:28:28.630: INFO: Pod "downwardapi-volume-eea8f523-1cf5-47f2-991a-1cf34bdee0da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011903422s
    STEP: Saw pod success 06/22/23 10:28:28.63
    Jun 22 10:28:28.631: INFO: Pod "downwardapi-volume-eea8f523-1cf5-47f2-991a-1cf34bdee0da" satisfied condition "Succeeded or Failed"
    Jun 22 10:28:28.636: INFO: Trying to get logs from node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 pod downwardapi-volume-eea8f523-1cf5-47f2-991a-1cf34bdee0da container client-container: <nil>
    STEP: delete the pod 06/22/23 10:28:28.656
    Jun 22 10:28:28.673: INFO: Waiting for pod downwardapi-volume-eea8f523-1cf5-47f2-991a-1cf34bdee0da to disappear
    Jun 22 10:28:28.678: INFO: Pod downwardapi-volume-eea8f523-1cf5-47f2-991a-1cf34bdee0da no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:28:28.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9856" for this suite. 06/22/23 10:28:28.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:28:28.7
Jun 22 10:28:28.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubectl 06/22/23 10:28:28.703
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:28:28.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:28:28.726
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 06/22/23 10:28:28.732
Jun 22 10:28:28.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 create -f -'
Jun 22 10:28:30.501: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"update-demo\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"update-demo\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"update-demo\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"update-demo\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Jun 22 10:28:30.501: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/22/23 10:28:30.501
Jun 22 10:28:30.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 10:28:30.597: INFO: stderr: ""
Jun 22 10:28:30.598: INFO: stdout: "update-demo-nautilus-k8rxn update-demo-nautilus-rsjjm "
Jun 22 10:28:30.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-k8rxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 10:28:30.685: INFO: stderr: ""
Jun 22 10:28:30.685: INFO: stdout: ""
Jun 22 10:28:30.685: INFO: update-demo-nautilus-k8rxn is created but not running
Jun 22 10:28:35.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 10:28:35.780: INFO: stderr: ""
Jun 22 10:28:35.780: INFO: stdout: "update-demo-nautilus-k8rxn update-demo-nautilus-rsjjm "
Jun 22 10:28:35.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-k8rxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 10:28:35.867: INFO: stderr: ""
Jun 22 10:28:35.867: INFO: stdout: "true"
Jun 22 10:28:35.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-k8rxn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 10:28:35.956: INFO: stderr: ""
Jun 22 10:28:35.956: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun 22 10:28:35.956: INFO: validating pod update-demo-nautilus-k8rxn
Jun 22 10:28:35.968: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 10:28:35.969: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 10:28:35.969: INFO: update-demo-nautilus-k8rxn is verified up and running
Jun 22 10:28:35.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-rsjjm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 10:28:36.058: INFO: stderr: ""
Jun 22 10:28:36.058: INFO: stdout: "true"
Jun 22 10:28:36.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-rsjjm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 10:28:36.145: INFO: stderr: ""
Jun 22 10:28:36.145: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun 22 10:28:36.145: INFO: validating pod update-demo-nautilus-rsjjm
Jun 22 10:28:36.157: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 10:28:36.157: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 10:28:36.157: INFO: update-demo-nautilus-rsjjm is verified up and running
STEP: scaling down the replication controller 06/22/23 10:28:36.157
Jun 22 10:28:36.162: INFO: scanned /root for discovery docs: <nil>
Jun 22 10:28:36.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jun 22 10:28:37.276: INFO: stderr: ""
Jun 22 10:28:37.276: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/22/23 10:28:37.277
Jun 22 10:28:37.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 10:28:37.375: INFO: stderr: ""
Jun 22 10:28:37.375: INFO: stdout: "update-demo-nautilus-k8rxn update-demo-nautilus-rsjjm "
STEP: Replicas for name=update-demo: expected=1 actual=2 06/22/23 10:28:37.375
Jun 22 10:28:42.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 10:28:42.553: INFO: stderr: ""
Jun 22 10:28:42.553: INFO: stdout: "update-demo-nautilus-k8rxn "
Jun 22 10:28:42.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-k8rxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 10:28:42.642: INFO: stderr: ""
Jun 22 10:28:42.642: INFO: stdout: "true"
Jun 22 10:28:42.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-k8rxn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 10:28:42.734: INFO: stderr: ""
Jun 22 10:28:42.734: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun 22 10:28:42.734: INFO: validating pod update-demo-nautilus-k8rxn
Jun 22 10:28:42.741: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 10:28:42.741: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 10:28:42.741: INFO: update-demo-nautilus-k8rxn is verified up and running
STEP: scaling up the replication controller 06/22/23 10:28:42.741
Jun 22 10:28:42.745: INFO: scanned /root for discovery docs: <nil>
Jun 22 10:28:42.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jun 22 10:28:43.874: INFO: stderr: ""
Jun 22 10:28:43.874: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/22/23 10:28:43.874
Jun 22 10:28:43.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 10:28:43.966: INFO: stderr: ""
Jun 22 10:28:43.966: INFO: stdout: "update-demo-nautilus-k8rxn update-demo-nautilus-tht49 "
Jun 22 10:28:43.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-k8rxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 10:28:44.056: INFO: stderr: ""
Jun 22 10:28:44.056: INFO: stdout: "true"
Jun 22 10:28:44.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-k8rxn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 10:28:44.144: INFO: stderr: ""
Jun 22 10:28:44.144: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun 22 10:28:44.144: INFO: validating pod update-demo-nautilus-k8rxn
Jun 22 10:28:44.151: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 10:28:44.151: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 10:28:44.151: INFO: update-demo-nautilus-k8rxn is verified up and running
Jun 22 10:28:44.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-tht49 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 10:28:44.240: INFO: stderr: ""
Jun 22 10:28:44.240: INFO: stdout: "true"
Jun 22 10:28:44.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-tht49 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 10:28:44.338: INFO: stderr: ""
Jun 22 10:28:44.338: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun 22 10:28:44.338: INFO: validating pod update-demo-nautilus-tht49
Jun 22 10:28:44.349: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 10:28:44.349: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 10:28:44.349: INFO: update-demo-nautilus-tht49 is verified up and running
STEP: using delete to clean up resources 06/22/23 10:28:44.349
Jun 22 10:28:44.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 delete --grace-period=0 --force -f -'
Jun 22 10:28:44.449: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 10:28:44.450: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 22 10:28:44.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get rc,svc -l name=update-demo --no-headers'
Jun 22 10:28:44.545: INFO: stderr: "No resources found in kubectl-4726 namespace.\n"
Jun 22 10:28:44.545: INFO: stdout: ""
Jun 22 10:28:44.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 10:28:44.637: INFO: stderr: ""
Jun 22 10:28:44.637: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun 22 10:28:44.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4726" for this suite. 06/22/23 10:28:44.648
------------------------------
• [SLOW TEST] [15.958 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:28:28.7
    Jun 22 10:28:28.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubectl 06/22/23 10:28:28.703
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:28:28.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:28:28.726
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 06/22/23 10:28:28.732
    Jun 22 10:28:28.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 create -f -'
    Jun 22 10:28:30.501: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"update-demo\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"update-demo\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"update-demo\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"update-demo\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
    Jun 22 10:28:30.501: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/22/23 10:28:30.501
    Jun 22 10:28:30.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 22 10:28:30.597: INFO: stderr: ""
    Jun 22 10:28:30.598: INFO: stdout: "update-demo-nautilus-k8rxn update-demo-nautilus-rsjjm "
    Jun 22 10:28:30.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-k8rxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 22 10:28:30.685: INFO: stderr: ""
    Jun 22 10:28:30.685: INFO: stdout: ""
    Jun 22 10:28:30.685: INFO: update-demo-nautilus-k8rxn is created but not running
    Jun 22 10:28:35.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 22 10:28:35.780: INFO: stderr: ""
    Jun 22 10:28:35.780: INFO: stdout: "update-demo-nautilus-k8rxn update-demo-nautilus-rsjjm "
    Jun 22 10:28:35.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-k8rxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 22 10:28:35.867: INFO: stderr: ""
    Jun 22 10:28:35.867: INFO: stdout: "true"
    Jun 22 10:28:35.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-k8rxn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 22 10:28:35.956: INFO: stderr: ""
    Jun 22 10:28:35.956: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun 22 10:28:35.956: INFO: validating pod update-demo-nautilus-k8rxn
    Jun 22 10:28:35.968: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 22 10:28:35.969: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 22 10:28:35.969: INFO: update-demo-nautilus-k8rxn is verified up and running
    Jun 22 10:28:35.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-rsjjm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 22 10:28:36.058: INFO: stderr: ""
    Jun 22 10:28:36.058: INFO: stdout: "true"
    Jun 22 10:28:36.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-rsjjm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 22 10:28:36.145: INFO: stderr: ""
    Jun 22 10:28:36.145: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun 22 10:28:36.145: INFO: validating pod update-demo-nautilus-rsjjm
    Jun 22 10:28:36.157: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 22 10:28:36.157: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 22 10:28:36.157: INFO: update-demo-nautilus-rsjjm is verified up and running
    STEP: scaling down the replication controller 06/22/23 10:28:36.157
    Jun 22 10:28:36.162: INFO: scanned /root for discovery docs: <nil>
    Jun 22 10:28:36.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jun 22 10:28:37.276: INFO: stderr: ""
    Jun 22 10:28:37.276: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/22/23 10:28:37.277
    Jun 22 10:28:37.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 22 10:28:37.375: INFO: stderr: ""
    Jun 22 10:28:37.375: INFO: stdout: "update-demo-nautilus-k8rxn update-demo-nautilus-rsjjm "
    STEP: Replicas for name=update-demo: expected=1 actual=2 06/22/23 10:28:37.375
    Jun 22 10:28:42.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 22 10:28:42.553: INFO: stderr: ""
    Jun 22 10:28:42.553: INFO: stdout: "update-demo-nautilus-k8rxn "
    Jun 22 10:28:42.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-k8rxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 22 10:28:42.642: INFO: stderr: ""
    Jun 22 10:28:42.642: INFO: stdout: "true"
    Jun 22 10:28:42.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-k8rxn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 22 10:28:42.734: INFO: stderr: ""
    Jun 22 10:28:42.734: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun 22 10:28:42.734: INFO: validating pod update-demo-nautilus-k8rxn
    Jun 22 10:28:42.741: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 22 10:28:42.741: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 22 10:28:42.741: INFO: update-demo-nautilus-k8rxn is verified up and running
    STEP: scaling up the replication controller 06/22/23 10:28:42.741
    Jun 22 10:28:42.745: INFO: scanned /root for discovery docs: <nil>
    Jun 22 10:28:42.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jun 22 10:28:43.874: INFO: stderr: ""
    Jun 22 10:28:43.874: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/22/23 10:28:43.874
    Jun 22 10:28:43.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 22 10:28:43.966: INFO: stderr: ""
    Jun 22 10:28:43.966: INFO: stdout: "update-demo-nautilus-k8rxn update-demo-nautilus-tht49 "
    Jun 22 10:28:43.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-k8rxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 22 10:28:44.056: INFO: stderr: ""
    Jun 22 10:28:44.056: INFO: stdout: "true"
    Jun 22 10:28:44.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-k8rxn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 22 10:28:44.144: INFO: stderr: ""
    Jun 22 10:28:44.144: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun 22 10:28:44.144: INFO: validating pod update-demo-nautilus-k8rxn
    Jun 22 10:28:44.151: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 22 10:28:44.151: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 22 10:28:44.151: INFO: update-demo-nautilus-k8rxn is verified up and running
    Jun 22 10:28:44.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-tht49 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 22 10:28:44.240: INFO: stderr: ""
    Jun 22 10:28:44.240: INFO: stdout: "true"
    Jun 22 10:28:44.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods update-demo-nautilus-tht49 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 22 10:28:44.338: INFO: stderr: ""
    Jun 22 10:28:44.338: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun 22 10:28:44.338: INFO: validating pod update-demo-nautilus-tht49
    Jun 22 10:28:44.349: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 22 10:28:44.349: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 22 10:28:44.349: INFO: update-demo-nautilus-tht49 is verified up and running
    STEP: using delete to clean up resources 06/22/23 10:28:44.349
    Jun 22 10:28:44.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 delete --grace-period=0 --force -f -'
    Jun 22 10:28:44.449: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 22 10:28:44.450: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jun 22 10:28:44.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get rc,svc -l name=update-demo --no-headers'
    Jun 22 10:28:44.545: INFO: stderr: "No resources found in kubectl-4726 namespace.\n"
    Jun 22 10:28:44.545: INFO: stdout: ""
    Jun 22 10:28:44.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-4726 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jun 22 10:28:44.637: INFO: stderr: ""
    Jun 22 10:28:44.637: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:28:44.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4726" for this suite. 06/22/23 10:28:44.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:28:44.665
Jun 22 10:28:44.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename namespaces 06/22/23 10:28:44.667
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:28:44.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:28:44.688
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 06/22/23 10:28:44.692
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:28:44.713
STEP: Creating a pod in the namespace 06/22/23 10:28:44.717
W0622 10:28:44.727378      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting for the pod to have running status 06/22/23 10:28:44.727
Jun 22 10:28:44.727: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9756" to be "running"
Jun 22 10:28:44.733: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.882093ms
Jun 22 10:28:46.741: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013556088s
Jun 22 10:28:46.741: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 06/22/23 10:28:46.741
STEP: Waiting for the namespace to be removed. 06/22/23 10:28:46.75
STEP: Recreating the namespace 06/22/23 10:28:57.757
STEP: Verifying there are no pods in the namespace 06/22/23 10:28:57.774
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:28:57.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9705" for this suite. 06/22/23 10:28:57.785
STEP: Destroying namespace "nsdeletetest-9756" for this suite. 06/22/23 10:28:57.796
Jun 22 10:28:57.801: INFO: Namespace nsdeletetest-9756 was already deleted
STEP: Destroying namespace "nsdeletetest-3251" for this suite. 06/22/23 10:28:57.801
------------------------------
• [SLOW TEST] [13.144 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:28:44.665
    Jun 22 10:28:44.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename namespaces 06/22/23 10:28:44.667
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:28:44.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:28:44.688
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 06/22/23 10:28:44.692
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:28:44.713
    STEP: Creating a pod in the namespace 06/22/23 10:28:44.717
    W0622 10:28:44.727378      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Waiting for the pod to have running status 06/22/23 10:28:44.727
    Jun 22 10:28:44.727: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9756" to be "running"
    Jun 22 10:28:44.733: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.882093ms
    Jun 22 10:28:46.741: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013556088s
    Jun 22 10:28:46.741: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 06/22/23 10:28:46.741
    STEP: Waiting for the namespace to be removed. 06/22/23 10:28:46.75
    STEP: Recreating the namespace 06/22/23 10:28:57.757
    STEP: Verifying there are no pods in the namespace 06/22/23 10:28:57.774
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:28:57.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9705" for this suite. 06/22/23 10:28:57.785
    STEP: Destroying namespace "nsdeletetest-9756" for this suite. 06/22/23 10:28:57.796
    Jun 22 10:28:57.801: INFO: Namespace nsdeletetest-9756 was already deleted
    STEP: Destroying namespace "nsdeletetest-3251" for this suite. 06/22/23 10:28:57.801
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:28:57.81
Jun 22 10:28:57.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename custom-resource-definition 06/22/23 10:28:57.812
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:28:57.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:28:57.919
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 06/22/23 10:28:57.923
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 06/22/23 10:28:57.925
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 06/22/23 10:28:57.925
STEP: fetching the /apis/apiextensions.k8s.io discovery document 06/22/23 10:28:57.926
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 06/22/23 10:28:57.927
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 06/22/23 10:28:57.927
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 06/22/23 10:28:57.929
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:28:57.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-4457" for this suite. 06/22/23 10:28:57.936
------------------------------
• [0.135 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:28:57.81
    Jun 22 10:28:57.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename custom-resource-definition 06/22/23 10:28:57.812
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:28:57.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:28:57.919
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 06/22/23 10:28:57.923
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 06/22/23 10:28:57.925
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 06/22/23 10:28:57.925
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 06/22/23 10:28:57.926
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 06/22/23 10:28:57.927
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 06/22/23 10:28:57.927
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 06/22/23 10:28:57.929
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:28:57.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-4457" for this suite. 06/22/23 10:28:57.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:28:57.946
Jun 22 10:28:57.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename secrets 06/22/23 10:28:57.947
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:28:57.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:28:57.967
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun 22 10:28:58.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8358" for this suite. 06/22/23 10:28:58.024
------------------------------
• [0.085 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:28:57.946
    Jun 22 10:28:57.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename secrets 06/22/23 10:28:57.947
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:28:57.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:28:57.967
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:28:58.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8358" for this suite. 06/22/23 10:28:58.024
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:28:58.031
Jun 22 10:28:58.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename services 06/22/23 10:28:58.033
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:28:58.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:28:58.051
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 06/22/23 10:28:58.055
Jun 22 10:28:58.055: INFO: Creating e2e-svc-a-rtrkg
Jun 22 10:28:58.079: INFO: Creating e2e-svc-b-d6xgp
Jun 22 10:28:58.098: INFO: Creating e2e-svc-c-b566c
STEP: deleting service collection 06/22/23 10:28:58.121
Jun 22 10:28:58.184: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun 22 10:28:58.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6200" for this suite. 06/22/23 10:28:58.197
------------------------------
• [0.180 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:28:58.031
    Jun 22 10:28:58.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename services 06/22/23 10:28:58.033
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:28:58.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:28:58.051
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 06/22/23 10:28:58.055
    Jun 22 10:28:58.055: INFO: Creating e2e-svc-a-rtrkg
    Jun 22 10:28:58.079: INFO: Creating e2e-svc-b-d6xgp
    Jun 22 10:28:58.098: INFO: Creating e2e-svc-c-b566c
    STEP: deleting service collection 06/22/23 10:28:58.121
    Jun 22 10:28:58.184: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:28:58.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6200" for this suite. 06/22/23 10:28:58.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:28:58.213
Jun 22 10:28:58.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename replication-controller 06/22/23 10:28:58.215
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:28:58.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:28:58.237
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 06/22/23 10:28:58.24
W0622 10:28:58.247014      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: When the matched label of one of its pods change 06/22/23 10:28:58.247
Jun 22 10:28:58.251: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 22 10:29:03.257: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 06/22/23 10:29:03.274
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jun 22 10:29:03.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6914" for this suite. 06/22/23 10:29:03.302
------------------------------
• [SLOW TEST] [5.100 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:28:58.213
    Jun 22 10:28:58.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename replication-controller 06/22/23 10:28:58.215
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:28:58.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:28:58.237
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 06/22/23 10:28:58.24
    W0622 10:28:58.247014      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: When the matched label of one of its pods change 06/22/23 10:28:58.247
    Jun 22 10:28:58.251: INFO: Pod name pod-release: Found 0 pods out of 1
    Jun 22 10:29:03.257: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 06/22/23 10:29:03.274
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:29:03.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6914" for this suite. 06/22/23 10:29:03.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:29:03.319
Jun 22 10:29:03.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename configmap 06/22/23 10:29:03.32
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:03.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:03.348
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-da0764f2-adf0-4972-bb4d-9e3fbe2d93ae 06/22/23 10:29:03.362
STEP: Creating the pod 06/22/23 10:29:03.368
W0622 10:29:03.380155      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:29:03.380: INFO: Waiting up to 5m0s for pod "pod-configmaps-87b4cf04-d2ef-494e-89a2-a7879281549e" in namespace "configmap-7524" to be "running and ready"
Jun 22 10:29:03.394: INFO: Pod "pod-configmaps-87b4cf04-d2ef-494e-89a2-a7879281549e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.994251ms
Jun 22 10:29:03.394: INFO: The phase of Pod pod-configmaps-87b4cf04-d2ef-494e-89a2-a7879281549e is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:29:05.400: INFO: Pod "pod-configmaps-87b4cf04-d2ef-494e-89a2-a7879281549e": Phase="Running", Reason="", readiness=true. Elapsed: 2.020415337s
Jun 22 10:29:05.400: INFO: The phase of Pod pod-configmaps-87b4cf04-d2ef-494e-89a2-a7879281549e is Running (Ready = true)
Jun 22 10:29:05.400: INFO: Pod "pod-configmaps-87b4cf04-d2ef-494e-89a2-a7879281549e" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-da0764f2-adf0-4972-bb4d-9e3fbe2d93ae 06/22/23 10:29:05.415
STEP: waiting to observe update in volume 06/22/23 10:29:05.422
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun 22 10:29:07.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7524" for this suite. 06/22/23 10:29:07.45
------------------------------
• [4.140 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:29:03.319
    Jun 22 10:29:03.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename configmap 06/22/23 10:29:03.32
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:03.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:03.348
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-da0764f2-adf0-4972-bb4d-9e3fbe2d93ae 06/22/23 10:29:03.362
    STEP: Creating the pod 06/22/23 10:29:03.368
    W0622 10:29:03.380155      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:29:03.380: INFO: Waiting up to 5m0s for pod "pod-configmaps-87b4cf04-d2ef-494e-89a2-a7879281549e" in namespace "configmap-7524" to be "running and ready"
    Jun 22 10:29:03.394: INFO: Pod "pod-configmaps-87b4cf04-d2ef-494e-89a2-a7879281549e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.994251ms
    Jun 22 10:29:03.394: INFO: The phase of Pod pod-configmaps-87b4cf04-d2ef-494e-89a2-a7879281549e is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:29:05.400: INFO: Pod "pod-configmaps-87b4cf04-d2ef-494e-89a2-a7879281549e": Phase="Running", Reason="", readiness=true. Elapsed: 2.020415337s
    Jun 22 10:29:05.400: INFO: The phase of Pod pod-configmaps-87b4cf04-d2ef-494e-89a2-a7879281549e is Running (Ready = true)
    Jun 22 10:29:05.400: INFO: Pod "pod-configmaps-87b4cf04-d2ef-494e-89a2-a7879281549e" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-da0764f2-adf0-4972-bb4d-9e3fbe2d93ae 06/22/23 10:29:05.415
    STEP: waiting to observe update in volume 06/22/23 10:29:05.422
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:29:07.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7524" for this suite. 06/22/23 10:29:07.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:29:07.462
Jun 22 10:29:07.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename secrets 06/22/23 10:29:07.463
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:07.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:07.485
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-9609/secret-test-11bba964-247e-4043-9eda-8b1a960233a5 06/22/23 10:29:07.489
STEP: Creating a pod to test consume secrets 06/22/23 10:29:07.495
W0622 10:29:07.517939      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "env-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "env-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "env-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "env-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:29:07.518: INFO: Waiting up to 5m0s for pod "pod-configmaps-7cc18d19-befb-48db-a50e-8716b89ea66b" in namespace "secrets-9609" to be "Succeeded or Failed"
Jun 22 10:29:07.526: INFO: Pod "pod-configmaps-7cc18d19-befb-48db-a50e-8716b89ea66b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.702887ms
Jun 22 10:29:09.534: INFO: Pod "pod-configmaps-7cc18d19-befb-48db-a50e-8716b89ea66b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016411168s
Jun 22 10:29:11.535: INFO: Pod "pod-configmaps-7cc18d19-befb-48db-a50e-8716b89ea66b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017219905s
STEP: Saw pod success 06/22/23 10:29:11.535
Jun 22 10:29:11.535: INFO: Pod "pod-configmaps-7cc18d19-befb-48db-a50e-8716b89ea66b" satisfied condition "Succeeded or Failed"
Jun 22 10:29:11.540: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-configmaps-7cc18d19-befb-48db-a50e-8716b89ea66b container env-test: <nil>
STEP: delete the pod 06/22/23 10:29:11.551
Jun 22 10:29:11.572: INFO: Waiting for pod pod-configmaps-7cc18d19-befb-48db-a50e-8716b89ea66b to disappear
Jun 22 10:29:11.577: INFO: Pod pod-configmaps-7cc18d19-befb-48db-a50e-8716b89ea66b no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jun 22 10:29:11.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9609" for this suite. 06/22/23 10:29:11.585
------------------------------
• [4.134 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:29:07.462
    Jun 22 10:29:07.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename secrets 06/22/23 10:29:07.463
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:07.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:07.485
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-9609/secret-test-11bba964-247e-4043-9eda-8b1a960233a5 06/22/23 10:29:07.489
    STEP: Creating a pod to test consume secrets 06/22/23 10:29:07.495
    W0622 10:29:07.517939      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "env-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "env-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "env-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "env-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:29:07.518: INFO: Waiting up to 5m0s for pod "pod-configmaps-7cc18d19-befb-48db-a50e-8716b89ea66b" in namespace "secrets-9609" to be "Succeeded or Failed"
    Jun 22 10:29:07.526: INFO: Pod "pod-configmaps-7cc18d19-befb-48db-a50e-8716b89ea66b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.702887ms
    Jun 22 10:29:09.534: INFO: Pod "pod-configmaps-7cc18d19-befb-48db-a50e-8716b89ea66b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016411168s
    Jun 22 10:29:11.535: INFO: Pod "pod-configmaps-7cc18d19-befb-48db-a50e-8716b89ea66b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017219905s
    STEP: Saw pod success 06/22/23 10:29:11.535
    Jun 22 10:29:11.535: INFO: Pod "pod-configmaps-7cc18d19-befb-48db-a50e-8716b89ea66b" satisfied condition "Succeeded or Failed"
    Jun 22 10:29:11.540: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-configmaps-7cc18d19-befb-48db-a50e-8716b89ea66b container env-test: <nil>
    STEP: delete the pod 06/22/23 10:29:11.551
    Jun 22 10:29:11.572: INFO: Waiting for pod pod-configmaps-7cc18d19-befb-48db-a50e-8716b89ea66b to disappear
    Jun 22 10:29:11.577: INFO: Pod pod-configmaps-7cc18d19-befb-48db-a50e-8716b89ea66b no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:29:11.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9609" for this suite. 06/22/23 10:29:11.585
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:29:11.596
Jun 22 10:29:11.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename tables 06/22/23 10:29:11.598
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:11.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:11.617
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Jun 22 10:29:11.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-4897" for this suite. 06/22/23 10:29:11.633
------------------------------
• [0.045 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:29:11.596
    Jun 22 10:29:11.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename tables 06/22/23 10:29:11.598
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:11.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:11.617
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:29:11.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-4897" for this suite. 06/22/23 10:29:11.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:29:11.643
Jun 22 10:29:11.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename downward-api 06/22/23 10:29:11.644
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:11.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:11.665
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 06/22/23 10:29:11.668
W0622 10:29:11.680339      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:29:11.680: INFO: Waiting up to 5m0s for pod "downward-api-6c31f9a1-b4d3-4d17-8c07-ef6e97280a59" in namespace "downward-api-8678" to be "Succeeded or Failed"
Jun 22 10:29:11.684: INFO: Pod "downward-api-6c31f9a1-b4d3-4d17-8c07-ef6e97280a59": Phase="Pending", Reason="", readiness=false. Elapsed: 4.388975ms
Jun 22 10:29:13.692: INFO: Pod "downward-api-6c31f9a1-b4d3-4d17-8c07-ef6e97280a59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012009195s
Jun 22 10:29:15.691: INFO: Pod "downward-api-6c31f9a1-b4d3-4d17-8c07-ef6e97280a59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010893365s
STEP: Saw pod success 06/22/23 10:29:15.691
Jun 22 10:29:15.691: INFO: Pod "downward-api-6c31f9a1-b4d3-4d17-8c07-ef6e97280a59" satisfied condition "Succeeded or Failed"
Jun 22 10:29:15.696: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downward-api-6c31f9a1-b4d3-4d17-8c07-ef6e97280a59 container dapi-container: <nil>
STEP: delete the pod 06/22/23 10:29:15.707
Jun 22 10:29:15.725: INFO: Waiting for pod downward-api-6c31f9a1-b4d3-4d17-8c07-ef6e97280a59 to disappear
Jun 22 10:29:15.728: INFO: Pod downward-api-6c31f9a1-b4d3-4d17-8c07-ef6e97280a59 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jun 22 10:29:15.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8678" for this suite. 06/22/23 10:29:15.737
------------------------------
• [4.102 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:29:11.643
    Jun 22 10:29:11.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename downward-api 06/22/23 10:29:11.644
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:11.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:11.665
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 06/22/23 10:29:11.668
    W0622 10:29:11.680339      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:29:11.680: INFO: Waiting up to 5m0s for pod "downward-api-6c31f9a1-b4d3-4d17-8c07-ef6e97280a59" in namespace "downward-api-8678" to be "Succeeded or Failed"
    Jun 22 10:29:11.684: INFO: Pod "downward-api-6c31f9a1-b4d3-4d17-8c07-ef6e97280a59": Phase="Pending", Reason="", readiness=false. Elapsed: 4.388975ms
    Jun 22 10:29:13.692: INFO: Pod "downward-api-6c31f9a1-b4d3-4d17-8c07-ef6e97280a59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012009195s
    Jun 22 10:29:15.691: INFO: Pod "downward-api-6c31f9a1-b4d3-4d17-8c07-ef6e97280a59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010893365s
    STEP: Saw pod success 06/22/23 10:29:15.691
    Jun 22 10:29:15.691: INFO: Pod "downward-api-6c31f9a1-b4d3-4d17-8c07-ef6e97280a59" satisfied condition "Succeeded or Failed"
    Jun 22 10:29:15.696: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downward-api-6c31f9a1-b4d3-4d17-8c07-ef6e97280a59 container dapi-container: <nil>
    STEP: delete the pod 06/22/23 10:29:15.707
    Jun 22 10:29:15.725: INFO: Waiting for pod downward-api-6c31f9a1-b4d3-4d17-8c07-ef6e97280a59 to disappear
    Jun 22 10:29:15.728: INFO: Pod downward-api-6c31f9a1-b4d3-4d17-8c07-ef6e97280a59 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:29:15.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8678" for this suite. 06/22/23 10:29:15.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:29:15.749
Jun 22 10:29:15.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename emptydir 06/22/23 10:29:15.751
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:15.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:15.772
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 06/22/23 10:29:15.776
W0622 10:29:15.788664      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:29:15.788: INFO: Waiting up to 5m0s for pod "pod-c435d8d4-391b-4365-919e-7a41e1cd21e6" in namespace "emptydir-5946" to be "Succeeded or Failed"
Jun 22 10:29:15.795: INFO: Pod "pod-c435d8d4-391b-4365-919e-7a41e1cd21e6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.930984ms
Jun 22 10:29:17.803: INFO: Pod "pod-c435d8d4-391b-4365-919e-7a41e1cd21e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015048495s
Jun 22 10:29:19.802: INFO: Pod "pod-c435d8d4-391b-4365-919e-7a41e1cd21e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013487484s
STEP: Saw pod success 06/22/23 10:29:19.802
Jun 22 10:29:19.802: INFO: Pod "pod-c435d8d4-391b-4365-919e-7a41e1cd21e6" satisfied condition "Succeeded or Failed"
Jun 22 10:29:19.807: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-c435d8d4-391b-4365-919e-7a41e1cd21e6 container test-container: <nil>
STEP: delete the pod 06/22/23 10:29:19.816
Jun 22 10:29:19.830: INFO: Waiting for pod pod-c435d8d4-391b-4365-919e-7a41e1cd21e6 to disappear
Jun 22 10:29:19.835: INFO: Pod pod-c435d8d4-391b-4365-919e-7a41e1cd21e6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun 22 10:29:19.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5946" for this suite. 06/22/23 10:29:19.844
------------------------------
• [4.103 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:29:15.749
    Jun 22 10:29:15.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename emptydir 06/22/23 10:29:15.751
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:15.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:15.772
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 06/22/23 10:29:15.776
    W0622 10:29:15.788664      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:29:15.788: INFO: Waiting up to 5m0s for pod "pod-c435d8d4-391b-4365-919e-7a41e1cd21e6" in namespace "emptydir-5946" to be "Succeeded or Failed"
    Jun 22 10:29:15.795: INFO: Pod "pod-c435d8d4-391b-4365-919e-7a41e1cd21e6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.930984ms
    Jun 22 10:29:17.803: INFO: Pod "pod-c435d8d4-391b-4365-919e-7a41e1cd21e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015048495s
    Jun 22 10:29:19.802: INFO: Pod "pod-c435d8d4-391b-4365-919e-7a41e1cd21e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013487484s
    STEP: Saw pod success 06/22/23 10:29:19.802
    Jun 22 10:29:19.802: INFO: Pod "pod-c435d8d4-391b-4365-919e-7a41e1cd21e6" satisfied condition "Succeeded or Failed"
    Jun 22 10:29:19.807: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-c435d8d4-391b-4365-919e-7a41e1cd21e6 container test-container: <nil>
    STEP: delete the pod 06/22/23 10:29:19.816
    Jun 22 10:29:19.830: INFO: Waiting for pod pod-c435d8d4-391b-4365-919e-7a41e1cd21e6 to disappear
    Jun 22 10:29:19.835: INFO: Pod pod-c435d8d4-391b-4365-919e-7a41e1cd21e6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:29:19.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5946" for this suite. 06/22/23 10:29:19.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:29:19.855
Jun 22 10:29:19.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename secrets 06/22/23 10:29:19.856
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:19.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:19.876
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-5cc4c283-9690-458e-b446-a14abbc0ebca 06/22/23 10:29:19.88
STEP: Creating a pod to test consume secrets 06/22/23 10:29:19.885
W0622 10:29:19.894948      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-env-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-env-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-env-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-env-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:29:19.895: INFO: Waiting up to 5m0s for pod "pod-secrets-d9651b98-cdd6-4b8b-b451-37cdfbc0ce23" in namespace "secrets-8316" to be "Succeeded or Failed"
Jun 22 10:29:19.898: INFO: Pod "pod-secrets-d9651b98-cdd6-4b8b-b451-37cdfbc0ce23": Phase="Pending", Reason="", readiness=false. Elapsed: 3.733947ms
Jun 22 10:29:21.905: INFO: Pod "pod-secrets-d9651b98-cdd6-4b8b-b451-37cdfbc0ce23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010677618s
Jun 22 10:29:23.905: INFO: Pod "pod-secrets-d9651b98-cdd6-4b8b-b451-37cdfbc0ce23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01004491s
STEP: Saw pod success 06/22/23 10:29:23.905
Jun 22 10:29:23.905: INFO: Pod "pod-secrets-d9651b98-cdd6-4b8b-b451-37cdfbc0ce23" satisfied condition "Succeeded or Failed"
Jun 22 10:29:23.909: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-secrets-d9651b98-cdd6-4b8b-b451-37cdfbc0ce23 container secret-env-test: <nil>
STEP: delete the pod 06/22/23 10:29:23.919
Jun 22 10:29:23.931: INFO: Waiting for pod pod-secrets-d9651b98-cdd6-4b8b-b451-37cdfbc0ce23 to disappear
Jun 22 10:29:23.935: INFO: Pod pod-secrets-d9651b98-cdd6-4b8b-b451-37cdfbc0ce23 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jun 22 10:29:23.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8316" for this suite. 06/22/23 10:29:23.944
------------------------------
• [4.097 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:29:19.855
    Jun 22 10:29:19.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename secrets 06/22/23 10:29:19.856
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:19.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:19.876
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-5cc4c283-9690-458e-b446-a14abbc0ebca 06/22/23 10:29:19.88
    STEP: Creating a pod to test consume secrets 06/22/23 10:29:19.885
    W0622 10:29:19.894948      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-env-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-env-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-env-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-env-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:29:19.895: INFO: Waiting up to 5m0s for pod "pod-secrets-d9651b98-cdd6-4b8b-b451-37cdfbc0ce23" in namespace "secrets-8316" to be "Succeeded or Failed"
    Jun 22 10:29:19.898: INFO: Pod "pod-secrets-d9651b98-cdd6-4b8b-b451-37cdfbc0ce23": Phase="Pending", Reason="", readiness=false. Elapsed: 3.733947ms
    Jun 22 10:29:21.905: INFO: Pod "pod-secrets-d9651b98-cdd6-4b8b-b451-37cdfbc0ce23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010677618s
    Jun 22 10:29:23.905: INFO: Pod "pod-secrets-d9651b98-cdd6-4b8b-b451-37cdfbc0ce23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01004491s
    STEP: Saw pod success 06/22/23 10:29:23.905
    Jun 22 10:29:23.905: INFO: Pod "pod-secrets-d9651b98-cdd6-4b8b-b451-37cdfbc0ce23" satisfied condition "Succeeded or Failed"
    Jun 22 10:29:23.909: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-secrets-d9651b98-cdd6-4b8b-b451-37cdfbc0ce23 container secret-env-test: <nil>
    STEP: delete the pod 06/22/23 10:29:23.919
    Jun 22 10:29:23.931: INFO: Waiting for pod pod-secrets-d9651b98-cdd6-4b8b-b451-37cdfbc0ce23 to disappear
    Jun 22 10:29:23.935: INFO: Pod pod-secrets-d9651b98-cdd6-4b8b-b451-37cdfbc0ce23 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:29:23.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8316" for this suite. 06/22/23 10:29:23.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:29:23.955
Jun 22 10:29:23.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename namespaces 06/22/23 10:29:23.956
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:23.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:23.978
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-1880" 06/22/23 10:29:23.982
Jun 22 10:29:23.992: INFO: Namespace "namespaces-1880" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"e5d41edb-1c5f-49df-95d4-4a50c9c0e5cf", "kubernetes.io/metadata.name":"namespaces-1880", "namespaces-1880":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:29:23.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1880" for this suite. 06/22/23 10:29:24
------------------------------
• [0.054 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:29:23.955
    Jun 22 10:29:23.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename namespaces 06/22/23 10:29:23.956
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:23.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:23.978
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-1880" 06/22/23 10:29:23.982
    Jun 22 10:29:23.992: INFO: Namespace "namespaces-1880" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"e5d41edb-1c5f-49df-95d4-4a50c9c0e5cf", "kubernetes.io/metadata.name":"namespaces-1880", "namespaces-1880":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:29:23.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1880" for this suite. 06/22/23 10:29:24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:29:24.013
Jun 22 10:29:24.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename daemonsets 06/22/23 10:29:24.014
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:24.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:24.034
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 06/22/23 10:29:24.062
W0622 10:29:24.071547      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Check that daemon pods launch on every node of the cluster. 06/22/23 10:29:24.071
Jun 22 10:29:24.085: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:29:24.085: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:29:24.085: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:29:24.090: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 10:29:24.091: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
Jun 22 10:29:25.098: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:29:25.098: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:29:25.098: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:29:25.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 22 10:29:25.103: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
Jun 22 10:29:26.098: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:29:26.098: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:29:26.098: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:29:26.104: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 22 10:29:26.104: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
Jun 22 10:29:27.099: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:29:27.099: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:29:27.099: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:29:27.104: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 22 10:29:27.104: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 06/22/23 10:29:27.108
Jun 22 10:29:27.132: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:29:27.132: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:29:27.132: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:29:27.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 22 10:29:27.138: INFO: Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw is running 0 daemon pod, expected 1
Jun 22 10:29:28.148: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:29:28.148: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:29:28.148: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:29:28.153: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 22 10:29:28.153: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 06/22/23 10:29:28.153
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 06/22/23 10:29:28.163
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9377, will wait for the garbage collector to delete the pods 06/22/23 10:29:28.164
Jun 22 10:29:28.229: INFO: Deleting DaemonSet.extensions daemon-set took: 10.028188ms
Jun 22 10:29:28.330: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.857676ms
Jun 22 10:29:31.036: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 10:29:31.036: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 22 10:29:31.045: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"129863"},"items":null}

Jun 22 10:29:31.049: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"129863"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:29:31.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9377" for this suite. 06/22/23 10:29:31.075
------------------------------
• [SLOW TEST] [7.073 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:29:24.013
    Jun 22 10:29:24.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename daemonsets 06/22/23 10:29:24.014
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:24.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:24.034
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 06/22/23 10:29:24.062
    W0622 10:29:24.071547      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Check that daemon pods launch on every node of the cluster. 06/22/23 10:29:24.071
    Jun 22 10:29:24.085: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:29:24.085: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:29:24.085: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:29:24.090: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 10:29:24.091: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
    Jun 22 10:29:25.098: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:29:25.098: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:29:25.098: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:29:25.103: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 22 10:29:25.103: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
    Jun 22 10:29:26.098: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:29:26.098: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:29:26.098: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:29:26.104: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 22 10:29:26.104: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
    Jun 22 10:29:27.099: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:29:27.099: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:29:27.099: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:29:27.104: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 22 10:29:27.104: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 06/22/23 10:29:27.108
    Jun 22 10:29:27.132: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:29:27.132: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:29:27.132: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:29:27.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 22 10:29:27.138: INFO: Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw is running 0 daemon pod, expected 1
    Jun 22 10:29:28.148: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:29:28.148: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:29:28.148: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:29:28.153: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 22 10:29:28.153: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 06/22/23 10:29:28.153
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 06/22/23 10:29:28.163
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9377, will wait for the garbage collector to delete the pods 06/22/23 10:29:28.164
    Jun 22 10:29:28.229: INFO: Deleting DaemonSet.extensions daemon-set took: 10.028188ms
    Jun 22 10:29:28.330: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.857676ms
    Jun 22 10:29:31.036: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 10:29:31.036: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 22 10:29:31.045: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"129863"},"items":null}

    Jun 22 10:29:31.049: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"129863"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:29:31.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9377" for this suite. 06/22/23 10:29:31.075
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:29:31.087
Jun 22 10:29:31.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename statefulset 06/22/23 10:29:31.088
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:31.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:31.105
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8365 06/22/23 10:29:31.109
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 06/22/23 10:29:31.12
STEP: Creating pod with conflicting port in namespace statefulset-8365 06/22/23 10:29:31.127
W0622 10:29:31.136974      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": hostPort (container "webserver" uses hostPort 21017), allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting until pod test-pod will start running in namespace statefulset-8365 06/22/23 10:29:31.137
Jun 22 10:29:31.137: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-8365" to be "running"
Jun 22 10:29:31.141: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.943775ms
Jun 22 10:29:33.147: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010525124s
Jun 22 10:29:33.147: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-8365 06/22/23 10:29:33.147
W0622 10:29:33.156663      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": hostPort (container "webserver" uses hostPort 21017), allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8365 06/22/23 10:29:33.156
Jun 22 10:29:33.188: INFO: Observed stateful pod in namespace: statefulset-8365, name: ss-0, uid: c4c30e82-f1d6-40c8-980b-e9d92e2f1f5e, status phase: Pending. Waiting for statefulset controller to delete.
Jun 22 10:29:33.207: INFO: Observed stateful pod in namespace: statefulset-8365, name: ss-0, uid: c4c30e82-f1d6-40c8-980b-e9d92e2f1f5e, status phase: Failed. Waiting for statefulset controller to delete.
Jun 22 10:29:33.221: INFO: Observed stateful pod in namespace: statefulset-8365, name: ss-0, uid: c4c30e82-f1d6-40c8-980b-e9d92e2f1f5e, status phase: Failed. Waiting for statefulset controller to delete.
Jun 22 10:29:33.229: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8365
STEP: Removing pod with conflicting port in namespace statefulset-8365 06/22/23 10:29:33.229
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8365 and will be in running state 06/22/23 10:29:33.252
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun 22 10:29:35.270: INFO: Deleting all statefulset in ns statefulset-8365
Jun 22 10:29:35.275: INFO: Scaling statefulset ss to 0
W0622 10:29:35.286589      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": hostPort (container "webserver" uses hostPort 21017), allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:29:45.299: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 10:29:45.303: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun 22 10:29:45.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8365" for this suite. 06/22/23 10:29:45.339
------------------------------
• [SLOW TEST] [14.268 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:29:31.087
    Jun 22 10:29:31.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename statefulset 06/22/23 10:29:31.088
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:31.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:31.105
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8365 06/22/23 10:29:31.109
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 06/22/23 10:29:31.12
    STEP: Creating pod with conflicting port in namespace statefulset-8365 06/22/23 10:29:31.127
    W0622 10:29:31.136974      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": hostPort (container "webserver" uses hostPort 21017), allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Waiting until pod test-pod will start running in namespace statefulset-8365 06/22/23 10:29:31.137
    Jun 22 10:29:31.137: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-8365" to be "running"
    Jun 22 10:29:31.141: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.943775ms
    Jun 22 10:29:33.147: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010525124s
    Jun 22 10:29:33.147: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-8365 06/22/23 10:29:33.147
    W0622 10:29:33.156663      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": hostPort (container "webserver" uses hostPort 21017), allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8365 06/22/23 10:29:33.156
    Jun 22 10:29:33.188: INFO: Observed stateful pod in namespace: statefulset-8365, name: ss-0, uid: c4c30e82-f1d6-40c8-980b-e9d92e2f1f5e, status phase: Pending. Waiting for statefulset controller to delete.
    Jun 22 10:29:33.207: INFO: Observed stateful pod in namespace: statefulset-8365, name: ss-0, uid: c4c30e82-f1d6-40c8-980b-e9d92e2f1f5e, status phase: Failed. Waiting for statefulset controller to delete.
    Jun 22 10:29:33.221: INFO: Observed stateful pod in namespace: statefulset-8365, name: ss-0, uid: c4c30e82-f1d6-40c8-980b-e9d92e2f1f5e, status phase: Failed. Waiting for statefulset controller to delete.
    Jun 22 10:29:33.229: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8365
    STEP: Removing pod with conflicting port in namespace statefulset-8365 06/22/23 10:29:33.229
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8365 and will be in running state 06/22/23 10:29:33.252
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun 22 10:29:35.270: INFO: Deleting all statefulset in ns statefulset-8365
    Jun 22 10:29:35.275: INFO: Scaling statefulset ss to 0
    W0622 10:29:35.286589      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": hostPort (container "webserver" uses hostPort 21017), allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:29:45.299: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 22 10:29:45.303: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:29:45.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8365" for this suite. 06/22/23 10:29:45.339
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:29:45.355
Jun 22 10:29:45.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename deployment 06/22/23 10:29:45.358
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:45.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:45.403
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jun 22 10:29:45.407: INFO: Creating deployment "webserver-deployment"
W0622 10:29:45.414186      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:29:45.414: INFO: Waiting for observed generation 1
Jun 22 10:29:47.440: INFO: Waiting for all required pods to come up
Jun 22 10:29:47.463: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 06/22/23 10:29:47.463
Jun 22 10:29:47.465: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-49br4" in namespace "deployment-5341" to be "running"
Jun 22 10:29:47.475: INFO: Pod "webserver-deployment-7f5969cbc7-49br4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.499729ms
Jun 22 10:29:49.483: INFO: Pod "webserver-deployment-7f5969cbc7-49br4": Phase="Running", Reason="", readiness=true. Elapsed: 2.017411413s
Jun 22 10:29:49.483: INFO: Pod "webserver-deployment-7f5969cbc7-49br4" satisfied condition "running"
Jun 22 10:29:49.483: INFO: Waiting for deployment "webserver-deployment" to complete
Jun 22 10:29:49.497: INFO: Updating deployment "webserver-deployment" with a non-existent image
W0622 10:29:49.518135      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:29:49.518: INFO: Updating deployment webserver-deployment
Jun 22 10:29:49.518: INFO: Waiting for observed generation 2
Jun 22 10:29:51.534: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 22 10:29:51.539: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 22 10:29:51.544: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 22 10:29:51.559: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 22 10:29:51.559: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 22 10:29:51.564: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 22 10:29:51.580: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun 22 10:29:51.580: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
W0622 10:29:51.620609      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:29:51.620: INFO: Updating deployment webserver-deployment
Jun 22 10:29:51.620: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun 22 10:29:51.647: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 22 10:29:51.660: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 22 10:29:51.847: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-5341  6b3d9eb2-3413-4d31-bb9d-ce3566b1a527 130261 3 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044fa2d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-06-22 10:29:49 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-22 10:29:51 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun 22 10:29:51.869: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-5341  3d79ab59-5edd-4dea-b585-0ca4a802ee1c 130249 3 2023-06-22 10:29:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6b3d9eb2-3413-4d31-bb9d-ce3566b1a527 0xc0045db117 0xc0045db118}] [] [{kube-controller-manager Update apps/v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b3d9eb2-3413-4d31-bb9d-ce3566b1a527\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045db1b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 10:29:51.869: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun 22 10:29:51.870: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-5341  78ba4257-883c-4b3a-986c-585a2280528e 130246 3 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6b3d9eb2-3413-4d31-bb9d-ce3566b1a527 0xc0045db027 0xc0045db028}] [] [{kube-controller-manager Update apps/v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b3d9eb2-3413-4d31-bb9d-ce3566b1a527\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045db0b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun 22 10:29:51.901: INFO: Pod "webserver-deployment-7f5969cbc7-28qgg" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-28qgg webserver-deployment-7f5969cbc7- deployment-5341  fdc4e4e2-f4db-4510-9f82-60a9789c899f 130125 0 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc0045db677 0xc0045db678}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cqtpn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cqtpn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.242,StartTime:2023-06-22 10:29:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:29:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aefa495223c85a1014a04349e12d5a1db6567b563c7d6f199c98fe98097daa9d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.242,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.901: INFO: Pod "webserver-deployment-7f5969cbc7-46pn9" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-46pn9 webserver-deployment-7f5969cbc7- deployment-5341  4f5ca0b4-b91d-45cb-977d-ab16098e1b41 130268 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc0045db857 0xc0045db858}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kl6gc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kl6gc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.903: INFO: Pod "webserver-deployment-7f5969cbc7-6f5r4" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6f5r4 webserver-deployment-7f5969cbc7- deployment-5341  3d7504db-fa6b-47a2-8cb5-13ac61d7d4d7 130120 0 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc0045db9a0 0xc0045db9a1}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bm4l2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bm4l2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.62,PodIP:100.96.2.191,StartTime:2023-06-22 10:29:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:29:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c7b3d8d3c179890b88abf3cc29f602672a78d04b190378c14003b5f268de82d0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.904: INFO: Pod "webserver-deployment-7f5969cbc7-77bwd" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-77bwd webserver-deployment-7f5969cbc7- deployment-5341  8da5de11-0239-4df3-8292-ab97c364f550 130124 0 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc0045dbb77 0xc0045dbb78}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.244\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xwvnt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xwvnt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.244,StartTime:2023-06-22 10:29:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:29:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://90f586df79598402cf1fee828eaf8f298a019f74630a43bc8b6c85fbc82ef506,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.244,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.905: INFO: Pod "webserver-deployment-7f5969cbc7-7rjwk" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7rjwk webserver-deployment-7f5969cbc7- deployment-5341  8fb12797-9e11-444b-bc61-e8e2dfdd0c80 130273 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc0045dbd57 0xc0045dbd58}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jrwtg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jrwtg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.62,PodIP:,StartTime:2023-06-22 10:29:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.906: INFO: Pod "webserver-deployment-7f5969cbc7-7rlpn" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7rlpn webserver-deployment-7f5969cbc7- deployment-5341  860f8577-1303-40c5-918d-de739a64241b 130283 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc0045dbf17 0xc0045dbf18}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jqpkq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jqpkq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.179,PodIP:,StartTime:2023-06-22 10:29:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.907: INFO: Pod "webserver-deployment-7f5969cbc7-8pm49" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8pm49 webserver-deployment-7f5969cbc7- deployment-5341  082e7fb7-6cea-41a7-99c5-38041729e9d9 130118 0 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc00467c0d7 0xc00467c0d8}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gcfqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gcfqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.62,PodIP:100.96.2.192,StartTime:2023-06-22 10:29:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:29:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f72f538c18a5c2733e62c1a8c04489202dd179b6c28d1240e733989778827afb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.192,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.907: INFO: Pod "webserver-deployment-7f5969cbc7-c2c59" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c2c59 webserver-deployment-7f5969cbc7- deployment-5341  aed65582-70dc-4187-b7b1-3b3993d77164 130117 0 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc00467c2b7 0xc00467c2b8}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.4.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l92bj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l92bj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.179,PodIP:100.96.4.175,StartTime:2023-06-22 10:29:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:29:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://247c28be03fd3da080910754079664b9a876dbf842c350e132b569ab99330281,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.4.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.908: INFO: Pod "webserver-deployment-7f5969cbc7-dwnr4" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dwnr4 webserver-deployment-7f5969cbc7- deployment-5341  7b04d2c0-0f26-4744-8a62-059f46886c31 130270 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc00467c497 0xc00467c498}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9xcrd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9xcrd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.909: INFO: Pod "webserver-deployment-7f5969cbc7-sf2fq" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sf2fq webserver-deployment-7f5969cbc7- deployment-5341  8c51e04d-2ada-404d-b7c3-7f20ea0b8805 130267 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc00467c5e0 0xc00467c5e1}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dr6b2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dr6b2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.909: INFO: Pod "webserver-deployment-7f5969cbc7-t9h9l" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t9h9l webserver-deployment-7f5969cbc7- deployment-5341  73f0c3a5-b7f6-413b-a296-9c551b880b6a 130121 0 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc00467c720 0xc00467c721}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.4.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qd5bz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qd5bz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.179,PodIP:100.96.4.173,StartTime:2023-06-22 10:29:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:29:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c9edfce624fe7667dfd6525eaafe8a30b7c27b72531ff29d316fa868da3e6335,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.4.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.910: INFO: Pod "webserver-deployment-7f5969cbc7-tj85v" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tj85v webserver-deployment-7f5969cbc7- deployment-5341  b0a29578-9582-46b7-96d6-924497bcca4f 130119 0 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc00467c8f7 0xc00467c8f8}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.4.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-59wfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-59wfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.179,PodIP:100.96.4.174,StartTime:2023-06-22 10:29:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:29:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a2acd371f0d6f4db190a7ec9bde447c0ff0a898d1dbb2c47223dbb1b7920efe5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.4.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.910: INFO: Pod "webserver-deployment-7f5969cbc7-wdrpn" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wdrpn webserver-deployment-7f5969cbc7- deployment-5341  6198fd6b-c50f-46fa-ace5-3e7ef0b8ed4b 130122 0 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc00467cad7 0xc00467cad8}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.193\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h2b2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h2b2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.62,PodIP:100.96.2.193,StartTime:2023-06-22 10:29:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:29:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3cec7c0107ecbffa0d69bb6d4075b3f50dc793ba2971f60699c7b3f3717db6de,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.193,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.910: INFO: Pod "webserver-deployment-7f5969cbc7-wl9gk" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wl9gk webserver-deployment-7f5969cbc7- deployment-5341  0bb37be1-a076-424d-a7c7-f858ad22eaf0 130253 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc00467ccb7 0xc00467ccb8}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hw54r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hw54r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:,StartTime:2023-06-22 10:29:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.911: INFO: Pod "webserver-deployment-7f5969cbc7-wsb2m" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wsb2m webserver-deployment-7f5969cbc7- deployment-5341  25aaa0ed-e28d-43d3-b2e7-a04fdf4d61f2 130272 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc00467ce77 0xc00467ce78}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l8cdn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l8cdn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:,StartTime:2023-06-22 10:29:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.911: INFO: Pod "webserver-deployment-d9f79cb5-9rt2s" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9rt2s webserver-deployment-d9f79cb5- deployment-5341  6f84c00b-4b19-475d-acaa-2502d4ee7426 130190 0 2023-06-22 10:29:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3d79ab59-5edd-4dea-b585-0ca4a802ee1c 0xc00467d037 0xc00467d038}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d79ab59-5edd-4dea-b585-0ca4a802ee1c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mnbfd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mnbfd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:,StartTime:2023-06-22 10:29:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.911: INFO: Pod "webserver-deployment-d9f79cb5-cx8sh" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-cx8sh webserver-deployment-d9f79cb5- deployment-5341  a3524a9c-21c5-44cd-a920-9590db42f1b2 130278 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3d79ab59-5edd-4dea-b585-0ca4a802ee1c 0xc00467d217 0xc00467d218}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d79ab59-5edd-4dea-b585-0ca4a802ee1c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pf8nk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pf8nk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.912: INFO: Pod "webserver-deployment-d9f79cb5-fwn8l" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fwn8l webserver-deployment-d9f79cb5- deployment-5341  8eaad55e-5a2c-4c54-af30-b15efb3f6486 130281 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3d79ab59-5edd-4dea-b585-0ca4a802ee1c 0xc00467d35f 0xc00467d370}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d79ab59-5edd-4dea-b585-0ca4a802ee1c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lwnwh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lwnwh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.912: INFO: Pod "webserver-deployment-d9f79cb5-kn5ck" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kn5ck webserver-deployment-d9f79cb5- deployment-5341  822544a3-2aaf-47ea-a513-377be7846524 130282 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3d79ab59-5edd-4dea-b585-0ca4a802ee1c 0xc00467d4bf 0xc00467d4d0}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d79ab59-5edd-4dea-b585-0ca4a802ee1c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n6vk9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n6vk9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.62,PodIP:,StartTime:2023-06-22 10:29:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.915: INFO: Pod "webserver-deployment-d9f79cb5-rx8zm" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rx8zm webserver-deployment-d9f79cb5- deployment-5341  8c1a444d-97a7-4a2f-ac44-706cda812380 130236 0 2023-06-22 10:29:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3d79ab59-5edd-4dea-b585-0ca4a802ee1c 0xc00467d6a7 0xc00467d6a8}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d79ab59-5edd-4dea-b585-0ca4a802ee1c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-brj6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-brj6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.245,StartTime:2023-06-22 10:29:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.916: INFO: Pod "webserver-deployment-d9f79cb5-th9rf" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-th9rf webserver-deployment-d9f79cb5- deployment-5341  11ae163d-9376-4453-b8fe-14637388efc9 130230 0 2023-06-22 10:29:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3d79ab59-5edd-4dea-b585-0ca4a802ee1c 0xc00467d8b7 0xc00467d8b8}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d79ab59-5edd-4dea-b585-0ca4a802ee1c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d4gcl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d4gcl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.62,PodIP:100.96.2.194,StartTime:2023-06-22 10:29:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.916: INFO: Pod "webserver-deployment-d9f79cb5-x8t7b" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-x8t7b webserver-deployment-d9f79cb5- deployment-5341  90b69762-d106-460a-842d-ace1291f230c 130195 0 2023-06-22 10:29:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3d79ab59-5edd-4dea-b585-0ca4a802ee1c 0xc00467dac7 0xc00467dac8}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d79ab59-5edd-4dea-b585-0ca4a802ee1c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6gk2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6gk2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.179,PodIP:,StartTime:2023-06-22 10:29:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:29:51.917: INFO: Pod "webserver-deployment-d9f79cb5-xbxt4" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xbxt4 webserver-deployment-d9f79cb5- deployment-5341  667c7ffc-1541-4086-a984-4e9c22bb9ee1 130176 0 2023-06-22 10:29:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3d79ab59-5edd-4dea-b585-0ca4a802ee1c 0xc00467dca7 0xc00467dca8}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d79ab59-5edd-4dea-b585-0ca4a802ee1c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4q6zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4q6zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.179,PodIP:,StartTime:2023-06-22 10:29:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun 22 10:29:51.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5341" for this suite. 06/22/23 10:29:51.941
------------------------------
• [SLOW TEST] [6.633 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:29:45.355
    Jun 22 10:29:45.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename deployment 06/22/23 10:29:45.358
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:45.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:45.403
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jun 22 10:29:45.407: INFO: Creating deployment "webserver-deployment"
    W0622 10:29:45.414186      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:29:45.414: INFO: Waiting for observed generation 1
    Jun 22 10:29:47.440: INFO: Waiting for all required pods to come up
    Jun 22 10:29:47.463: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 06/22/23 10:29:47.463
    Jun 22 10:29:47.465: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-49br4" in namespace "deployment-5341" to be "running"
    Jun 22 10:29:47.475: INFO: Pod "webserver-deployment-7f5969cbc7-49br4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.499729ms
    Jun 22 10:29:49.483: INFO: Pod "webserver-deployment-7f5969cbc7-49br4": Phase="Running", Reason="", readiness=true. Elapsed: 2.017411413s
    Jun 22 10:29:49.483: INFO: Pod "webserver-deployment-7f5969cbc7-49br4" satisfied condition "running"
    Jun 22 10:29:49.483: INFO: Waiting for deployment "webserver-deployment" to complete
    Jun 22 10:29:49.497: INFO: Updating deployment "webserver-deployment" with a non-existent image
    W0622 10:29:49.518135      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:29:49.518: INFO: Updating deployment webserver-deployment
    Jun 22 10:29:49.518: INFO: Waiting for observed generation 2
    Jun 22 10:29:51.534: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jun 22 10:29:51.539: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jun 22 10:29:51.544: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jun 22 10:29:51.559: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jun 22 10:29:51.559: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jun 22 10:29:51.564: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jun 22 10:29:51.580: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jun 22 10:29:51.580: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    W0622 10:29:51.620609      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:29:51.620: INFO: Updating deployment webserver-deployment
    Jun 22 10:29:51.620: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jun 22 10:29:51.647: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jun 22 10:29:51.660: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 22 10:29:51.847: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-5341  6b3d9eb2-3413-4d31-bb9d-ce3566b1a527 130261 3 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044fa2d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-06-22 10:29:49 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-22 10:29:51 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jun 22 10:29:51.869: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-5341  3d79ab59-5edd-4dea-b585-0ca4a802ee1c 130249 3 2023-06-22 10:29:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6b3d9eb2-3413-4d31-bb9d-ce3566b1a527 0xc0045db117 0xc0045db118}] [] [{kube-controller-manager Update apps/v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b3d9eb2-3413-4d31-bb9d-ce3566b1a527\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045db1b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 22 10:29:51.869: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jun 22 10:29:51.870: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-5341  78ba4257-883c-4b3a-986c-585a2280528e 130246 3 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6b3d9eb2-3413-4d31-bb9d-ce3566b1a527 0xc0045db027 0xc0045db028}] [] [{kube-controller-manager Update apps/v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b3d9eb2-3413-4d31-bb9d-ce3566b1a527\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045db0b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jun 22 10:29:51.901: INFO: Pod "webserver-deployment-7f5969cbc7-28qgg" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-28qgg webserver-deployment-7f5969cbc7- deployment-5341  fdc4e4e2-f4db-4510-9f82-60a9789c899f 130125 0 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc0045db677 0xc0045db678}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.242\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cqtpn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cqtpn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.242,StartTime:2023-06-22 10:29:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:29:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://aefa495223c85a1014a04349e12d5a1db6567b563c7d6f199c98fe98097daa9d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.242,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.901: INFO: Pod "webserver-deployment-7f5969cbc7-46pn9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-46pn9 webserver-deployment-7f5969cbc7- deployment-5341  4f5ca0b4-b91d-45cb-977d-ab16098e1b41 130268 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc0045db857 0xc0045db858}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kl6gc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kl6gc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.903: INFO: Pod "webserver-deployment-7f5969cbc7-6f5r4" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6f5r4 webserver-deployment-7f5969cbc7- deployment-5341  3d7504db-fa6b-47a2-8cb5-13ac61d7d4d7 130120 0 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc0045db9a0 0xc0045db9a1}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bm4l2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bm4l2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.62,PodIP:100.96.2.191,StartTime:2023-06-22 10:29:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:29:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c7b3d8d3c179890b88abf3cc29f602672a78d04b190378c14003b5f268de82d0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.904: INFO: Pod "webserver-deployment-7f5969cbc7-77bwd" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-77bwd webserver-deployment-7f5969cbc7- deployment-5341  8da5de11-0239-4df3-8292-ab97c364f550 130124 0 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc0045dbb77 0xc0045dbb78}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.244\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xwvnt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xwvnt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.244,StartTime:2023-06-22 10:29:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:29:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://90f586df79598402cf1fee828eaf8f298a019f74630a43bc8b6c85fbc82ef506,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.244,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.905: INFO: Pod "webserver-deployment-7f5969cbc7-7rjwk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7rjwk webserver-deployment-7f5969cbc7- deployment-5341  8fb12797-9e11-444b-bc61-e8e2dfdd0c80 130273 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc0045dbd57 0xc0045dbd58}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jrwtg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jrwtg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.62,PodIP:,StartTime:2023-06-22 10:29:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.906: INFO: Pod "webserver-deployment-7f5969cbc7-7rlpn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7rlpn webserver-deployment-7f5969cbc7- deployment-5341  860f8577-1303-40c5-918d-de739a64241b 130283 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc0045dbf17 0xc0045dbf18}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jqpkq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jqpkq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.179,PodIP:,StartTime:2023-06-22 10:29:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.907: INFO: Pod "webserver-deployment-7f5969cbc7-8pm49" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8pm49 webserver-deployment-7f5969cbc7- deployment-5341  082e7fb7-6cea-41a7-99c5-38041729e9d9 130118 0 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc00467c0d7 0xc00467c0d8}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gcfqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gcfqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.62,PodIP:100.96.2.192,StartTime:2023-06-22 10:29:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:29:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f72f538c18a5c2733e62c1a8c04489202dd179b6c28d1240e733989778827afb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.192,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.907: INFO: Pod "webserver-deployment-7f5969cbc7-c2c59" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-c2c59 webserver-deployment-7f5969cbc7- deployment-5341  aed65582-70dc-4187-b7b1-3b3993d77164 130117 0 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc00467c2b7 0xc00467c2b8}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.4.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l92bj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l92bj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.179,PodIP:100.96.4.175,StartTime:2023-06-22 10:29:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:29:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://247c28be03fd3da080910754079664b9a876dbf842c350e132b569ab99330281,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.4.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.908: INFO: Pod "webserver-deployment-7f5969cbc7-dwnr4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dwnr4 webserver-deployment-7f5969cbc7- deployment-5341  7b04d2c0-0f26-4744-8a62-059f46886c31 130270 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc00467c497 0xc00467c498}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9xcrd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9xcrd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.909: INFO: Pod "webserver-deployment-7f5969cbc7-sf2fq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sf2fq webserver-deployment-7f5969cbc7- deployment-5341  8c51e04d-2ada-404d-b7c3-7f20ea0b8805 130267 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc00467c5e0 0xc00467c5e1}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dr6b2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dr6b2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.909: INFO: Pod "webserver-deployment-7f5969cbc7-t9h9l" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-t9h9l webserver-deployment-7f5969cbc7- deployment-5341  73f0c3a5-b7f6-413b-a296-9c551b880b6a 130121 0 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc00467c720 0xc00467c721}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.4.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qd5bz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qd5bz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.179,PodIP:100.96.4.173,StartTime:2023-06-22 10:29:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:29:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c9edfce624fe7667dfd6525eaafe8a30b7c27b72531ff29d316fa868da3e6335,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.4.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.910: INFO: Pod "webserver-deployment-7f5969cbc7-tj85v" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-tj85v webserver-deployment-7f5969cbc7- deployment-5341  b0a29578-9582-46b7-96d6-924497bcca4f 130119 0 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc00467c8f7 0xc00467c8f8}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.4.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-59wfb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-59wfb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.179,PodIP:100.96.4.174,StartTime:2023-06-22 10:29:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:29:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a2acd371f0d6f4db190a7ec9bde447c0ff0a898d1dbb2c47223dbb1b7920efe5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.4.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.910: INFO: Pod "webserver-deployment-7f5969cbc7-wdrpn" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wdrpn webserver-deployment-7f5969cbc7- deployment-5341  6198fd6b-c50f-46fa-ace5-3e7ef0b8ed4b 130122 0 2023-06-22 10:29:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc00467cad7 0xc00467cad8}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.193\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h2b2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h2b2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.62,PodIP:100.96.2.193,StartTime:2023-06-22 10:29:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:29:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3cec7c0107ecbffa0d69bb6d4075b3f50dc793ba2971f60699c7b3f3717db6de,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.193,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.910: INFO: Pod "webserver-deployment-7f5969cbc7-wl9gk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wl9gk webserver-deployment-7f5969cbc7- deployment-5341  0bb37be1-a076-424d-a7c7-f858ad22eaf0 130253 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc00467ccb7 0xc00467ccb8}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hw54r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hw54r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:,StartTime:2023-06-22 10:29:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.911: INFO: Pod "webserver-deployment-7f5969cbc7-wsb2m" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wsb2m webserver-deployment-7f5969cbc7- deployment-5341  25aaa0ed-e28d-43d3-b2e7-a04fdf4d61f2 130272 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 78ba4257-883c-4b3a-986c-585a2280528e 0xc00467ce77 0xc00467ce78}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78ba4257-883c-4b3a-986c-585a2280528e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l8cdn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l8cdn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:,StartTime:2023-06-22 10:29:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.911: INFO: Pod "webserver-deployment-d9f79cb5-9rt2s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-9rt2s webserver-deployment-d9f79cb5- deployment-5341  6f84c00b-4b19-475d-acaa-2502d4ee7426 130190 0 2023-06-22 10:29:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3d79ab59-5edd-4dea-b585-0ca4a802ee1c 0xc00467d037 0xc00467d038}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d79ab59-5edd-4dea-b585-0ca4a802ee1c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mnbfd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mnbfd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:,StartTime:2023-06-22 10:29:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.911: INFO: Pod "webserver-deployment-d9f79cb5-cx8sh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-cx8sh webserver-deployment-d9f79cb5- deployment-5341  a3524a9c-21c5-44cd-a920-9590db42f1b2 130278 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3d79ab59-5edd-4dea-b585-0ca4a802ee1c 0xc00467d217 0xc00467d218}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d79ab59-5edd-4dea-b585-0ca4a802ee1c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pf8nk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pf8nk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.912: INFO: Pod "webserver-deployment-d9f79cb5-fwn8l" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fwn8l webserver-deployment-d9f79cb5- deployment-5341  8eaad55e-5a2c-4c54-af30-b15efb3f6486 130281 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3d79ab59-5edd-4dea-b585-0ca4a802ee1c 0xc00467d35f 0xc00467d370}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d79ab59-5edd-4dea-b585-0ca4a802ee1c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lwnwh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lwnwh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.912: INFO: Pod "webserver-deployment-d9f79cb5-kn5ck" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kn5ck webserver-deployment-d9f79cb5- deployment-5341  822544a3-2aaf-47ea-a513-377be7846524 130282 0 2023-06-22 10:29:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3d79ab59-5edd-4dea-b585-0ca4a802ee1c 0xc00467d4bf 0xc00467d4d0}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d79ab59-5edd-4dea-b585-0ca4a802ee1c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n6vk9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n6vk9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.62,PodIP:,StartTime:2023-06-22 10:29:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.915: INFO: Pod "webserver-deployment-d9f79cb5-rx8zm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rx8zm webserver-deployment-d9f79cb5- deployment-5341  8c1a444d-97a7-4a2f-ac44-706cda812380 130236 0 2023-06-22 10:29:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3d79ab59-5edd-4dea-b585-0ca4a802ee1c 0xc00467d6a7 0xc00467d6a8}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d79ab59-5edd-4dea-b585-0ca4a802ee1c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.245\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-brj6q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-brj6q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.245,StartTime:2023-06-22 10:29:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.245,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.916: INFO: Pod "webserver-deployment-d9f79cb5-th9rf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-th9rf webserver-deployment-d9f79cb5- deployment-5341  11ae163d-9376-4453-b8fe-14637388efc9 130230 0 2023-06-22 10:29:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3d79ab59-5edd-4dea-b585-0ca4a802ee1c 0xc00467d8b7 0xc00467d8b8}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d79ab59-5edd-4dea-b585-0ca4a802ee1c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d4gcl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d4gcl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.62,PodIP:100.96.2.194,StartTime:2023-06-22 10:29:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.916: INFO: Pod "webserver-deployment-d9f79cb5-x8t7b" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-x8t7b webserver-deployment-d9f79cb5- deployment-5341  90b69762-d106-460a-842d-ace1291f230c 130195 0 2023-06-22 10:29:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3d79ab59-5edd-4dea-b585-0ca4a802ee1c 0xc00467dac7 0xc00467dac8}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d79ab59-5edd-4dea-b585-0ca4a802ee1c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6gk2h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6gk2h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.179,PodIP:,StartTime:2023-06-22 10:29:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:29:51.917: INFO: Pod "webserver-deployment-d9f79cb5-xbxt4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xbxt4 webserver-deployment-d9f79cb5- deployment-5341  667c7ffc-1541-4086-a984-4e9c22bb9ee1 130176 0 2023-06-22 10:29:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 3d79ab59-5edd-4dea-b585-0ca4a802ee1c 0xc00467dca7 0xc00467dca8}] [] [{kube-controller-manager Update v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d79ab59-5edd-4dea-b585-0ca4a802ee1c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:29:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4q6zx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4q6zx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:29:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.179,PodIP:,StartTime:2023-06-22 10:29:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:29:51.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5341" for this suite. 06/22/23 10:29:51.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:29:51.997
Jun 22 10:29:51.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename container-lifecycle-hook 06/22/23 10:29:52.007
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:52.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:52.056
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 06/22/23 10:29:52.07
W0622 10:29:52.083508      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:29:52.083: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8884" to be "running and ready"
Jun 22 10:29:52.088: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.544196ms
Jun 22 10:29:52.088: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:29:54.095: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011737153s
Jun 22 10:29:54.095: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:29:56.095: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01185203s
Jun 22 10:29:56.095: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:29:58.097: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 6.013278983s
Jun 22 10:29:58.097: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun 22 10:29:58.097: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 06/22/23 10:29:58.104
W0622 10:29:58.112132      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-with-prestop-exec-hook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-with-prestop-exec-hook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-with-prestop-exec-hook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-with-prestop-exec-hook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:29:58.112: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-8884" to be "running and ready"
Jun 22 10:29:58.121: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.549595ms
Jun 22 10:29:58.122: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:30:00.129: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017281323s
Jun 22 10:30:00.129: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:30:02.128: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.016209067s
Jun 22 10:30:02.128: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jun 22 10:30:02.128: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 06/22/23 10:30:02.134
Jun 22 10:30:02.145: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 10:30:02.150: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 10:30:04.152: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 10:30:04.158: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 06/22/23 10:30:04.158
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jun 22 10:30:04.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-8884" for this suite. 06/22/23 10:30:04.213
------------------------------
• [SLOW TEST] [12.231 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:29:51.997
    Jun 22 10:29:51.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/22/23 10:29:52.007
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:29:52.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:29:52.056
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 06/22/23 10:29:52.07
    W0622 10:29:52.083508      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:29:52.083: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8884" to be "running and ready"
    Jun 22 10:29:52.088: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.544196ms
    Jun 22 10:29:52.088: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:29:54.095: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011737153s
    Jun 22 10:29:54.095: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:29:56.095: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01185203s
    Jun 22 10:29:56.095: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:29:58.097: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 6.013278983s
    Jun 22 10:29:58.097: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun 22 10:29:58.097: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 06/22/23 10:29:58.104
    W0622 10:29:58.112132      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-with-prestop-exec-hook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-with-prestop-exec-hook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-with-prestop-exec-hook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-with-prestop-exec-hook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:29:58.112: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-8884" to be "running and ready"
    Jun 22 10:29:58.121: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.549595ms
    Jun 22 10:29:58.122: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:30:00.129: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017281323s
    Jun 22 10:30:00.129: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:30:02.128: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.016209067s
    Jun 22 10:30:02.128: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jun 22 10:30:02.128: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 06/22/23 10:30:02.134
    Jun 22 10:30:02.145: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jun 22 10:30:02.150: INFO: Pod pod-with-prestop-exec-hook still exists
    Jun 22 10:30:04.152: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jun 22 10:30:04.158: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 06/22/23 10:30:04.158
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:30:04.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-8884" for this suite. 06/22/23 10:30:04.213
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:30:04.234
Jun 22 10:30:04.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename var-expansion 06/22/23 10:30:04.236
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:30:04.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:30:04.26
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
W0622 10:30:04.280790      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:30:04.281: INFO: Waiting up to 2m0s for pod "var-expansion-6a40c21e-556c-4978-a2d4-368f9f8e85fb" in namespace "var-expansion-7271" to be "container 0 failed with reason CreateContainerConfigError"
Jun 22 10:30:04.286: INFO: Pod "var-expansion-6a40c21e-556c-4978-a2d4-368f9f8e85fb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.242835ms
Jun 22 10:30:06.294: INFO: Pod "var-expansion-6a40c21e-556c-4978-a2d4-368f9f8e85fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013464317s
Jun 22 10:30:06.294: INFO: Pod "var-expansion-6a40c21e-556c-4978-a2d4-368f9f8e85fb" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jun 22 10:30:06.294: INFO: Deleting pod "var-expansion-6a40c21e-556c-4978-a2d4-368f9f8e85fb" in namespace "var-expansion-7271"
Jun 22 10:30:06.306: INFO: Wait up to 5m0s for pod "var-expansion-6a40c21e-556c-4978-a2d4-368f9f8e85fb" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun 22 10:30:08.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7271" for this suite. 06/22/23 10:30:08.326
------------------------------
• [4.110 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:30:04.234
    Jun 22 10:30:04.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename var-expansion 06/22/23 10:30:04.236
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:30:04.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:30:04.26
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    W0622 10:30:04.280790      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:30:04.281: INFO: Waiting up to 2m0s for pod "var-expansion-6a40c21e-556c-4978-a2d4-368f9f8e85fb" in namespace "var-expansion-7271" to be "container 0 failed with reason CreateContainerConfigError"
    Jun 22 10:30:04.286: INFO: Pod "var-expansion-6a40c21e-556c-4978-a2d4-368f9f8e85fb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.242835ms
    Jun 22 10:30:06.294: INFO: Pod "var-expansion-6a40c21e-556c-4978-a2d4-368f9f8e85fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013464317s
    Jun 22 10:30:06.294: INFO: Pod "var-expansion-6a40c21e-556c-4978-a2d4-368f9f8e85fb" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jun 22 10:30:06.294: INFO: Deleting pod "var-expansion-6a40c21e-556c-4978-a2d4-368f9f8e85fb" in namespace "var-expansion-7271"
    Jun 22 10:30:06.306: INFO: Wait up to 5m0s for pod "var-expansion-6a40c21e-556c-4978-a2d4-368f9f8e85fb" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:30:08.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7271" for this suite. 06/22/23 10:30:08.326
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:30:08.347
Jun 22 10:30:08.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename emptydir 06/22/23 10:30:08.348
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:30:08.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:30:08.371
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 06/22/23 10:30:08.374
W0622 10:30:08.387075      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:30:08.387: INFO: Waiting up to 5m0s for pod "pod-b5e3b096-0005-4dee-aac4-54715c7dc577" in namespace "emptydir-4977" to be "Succeeded or Failed"
Jun 22 10:30:08.392: INFO: Pod "pod-b5e3b096-0005-4dee-aac4-54715c7dc577": Phase="Pending", Reason="", readiness=false. Elapsed: 4.809136ms
Jun 22 10:30:10.399: INFO: Pod "pod-b5e3b096-0005-4dee-aac4-54715c7dc577": Phase="Running", Reason="", readiness=false. Elapsed: 2.012050038s
Jun 22 10:30:12.399: INFO: Pod "pod-b5e3b096-0005-4dee-aac4-54715c7dc577": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012078745s
STEP: Saw pod success 06/22/23 10:30:12.399
Jun 22 10:30:12.399: INFO: Pod "pod-b5e3b096-0005-4dee-aac4-54715c7dc577" satisfied condition "Succeeded or Failed"
Jun 22 10:30:12.404: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-b5e3b096-0005-4dee-aac4-54715c7dc577 container test-container: <nil>
STEP: delete the pod 06/22/23 10:30:12.413
Jun 22 10:30:12.431: INFO: Waiting for pod pod-b5e3b096-0005-4dee-aac4-54715c7dc577 to disappear
Jun 22 10:30:12.436: INFO: Pod pod-b5e3b096-0005-4dee-aac4-54715c7dc577 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun 22 10:30:12.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4977" for this suite. 06/22/23 10:30:12.444
------------------------------
• [4.105 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:30:08.347
    Jun 22 10:30:08.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename emptydir 06/22/23 10:30:08.348
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:30:08.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:30:08.371
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 06/22/23 10:30:08.374
    W0622 10:30:08.387075      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:30:08.387: INFO: Waiting up to 5m0s for pod "pod-b5e3b096-0005-4dee-aac4-54715c7dc577" in namespace "emptydir-4977" to be "Succeeded or Failed"
    Jun 22 10:30:08.392: INFO: Pod "pod-b5e3b096-0005-4dee-aac4-54715c7dc577": Phase="Pending", Reason="", readiness=false. Elapsed: 4.809136ms
    Jun 22 10:30:10.399: INFO: Pod "pod-b5e3b096-0005-4dee-aac4-54715c7dc577": Phase="Running", Reason="", readiness=false. Elapsed: 2.012050038s
    Jun 22 10:30:12.399: INFO: Pod "pod-b5e3b096-0005-4dee-aac4-54715c7dc577": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012078745s
    STEP: Saw pod success 06/22/23 10:30:12.399
    Jun 22 10:30:12.399: INFO: Pod "pod-b5e3b096-0005-4dee-aac4-54715c7dc577" satisfied condition "Succeeded or Failed"
    Jun 22 10:30:12.404: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-b5e3b096-0005-4dee-aac4-54715c7dc577 container test-container: <nil>
    STEP: delete the pod 06/22/23 10:30:12.413
    Jun 22 10:30:12.431: INFO: Waiting for pod pod-b5e3b096-0005-4dee-aac4-54715c7dc577 to disappear
    Jun 22 10:30:12.436: INFO: Pod pod-b5e3b096-0005-4dee-aac4-54715c7dc577 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:30:12.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4977" for this suite. 06/22/23 10:30:12.444
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:30:12.452
Jun 22 10:30:12.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename crd-publish-openapi 06/22/23 10:30:12.454
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:30:12.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:30:12.475
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Jun 22 10:30:12.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 06/22/23 10:30:15.514
Jun 22 10:30:15.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 --namespace=crd-publish-openapi-5888 create -f -'
Jun 22 10:30:16.953: INFO: stderr: ""
Jun 22 10:30:16.953: INFO: stdout: "e2e-test-crd-publish-openapi-4668-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 22 10:30:16.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 --namespace=crd-publish-openapi-5888 delete e2e-test-crd-publish-openapi-4668-crds test-foo'
Jun 22 10:30:17.090: INFO: stderr: ""
Jun 22 10:30:17.090: INFO: stdout: "e2e-test-crd-publish-openapi-4668-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun 22 10:30:17.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 --namespace=crd-publish-openapi-5888 apply -f -'
Jun 22 10:30:17.476: INFO: stderr: ""
Jun 22 10:30:17.476: INFO: stdout: "e2e-test-crd-publish-openapi-4668-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 22 10:30:17.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 --namespace=crd-publish-openapi-5888 delete e2e-test-crd-publish-openapi-4668-crds test-foo'
Jun 22 10:30:17.568: INFO: stderr: ""
Jun 22 10:30:17.568: INFO: stdout: "e2e-test-crd-publish-openapi-4668-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 06/22/23 10:30:17.568
Jun 22 10:30:17.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 --namespace=crd-publish-openapi-5888 create -f -'
Jun 22 10:30:18.640: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 06/22/23 10:30:18.64
Jun 22 10:30:18.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 --namespace=crd-publish-openapi-5888 create -f -'
Jun 22 10:30:19.006: INFO: rc: 1
Jun 22 10:30:19.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 --namespace=crd-publish-openapi-5888 apply -f -'
Jun 22 10:30:19.362: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 06/22/23 10:30:19.362
Jun 22 10:30:19.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 --namespace=crd-publish-openapi-5888 create -f -'
Jun 22 10:30:19.831: INFO: rc: 1
Jun 22 10:30:19.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 --namespace=crd-publish-openapi-5888 apply -f -'
Jun 22 10:30:20.191: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 06/22/23 10:30:20.191
Jun 22 10:30:20.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 explain e2e-test-crd-publish-openapi-4668-crds'
Jun 22 10:30:20.560: INFO: stderr: ""
Jun 22 10:30:20.560: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4668-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 06/22/23 10:30:20.56
Jun 22 10:30:20.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 explain e2e-test-crd-publish-openapi-4668-crds.metadata'
Jun 22 10:30:21.588: INFO: stderr: ""
Jun 22 10:30:21.588: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4668-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun 22 10:30:21.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 explain e2e-test-crd-publish-openapi-4668-crds.spec'
Jun 22 10:30:21.952: INFO: stderr: ""
Jun 22 10:30:21.952: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4668-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun 22 10:30:21.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 explain e2e-test-crd-publish-openapi-4668-crds.spec.bars'
Jun 22 10:30:22.300: INFO: stderr: ""
Jun 22 10:30:22.300: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4668-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 06/22/23 10:30:22.3
Jun 22 10:30:22.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 explain e2e-test-crd-publish-openapi-4668-crds.spec.bars2'
Jun 22 10:30:22.657: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:30:25.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5888" for this suite. 06/22/23 10:30:25.476
------------------------------
• [SLOW TEST] [13.031 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:30:12.452
    Jun 22 10:30:12.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename crd-publish-openapi 06/22/23 10:30:12.454
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:30:12.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:30:12.475
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Jun 22 10:30:12.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 06/22/23 10:30:15.514
    Jun 22 10:30:15.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 --namespace=crd-publish-openapi-5888 create -f -'
    Jun 22 10:30:16.953: INFO: stderr: ""
    Jun 22 10:30:16.953: INFO: stdout: "e2e-test-crd-publish-openapi-4668-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jun 22 10:30:16.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 --namespace=crd-publish-openapi-5888 delete e2e-test-crd-publish-openapi-4668-crds test-foo'
    Jun 22 10:30:17.090: INFO: stderr: ""
    Jun 22 10:30:17.090: INFO: stdout: "e2e-test-crd-publish-openapi-4668-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jun 22 10:30:17.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 --namespace=crd-publish-openapi-5888 apply -f -'
    Jun 22 10:30:17.476: INFO: stderr: ""
    Jun 22 10:30:17.476: INFO: stdout: "e2e-test-crd-publish-openapi-4668-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jun 22 10:30:17.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 --namespace=crd-publish-openapi-5888 delete e2e-test-crd-publish-openapi-4668-crds test-foo'
    Jun 22 10:30:17.568: INFO: stderr: ""
    Jun 22 10:30:17.568: INFO: stdout: "e2e-test-crd-publish-openapi-4668-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 06/22/23 10:30:17.568
    Jun 22 10:30:17.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 --namespace=crd-publish-openapi-5888 create -f -'
    Jun 22 10:30:18.640: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 06/22/23 10:30:18.64
    Jun 22 10:30:18.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 --namespace=crd-publish-openapi-5888 create -f -'
    Jun 22 10:30:19.006: INFO: rc: 1
    Jun 22 10:30:19.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 --namespace=crd-publish-openapi-5888 apply -f -'
    Jun 22 10:30:19.362: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 06/22/23 10:30:19.362
    Jun 22 10:30:19.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 --namespace=crd-publish-openapi-5888 create -f -'
    Jun 22 10:30:19.831: INFO: rc: 1
    Jun 22 10:30:19.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 --namespace=crd-publish-openapi-5888 apply -f -'
    Jun 22 10:30:20.191: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 06/22/23 10:30:20.191
    Jun 22 10:30:20.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 explain e2e-test-crd-publish-openapi-4668-crds'
    Jun 22 10:30:20.560: INFO: stderr: ""
    Jun 22 10:30:20.560: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4668-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 06/22/23 10:30:20.56
    Jun 22 10:30:20.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 explain e2e-test-crd-publish-openapi-4668-crds.metadata'
    Jun 22 10:30:21.588: INFO: stderr: ""
    Jun 22 10:30:21.588: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4668-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jun 22 10:30:21.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 explain e2e-test-crd-publish-openapi-4668-crds.spec'
    Jun 22 10:30:21.952: INFO: stderr: ""
    Jun 22 10:30:21.952: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4668-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jun 22 10:30:21.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 explain e2e-test-crd-publish-openapi-4668-crds.spec.bars'
    Jun 22 10:30:22.300: INFO: stderr: ""
    Jun 22 10:30:22.300: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4668-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 06/22/23 10:30:22.3
    Jun 22 10:30:22.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-5888 explain e2e-test-crd-publish-openapi-4668-crds.spec.bars2'
    Jun 22 10:30:22.657: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:30:25.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5888" for this suite. 06/22/23 10:30:25.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:30:25.487
Jun 22 10:30:25.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 10:30:25.488
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:30:25.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:30:25.513
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 06/22/23 10:30:25.517
W0622 10:30:25.530012      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:30:25.530: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1370d2d9-abab-44a9-8dd2-379b8f8bc317" in namespace "projected-7884" to be "Succeeded or Failed"
Jun 22 10:30:25.533: INFO: Pod "downwardapi-volume-1370d2d9-abab-44a9-8dd2-379b8f8bc317": Phase="Pending", Reason="", readiness=false. Elapsed: 3.63891ms
Jun 22 10:30:27.540: INFO: Pod "downwardapi-volume-1370d2d9-abab-44a9-8dd2-379b8f8bc317": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010208449s
Jun 22 10:30:29.540: INFO: Pod "downwardapi-volume-1370d2d9-abab-44a9-8dd2-379b8f8bc317": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01060697s
STEP: Saw pod success 06/22/23 10:30:29.54
Jun 22 10:30:29.541: INFO: Pod "downwardapi-volume-1370d2d9-abab-44a9-8dd2-379b8f8bc317" satisfied condition "Succeeded or Failed"
Jun 22 10:30:29.545: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-1370d2d9-abab-44a9-8dd2-379b8f8bc317 container client-container: <nil>
STEP: delete the pod 06/22/23 10:30:29.561
Jun 22 10:30:29.576: INFO: Waiting for pod downwardapi-volume-1370d2d9-abab-44a9-8dd2-379b8f8bc317 to disappear
Jun 22 10:30:29.580: INFO: Pod downwardapi-volume-1370d2d9-abab-44a9-8dd2-379b8f8bc317 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun 22 10:30:29.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7884" for this suite. 06/22/23 10:30:29.589
------------------------------
• [4.111 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:30:25.487
    Jun 22 10:30:25.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 10:30:25.488
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:30:25.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:30:25.513
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 06/22/23 10:30:25.517
    W0622 10:30:25.530012      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:30:25.530: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1370d2d9-abab-44a9-8dd2-379b8f8bc317" in namespace "projected-7884" to be "Succeeded or Failed"
    Jun 22 10:30:25.533: INFO: Pod "downwardapi-volume-1370d2d9-abab-44a9-8dd2-379b8f8bc317": Phase="Pending", Reason="", readiness=false. Elapsed: 3.63891ms
    Jun 22 10:30:27.540: INFO: Pod "downwardapi-volume-1370d2d9-abab-44a9-8dd2-379b8f8bc317": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010208449s
    Jun 22 10:30:29.540: INFO: Pod "downwardapi-volume-1370d2d9-abab-44a9-8dd2-379b8f8bc317": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01060697s
    STEP: Saw pod success 06/22/23 10:30:29.54
    Jun 22 10:30:29.541: INFO: Pod "downwardapi-volume-1370d2d9-abab-44a9-8dd2-379b8f8bc317" satisfied condition "Succeeded or Failed"
    Jun 22 10:30:29.545: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-1370d2d9-abab-44a9-8dd2-379b8f8bc317 container client-container: <nil>
    STEP: delete the pod 06/22/23 10:30:29.561
    Jun 22 10:30:29.576: INFO: Waiting for pod downwardapi-volume-1370d2d9-abab-44a9-8dd2-379b8f8bc317 to disappear
    Jun 22 10:30:29.580: INFO: Pod downwardapi-volume-1370d2d9-abab-44a9-8dd2-379b8f8bc317 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:30:29.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7884" for this suite. 06/22/23 10:30:29.589
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:30:29.599
Jun 22 10:30:29.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubelet-test 06/22/23 10:30:29.6
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:30:29.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:30:29.625
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
W0622 10:30:29.639984      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:30:29.640: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b" in namespace "kubelet-test-2975" to be "running and ready"
Jun 22 10:30:29.647: INFO: Pod "busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.350285ms
Jun 22 10:30:29.647: INFO: The phase of Pod busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:30:31.653: INFO: Pod "busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b": Phase="Running", Reason="", readiness=true. Elapsed: 2.013048697s
Jun 22 10:30:31.653: INFO: The phase of Pod busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b is Running (Ready = true)
Jun 22 10:30:31.653: INFO: Pod "busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jun 22 10:30:31.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2975" for this suite. 06/22/23 10:30:31.673
------------------------------
• [2.097 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:30:29.599
    Jun 22 10:30:29.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubelet-test 06/22/23 10:30:29.6
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:30:29.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:30:29.625
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    W0622 10:30:29.639984      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:30:29.640: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b" in namespace "kubelet-test-2975" to be "running and ready"
    Jun 22 10:30:29.647: INFO: Pod "busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.350285ms
    Jun 22 10:30:29.647: INFO: The phase of Pod busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:30:31.653: INFO: Pod "busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b": Phase="Running", Reason="", readiness=true. Elapsed: 2.013048697s
    Jun 22 10:30:31.653: INFO: The phase of Pod busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b is Running (Ready = true)
    Jun 22 10:30:31.653: INFO: Pod "busybox-readonly-fsdb284970-68d0-4215-914c-7a65978b180b" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:30:31.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2975" for this suite. 06/22/23 10:30:31.673
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:30:31.696
Jun 22 10:30:31.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename namespaces 06/22/23 10:30:31.698
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:30:31.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:30:31.719
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 06/22/23 10:30:31.723
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:30:31.753
STEP: Creating a service in the namespace 06/22/23 10:30:31.758
STEP: Deleting the namespace 06/22/23 10:30:31.812
STEP: Waiting for the namespace to be removed. 06/22/23 10:30:31.831
STEP: Recreating the namespace 06/22/23 10:30:37.837
STEP: Verifying there is no service in the namespace 06/22/23 10:30:37.859
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:30:37.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-8827" for this suite. 06/22/23 10:30:37.872
STEP: Destroying namespace "nsdeletetest-8486" for this suite. 06/22/23 10:30:37.879
Jun 22 10:30:37.884: INFO: Namespace nsdeletetest-8486 was already deleted
STEP: Destroying namespace "nsdeletetest-3850" for this suite. 06/22/23 10:30:37.884
------------------------------
• [SLOW TEST] [6.196 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:30:31.696
    Jun 22 10:30:31.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename namespaces 06/22/23 10:30:31.698
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:30:31.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:30:31.719
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 06/22/23 10:30:31.723
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:30:31.753
    STEP: Creating a service in the namespace 06/22/23 10:30:31.758
    STEP: Deleting the namespace 06/22/23 10:30:31.812
    STEP: Waiting for the namespace to be removed. 06/22/23 10:30:31.831
    STEP: Recreating the namespace 06/22/23 10:30:37.837
    STEP: Verifying there is no service in the namespace 06/22/23 10:30:37.859
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:30:37.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-8827" for this suite. 06/22/23 10:30:37.872
    STEP: Destroying namespace "nsdeletetest-8486" for this suite. 06/22/23 10:30:37.879
    Jun 22 10:30:37.884: INFO: Namespace nsdeletetest-8486 was already deleted
    STEP: Destroying namespace "nsdeletetest-3850" for this suite. 06/22/23 10:30:37.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:30:37.893
Jun 22 10:30:37.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 10:30:37.894
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:30:37.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:30:37.92
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 06/22/23 10:30:37.924
W0622 10:30:37.937436      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:30:37.937: INFO: Waiting up to 5m0s for pod "annotationupdatea61091d7-90c2-48f6-b828-43ec834a7a5c" in namespace "projected-3385" to be "running and ready"
Jun 22 10:30:37.942: INFO: Pod "annotationupdatea61091d7-90c2-48f6-b828-43ec834a7a5c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.752812ms
Jun 22 10:30:37.942: INFO: The phase of Pod annotationupdatea61091d7-90c2-48f6-b828-43ec834a7a5c is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:30:39.949: INFO: Pod "annotationupdatea61091d7-90c2-48f6-b828-43ec834a7a5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.011612234s
Jun 22 10:30:39.949: INFO: The phase of Pod annotationupdatea61091d7-90c2-48f6-b828-43ec834a7a5c is Running (Ready = true)
Jun 22 10:30:39.949: INFO: Pod "annotationupdatea61091d7-90c2-48f6-b828-43ec834a7a5c" satisfied condition "running and ready"
Jun 22 10:30:40.481: INFO: Successfully updated pod "annotationupdatea61091d7-90c2-48f6-b828-43ec834a7a5c"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun 22 10:30:44.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3385" for this suite. 06/22/23 10:30:44.52
------------------------------
• [SLOW TEST] [6.637 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:30:37.893
    Jun 22 10:30:37.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 10:30:37.894
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:30:37.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:30:37.92
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 06/22/23 10:30:37.924
    W0622 10:30:37.937436      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:30:37.937: INFO: Waiting up to 5m0s for pod "annotationupdatea61091d7-90c2-48f6-b828-43ec834a7a5c" in namespace "projected-3385" to be "running and ready"
    Jun 22 10:30:37.942: INFO: Pod "annotationupdatea61091d7-90c2-48f6-b828-43ec834a7a5c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.752812ms
    Jun 22 10:30:37.942: INFO: The phase of Pod annotationupdatea61091d7-90c2-48f6-b828-43ec834a7a5c is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:30:39.949: INFO: Pod "annotationupdatea61091d7-90c2-48f6-b828-43ec834a7a5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.011612234s
    Jun 22 10:30:39.949: INFO: The phase of Pod annotationupdatea61091d7-90c2-48f6-b828-43ec834a7a5c is Running (Ready = true)
    Jun 22 10:30:39.949: INFO: Pod "annotationupdatea61091d7-90c2-48f6-b828-43ec834a7a5c" satisfied condition "running and ready"
    Jun 22 10:30:40.481: INFO: Successfully updated pod "annotationupdatea61091d7-90c2-48f6-b828-43ec834a7a5c"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:30:44.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3385" for this suite. 06/22/23 10:30:44.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:30:44.533
Jun 22 10:30:44.533: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename crd-watch 06/22/23 10:30:44.534
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:30:44.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:30:44.558
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jun 22 10:30:44.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Creating first CR  06/22/23 10:30:47.122
Jun 22 10:30:47.128: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-22T10:30:47Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-22T10:30:47Z]] name:name1 resourceVersion:130953 uid:61c61c59-5fba-4eec-bdef-ab0f613e6fec] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 06/22/23 10:30:57.131
Jun 22 10:30:57.138: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-22T10:30:57Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-22T10:30:57Z]] name:name2 resourceVersion:131004 uid:c87bb8e8-7431-4e7b-b0f4-ea3ff95c804e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 06/22/23 10:31:07.141
Jun 22 10:31:07.149: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-22T10:30:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-22T10:31:07Z]] name:name1 resourceVersion:131040 uid:61c61c59-5fba-4eec-bdef-ab0f613e6fec] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 06/22/23 10:31:17.152
Jun 22 10:31:17.160: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-22T10:30:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-22T10:31:17Z]] name:name2 resourceVersion:131091 uid:c87bb8e8-7431-4e7b-b0f4-ea3ff95c804e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 06/22/23 10:31:27.161
Jun 22 10:31:27.172: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-22T10:30:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-22T10:31:07Z]] name:name1 resourceVersion:131130 uid:61c61c59-5fba-4eec-bdef-ab0f613e6fec] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 06/22/23 10:31:37.175
Jun 22 10:31:37.187: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-22T10:30:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-22T10:31:17Z]] name:name2 resourceVersion:131167 uid:c87bb8e8-7431-4e7b-b0f4-ea3ff95c804e] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:31:47.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-5678" for this suite. 06/22/23 10:31:47.715
------------------------------
• [SLOW TEST] [63.191 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:30:44.533
    Jun 22 10:30:44.533: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename crd-watch 06/22/23 10:30:44.534
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:30:44.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:30:44.558
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jun 22 10:30:44.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Creating first CR  06/22/23 10:30:47.122
    Jun 22 10:30:47.128: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-22T10:30:47Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-22T10:30:47Z]] name:name1 resourceVersion:130953 uid:61c61c59-5fba-4eec-bdef-ab0f613e6fec] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 06/22/23 10:30:57.131
    Jun 22 10:30:57.138: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-22T10:30:57Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-22T10:30:57Z]] name:name2 resourceVersion:131004 uid:c87bb8e8-7431-4e7b-b0f4-ea3ff95c804e] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 06/22/23 10:31:07.141
    Jun 22 10:31:07.149: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-22T10:30:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-22T10:31:07Z]] name:name1 resourceVersion:131040 uid:61c61c59-5fba-4eec-bdef-ab0f613e6fec] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 06/22/23 10:31:17.152
    Jun 22 10:31:17.160: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-22T10:30:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-22T10:31:17Z]] name:name2 resourceVersion:131091 uid:c87bb8e8-7431-4e7b-b0f4-ea3ff95c804e] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 06/22/23 10:31:27.161
    Jun 22 10:31:27.172: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-22T10:30:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-22T10:31:07Z]] name:name1 resourceVersion:131130 uid:61c61c59-5fba-4eec-bdef-ab0f613e6fec] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 06/22/23 10:31:37.175
    Jun 22 10:31:37.187: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-22T10:30:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-22T10:31:17Z]] name:name2 resourceVersion:131167 uid:c87bb8e8-7431-4e7b-b0f4-ea3ff95c804e] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:31:47.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-5678" for this suite. 06/22/23 10:31:47.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:31:47.728
Jun 22 10:31:47.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename resourcequota 06/22/23 10:31:47.73
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:31:47.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:31:47.757
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 06/22/23 10:31:47.761
STEP: Getting a ResourceQuota 06/22/23 10:31:47.767
STEP: Listing all ResourceQuotas with LabelSelector 06/22/23 10:31:47.771
STEP: Patching the ResourceQuota 06/22/23 10:31:47.776
STEP: Deleting a Collection of ResourceQuotas 06/22/23 10:31:47.786
STEP: Verifying the deleted ResourceQuota 06/22/23 10:31:47.795
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun 22 10:31:47.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4772" for this suite. 06/22/23 10:31:47.814
------------------------------
• [0.093 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:31:47.728
    Jun 22 10:31:47.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename resourcequota 06/22/23 10:31:47.73
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:31:47.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:31:47.757
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 06/22/23 10:31:47.761
    STEP: Getting a ResourceQuota 06/22/23 10:31:47.767
    STEP: Listing all ResourceQuotas with LabelSelector 06/22/23 10:31:47.771
    STEP: Patching the ResourceQuota 06/22/23 10:31:47.776
    STEP: Deleting a Collection of ResourceQuotas 06/22/23 10:31:47.786
    STEP: Verifying the deleted ResourceQuota 06/22/23 10:31:47.795
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:31:47.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4772" for this suite. 06/22/23 10:31:47.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:31:47.822
Jun 22 10:31:47.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 10:31:47.824
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:31:47.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:31:47.849
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 06/22/23 10:31:47.854
W0622 10:31:47.867264      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:31:47.867: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6acd0f1b-b719-4841-8baf-7ed1dee5f86e" in namespace "projected-4005" to be "Succeeded or Failed"
Jun 22 10:31:47.871: INFO: Pod "downwardapi-volume-6acd0f1b-b719-4841-8baf-7ed1dee5f86e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.906279ms
Jun 22 10:31:49.878: INFO: Pod "downwardapi-volume-6acd0f1b-b719-4841-8baf-7ed1dee5f86e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011033154s
Jun 22 10:31:51.878: INFO: Pod "downwardapi-volume-6acd0f1b-b719-4841-8baf-7ed1dee5f86e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01146257s
STEP: Saw pod success 06/22/23 10:31:51.879
Jun 22 10:31:51.879: INFO: Pod "downwardapi-volume-6acd0f1b-b719-4841-8baf-7ed1dee5f86e" satisfied condition "Succeeded or Failed"
Jun 22 10:31:51.884: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-6acd0f1b-b719-4841-8baf-7ed1dee5f86e container client-container: <nil>
STEP: delete the pod 06/22/23 10:31:51.892
Jun 22 10:31:51.907: INFO: Waiting for pod downwardapi-volume-6acd0f1b-b719-4841-8baf-7ed1dee5f86e to disappear
Jun 22 10:31:51.911: INFO: Pod downwardapi-volume-6acd0f1b-b719-4841-8baf-7ed1dee5f86e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun 22 10:31:51.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4005" for this suite. 06/22/23 10:31:51.919
------------------------------
• [4.103 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:31:47.822
    Jun 22 10:31:47.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 10:31:47.824
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:31:47.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:31:47.849
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 06/22/23 10:31:47.854
    W0622 10:31:47.867264      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:31:47.867: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6acd0f1b-b719-4841-8baf-7ed1dee5f86e" in namespace "projected-4005" to be "Succeeded or Failed"
    Jun 22 10:31:47.871: INFO: Pod "downwardapi-volume-6acd0f1b-b719-4841-8baf-7ed1dee5f86e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.906279ms
    Jun 22 10:31:49.878: INFO: Pod "downwardapi-volume-6acd0f1b-b719-4841-8baf-7ed1dee5f86e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011033154s
    Jun 22 10:31:51.878: INFO: Pod "downwardapi-volume-6acd0f1b-b719-4841-8baf-7ed1dee5f86e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01146257s
    STEP: Saw pod success 06/22/23 10:31:51.879
    Jun 22 10:31:51.879: INFO: Pod "downwardapi-volume-6acd0f1b-b719-4841-8baf-7ed1dee5f86e" satisfied condition "Succeeded or Failed"
    Jun 22 10:31:51.884: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-6acd0f1b-b719-4841-8baf-7ed1dee5f86e container client-container: <nil>
    STEP: delete the pod 06/22/23 10:31:51.892
    Jun 22 10:31:51.907: INFO: Waiting for pod downwardapi-volume-6acd0f1b-b719-4841-8baf-7ed1dee5f86e to disappear
    Jun 22 10:31:51.911: INFO: Pod downwardapi-volume-6acd0f1b-b719-4841-8baf-7ed1dee5f86e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:31:51.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4005" for this suite. 06/22/23 10:31:51.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:31:51.928
Jun 22 10:31:51.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename services 06/22/23 10:31:51.929
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:31:51.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:31:51.954
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-8961 06/22/23 10:31:51.958
STEP: creating replication controller nodeport-test in namespace services-8961 06/22/23 10:31:51.977
W0622 10:31:51.991115      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nodeport-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nodeport-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nodeport-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nodeport-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0622 10:31:51.992430      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8961, replica count: 2
I0622 10:31:55.044106      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 10:31:55.044: INFO: Creating new exec pod
W0622 10:31:55.055935      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:31:55.056: INFO: Waiting up to 5m0s for pod "execpod5hfbz" in namespace "services-8961" to be "running"
Jun 22 10:31:55.063: INFO: Pod "execpod5hfbz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.883459ms
Jun 22 10:31:57.069: INFO: Pod "execpod5hfbz": Phase="Running", Reason="", readiness=true. Elapsed: 2.013362101s
Jun 22 10:31:57.069: INFO: Pod "execpod5hfbz" satisfied condition "running"
Jun 22 10:31:58.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-8961 exec execpod5hfbz -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Jun 22 10:31:58.559: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 22 10:31:58.559: INFO: stdout: ""
Jun 22 10:31:58.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-8961 exec execpod5hfbz -- /bin/sh -x -c nc -v -z -w 2 100.65.4.105 80'
Jun 22 10:31:58.752: INFO: stderr: "+ nc -v -z -w 2 100.65.4.105 80\nConnection to 100.65.4.105 80 port [tcp/http] succeeded!\n"
Jun 22 10:31:58.752: INFO: stdout: ""
Jun 22 10:31:58.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-8961 exec execpod5hfbz -- /bin/sh -x -c nc -v -z -w 2 10.92.224.62 31607'
Jun 22 10:31:58.931: INFO: stderr: "+ nc -v -z -w 2 10.92.224.62 31607\nConnection to 10.92.224.62 31607 port [tcp/*] succeeded!\n"
Jun 22 10:31:58.931: INFO: stdout: ""
Jun 22 10:31:58.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-8961 exec execpod5hfbz -- /bin/sh -x -c nc -v -z -w 2 10.92.224.179 31607'
Jun 22 10:31:59.114: INFO: stderr: "+ nc -v -z -w 2 10.92.224.179 31607\nConnection to 10.92.224.179 31607 port [tcp/*] succeeded!\n"
Jun 22 10:31:59.114: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun 22 10:31:59.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8961" for this suite. 06/22/23 10:31:59.123
------------------------------
• [SLOW TEST] [7.203 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:31:51.928
    Jun 22 10:31:51.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename services 06/22/23 10:31:51.929
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:31:51.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:31:51.954
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-8961 06/22/23 10:31:51.958
    STEP: creating replication controller nodeport-test in namespace services-8961 06/22/23 10:31:51.977
    W0622 10:31:51.991115      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nodeport-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nodeport-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nodeport-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nodeport-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0622 10:31:51.992430      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8961, replica count: 2
    I0622 10:31:55.044106      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 22 10:31:55.044: INFO: Creating new exec pod
    W0622 10:31:55.055935      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:31:55.056: INFO: Waiting up to 5m0s for pod "execpod5hfbz" in namespace "services-8961" to be "running"
    Jun 22 10:31:55.063: INFO: Pod "execpod5hfbz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.883459ms
    Jun 22 10:31:57.069: INFO: Pod "execpod5hfbz": Phase="Running", Reason="", readiness=true. Elapsed: 2.013362101s
    Jun 22 10:31:57.069: INFO: Pod "execpod5hfbz" satisfied condition "running"
    Jun 22 10:31:58.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-8961 exec execpod5hfbz -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Jun 22 10:31:58.559: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 22 10:31:58.559: INFO: stdout: ""
    Jun 22 10:31:58.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-8961 exec execpod5hfbz -- /bin/sh -x -c nc -v -z -w 2 100.65.4.105 80'
    Jun 22 10:31:58.752: INFO: stderr: "+ nc -v -z -w 2 100.65.4.105 80\nConnection to 100.65.4.105 80 port [tcp/http] succeeded!\n"
    Jun 22 10:31:58.752: INFO: stdout: ""
    Jun 22 10:31:58.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-8961 exec execpod5hfbz -- /bin/sh -x -c nc -v -z -w 2 10.92.224.62 31607'
    Jun 22 10:31:58.931: INFO: stderr: "+ nc -v -z -w 2 10.92.224.62 31607\nConnection to 10.92.224.62 31607 port [tcp/*] succeeded!\n"
    Jun 22 10:31:58.931: INFO: stdout: ""
    Jun 22 10:31:58.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-8961 exec execpod5hfbz -- /bin/sh -x -c nc -v -z -w 2 10.92.224.179 31607'
    Jun 22 10:31:59.114: INFO: stderr: "+ nc -v -z -w 2 10.92.224.179 31607\nConnection to 10.92.224.179 31607 port [tcp/*] succeeded!\n"
    Jun 22 10:31:59.114: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:31:59.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8961" for this suite. 06/22/23 10:31:59.123
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:31:59.14
Jun 22 10:31:59.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename daemonsets 06/22/23 10:31:59.142
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:31:59.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:31:59.169
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 06/22/23 10:31:59.208
W0622 10:31:59.218581      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Check that daemon pods launch on every node of the cluster. 06/22/23 10:31:59.218
Jun 22 10:31:59.225: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:31:59.225: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:31:59.226: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:31:59.230: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 10:31:59.230: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
Jun 22 10:32:00.242: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:32:00.242: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:32:00.242: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:32:00.247: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 10:32:00.247: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
Jun 22 10:32:01.239: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:32:01.239: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:32:01.239: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:32:01.244: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 22 10:32:01.244: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 06/22/23 10:32:01.249
STEP: DeleteCollection of the DaemonSets 06/22/23 10:32:01.254
STEP: Verify that ReplicaSets have been deleted 06/22/23 10:32:01.264
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Jun 22 10:32:01.279: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"131395"},"items":null}

Jun 22 10:32:01.286: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"131395"},"items":[{"metadata":{"name":"daemon-set-4vw5v","generateName":"daemon-set-","namespace":"daemonsets-3214","uid":"28a7c0af-a9f4-4a26-a47e-7d51fd981c80","resourceVersion":"131384","creationTimestamp":"2023-06-22T10:31:59Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d83ad024-68cd-4894-8cfe-dfcbd08465d3","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-22T10:31:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d83ad024-68cd-4894-8cfe-dfcbd08465d3\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-22T10:32:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.4.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xvg2v","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xvg2v","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:31:59Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:32:00Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:32:00Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:31:59Z"}],"hostIP":"10.92.224.179","podIP":"100.96.4.179","podIPs":[{"ip":"100.96.4.179"}],"startTime":"2023-06-22T10:31:59Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-22T10:31:59Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://2f192bf2d967d0b53e294a2f3846a4038767341d17d2418486d68dcc8a08b22c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tzv8g","generateName":"daemon-set-","namespace":"daemonsets-3214","uid":"55428b90-9925-453b-8d55-07ad34013790","resourceVersion":"131386","creationTimestamp":"2023-06-22T10:31:59Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d83ad024-68cd-4894-8cfe-dfcbd08465d3","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-22T10:31:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d83ad024-68cd-4894-8cfe-dfcbd08465d3\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-22T10:32:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-c7hk9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-c7hk9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:31:59Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:32:00Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:32:00Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:31:59Z"}],"hostIP":"10.92.224.62","podIP":"100.96.2.196","podIPs":[{"ip":"100.96.2.196"}],"startTime":"2023-06-22T10:31:59Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-22T10:31:59Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://12659b480da79c4944fc999c87abfd8639f9a1287ee2c1a781b80ddb2db21edc","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-v8q2l","generateName":"daemon-set-","namespace":"daemonsets-3214","uid":"9fbc27ec-5dc0-459f-9ce1-7c5a02c59f9a","resourceVersion":"131393","creationTimestamp":"2023-06-22T10:31:59Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d83ad024-68cd-4894-8cfe-dfcbd08465d3","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-22T10:31:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d83ad024-68cd-4894-8cfe-dfcbd08465d3\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-22T10:32:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-p9wkf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-p9wkf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:31:59Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:32:00Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:32:00Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:31:59Z"}],"hostIP":"10.92.226.162","podIP":"100.96.3.4","podIPs":[{"ip":"100.96.3.4"}],"startTime":"2023-06-22T10:31:59Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-22T10:32:00Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://899c93b511700190555e083748eb0e1ddca9d561f326d04ab50e4a72bde189b5","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:32:01.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3214" for this suite. 06/22/23 10:32:01.314
------------------------------
• [2.186 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:31:59.14
    Jun 22 10:31:59.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename daemonsets 06/22/23 10:31:59.142
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:31:59.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:31:59.169
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 06/22/23 10:31:59.208
    W0622 10:31:59.218581      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Check that daemon pods launch on every node of the cluster. 06/22/23 10:31:59.218
    Jun 22 10:31:59.225: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:31:59.225: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:31:59.226: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:31:59.230: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 10:31:59.230: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
    Jun 22 10:32:00.242: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:32:00.242: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:32:00.242: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:32:00.247: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 10:32:00.247: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
    Jun 22 10:32:01.239: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:32:01.239: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:32:01.239: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:32:01.244: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 22 10:32:01.244: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 06/22/23 10:32:01.249
    STEP: DeleteCollection of the DaemonSets 06/22/23 10:32:01.254
    STEP: Verify that ReplicaSets have been deleted 06/22/23 10:32:01.264
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Jun 22 10:32:01.279: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"131395"},"items":null}

    Jun 22 10:32:01.286: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"131395"},"items":[{"metadata":{"name":"daemon-set-4vw5v","generateName":"daemon-set-","namespace":"daemonsets-3214","uid":"28a7c0af-a9f4-4a26-a47e-7d51fd981c80","resourceVersion":"131384","creationTimestamp":"2023-06-22T10:31:59Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d83ad024-68cd-4894-8cfe-dfcbd08465d3","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-22T10:31:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d83ad024-68cd-4894-8cfe-dfcbd08465d3\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-22T10:32:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.4.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xvg2v","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xvg2v","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:31:59Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:32:00Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:32:00Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:31:59Z"}],"hostIP":"10.92.224.179","podIP":"100.96.4.179","podIPs":[{"ip":"100.96.4.179"}],"startTime":"2023-06-22T10:31:59Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-22T10:31:59Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://2f192bf2d967d0b53e294a2f3846a4038767341d17d2418486d68dcc8a08b22c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tzv8g","generateName":"daemon-set-","namespace":"daemonsets-3214","uid":"55428b90-9925-453b-8d55-07ad34013790","resourceVersion":"131386","creationTimestamp":"2023-06-22T10:31:59Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d83ad024-68cd-4894-8cfe-dfcbd08465d3","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-22T10:31:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d83ad024-68cd-4894-8cfe-dfcbd08465d3\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-22T10:32:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-c7hk9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-c7hk9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:31:59Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:32:00Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:32:00Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:31:59Z"}],"hostIP":"10.92.224.62","podIP":"100.96.2.196","podIPs":[{"ip":"100.96.2.196"}],"startTime":"2023-06-22T10:31:59Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-22T10:31:59Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://12659b480da79c4944fc999c87abfd8639f9a1287ee2c1a781b80ddb2db21edc","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-v8q2l","generateName":"daemon-set-","namespace":"daemonsets-3214","uid":"9fbc27ec-5dc0-459f-9ce1-7c5a02c59f9a","resourceVersion":"131393","creationTimestamp":"2023-06-22T10:31:59Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"d83ad024-68cd-4894-8cfe-dfcbd08465d3","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-22T10:31:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d83ad024-68cd-4894-8cfe-dfcbd08465d3\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-22T10:32:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-p9wkf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-p9wkf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:31:59Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:32:00Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:32:00Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-22T10:31:59Z"}],"hostIP":"10.92.226.162","podIP":"100.96.3.4","podIPs":[{"ip":"100.96.3.4"}],"startTime":"2023-06-22T10:31:59Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-22T10:32:00Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://899c93b511700190555e083748eb0e1ddca9d561f326d04ab50e4a72bde189b5","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:32:01.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3214" for this suite. 06/22/23 10:32:01.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:32:01.328
Jun 22 10:32:01.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubectl 06/22/23 10:32:01.33
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:01.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:01.356
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 06/22/23 10:32:01.361
Jun 22 10:32:01.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-1159 cluster-info'
Jun 22 10:32:01.463: INFO: stderr: ""
Jun 22 10:32:01.463: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun 22 10:32:01.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1159" for this suite. 06/22/23 10:32:01.472
------------------------------
• [0.157 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:32:01.328
    Jun 22 10:32:01.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubectl 06/22/23 10:32:01.33
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:01.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:01.356
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 06/22/23 10:32:01.361
    Jun 22 10:32:01.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-1159 cluster-info'
    Jun 22 10:32:01.463: INFO: stderr: ""
    Jun 22 10:32:01.463: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://100.64.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:32:01.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1159" for this suite. 06/22/23 10:32:01.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:32:01.486
Jun 22 10:32:01.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename replication-controller 06/22/23 10:32:01.487
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:01.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:01.511
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Jun 22 10:32:01.515: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 06/22/23 10:32:02.531
W0622 10:32:02.540342      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Checking rc "condition-test" has the desired failure condition set 06/22/23 10:32:02.54
STEP: Scaling down rc "condition-test" to satisfy pod quota 06/22/23 10:32:03.551
W0622 10:32:03.567195      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:32:03.567: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 06/22/23 10:32:03.567
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jun 22 10:32:04.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8601" for this suite. 06/22/23 10:32:04.587
------------------------------
• [3.109 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:32:01.486
    Jun 22 10:32:01.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename replication-controller 06/22/23 10:32:01.487
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:01.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:01.511
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Jun 22 10:32:01.515: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 06/22/23 10:32:02.531
    W0622 10:32:02.540342      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Checking rc "condition-test" has the desired failure condition set 06/22/23 10:32:02.54
    STEP: Scaling down rc "condition-test" to satisfy pod quota 06/22/23 10:32:03.551
    W0622 10:32:03.567195      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:32:03.567: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 06/22/23 10:32:03.567
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:32:04.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8601" for this suite. 06/22/23 10:32:04.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:32:04.596
Jun 22 10:32:04.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename secrets 06/22/23 10:32:04.598
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:04.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:04.624
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-e0d338b2-dc45-4e76-ba2b-19763316e127 06/22/23 10:32:04.628
STEP: Creating a pod to test consume secrets 06/22/23 10:32:04.634
W0622 10:32:04.646294      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:32:04.646: INFO: Waiting up to 5m0s for pod "pod-secrets-9a117ee4-df02-425b-9cf1-56ff1663f068" in namespace "secrets-6238" to be "Succeeded or Failed"
Jun 22 10:32:04.650: INFO: Pod "pod-secrets-9a117ee4-df02-425b-9cf1-56ff1663f068": Phase="Pending", Reason="", readiness=false. Elapsed: 3.657068ms
Jun 22 10:32:06.657: INFO: Pod "pod-secrets-9a117ee4-df02-425b-9cf1-56ff1663f068": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010987195s
Jun 22 10:32:08.656: INFO: Pod "pod-secrets-9a117ee4-df02-425b-9cf1-56ff1663f068": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010299776s
STEP: Saw pod success 06/22/23 10:32:08.656
Jun 22 10:32:08.657: INFO: Pod "pod-secrets-9a117ee4-df02-425b-9cf1-56ff1663f068" satisfied condition "Succeeded or Failed"
Jun 22 10:32:08.661: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-secrets-9a117ee4-df02-425b-9cf1-56ff1663f068 container secret-volume-test: <nil>
STEP: delete the pod 06/22/23 10:32:08.67
Jun 22 10:32:08.682: INFO: Waiting for pod pod-secrets-9a117ee4-df02-425b-9cf1-56ff1663f068 to disappear
Jun 22 10:32:08.685: INFO: Pod pod-secrets-9a117ee4-df02-425b-9cf1-56ff1663f068 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun 22 10:32:08.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6238" for this suite. 06/22/23 10:32:08.692
------------------------------
• [4.102 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:32:04.596
    Jun 22 10:32:04.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename secrets 06/22/23 10:32:04.598
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:04.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:04.624
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-e0d338b2-dc45-4e76-ba2b-19763316e127 06/22/23 10:32:04.628
    STEP: Creating a pod to test consume secrets 06/22/23 10:32:04.634
    W0622 10:32:04.646294      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:32:04.646: INFO: Waiting up to 5m0s for pod "pod-secrets-9a117ee4-df02-425b-9cf1-56ff1663f068" in namespace "secrets-6238" to be "Succeeded or Failed"
    Jun 22 10:32:04.650: INFO: Pod "pod-secrets-9a117ee4-df02-425b-9cf1-56ff1663f068": Phase="Pending", Reason="", readiness=false. Elapsed: 3.657068ms
    Jun 22 10:32:06.657: INFO: Pod "pod-secrets-9a117ee4-df02-425b-9cf1-56ff1663f068": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010987195s
    Jun 22 10:32:08.656: INFO: Pod "pod-secrets-9a117ee4-df02-425b-9cf1-56ff1663f068": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010299776s
    STEP: Saw pod success 06/22/23 10:32:08.656
    Jun 22 10:32:08.657: INFO: Pod "pod-secrets-9a117ee4-df02-425b-9cf1-56ff1663f068" satisfied condition "Succeeded or Failed"
    Jun 22 10:32:08.661: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-secrets-9a117ee4-df02-425b-9cf1-56ff1663f068 container secret-volume-test: <nil>
    STEP: delete the pod 06/22/23 10:32:08.67
    Jun 22 10:32:08.682: INFO: Waiting for pod pod-secrets-9a117ee4-df02-425b-9cf1-56ff1663f068 to disappear
    Jun 22 10:32:08.685: INFO: Pod pod-secrets-9a117ee4-df02-425b-9cf1-56ff1663f068 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:32:08.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6238" for this suite. 06/22/23 10:32:08.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:32:08.701
Jun 22 10:32:08.701: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename webhook 06/22/23 10:32:08.702
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:08.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:08.726
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/22/23 10:32:08.746
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 10:32:09.465
STEP: Deploying the webhook pod 06/22/23 10:32:09.476
W0622 10:32:09.493653      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 10:32:09.493
Jun 22 10:32:09.504: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/22/23 10:32:11.517
STEP: Verifying the service has paired with the endpoint 06/22/23 10:32:11.532
Jun 22 10:32:12.532: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 06/22/23 10:32:12.538
STEP: Creating a custom resource definition that should be denied by the webhook 06/22/23 10:32:12.566
Jun 22 10:32:12.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:32:12.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6934" for this suite. 06/22/23 10:32:12.7
STEP: Destroying namespace "webhook-6934-markers" for this suite. 06/22/23 10:32:12.707
------------------------------
• [4.013 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:32:08.701
    Jun 22 10:32:08.701: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename webhook 06/22/23 10:32:08.702
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:08.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:08.726
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/22/23 10:32:08.746
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 10:32:09.465
    STEP: Deploying the webhook pod 06/22/23 10:32:09.476
    W0622 10:32:09.493653      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 10:32:09.493
    Jun 22 10:32:09.504: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/22/23 10:32:11.517
    STEP: Verifying the service has paired with the endpoint 06/22/23 10:32:11.532
    Jun 22 10:32:12.532: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 06/22/23 10:32:12.538
    STEP: Creating a custom resource definition that should be denied by the webhook 06/22/23 10:32:12.566
    Jun 22 10:32:12.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:32:12.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6934" for this suite. 06/22/23 10:32:12.7
    STEP: Destroying namespace "webhook-6934-markers" for this suite. 06/22/23 10:32:12.707
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:32:12.718
Jun 22 10:32:12.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubectl 06/22/23 10:32:12.719
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:12.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:12.744
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/22/23 10:32:12.748
Jun 22 10:32:12.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9094 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun 22 10:32:12.838: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"e2e-test-httpd-pod\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"e2e-test-httpd-pod\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"e2e-test-httpd-pod\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"e2e-test-httpd-pod\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Jun 22 10:32:12.838: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 06/22/23 10:32:12.838
Jun 22 10:32:12.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9094 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Jun 22 10:32:14.286: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"e2e-test-httpd-pod\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"e2e-test-httpd-pod\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"e2e-test-httpd-pod\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"e2e-test-httpd-pod\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Jun 22 10:32:14.286: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/22/23 10:32:14.286
Jun 22 10:32:14.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9094 delete pods e2e-test-httpd-pod'
Jun 22 10:32:16.579: INFO: stderr: ""
Jun 22 10:32:16.579: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun 22 10:32:16.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9094" for this suite. 06/22/23 10:32:16.586
------------------------------
• [3.877 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:32:12.718
    Jun 22 10:32:12.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubectl 06/22/23 10:32:12.719
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:12.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:12.744
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/22/23 10:32:12.748
    Jun 22 10:32:12.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9094 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jun 22 10:32:12.838: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"e2e-test-httpd-pod\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"e2e-test-httpd-pod\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"e2e-test-httpd-pod\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"e2e-test-httpd-pod\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
    Jun 22 10:32:12.838: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 06/22/23 10:32:12.838
    Jun 22 10:32:12.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9094 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Jun 22 10:32:14.286: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"e2e-test-httpd-pod\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"e2e-test-httpd-pod\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"e2e-test-httpd-pod\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"e2e-test-httpd-pod\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
    Jun 22 10:32:14.286: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/22/23 10:32:14.286
    Jun 22 10:32:14.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9094 delete pods e2e-test-httpd-pod'
    Jun 22 10:32:16.579: INFO: stderr: ""
    Jun 22 10:32:16.579: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:32:16.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9094" for this suite. 06/22/23 10:32:16.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:32:16.595
Jun 22 10:32:16.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename secrets 06/22/23 10:32:16.597
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:16.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:16.623
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-316695c2-07cf-4970-89b4-c80b7d0194df 06/22/23 10:32:16.63
STEP: Creating a pod to test consume secrets 06/22/23 10:32:16.638
W0622 10:32:16.650207      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:32:16.650: INFO: Waiting up to 5m0s for pod "pod-secrets-c6b9a013-9a42-4c4f-a618-d6e3b14b24cf" in namespace "secrets-3301" to be "Succeeded or Failed"
Jun 22 10:32:16.655: INFO: Pod "pod-secrets-c6b9a013-9a42-4c4f-a618-d6e3b14b24cf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.121513ms
Jun 22 10:32:18.661: INFO: Pod "pod-secrets-c6b9a013-9a42-4c4f-a618-d6e3b14b24cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010955635s
Jun 22 10:32:20.662: INFO: Pod "pod-secrets-c6b9a013-9a42-4c4f-a618-d6e3b14b24cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011869527s
STEP: Saw pod success 06/22/23 10:32:20.662
Jun 22 10:32:20.662: INFO: Pod "pod-secrets-c6b9a013-9a42-4c4f-a618-d6e3b14b24cf" satisfied condition "Succeeded or Failed"
Jun 22 10:32:20.667: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-secrets-c6b9a013-9a42-4c4f-a618-d6e3b14b24cf container secret-volume-test: <nil>
STEP: delete the pod 06/22/23 10:32:20.676
Jun 22 10:32:20.693: INFO: Waiting for pod pod-secrets-c6b9a013-9a42-4c4f-a618-d6e3b14b24cf to disappear
Jun 22 10:32:20.697: INFO: Pod pod-secrets-c6b9a013-9a42-4c4f-a618-d6e3b14b24cf no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun 22 10:32:20.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3301" for this suite. 06/22/23 10:32:20.705
------------------------------
• [4.120 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:32:16.595
    Jun 22 10:32:16.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename secrets 06/22/23 10:32:16.597
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:16.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:16.623
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-316695c2-07cf-4970-89b4-c80b7d0194df 06/22/23 10:32:16.63
    STEP: Creating a pod to test consume secrets 06/22/23 10:32:16.638
    W0622 10:32:16.650207      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:32:16.650: INFO: Waiting up to 5m0s for pod "pod-secrets-c6b9a013-9a42-4c4f-a618-d6e3b14b24cf" in namespace "secrets-3301" to be "Succeeded or Failed"
    Jun 22 10:32:16.655: INFO: Pod "pod-secrets-c6b9a013-9a42-4c4f-a618-d6e3b14b24cf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.121513ms
    Jun 22 10:32:18.661: INFO: Pod "pod-secrets-c6b9a013-9a42-4c4f-a618-d6e3b14b24cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010955635s
    Jun 22 10:32:20.662: INFO: Pod "pod-secrets-c6b9a013-9a42-4c4f-a618-d6e3b14b24cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011869527s
    STEP: Saw pod success 06/22/23 10:32:20.662
    Jun 22 10:32:20.662: INFO: Pod "pod-secrets-c6b9a013-9a42-4c4f-a618-d6e3b14b24cf" satisfied condition "Succeeded or Failed"
    Jun 22 10:32:20.667: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-secrets-c6b9a013-9a42-4c4f-a618-d6e3b14b24cf container secret-volume-test: <nil>
    STEP: delete the pod 06/22/23 10:32:20.676
    Jun 22 10:32:20.693: INFO: Waiting for pod pod-secrets-c6b9a013-9a42-4c4f-a618-d6e3b14b24cf to disappear
    Jun 22 10:32:20.697: INFO: Pod pod-secrets-c6b9a013-9a42-4c4f-a618-d6e3b14b24cf no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:32:20.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3301" for this suite. 06/22/23 10:32:20.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:32:20.717
Jun 22 10:32:20.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 06/22/23 10:32:20.719
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:20.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:20.744
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 06/22/23 10:32:20.748
STEP: Creating hostNetwork=false pod 06/22/23 10:32:20.748
W0622 10:32:20.763257      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "busybox-1", "busybox-2", "busybox-3" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "busybox-1", "busybox-2", "busybox-3" must set securityContext.capabilities.drop=["ALL"]), restricted volume types (volume "host-etc-hosts" uses restricted volume type "hostPath"), runAsNonRoot != true (pod or containers "busybox-1", "busybox-2", "busybox-3" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "busybox-1", "busybox-2", "busybox-3" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:32:20.763: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-6731" to be "running and ready"
Jun 22 10:32:20.768: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.064659ms
Jun 22 10:32:20.768: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:32:22.774: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011266405s
Jun 22 10:32:22.774: INFO: The phase of Pod test-pod is Running (Ready = true)
Jun 22 10:32:22.774: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 06/22/23 10:32:22.779
W0622 10:32:22.789058      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": host namespaces (hostNetwork=true), allowPrivilegeEscalation != false (containers "busybox-1", "busybox-2" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "busybox-1", "busybox-2" must set securityContext.capabilities.drop=["ALL"]), restricted volume types (volume "host-etc-hosts" uses restricted volume type "hostPath"), runAsNonRoot != true (pod or containers "busybox-1", "busybox-2" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "busybox-1", "busybox-2" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:32:22.789: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-6731" to be "running and ready"
Jun 22 10:32:22.793: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.084217ms
Jun 22 10:32:22.793: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:32:24.799: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010207164s
Jun 22 10:32:24.799: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jun 22 10:32:24.799: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 06/22/23 10:32:24.803
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 06/22/23 10:32:24.803
Jun 22 10:32:24.804: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:32:24.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:32:24.805: INFO: ExecWithOptions: Clientset creation
Jun 22 10:32:24.805: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 22 10:32:24.902: INFO: Exec stderr: ""
Jun 22 10:32:24.902: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:32:24.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:32:24.903: INFO: ExecWithOptions: Clientset creation
Jun 22 10:32:24.903: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 22 10:32:25.004: INFO: Exec stderr: ""
Jun 22 10:32:25.005: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:32:25.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:32:25.005: INFO: ExecWithOptions: Clientset creation
Jun 22 10:32:25.005: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 22 10:32:25.104: INFO: Exec stderr: ""
Jun 22 10:32:25.104: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:32:25.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:32:25.104: INFO: ExecWithOptions: Clientset creation
Jun 22 10:32:25.105: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 22 10:32:25.195: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 06/22/23 10:32:25.196
Jun 22 10:32:25.196: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:32:25.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:32:25.197: INFO: ExecWithOptions: Clientset creation
Jun 22 10:32:25.197: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jun 22 10:32:25.285: INFO: Exec stderr: ""
Jun 22 10:32:25.285: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:32:25.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:32:25.286: INFO: ExecWithOptions: Clientset creation
Jun 22 10:32:25.286: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jun 22 10:32:25.373: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 06/22/23 10:32:25.373
Jun 22 10:32:25.373: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:32:25.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:32:25.374: INFO: ExecWithOptions: Clientset creation
Jun 22 10:32:25.374: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 22 10:32:25.500: INFO: Exec stderr: ""
Jun 22 10:32:25.500: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:32:25.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:32:25.501: INFO: ExecWithOptions: Clientset creation
Jun 22 10:32:25.501: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 22 10:32:25.609: INFO: Exec stderr: ""
Jun 22 10:32:25.609: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:32:25.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:32:25.610: INFO: ExecWithOptions: Clientset creation
Jun 22 10:32:25.610: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 22 10:32:25.707: INFO: Exec stderr: ""
Jun 22 10:32:25.708: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:32:25.708: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:32:25.708: INFO: ExecWithOptions: Clientset creation
Jun 22 10:32:25.709: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 22 10:32:25.809: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Jun 22 10:32:25.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6731" for this suite. 06/22/23 10:32:25.816
------------------------------
• [SLOW TEST] [5.106 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:32:20.717
    Jun 22 10:32:20.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 06/22/23 10:32:20.719
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:20.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:20.744
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 06/22/23 10:32:20.748
    STEP: Creating hostNetwork=false pod 06/22/23 10:32:20.748
    W0622 10:32:20.763257      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "busybox-1", "busybox-2", "busybox-3" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "busybox-1", "busybox-2", "busybox-3" must set securityContext.capabilities.drop=["ALL"]), restricted volume types (volume "host-etc-hosts" uses restricted volume type "hostPath"), runAsNonRoot != true (pod or containers "busybox-1", "busybox-2", "busybox-3" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "busybox-1", "busybox-2", "busybox-3" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:32:20.763: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-6731" to be "running and ready"
    Jun 22 10:32:20.768: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.064659ms
    Jun 22 10:32:20.768: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:32:22.774: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011266405s
    Jun 22 10:32:22.774: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jun 22 10:32:22.774: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 06/22/23 10:32:22.779
    W0622 10:32:22.789058      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": host namespaces (hostNetwork=true), allowPrivilegeEscalation != false (containers "busybox-1", "busybox-2" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "busybox-1", "busybox-2" must set securityContext.capabilities.drop=["ALL"]), restricted volume types (volume "host-etc-hosts" uses restricted volume type "hostPath"), runAsNonRoot != true (pod or containers "busybox-1", "busybox-2" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "busybox-1", "busybox-2" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:32:22.789: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-6731" to be "running and ready"
    Jun 22 10:32:22.793: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.084217ms
    Jun 22 10:32:22.793: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:32:24.799: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010207164s
    Jun 22 10:32:24.799: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jun 22 10:32:24.799: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 06/22/23 10:32:24.803
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 06/22/23 10:32:24.803
    Jun 22 10:32:24.804: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:32:24.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:32:24.805: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:32:24.805: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun 22 10:32:24.902: INFO: Exec stderr: ""
    Jun 22 10:32:24.902: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:32:24.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:32:24.903: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:32:24.903: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun 22 10:32:25.004: INFO: Exec stderr: ""
    Jun 22 10:32:25.005: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:32:25.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:32:25.005: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:32:25.005: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun 22 10:32:25.104: INFO: Exec stderr: ""
    Jun 22 10:32:25.104: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:32:25.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:32:25.104: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:32:25.105: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun 22 10:32:25.195: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 06/22/23 10:32:25.196
    Jun 22 10:32:25.196: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:32:25.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:32:25.197: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:32:25.197: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jun 22 10:32:25.285: INFO: Exec stderr: ""
    Jun 22 10:32:25.285: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:32:25.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:32:25.286: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:32:25.286: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jun 22 10:32:25.373: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 06/22/23 10:32:25.373
    Jun 22 10:32:25.373: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:32:25.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:32:25.374: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:32:25.374: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun 22 10:32:25.500: INFO: Exec stderr: ""
    Jun 22 10:32:25.500: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:32:25.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:32:25.501: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:32:25.501: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun 22 10:32:25.609: INFO: Exec stderr: ""
    Jun 22 10:32:25.609: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:32:25.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:32:25.610: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:32:25.610: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun 22 10:32:25.707: INFO: Exec stderr: ""
    Jun 22 10:32:25.708: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6731 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:32:25.708: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:32:25.708: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:32:25.709: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6731/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun 22 10:32:25.809: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:32:25.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-6731" for this suite. 06/22/23 10:32:25.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:32:25.824
Jun 22 10:32:25.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename secrets 06/22/23 10:32:25.825
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:25.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:25.853
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-216d664f-c1b7-444c-bd6f-8e151fbce061 06/22/23 10:32:25.857
STEP: Creating a pod to test consume secrets 06/22/23 10:32:25.863
W0622 10:32:25.876442      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:32:25.876: INFO: Waiting up to 5m0s for pod "pod-secrets-39a592f3-ede3-4ceb-b5f6-793543cb7c79" in namespace "secrets-5296" to be "Succeeded or Failed"
Jun 22 10:32:25.880: INFO: Pod "pod-secrets-39a592f3-ede3-4ceb-b5f6-793543cb7c79": Phase="Pending", Reason="", readiness=false. Elapsed: 4.328974ms
Jun 22 10:32:27.886: INFO: Pod "pod-secrets-39a592f3-ede3-4ceb-b5f6-793543cb7c79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010216708s
Jun 22 10:32:29.900: INFO: Pod "pod-secrets-39a592f3-ede3-4ceb-b5f6-793543cb7c79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023802202s
STEP: Saw pod success 06/22/23 10:32:29.901
Jun 22 10:32:29.904: INFO: Pod "pod-secrets-39a592f3-ede3-4ceb-b5f6-793543cb7c79" satisfied condition "Succeeded or Failed"
Jun 22 10:32:29.909: INFO: Trying to get logs from node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k pod pod-secrets-39a592f3-ede3-4ceb-b5f6-793543cb7c79 container secret-volume-test: <nil>
STEP: delete the pod 06/22/23 10:32:29.936
Jun 22 10:32:29.955: INFO: Waiting for pod pod-secrets-39a592f3-ede3-4ceb-b5f6-793543cb7c79 to disappear
Jun 22 10:32:29.960: INFO: Pod pod-secrets-39a592f3-ede3-4ceb-b5f6-793543cb7c79 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun 22 10:32:29.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5296" for this suite. 06/22/23 10:32:29.975
------------------------------
• [4.160 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:32:25.824
    Jun 22 10:32:25.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename secrets 06/22/23 10:32:25.825
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:25.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:25.853
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-216d664f-c1b7-444c-bd6f-8e151fbce061 06/22/23 10:32:25.857
    STEP: Creating a pod to test consume secrets 06/22/23 10:32:25.863
    W0622 10:32:25.876442      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:32:25.876: INFO: Waiting up to 5m0s for pod "pod-secrets-39a592f3-ede3-4ceb-b5f6-793543cb7c79" in namespace "secrets-5296" to be "Succeeded or Failed"
    Jun 22 10:32:25.880: INFO: Pod "pod-secrets-39a592f3-ede3-4ceb-b5f6-793543cb7c79": Phase="Pending", Reason="", readiness=false. Elapsed: 4.328974ms
    Jun 22 10:32:27.886: INFO: Pod "pod-secrets-39a592f3-ede3-4ceb-b5f6-793543cb7c79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010216708s
    Jun 22 10:32:29.900: INFO: Pod "pod-secrets-39a592f3-ede3-4ceb-b5f6-793543cb7c79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023802202s
    STEP: Saw pod success 06/22/23 10:32:29.901
    Jun 22 10:32:29.904: INFO: Pod "pod-secrets-39a592f3-ede3-4ceb-b5f6-793543cb7c79" satisfied condition "Succeeded or Failed"
    Jun 22 10:32:29.909: INFO: Trying to get logs from node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k pod pod-secrets-39a592f3-ede3-4ceb-b5f6-793543cb7c79 container secret-volume-test: <nil>
    STEP: delete the pod 06/22/23 10:32:29.936
    Jun 22 10:32:29.955: INFO: Waiting for pod pod-secrets-39a592f3-ede3-4ceb-b5f6-793543cb7c79 to disappear
    Jun 22 10:32:29.960: INFO: Pod pod-secrets-39a592f3-ede3-4ceb-b5f6-793543cb7c79 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:32:29.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5296" for this suite. 06/22/23 10:32:29.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:32:29.99
Jun 22 10:32:29.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename webhook 06/22/23 10:32:30.001
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:30.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:30.039
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/22/23 10:32:30.061
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 10:32:31.147
STEP: Deploying the webhook pod 06/22/23 10:32:31.154
W0622 10:32:31.173624      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 10:32:31.174
Jun 22 10:32:31.183: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 06/22/23 10:32:33.201
STEP: Verifying the service has paired with the endpoint 06/22/23 10:32:33.218
Jun 22 10:32:34.218: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 06/22/23 10:32:34.224
STEP: Creating a configMap that does not comply to the validation webhook rules 06/22/23 10:32:34.25
STEP: Updating a validating webhook configuration's rules to not include the create operation 06/22/23 10:32:34.261
STEP: Creating a configMap that does not comply to the validation webhook rules 06/22/23 10:32:34.284
STEP: Patching a validating webhook configuration's rules to include the create operation 06/22/23 10:32:34.298
STEP: Creating a configMap that does not comply to the validation webhook rules 06/22/23 10:32:34.309
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:32:34.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1605" for this suite. 06/22/23 10:32:34.379
STEP: Destroying namespace "webhook-1605-markers" for this suite. 06/22/23 10:32:34.386
------------------------------
• [4.405 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:32:29.99
    Jun 22 10:32:29.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename webhook 06/22/23 10:32:30.001
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:30.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:30.039
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/22/23 10:32:30.061
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 10:32:31.147
    STEP: Deploying the webhook pod 06/22/23 10:32:31.154
    W0622 10:32:31.173624      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 10:32:31.174
    Jun 22 10:32:31.183: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 06/22/23 10:32:33.201
    STEP: Verifying the service has paired with the endpoint 06/22/23 10:32:33.218
    Jun 22 10:32:34.218: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 06/22/23 10:32:34.224
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/22/23 10:32:34.25
    STEP: Updating a validating webhook configuration's rules to not include the create operation 06/22/23 10:32:34.261
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/22/23 10:32:34.284
    STEP: Patching a validating webhook configuration's rules to include the create operation 06/22/23 10:32:34.298
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/22/23 10:32:34.309
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:32:34.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1605" for this suite. 06/22/23 10:32:34.379
    STEP: Destroying namespace "webhook-1605-markers" for this suite. 06/22/23 10:32:34.386
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:32:34.396
Jun 22 10:32:34.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename custom-resource-definition 06/22/23 10:32:34.399
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:34.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:34.434
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jun 22 10:32:34.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:32:40.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6385" for this suite. 06/22/23 10:32:40.864
------------------------------
• [SLOW TEST] [6.478 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:32:34.396
    Jun 22 10:32:34.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename custom-resource-definition 06/22/23 10:32:34.399
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:34.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:34.434
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jun 22 10:32:34.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:32:40.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6385" for this suite. 06/22/23 10:32:40.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:32:40.877
Jun 22 10:32:40.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename configmap 06/22/23 10:32:40.878
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:40.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:40.906
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-9479aa57-b661-4dd6-90b0-05159816a31a 06/22/23 10:32:40.919
STEP: Creating the pod 06/22/23 10:32:40.929
W0622 10:32:40.942086      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "agnhost-container", "configmap-volume-binary-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "agnhost-container", "configmap-volume-binary-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "agnhost-container", "configmap-volume-binary-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "agnhost-container", "configmap-volume-binary-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:32:40.942: INFO: Waiting up to 5m0s for pod "pod-configmaps-cec75cf7-2176-4259-a4cc-5b7756a0a465" in namespace "configmap-8404" to be "running"
Jun 22 10:32:40.946: INFO: Pod "pod-configmaps-cec75cf7-2176-4259-a4cc-5b7756a0a465": Phase="Pending", Reason="", readiness=false. Elapsed: 4.676449ms
Jun 22 10:32:42.956: INFO: Pod "pod-configmaps-cec75cf7-2176-4259-a4cc-5b7756a0a465": Phase="Running", Reason="", readiness=false. Elapsed: 2.013948542s
Jun 22 10:32:42.956: INFO: Pod "pod-configmaps-cec75cf7-2176-4259-a4cc-5b7756a0a465" satisfied condition "running"
STEP: Waiting for pod with text data 06/22/23 10:32:42.956
STEP: Waiting for pod with binary data 06/22/23 10:32:42.966
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun 22 10:32:42.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8404" for this suite. 06/22/23 10:32:42.983
------------------------------
• [2.114 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:32:40.877
    Jun 22 10:32:40.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename configmap 06/22/23 10:32:40.878
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:40.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:40.906
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-9479aa57-b661-4dd6-90b0-05159816a31a 06/22/23 10:32:40.919
    STEP: Creating the pod 06/22/23 10:32:40.929
    W0622 10:32:40.942086      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "agnhost-container", "configmap-volume-binary-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "agnhost-container", "configmap-volume-binary-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "agnhost-container", "configmap-volume-binary-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "agnhost-container", "configmap-volume-binary-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:32:40.942: INFO: Waiting up to 5m0s for pod "pod-configmaps-cec75cf7-2176-4259-a4cc-5b7756a0a465" in namespace "configmap-8404" to be "running"
    Jun 22 10:32:40.946: INFO: Pod "pod-configmaps-cec75cf7-2176-4259-a4cc-5b7756a0a465": Phase="Pending", Reason="", readiness=false. Elapsed: 4.676449ms
    Jun 22 10:32:42.956: INFO: Pod "pod-configmaps-cec75cf7-2176-4259-a4cc-5b7756a0a465": Phase="Running", Reason="", readiness=false. Elapsed: 2.013948542s
    Jun 22 10:32:42.956: INFO: Pod "pod-configmaps-cec75cf7-2176-4259-a4cc-5b7756a0a465" satisfied condition "running"
    STEP: Waiting for pod with text data 06/22/23 10:32:42.956
    STEP: Waiting for pod with binary data 06/22/23 10:32:42.966
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:32:42.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8404" for this suite. 06/22/23 10:32:42.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:32:42.994
Jun 22 10:32:42.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename job 06/22/23 10:32:42.996
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:43.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:43.022
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 06/22/23 10:32:43.026
W0622 10:32:43.037715      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions 06/22/23 10:32:43.037
STEP: Ensuring pods with index for job exist 06/22/23 10:32:51.044
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jun 22 10:32:51.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6637" for this suite. 06/22/23 10:32:51.061
------------------------------
• [SLOW TEST] [8.078 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:32:42.994
    Jun 22 10:32:42.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename job 06/22/23 10:32:42.996
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:43.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:43.022
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 06/22/23 10:32:43.026
    W0622 10:32:43.037715      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring job reaches completions 06/22/23 10:32:43.037
    STEP: Ensuring pods with index for job exist 06/22/23 10:32:51.044
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:32:51.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6637" for this suite. 06/22/23 10:32:51.061
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:32:51.074
Jun 22 10:32:51.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename cronjob 06/22/23 10:32:51.076
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:51.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:51.099
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 06/22/23 10:32:51.104
W0622 10:32:51.113118      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a job is scheduled 06/22/23 10:32:51.113
STEP: Ensuring exactly one is scheduled 06/22/23 10:33:01.119
STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/22/23 10:33:01.124
STEP: Ensuring the job is replaced with a new one 06/22/23 10:33:01.13
STEP: Removing cronjob 06/22/23 10:34:01.137
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jun 22 10:34:01.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1320" for this suite. 06/22/23 10:34:01.154
------------------------------
• [SLOW TEST] [70.087 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:32:51.074
    Jun 22 10:32:51.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename cronjob 06/22/23 10:32:51.076
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:32:51.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:32:51.099
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 06/22/23 10:32:51.104
    W0622 10:32:51.113118      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring a job is scheduled 06/22/23 10:32:51.113
    STEP: Ensuring exactly one is scheduled 06/22/23 10:33:01.119
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/22/23 10:33:01.124
    STEP: Ensuring the job is replaced with a new one 06/22/23 10:33:01.13
    STEP: Removing cronjob 06/22/23 10:34:01.137
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:34:01.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1320" for this suite. 06/22/23 10:34:01.154
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:34:01.162
Jun 22 10:34:01.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubectl 06/22/23 10:34:01.166
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:34:01.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:34:01.192
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/22/23 10:34:01.197
Jun 22 10:34:01.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-8722 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Jun 22 10:34:01.359: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"e2e-test-httpd-pod\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"e2e-test-httpd-pod\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"e2e-test-httpd-pod\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"e2e-test-httpd-pod\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Jun 22 10:34:01.359: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 06/22/23 10:34:01.359
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Jun 22 10:34:01.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-8722 delete pods e2e-test-httpd-pod'
Jun 22 10:34:03.780: INFO: stderr: ""
Jun 22 10:34:03.780: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun 22 10:34:03.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8722" for this suite. 06/22/23 10:34:03.788
------------------------------
• [2.633 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:34:01.162
    Jun 22 10:34:01.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubectl 06/22/23 10:34:01.166
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:34:01.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:34:01.192
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/22/23 10:34:01.197
    Jun 22 10:34:01.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-8722 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Jun 22 10:34:01.359: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"e2e-test-httpd-pod\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"e2e-test-httpd-pod\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"e2e-test-httpd-pod\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"e2e-test-httpd-pod\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
    Jun 22 10:34:01.359: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 06/22/23 10:34:01.359
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Jun 22 10:34:01.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-8722 delete pods e2e-test-httpd-pod'
    Jun 22 10:34:03.780: INFO: stderr: ""
    Jun 22 10:34:03.780: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:34:03.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8722" for this suite. 06/22/23 10:34:03.788
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:34:03.796
Jun 22 10:34:03.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename podtemplate 06/22/23 10:34:03.797
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:34:03.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:34:03.823
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 06/22/23 10:34:03.827
W0622 10:34:03.841342      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Replace a pod template 06/22/23 10:34:03.841
W0622 10:34:03.857129      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:34:03.857: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jun 22 10:34:03.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-6532" for this suite. 06/22/23 10:34:03.866
------------------------------
• [0.078 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:34:03.796
    Jun 22 10:34:03.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename podtemplate 06/22/23 10:34:03.797
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:34:03.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:34:03.823
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 06/22/23 10:34:03.827
    W0622 10:34:03.841342      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Replace a pod template 06/22/23 10:34:03.841
    W0622 10:34:03.857129      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:34:03.857: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:34:03.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-6532" for this suite. 06/22/23 10:34:03.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:34:03.878
Jun 22 10:34:03.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename container-runtime 06/22/23 10:34:03.88
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:34:03.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:34:03.905
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 06/22/23 10:34:03.909
W0622 10:34:03.921489      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: wait for the container to reach Succeeded 06/22/23 10:34:03.921
STEP: get the container status 06/22/23 10:34:06.948
STEP: the container should be terminated 06/22/23 10:34:06.953
STEP: the termination message should be set 06/22/23 10:34:06.953
Jun 22 10:34:06.953: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 06/22/23 10:34:06.953
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jun 22 10:34:06.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4936" for this suite. 06/22/23 10:34:06.977
------------------------------
• [3.107 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:34:03.878
    Jun 22 10:34:03.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename container-runtime 06/22/23 10:34:03.88
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:34:03.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:34:03.905
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 06/22/23 10:34:03.909
    W0622 10:34:03.921489      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: wait for the container to reach Succeeded 06/22/23 10:34:03.921
    STEP: get the container status 06/22/23 10:34:06.948
    STEP: the container should be terminated 06/22/23 10:34:06.953
    STEP: the termination message should be set 06/22/23 10:34:06.953
    Jun 22 10:34:06.953: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 06/22/23 10:34:06.953
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:34:06.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4936" for this suite. 06/22/23 10:34:06.977
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:34:06.986
Jun 22 10:34:06.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename webhook 06/22/23 10:34:06.987
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:34:07.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:34:07.01
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/22/23 10:34:07.029
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 10:34:07.362
STEP: Deploying the webhook pod 06/22/23 10:34:07.374
W0622 10:34:07.390062      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 10:34:07.39
Jun 22 10:34:07.399: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/22/23 10:34:09.413
STEP: Verifying the service has paired with the endpoint 06/22/23 10:34:09.431
Jun 22 10:34:10.432: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Jun 22 10:34:10.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4236-crds.webhook.example.com via the AdmissionRegistration API 06/22/23 10:34:10.954
STEP: Creating a custom resource that should be mutated by the webhook 06/22/23 10:34:10.978
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:34:13.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6178" for this suite. 06/22/23 10:34:13.607
STEP: Destroying namespace "webhook-6178-markers" for this suite. 06/22/23 10:34:13.617
------------------------------
• [SLOW TEST] [6.639 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:34:06.986
    Jun 22 10:34:06.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename webhook 06/22/23 10:34:06.987
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:34:07.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:34:07.01
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/22/23 10:34:07.029
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 10:34:07.362
    STEP: Deploying the webhook pod 06/22/23 10:34:07.374
    W0622 10:34:07.390062      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 10:34:07.39
    Jun 22 10:34:07.399: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/22/23 10:34:09.413
    STEP: Verifying the service has paired with the endpoint 06/22/23 10:34:09.431
    Jun 22 10:34:10.432: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Jun 22 10:34:10.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4236-crds.webhook.example.com via the AdmissionRegistration API 06/22/23 10:34:10.954
    STEP: Creating a custom resource that should be mutated by the webhook 06/22/23 10:34:10.978
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:34:13.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6178" for this suite. 06/22/23 10:34:13.607
    STEP: Destroying namespace "webhook-6178-markers" for this suite. 06/22/23 10:34:13.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:34:13.632
Jun 22 10:34:13.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename cronjob 06/22/23 10:34:13.633
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:34:13.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:34:13.659
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 06/22/23 10:34:13.663
STEP: creating 06/22/23 10:34:13.663
W0622 10:34:13.671688      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: getting 06/22/23 10:34:13.671
STEP: listing 06/22/23 10:34:13.675
STEP: watching 06/22/23 10:34:13.679
Jun 22 10:34:13.679: INFO: starting watch
STEP: cluster-wide listing 06/22/23 10:34:13.682
STEP: cluster-wide watching 06/22/23 10:34:13.69
Jun 22 10:34:13.690: INFO: starting watch
STEP: patching 06/22/23 10:34:13.692
W0622 10:34:13.701410      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: updating 06/22/23 10:34:13.701
W0622 10:34:13.716350      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:34:13.716: INFO: waiting for watch events with expected annotations
Jun 22 10:34:13.716: INFO: saw patched and updated annotations
STEP: patching /status 06/22/23 10:34:13.716
STEP: updating /status 06/22/23 10:34:13.725
STEP: get /status 06/22/23 10:34:13.735
STEP: deleting 06/22/23 10:34:13.739
W0622 10:34:13.746826      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: deleting a collection 06/22/23 10:34:13.759
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jun 22 10:34:13.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-377" for this suite. 06/22/23 10:34:13.779
------------------------------
• [0.155 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:34:13.632
    Jun 22 10:34:13.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename cronjob 06/22/23 10:34:13.633
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:34:13.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:34:13.659
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 06/22/23 10:34:13.663
    STEP: creating 06/22/23 10:34:13.663
    W0622 10:34:13.671688      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: getting 06/22/23 10:34:13.671
    STEP: listing 06/22/23 10:34:13.675
    STEP: watching 06/22/23 10:34:13.679
    Jun 22 10:34:13.679: INFO: starting watch
    STEP: cluster-wide listing 06/22/23 10:34:13.682
    STEP: cluster-wide watching 06/22/23 10:34:13.69
    Jun 22 10:34:13.690: INFO: starting watch
    STEP: patching 06/22/23 10:34:13.692
    W0622 10:34:13.701410      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: updating 06/22/23 10:34:13.701
    W0622 10:34:13.716350      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:34:13.716: INFO: waiting for watch events with expected annotations
    Jun 22 10:34:13.716: INFO: saw patched and updated annotations
    STEP: patching /status 06/22/23 10:34:13.716
    STEP: updating /status 06/22/23 10:34:13.725
    STEP: get /status 06/22/23 10:34:13.735
    STEP: deleting 06/22/23 10:34:13.739
    W0622 10:34:13.746826      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: deleting a collection 06/22/23 10:34:13.759
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:34:13.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-377" for this suite. 06/22/23 10:34:13.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:34:13.788
Jun 22 10:34:13.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename daemonsets 06/22/23 10:34:13.789
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:34:13.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:34:13.816
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 06/22/23 10:34:13.85
W0622 10:34:13.859124      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Check that daemon pods launch on every node of the cluster. 06/22/23 10:34:13.859
Jun 22 10:34:13.866: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:34:13.866: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:34:13.866: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:34:13.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 10:34:13.870: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
Jun 22 10:34:14.878: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:34:14.878: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:34:14.878: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:34:14.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 22 10:34:14.883: INFO: Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw is running 0 daemon pod, expected 1
Jun 22 10:34:15.879: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:34:15.879: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:34:15.879: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 10:34:15.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 22 10:34:15.884: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 06/22/23 10:34:15.888
Jun 22 10:34:15.893: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 06/22/23 10:34:15.893
Jun 22 10:34:15.907: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 06/22/23 10:34:15.907
Jun 22 10:34:15.911: INFO: Observed &DaemonSet event: ADDED
Jun 22 10:34:15.911: INFO: Observed &DaemonSet event: MODIFIED
Jun 22 10:34:15.911: INFO: Observed &DaemonSet event: MODIFIED
Jun 22 10:34:15.912: INFO: Observed &DaemonSet event: MODIFIED
Jun 22 10:34:15.912: INFO: Observed &DaemonSet event: MODIFIED
Jun 22 10:34:15.912: INFO: Observed &DaemonSet event: MODIFIED
Jun 22 10:34:15.912: INFO: Found daemon set daemon-set in namespace daemonsets-9979 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 22 10:34:15.912: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 06/22/23 10:34:15.912
STEP: watching for the daemon set status to be patched 06/22/23 10:34:15.931
Jun 22 10:34:15.935: INFO: Observed &DaemonSet event: ADDED
Jun 22 10:34:15.935: INFO: Observed &DaemonSet event: MODIFIED
Jun 22 10:34:15.935: INFO: Observed &DaemonSet event: MODIFIED
Jun 22 10:34:15.935: INFO: Observed &DaemonSet event: MODIFIED
Jun 22 10:34:15.936: INFO: Observed &DaemonSet event: MODIFIED
Jun 22 10:34:15.936: INFO: Observed &DaemonSet event: MODIFIED
Jun 22 10:34:15.936: INFO: Observed daemon set daemon-set in namespace daemonsets-9979 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 22 10:34:15.936: INFO: Observed &DaemonSet event: MODIFIED
Jun 22 10:34:15.936: INFO: Found daemon set daemon-set in namespace daemonsets-9979 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jun 22 10:34:15.936: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 06/22/23 10:34:15.943
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9979, will wait for the garbage collector to delete the pods 06/22/23 10:34:15.943
Jun 22 10:34:16.006: INFO: Deleting DaemonSet.extensions daemon-set took: 8.013478ms
Jun 22 10:34:16.107: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.851179ms
Jun 22 10:34:18.013: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 10:34:18.013: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 22 10:34:18.017: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"132814"},"items":null}

Jun 22 10:34:18.021: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"132814"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:34:18.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9979" for this suite. 06/22/23 10:34:18.044
------------------------------
• [4.263 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:34:13.788
    Jun 22 10:34:13.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename daemonsets 06/22/23 10:34:13.789
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:34:13.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:34:13.816
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 06/22/23 10:34:13.85
    W0622 10:34:13.859124      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Check that daemon pods launch on every node of the cluster. 06/22/23 10:34:13.859
    Jun 22 10:34:13.866: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:34:13.866: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:34:13.866: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:34:13.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 10:34:13.870: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
    Jun 22 10:34:14.878: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:34:14.878: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:34:14.878: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:34:14.883: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 22 10:34:14.883: INFO: Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw is running 0 daemon pod, expected 1
    Jun 22 10:34:15.879: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:34:15.879: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:34:15.879: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 10:34:15.884: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 22 10:34:15.884: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 06/22/23 10:34:15.888
    Jun 22 10:34:15.893: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 06/22/23 10:34:15.893
    Jun 22 10:34:15.907: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 06/22/23 10:34:15.907
    Jun 22 10:34:15.911: INFO: Observed &DaemonSet event: ADDED
    Jun 22 10:34:15.911: INFO: Observed &DaemonSet event: MODIFIED
    Jun 22 10:34:15.911: INFO: Observed &DaemonSet event: MODIFIED
    Jun 22 10:34:15.912: INFO: Observed &DaemonSet event: MODIFIED
    Jun 22 10:34:15.912: INFO: Observed &DaemonSet event: MODIFIED
    Jun 22 10:34:15.912: INFO: Observed &DaemonSet event: MODIFIED
    Jun 22 10:34:15.912: INFO: Found daemon set daemon-set in namespace daemonsets-9979 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun 22 10:34:15.912: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 06/22/23 10:34:15.912
    STEP: watching for the daemon set status to be patched 06/22/23 10:34:15.931
    Jun 22 10:34:15.935: INFO: Observed &DaemonSet event: ADDED
    Jun 22 10:34:15.935: INFO: Observed &DaemonSet event: MODIFIED
    Jun 22 10:34:15.935: INFO: Observed &DaemonSet event: MODIFIED
    Jun 22 10:34:15.935: INFO: Observed &DaemonSet event: MODIFIED
    Jun 22 10:34:15.936: INFO: Observed &DaemonSet event: MODIFIED
    Jun 22 10:34:15.936: INFO: Observed &DaemonSet event: MODIFIED
    Jun 22 10:34:15.936: INFO: Observed daemon set daemon-set in namespace daemonsets-9979 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun 22 10:34:15.936: INFO: Observed &DaemonSet event: MODIFIED
    Jun 22 10:34:15.936: INFO: Found daemon set daemon-set in namespace daemonsets-9979 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jun 22 10:34:15.936: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 06/22/23 10:34:15.943
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9979, will wait for the garbage collector to delete the pods 06/22/23 10:34:15.943
    Jun 22 10:34:16.006: INFO: Deleting DaemonSet.extensions daemon-set took: 8.013478ms
    Jun 22 10:34:16.107: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.851179ms
    Jun 22 10:34:18.013: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 10:34:18.013: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 22 10:34:18.017: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"132814"},"items":null}

    Jun 22 10:34:18.021: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"132814"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:34:18.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9979" for this suite. 06/22/23 10:34:18.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:34:18.054
Jun 22 10:34:18.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename replication-controller 06/22/23 10:34:18.055
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:34:18.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:34:18.082
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 06/22/23 10:34:18.087
W0622 10:34:18.098759      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-adoption" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:34:18.098: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2157" to be "running and ready"
Jun 22 10:34:18.102: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 3.908736ms
Jun 22 10:34:18.102: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:34:20.108: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.009618257s
Jun 22 10:34:20.108: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jun 22 10:34:20.108: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 06/22/23 10:34:20.112
W0622 10:34:20.121478      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-adoption" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Then the orphan pod is adopted 06/22/23 10:34:20.121
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jun 22 10:34:21.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2157" for this suite. 06/22/23 10:34:21.138
------------------------------
• [3.093 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:34:18.054
    Jun 22 10:34:18.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename replication-controller 06/22/23 10:34:18.055
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:34:18.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:34:18.082
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 06/22/23 10:34:18.087
    W0622 10:34:18.098759      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-adoption" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:34:18.098: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2157" to be "running and ready"
    Jun 22 10:34:18.102: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 3.908736ms
    Jun 22 10:34:18.102: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:34:20.108: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.009618257s
    Jun 22 10:34:20.108: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jun 22 10:34:20.108: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 06/22/23 10:34:20.112
    W0622 10:34:20.121478      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-adoption" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Then the orphan pod is adopted 06/22/23 10:34:20.121
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:34:21.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2157" for this suite. 06/22/23 10:34:21.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:34:21.149
Jun 22 10:34:21.149: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename emptydir-wrapper 06/22/23 10:34:21.15
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:34:21.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:34:21.174
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 06/22/23 10:34:21.178
STEP: Creating RC which spawns configmap-volume pods 06/22/23 10:34:21.433
W0622 10:34:21.464789      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:34:21.508: INFO: Pod name wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb: Found 3 pods out of 5
Jun 22 10:34:26.521: INFO: Pod name wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb: Found 5 pods out of 5
STEP: Ensuring each pod is running 06/22/23 10:34:26.521
Jun 22 10:34:26.521: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-2thhl" in namespace "emptydir-wrapper-2477" to be "running"
Jun 22 10:34:26.526: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-2thhl": Phase="Pending", Reason="", readiness=false. Elapsed: 5.747153ms
Jun 22 10:34:28.536: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-2thhl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015443424s
Jun 22 10:34:30.534: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-2thhl": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013157894s
Jun 22 10:34:32.534: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-2thhl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013002716s
Jun 22 10:34:34.533: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-2thhl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012451556s
Jun 22 10:34:36.533: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-2thhl": Phase="Running", Reason="", readiness=true. Elapsed: 10.012143232s
Jun 22 10:34:36.533: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-2thhl" satisfied condition "running"
Jun 22 10:34:36.533: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-6rwqp" in namespace "emptydir-wrapper-2477" to be "running"
Jun 22 10:34:36.538: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-6rwqp": Phase="Running", Reason="", readiness=true. Elapsed: 4.793953ms
Jun 22 10:34:36.538: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-6rwqp" satisfied condition "running"
Jun 22 10:34:36.538: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-hxfjj" in namespace "emptydir-wrapper-2477" to be "running"
Jun 22 10:34:36.543: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-hxfjj": Phase="Running", Reason="", readiness=true. Elapsed: 5.213114ms
Jun 22 10:34:36.543: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-hxfjj" satisfied condition "running"
Jun 22 10:34:36.543: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-kvv4c" in namespace "emptydir-wrapper-2477" to be "running"
Jun 22 10:34:36.548: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-kvv4c": Phase="Running", Reason="", readiness=true. Elapsed: 5.076057ms
Jun 22 10:34:36.548: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-kvv4c" satisfied condition "running"
Jun 22 10:34:36.548: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-mmkvn" in namespace "emptydir-wrapper-2477" to be "running"
Jun 22 10:34:36.553: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-mmkvn": Phase="Running", Reason="", readiness=true. Elapsed: 4.766207ms
Jun 22 10:34:36.553: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-mmkvn" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb in namespace emptydir-wrapper-2477, will wait for the garbage collector to delete the pods 06/22/23 10:34:36.553
Jun 22 10:34:36.622: INFO: Deleting ReplicationController wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb took: 12.068062ms
Jun 22 10:34:36.723: INFO: Terminating ReplicationController wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb pods took: 101.303944ms
STEP: Creating RC which spawns configmap-volume pods 06/22/23 10:34:40.663
W0622 10:34:40.719097      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:34:40.728: INFO: Pod name wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4: Found 1 pods out of 5
Jun 22 10:34:45.742: INFO: Pod name wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4: Found 5 pods out of 5
STEP: Ensuring each pod is running 06/22/23 10:34:45.742
Jun 22 10:34:45.742: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-8n6j2" in namespace "emptydir-wrapper-2477" to be "running"
Jun 22 10:34:45.749: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-8n6j2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.685362ms
Jun 22 10:34:47.759: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-8n6j2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017035819s
Jun 22 10:34:49.759: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-8n6j2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01695864s
Jun 22 10:34:51.760: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-8n6j2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01793344s
Jun 22 10:34:53.760: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-8n6j2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018221806s
Jun 22 10:34:55.758: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-8n6j2": Phase="Running", Reason="", readiness=true. Elapsed: 10.015772746s
Jun 22 10:34:55.758: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-8n6j2" satisfied condition "running"
Jun 22 10:34:55.758: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-9m5ft" in namespace "emptydir-wrapper-2477" to be "running"
Jun 22 10:34:55.763: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-9m5ft": Phase="Running", Reason="", readiness=true. Elapsed: 5.434768ms
Jun 22 10:34:55.763: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-9m5ft" satisfied condition "running"
Jun 22 10:34:55.763: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-jtf4k" in namespace "emptydir-wrapper-2477" to be "running"
Jun 22 10:34:55.768: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-jtf4k": Phase="Running", Reason="", readiness=true. Elapsed: 5.083965ms
Jun 22 10:34:55.768: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-jtf4k" satisfied condition "running"
Jun 22 10:34:55.768: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-p62pr" in namespace "emptydir-wrapper-2477" to be "running"
Jun 22 10:34:55.774: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-p62pr": Phase="Running", Reason="", readiness=true. Elapsed: 5.358957ms
Jun 22 10:34:55.774: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-p62pr" satisfied condition "running"
Jun 22 10:34:55.774: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-zmkg6" in namespace "emptydir-wrapper-2477" to be "running"
Jun 22 10:34:55.779: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-zmkg6": Phase="Running", Reason="", readiness=true. Elapsed: 5.482871ms
Jun 22 10:34:55.779: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-zmkg6" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4 in namespace emptydir-wrapper-2477, will wait for the garbage collector to delete the pods 06/22/23 10:34:55.779
Jun 22 10:34:55.847: INFO: Deleting ReplicationController wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4 took: 12.016719ms
Jun 22 10:34:55.948: INFO: Terminating ReplicationController wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4 pods took: 100.851634ms
STEP: Creating RC which spawns configmap-volume pods 06/22/23 10:34:59.457
W0622 10:34:59.476396      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:34:59.484: INFO: Pod name wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478: Found 0 pods out of 5
Jun 22 10:35:04.494: INFO: Pod name wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478: Found 5 pods out of 5
STEP: Ensuring each pod is running 06/22/23 10:35:04.494
Jun 22 10:35:04.494: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-2hlps" in namespace "emptydir-wrapper-2477" to be "running"
Jun 22 10:35:04.499: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-2hlps": Phase="Pending", Reason="", readiness=false. Elapsed: 5.073762ms
Jun 22 10:35:06.507: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-2hlps": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013440241s
Jun 22 10:35:08.507: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-2hlps": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013245397s
Jun 22 10:35:10.512: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-2hlps": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018549044s
Jun 22 10:35:12.507: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-2hlps": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013320626s
Jun 22 10:35:14.506: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-2hlps": Phase="Running", Reason="", readiness=true. Elapsed: 10.011841887s
Jun 22 10:35:14.506: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-2hlps" satisfied condition "running"
Jun 22 10:35:14.506: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-68p8l" in namespace "emptydir-wrapper-2477" to be "running"
Jun 22 10:35:14.511: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-68p8l": Phase="Running", Reason="", readiness=true. Elapsed: 5.407016ms
Jun 22 10:35:14.511: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-68p8l" satisfied condition "running"
Jun 22 10:35:14.511: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-6sl2t" in namespace "emptydir-wrapper-2477" to be "running"
Jun 22 10:35:14.517: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-6sl2t": Phase="Running", Reason="", readiness=true. Elapsed: 6.198158ms
Jun 22 10:35:14.517: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-6sl2t" satisfied condition "running"
Jun 22 10:35:14.517: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-9szqt" in namespace "emptydir-wrapper-2477" to be "running"
Jun 22 10:35:14.522: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-9szqt": Phase="Running", Reason="", readiness=true. Elapsed: 4.980654ms
Jun 22 10:35:14.523: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-9szqt" satisfied condition "running"
Jun 22 10:35:14.523: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-nzks7" in namespace "emptydir-wrapper-2477" to be "running"
Jun 22 10:35:14.528: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-nzks7": Phase="Running", Reason="", readiness=true. Elapsed: 5.13058ms
Jun 22 10:35:14.528: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-nzks7" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478 in namespace emptydir-wrapper-2477, will wait for the garbage collector to delete the pods 06/22/23 10:35:14.528
Jun 22 10:35:14.595: INFO: Deleting ReplicationController wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478 took: 10.523904ms
Jun 22 10:35:14.696: INFO: Terminating ReplicationController wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478 pods took: 101.110786ms
STEP: Cleaning up the configMaps 06/22/23 10:35:17.596
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jun 22 10:35:17.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-2477" for this suite. 06/22/23 10:35:17.987
------------------------------
• [SLOW TEST] [56.846 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:34:21.149
    Jun 22 10:34:21.149: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename emptydir-wrapper 06/22/23 10:34:21.15
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:34:21.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:34:21.174
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 06/22/23 10:34:21.178
    STEP: Creating RC which spawns configmap-volume pods 06/22/23 10:34:21.433
    W0622 10:34:21.464789      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:34:21.508: INFO: Pod name wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb: Found 3 pods out of 5
    Jun 22 10:34:26.521: INFO: Pod name wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb: Found 5 pods out of 5
    STEP: Ensuring each pod is running 06/22/23 10:34:26.521
    Jun 22 10:34:26.521: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-2thhl" in namespace "emptydir-wrapper-2477" to be "running"
    Jun 22 10:34:26.526: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-2thhl": Phase="Pending", Reason="", readiness=false. Elapsed: 5.747153ms
    Jun 22 10:34:28.536: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-2thhl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015443424s
    Jun 22 10:34:30.534: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-2thhl": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013157894s
    Jun 22 10:34:32.534: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-2thhl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013002716s
    Jun 22 10:34:34.533: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-2thhl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012451556s
    Jun 22 10:34:36.533: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-2thhl": Phase="Running", Reason="", readiness=true. Elapsed: 10.012143232s
    Jun 22 10:34:36.533: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-2thhl" satisfied condition "running"
    Jun 22 10:34:36.533: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-6rwqp" in namespace "emptydir-wrapper-2477" to be "running"
    Jun 22 10:34:36.538: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-6rwqp": Phase="Running", Reason="", readiness=true. Elapsed: 4.793953ms
    Jun 22 10:34:36.538: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-6rwqp" satisfied condition "running"
    Jun 22 10:34:36.538: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-hxfjj" in namespace "emptydir-wrapper-2477" to be "running"
    Jun 22 10:34:36.543: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-hxfjj": Phase="Running", Reason="", readiness=true. Elapsed: 5.213114ms
    Jun 22 10:34:36.543: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-hxfjj" satisfied condition "running"
    Jun 22 10:34:36.543: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-kvv4c" in namespace "emptydir-wrapper-2477" to be "running"
    Jun 22 10:34:36.548: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-kvv4c": Phase="Running", Reason="", readiness=true. Elapsed: 5.076057ms
    Jun 22 10:34:36.548: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-kvv4c" satisfied condition "running"
    Jun 22 10:34:36.548: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-mmkvn" in namespace "emptydir-wrapper-2477" to be "running"
    Jun 22 10:34:36.553: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-mmkvn": Phase="Running", Reason="", readiness=true. Elapsed: 4.766207ms
    Jun 22 10:34:36.553: INFO: Pod "wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb-mmkvn" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb in namespace emptydir-wrapper-2477, will wait for the garbage collector to delete the pods 06/22/23 10:34:36.553
    Jun 22 10:34:36.622: INFO: Deleting ReplicationController wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb took: 12.068062ms
    Jun 22 10:34:36.723: INFO: Terminating ReplicationController wrapped-volume-race-17df186f-01b9-4541-b70a-e11428cd5aeb pods took: 101.303944ms
    STEP: Creating RC which spawns configmap-volume pods 06/22/23 10:34:40.663
    W0622 10:34:40.719097      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:34:40.728: INFO: Pod name wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4: Found 1 pods out of 5
    Jun 22 10:34:45.742: INFO: Pod name wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4: Found 5 pods out of 5
    STEP: Ensuring each pod is running 06/22/23 10:34:45.742
    Jun 22 10:34:45.742: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-8n6j2" in namespace "emptydir-wrapper-2477" to be "running"
    Jun 22 10:34:45.749: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-8n6j2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.685362ms
    Jun 22 10:34:47.759: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-8n6j2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017035819s
    Jun 22 10:34:49.759: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-8n6j2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01695864s
    Jun 22 10:34:51.760: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-8n6j2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01793344s
    Jun 22 10:34:53.760: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-8n6j2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018221806s
    Jun 22 10:34:55.758: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-8n6j2": Phase="Running", Reason="", readiness=true. Elapsed: 10.015772746s
    Jun 22 10:34:55.758: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-8n6j2" satisfied condition "running"
    Jun 22 10:34:55.758: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-9m5ft" in namespace "emptydir-wrapper-2477" to be "running"
    Jun 22 10:34:55.763: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-9m5ft": Phase="Running", Reason="", readiness=true. Elapsed: 5.434768ms
    Jun 22 10:34:55.763: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-9m5ft" satisfied condition "running"
    Jun 22 10:34:55.763: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-jtf4k" in namespace "emptydir-wrapper-2477" to be "running"
    Jun 22 10:34:55.768: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-jtf4k": Phase="Running", Reason="", readiness=true. Elapsed: 5.083965ms
    Jun 22 10:34:55.768: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-jtf4k" satisfied condition "running"
    Jun 22 10:34:55.768: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-p62pr" in namespace "emptydir-wrapper-2477" to be "running"
    Jun 22 10:34:55.774: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-p62pr": Phase="Running", Reason="", readiness=true. Elapsed: 5.358957ms
    Jun 22 10:34:55.774: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-p62pr" satisfied condition "running"
    Jun 22 10:34:55.774: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-zmkg6" in namespace "emptydir-wrapper-2477" to be "running"
    Jun 22 10:34:55.779: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-zmkg6": Phase="Running", Reason="", readiness=true. Elapsed: 5.482871ms
    Jun 22 10:34:55.779: INFO: Pod "wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4-zmkg6" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4 in namespace emptydir-wrapper-2477, will wait for the garbage collector to delete the pods 06/22/23 10:34:55.779
    Jun 22 10:34:55.847: INFO: Deleting ReplicationController wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4 took: 12.016719ms
    Jun 22 10:34:55.948: INFO: Terminating ReplicationController wrapped-volume-race-96db58f5-413d-442f-b34f-fcf4fb0c46f4 pods took: 100.851634ms
    STEP: Creating RC which spawns configmap-volume pods 06/22/23 10:34:59.457
    W0622 10:34:59.476396      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:34:59.484: INFO: Pod name wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478: Found 0 pods out of 5
    Jun 22 10:35:04.494: INFO: Pod name wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478: Found 5 pods out of 5
    STEP: Ensuring each pod is running 06/22/23 10:35:04.494
    Jun 22 10:35:04.494: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-2hlps" in namespace "emptydir-wrapper-2477" to be "running"
    Jun 22 10:35:04.499: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-2hlps": Phase="Pending", Reason="", readiness=false. Elapsed: 5.073762ms
    Jun 22 10:35:06.507: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-2hlps": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013440241s
    Jun 22 10:35:08.507: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-2hlps": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013245397s
    Jun 22 10:35:10.512: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-2hlps": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018549044s
    Jun 22 10:35:12.507: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-2hlps": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013320626s
    Jun 22 10:35:14.506: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-2hlps": Phase="Running", Reason="", readiness=true. Elapsed: 10.011841887s
    Jun 22 10:35:14.506: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-2hlps" satisfied condition "running"
    Jun 22 10:35:14.506: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-68p8l" in namespace "emptydir-wrapper-2477" to be "running"
    Jun 22 10:35:14.511: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-68p8l": Phase="Running", Reason="", readiness=true. Elapsed: 5.407016ms
    Jun 22 10:35:14.511: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-68p8l" satisfied condition "running"
    Jun 22 10:35:14.511: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-6sl2t" in namespace "emptydir-wrapper-2477" to be "running"
    Jun 22 10:35:14.517: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-6sl2t": Phase="Running", Reason="", readiness=true. Elapsed: 6.198158ms
    Jun 22 10:35:14.517: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-6sl2t" satisfied condition "running"
    Jun 22 10:35:14.517: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-9szqt" in namespace "emptydir-wrapper-2477" to be "running"
    Jun 22 10:35:14.522: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-9szqt": Phase="Running", Reason="", readiness=true. Elapsed: 4.980654ms
    Jun 22 10:35:14.523: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-9szqt" satisfied condition "running"
    Jun 22 10:35:14.523: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-nzks7" in namespace "emptydir-wrapper-2477" to be "running"
    Jun 22 10:35:14.528: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-nzks7": Phase="Running", Reason="", readiness=true. Elapsed: 5.13058ms
    Jun 22 10:35:14.528: INFO: Pod "wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478-nzks7" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478 in namespace emptydir-wrapper-2477, will wait for the garbage collector to delete the pods 06/22/23 10:35:14.528
    Jun 22 10:35:14.595: INFO: Deleting ReplicationController wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478 took: 10.523904ms
    Jun 22 10:35:14.696: INFO: Terminating ReplicationController wrapped-volume-race-5e012e2f-58de-49f0-a0e5-4d047f640478 pods took: 101.110786ms
    STEP: Cleaning up the configMaps 06/22/23 10:35:17.596
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:35:17.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-2477" for this suite. 06/22/23 10:35:17.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:35:17.997
Jun 22 10:35:17.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubectl 06/22/23 10:35:17.998
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:35:18.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:35:18.022
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Jun 22 10:35:18.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5002 create -f -'
Jun 22 10:35:19.652: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"agnhost-primary\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"agnhost-primary\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"agnhost-primary\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"agnhost-primary\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Jun 22 10:35:19.652: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jun 22 10:35:19.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5002 create -f -'
Jun 22 10:35:21.300: INFO: stderr: ""
Jun 22 10:35:21.300: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 06/22/23 10:35:21.3
Jun 22 10:35:22.307: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 10:35:22.307: INFO: Found 1 / 1
Jun 22 10:35:22.307: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 22 10:35:22.312: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 10:35:22.312: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 10:35:22.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5002 describe pod agnhost-primary-xjphq'
Jun 22 10:35:22.417: INFO: stderr: ""
Jun 22 10:35:22.417: INFO: stdout: "Name:             agnhost-primary-xjphq\nNamespace:        kubectl-5002\nPriority:         0\nService Account:  default\nNode:             wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw/10.92.226.162\nStart Time:       Thu, 22 Jun 2023 10:35:19 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               100.96.3.27\nIPs:\n  IP:           100.96.3.27\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://26c37a647994a0a3da51acdb8fc9352101f528e6d0bef8bb4103c8daf779dc09\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 22 Jun 2023 10:35:20 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kn8x9 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-kn8x9:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-5002/agnhost-primary-xjphq to wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw\n"
Jun 22 10:35:22.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5002 describe rc agnhost-primary'
Jun 22 10:35:22.543: INFO: stderr: ""
Jun 22 10:35:22.543: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5002\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-xjphq\n"
Jun 22 10:35:22.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5002 describe service agnhost-primary'
Jun 22 10:35:22.635: INFO: stderr: ""
Jun 22 10:35:22.635: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5002\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                100.70.71.30\nIPs:               100.70.71.30\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.96.3.27:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 22 10:35:22.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5002 describe node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k'
Jun 22 10:35:22.778: INFO: stderr: ""
Jun 22 10:35:22.778: INFO: stdout: "Name:               wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=vsphere-vm.cpu-4.mem-16gb.os-ubuntu\n                    beta.kubernetes.io/os=linux\n                    image-type=ova\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k\n                    kubernetes.io/os=linux\n                    node.cluster.x-k8s.io/esxi-host=10.92.226.169\n                    node.kubernetes.io/instance-type=vsphere-vm.cpu-4.mem-16gb.os-ubuntu\n                    os-name=ubuntu\n                    os-type=linux\nAnnotations:        cluster.x-k8s.io/cluster-name: wl-antrea\n                    cluster.x-k8s.io/cluster-namespace: default\n                    cluster.x-k8s.io/labels-from-machine: node.cluster.x-k8s.io/esxi-host\n                    cluster.x-k8s.io/machine: wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k\n                    cluster.x-k8s.io/owner-kind: MachineSet\n                    cluster.x-k8s.io/owner-name: wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz\n                    csi.volume.kubernetes.io/nodeid: {\"csi.vsphere.vmware.com\":\"420ac87c-ff2a-d924-b7ca-5cca9d75b05f\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 22 Jun 2023 05:45:27 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 22 Jun 2023 10:35:18 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 22 Jun 2023 10:35:01 +0000   Thu, 22 Jun 2023 05:45:27 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 22 Jun 2023 10:35:01 +0000   Thu, 22 Jun 2023 05:45:27 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 22 Jun 2023 10:35:01 +0000   Thu, 22 Jun 2023 05:45:27 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 22 Jun 2023 10:35:01 +0000   Thu, 22 Jun 2023 05:46:23 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  Hostname:    wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k\n  InternalIP:  10.92.224.179\n  ExternalIP:  10.92.224.179\nCapacity:\n  cpu:                4\n  ephemeral-storage:  40419920Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16392924Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  37250998211\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16290524Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 004a564c1ee34c3ba3eca437304d1e81\n  System UUID:                7cc80a42-2aff-24d9-b7ca-5cca9d75b05f\n  Boot ID:                    c4e73aa1-0d8c-46a0-afc5-261924c903d4\n  Kernel Version:             5.4.0-150-generic\n  OS Image:                   Ubuntu 20.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.18-1-gdbc99e5b1\n  Kubelet Version:            v1.26.5+vmware.1\n  Kube-Proxy Version:         v1.26.5+vmware.1\nPodCIDR:                      100.96.4.0/24\nPodCIDRs:                     100.96.4.0/24\nProviderID:                   vsphere://420ac87c-ff2a-d924-b7ca-5cca9d75b05f\nNon-terminated Pods:          (6 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  corgi-test-privileged       corgi-test-8c78878b-6r5tv                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h23m\n  kube-system                 antrea-agent-dxs2v                                         400m (10%)    0 (0%)      0 (0%)           0 (0%)         4h49m\n  kube-system                 kube-proxy-czx75                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h49m\n  sonobuoy                    sonobuoy-e2e-job-38d4831e07f74ae5                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-5d74n    0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\n  vmware-system-csi           vsphere-csi-node-mp5v2                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h49m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                400m (10%)  0 (0%)\n  memory             0 (0%)      0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type    Reason          Age   From             Message\n  ----    ------          ----  ----             -------\n  Normal  RegisteredNode  76m   node-controller  Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k event: Registered Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k in Controller\n  Normal  RegisteredNode  14m   node-controller  Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k event: Registered Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k in Controller\n"
Jun 22 10:35:22.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5002 describe namespace kubectl-5002'
Jun 22 10:35:22.873: INFO: stderr: ""
Jun 22 10:35:22.873: INFO: stdout: "Name:         kubectl-5002\nLabels:       e2e-framework=kubectl\n              e2e-run=e5d41edb-1c5f-49df-95d4-4a50c9c0e5cf\n              kubernetes.io/metadata.name=kubectl-5002\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun 22 10:35:22.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5002" for this suite. 06/22/23 10:35:22.885
------------------------------
• [4.898 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:35:17.997
    Jun 22 10:35:17.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubectl 06/22/23 10:35:17.998
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:35:18.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:35:18.022
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Jun 22 10:35:18.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5002 create -f -'
    Jun 22 10:35:19.652: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"agnhost-primary\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"agnhost-primary\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"agnhost-primary\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"agnhost-primary\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
    Jun 22 10:35:19.652: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jun 22 10:35:19.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5002 create -f -'
    Jun 22 10:35:21.300: INFO: stderr: ""
    Jun 22 10:35:21.300: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 06/22/23 10:35:21.3
    Jun 22 10:35:22.307: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 22 10:35:22.307: INFO: Found 1 / 1
    Jun 22 10:35:22.307: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jun 22 10:35:22.312: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 22 10:35:22.312: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jun 22 10:35:22.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5002 describe pod agnhost-primary-xjphq'
    Jun 22 10:35:22.417: INFO: stderr: ""
    Jun 22 10:35:22.417: INFO: stdout: "Name:             agnhost-primary-xjphq\nNamespace:        kubectl-5002\nPriority:         0\nService Account:  default\nNode:             wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw/10.92.226.162\nStart Time:       Thu, 22 Jun 2023 10:35:19 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               100.96.3.27\nIPs:\n  IP:           100.96.3.27\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://26c37a647994a0a3da51acdb8fc9352101f528e6d0bef8bb4103c8daf779dc09\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 22 Jun 2023 10:35:20 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kn8x9 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-kn8x9:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-5002/agnhost-primary-xjphq to wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw\n"
    Jun 22 10:35:22.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5002 describe rc agnhost-primary'
    Jun 22 10:35:22.543: INFO: stderr: ""
    Jun 22 10:35:22.543: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5002\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-xjphq\n"
    Jun 22 10:35:22.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5002 describe service agnhost-primary'
    Jun 22 10:35:22.635: INFO: stderr: ""
    Jun 22 10:35:22.635: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5002\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                100.70.71.30\nIPs:               100.70.71.30\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         100.96.3.27:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jun 22 10:35:22.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5002 describe node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k'
    Jun 22 10:35:22.778: INFO: stderr: ""
    Jun 22 10:35:22.778: INFO: stdout: "Name:               wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=vsphere-vm.cpu-4.mem-16gb.os-ubuntu\n                    beta.kubernetes.io/os=linux\n                    image-type=ova\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k\n                    kubernetes.io/os=linux\n                    node.cluster.x-k8s.io/esxi-host=10.92.226.169\n                    node.kubernetes.io/instance-type=vsphere-vm.cpu-4.mem-16gb.os-ubuntu\n                    os-name=ubuntu\n                    os-type=linux\nAnnotations:        cluster.x-k8s.io/cluster-name: wl-antrea\n                    cluster.x-k8s.io/cluster-namespace: default\n                    cluster.x-k8s.io/labels-from-machine: node.cluster.x-k8s.io/esxi-host\n                    cluster.x-k8s.io/machine: wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k\n                    cluster.x-k8s.io/owner-kind: MachineSet\n                    cluster.x-k8s.io/owner-name: wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz\n                    csi.volume.kubernetes.io/nodeid: {\"csi.vsphere.vmware.com\":\"420ac87c-ff2a-d924-b7ca-5cca9d75b05f\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 22 Jun 2023 05:45:27 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 22 Jun 2023 10:35:18 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 22 Jun 2023 10:35:01 +0000   Thu, 22 Jun 2023 05:45:27 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 22 Jun 2023 10:35:01 +0000   Thu, 22 Jun 2023 05:45:27 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 22 Jun 2023 10:35:01 +0000   Thu, 22 Jun 2023 05:45:27 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 22 Jun 2023 10:35:01 +0000   Thu, 22 Jun 2023 05:46:23 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  Hostname:    wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k\n  InternalIP:  10.92.224.179\n  ExternalIP:  10.92.224.179\nCapacity:\n  cpu:                4\n  ephemeral-storage:  40419920Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16392924Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  37250998211\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16290524Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 004a564c1ee34c3ba3eca437304d1e81\n  System UUID:                7cc80a42-2aff-24d9-b7ca-5cca9d75b05f\n  Boot ID:                    c4e73aa1-0d8c-46a0-afc5-261924c903d4\n  Kernel Version:             5.4.0-150-generic\n  OS Image:                   Ubuntu 20.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.18-1-gdbc99e5b1\n  Kubelet Version:            v1.26.5+vmware.1\n  Kube-Proxy Version:         v1.26.5+vmware.1\nPodCIDR:                      100.96.4.0/24\nPodCIDRs:                     100.96.4.0/24\nProviderID:                   vsphere://420ac87c-ff2a-d924-b7ca-5cca9d75b05f\nNon-terminated Pods:          (6 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  corgi-test-privileged       corgi-test-8c78878b-6r5tv                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h23m\n  kube-system                 antrea-agent-dxs2v                                         400m (10%)    0 (0%)      0 (0%)           0 (0%)         4h49m\n  kube-system                 kube-proxy-czx75                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h49m\n  sonobuoy                    sonobuoy-e2e-job-38d4831e07f74ae5                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-5d74n    0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\n  vmware-system-csi           vsphere-csi-node-mp5v2                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h49m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                400m (10%)  0 (0%)\n  memory             0 (0%)      0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type    Reason          Age   From             Message\n  ----    ------          ----  ----             -------\n  Normal  RegisteredNode  76m   node-controller  Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k event: Registered Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k in Controller\n  Normal  RegisteredNode  14m   node-controller  Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k event: Registered Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k in Controller\n"
    Jun 22 10:35:22.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5002 describe namespace kubectl-5002'
    Jun 22 10:35:22.873: INFO: stderr: ""
    Jun 22 10:35:22.873: INFO: stdout: "Name:         kubectl-5002\nLabels:       e2e-framework=kubectl\n              e2e-run=e5d41edb-1c5f-49df-95d4-4a50c9c0e5cf\n              kubernetes.io/metadata.name=kubectl-5002\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:35:22.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5002" for this suite. 06/22/23 10:35:22.885
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:35:22.895
Jun 22 10:35:22.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename container-lifecycle-hook 06/22/23 10:35:22.896
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:35:22.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:35:22.922
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 06/22/23 10:35:22.934
W0622 10:35:22.948140      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:35:22.948: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6827" to be "running and ready"
Jun 22 10:35:22.953: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.09841ms
Jun 22 10:35:22.953: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:35:24.959: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010850802s
Jun 22 10:35:24.959: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun 22 10:35:24.959: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 06/22/23 10:35:24.963
W0622 10:35:24.971426      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-with-poststart-exec-hook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-with-poststart-exec-hook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-with-poststart-exec-hook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-with-poststart-exec-hook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:35:24.971: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6827" to be "running and ready"
Jun 22 10:35:24.976: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.454995ms
Jun 22 10:35:24.976: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:35:26.982: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010404058s
Jun 22 10:35:26.982: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jun 22 10:35:26.982: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 06/22/23 10:35:26.985
STEP: delete the pod with lifecycle hook 06/22/23 10:35:27.011
Jun 22 10:35:27.020: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 10:35:27.025: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 10:35:29.026: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 10:35:29.031: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 10:35:31.026: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 10:35:31.031: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jun 22 10:35:31.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6827" for this suite. 06/22/23 10:35:31.038
------------------------------
• [SLOW TEST] [8.150 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:35:22.895
    Jun 22 10:35:22.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/22/23 10:35:22.896
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:35:22.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:35:22.922
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 06/22/23 10:35:22.934
    W0622 10:35:22.948140      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:35:22.948: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6827" to be "running and ready"
    Jun 22 10:35:22.953: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.09841ms
    Jun 22 10:35:22.953: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:35:24.959: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010850802s
    Jun 22 10:35:24.959: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun 22 10:35:24.959: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 06/22/23 10:35:24.963
    W0622 10:35:24.971426      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-with-poststart-exec-hook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-with-poststart-exec-hook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-with-poststart-exec-hook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-with-poststart-exec-hook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:35:24.971: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6827" to be "running and ready"
    Jun 22 10:35:24.976: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.454995ms
    Jun 22 10:35:24.976: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:35:26.982: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010404058s
    Jun 22 10:35:26.982: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jun 22 10:35:26.982: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 06/22/23 10:35:26.985
    STEP: delete the pod with lifecycle hook 06/22/23 10:35:27.011
    Jun 22 10:35:27.020: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jun 22 10:35:27.025: INFO: Pod pod-with-poststart-exec-hook still exists
    Jun 22 10:35:29.026: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jun 22 10:35:29.031: INFO: Pod pod-with-poststart-exec-hook still exists
    Jun 22 10:35:31.026: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jun 22 10:35:31.031: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:35:31.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6827" for this suite. 06/22/23 10:35:31.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:35:31.046
Jun 22 10:35:31.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename secrets 06/22/23 10:35:31.047
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:35:31.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:35:31.07
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-74d2ceff-6421-40f6-a839-6388a038d4f5 06/22/23 10:35:31.074
STEP: Creating a pod to test consume secrets 06/22/23 10:35:31.082
W0622 10:35:31.092763      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:35:31.092: INFO: Waiting up to 5m0s for pod "pod-secrets-ca0ee0a9-c397-4884-868d-a087bd2d16a2" in namespace "secrets-5795" to be "Succeeded or Failed"
Jun 22 10:35:31.096: INFO: Pod "pod-secrets-ca0ee0a9-c397-4884-868d-a087bd2d16a2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.580168ms
Jun 22 10:35:33.103: INFO: Pod "pod-secrets-ca0ee0a9-c397-4884-868d-a087bd2d16a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010454228s
Jun 22 10:35:35.101: INFO: Pod "pod-secrets-ca0ee0a9-c397-4884-868d-a087bd2d16a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008615048s
STEP: Saw pod success 06/22/23 10:35:35.101
Jun 22 10:35:35.101: INFO: Pod "pod-secrets-ca0ee0a9-c397-4884-868d-a087bd2d16a2" satisfied condition "Succeeded or Failed"
Jun 22 10:35:35.106: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-secrets-ca0ee0a9-c397-4884-868d-a087bd2d16a2 container secret-volume-test: <nil>
STEP: delete the pod 06/22/23 10:35:35.126
Jun 22 10:35:35.139: INFO: Waiting for pod pod-secrets-ca0ee0a9-c397-4884-868d-a087bd2d16a2 to disappear
Jun 22 10:35:35.142: INFO: Pod pod-secrets-ca0ee0a9-c397-4884-868d-a087bd2d16a2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun 22 10:35:35.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5795" for this suite. 06/22/23 10:35:35.149
------------------------------
• [4.109 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:35:31.046
    Jun 22 10:35:31.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename secrets 06/22/23 10:35:31.047
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:35:31.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:35:31.07
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-74d2ceff-6421-40f6-a839-6388a038d4f5 06/22/23 10:35:31.074
    STEP: Creating a pod to test consume secrets 06/22/23 10:35:31.082
    W0622 10:35:31.092763      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:35:31.092: INFO: Waiting up to 5m0s for pod "pod-secrets-ca0ee0a9-c397-4884-868d-a087bd2d16a2" in namespace "secrets-5795" to be "Succeeded or Failed"
    Jun 22 10:35:31.096: INFO: Pod "pod-secrets-ca0ee0a9-c397-4884-868d-a087bd2d16a2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.580168ms
    Jun 22 10:35:33.103: INFO: Pod "pod-secrets-ca0ee0a9-c397-4884-868d-a087bd2d16a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010454228s
    Jun 22 10:35:35.101: INFO: Pod "pod-secrets-ca0ee0a9-c397-4884-868d-a087bd2d16a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008615048s
    STEP: Saw pod success 06/22/23 10:35:35.101
    Jun 22 10:35:35.101: INFO: Pod "pod-secrets-ca0ee0a9-c397-4884-868d-a087bd2d16a2" satisfied condition "Succeeded or Failed"
    Jun 22 10:35:35.106: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-secrets-ca0ee0a9-c397-4884-868d-a087bd2d16a2 container secret-volume-test: <nil>
    STEP: delete the pod 06/22/23 10:35:35.126
    Jun 22 10:35:35.139: INFO: Waiting for pod pod-secrets-ca0ee0a9-c397-4884-868d-a087bd2d16a2 to disappear
    Jun 22 10:35:35.142: INFO: Pod pod-secrets-ca0ee0a9-c397-4884-868d-a087bd2d16a2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:35:35.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5795" for this suite. 06/22/23 10:35:35.149
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:35:35.156
Jun 22 10:35:35.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename sysctl 06/22/23 10:35:35.157
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:35:35.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:35:35.18
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 06/22/23 10:35:35.185
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:35:35.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-5309" for this suite. 06/22/23 10:35:35.199
------------------------------
• [0.052 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:35:35.156
    Jun 22 10:35:35.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename sysctl 06/22/23 10:35:35.157
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:35:35.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:35:35.18
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 06/22/23 10:35:35.185
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:35:35.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-5309" for this suite. 06/22/23 10:35:35.199
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:35:35.208
Jun 22 10:35:35.208: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename pods 06/22/23 10:35:35.21
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:35:35.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:35:35.233
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 06/22/23 10:35:35.237
Jun 22 10:35:35.247: INFO: Waiting up to 5m0s for pod "pod-8s9dh" in namespace "pods-3922" to be "running"
Jun 22 10:35:35.251: INFO: Pod "pod-8s9dh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.586258ms
Jun 22 10:35:37.256: INFO: Pod "pod-8s9dh": Phase="Running", Reason="", readiness=true. Elapsed: 2.009354596s
Jun 22 10:35:37.256: INFO: Pod "pod-8s9dh" satisfied condition "running"
STEP: patching /status 06/22/23 10:35:37.257
Jun 22 10:35:37.267: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun 22 10:35:37.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3922" for this suite. 06/22/23 10:35:37.273
------------------------------
• [2.074 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:35:35.208
    Jun 22 10:35:35.208: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename pods 06/22/23 10:35:35.21
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:35:35.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:35:35.233
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 06/22/23 10:35:35.237
    Jun 22 10:35:35.247: INFO: Waiting up to 5m0s for pod "pod-8s9dh" in namespace "pods-3922" to be "running"
    Jun 22 10:35:35.251: INFO: Pod "pod-8s9dh": Phase="Pending", Reason="", readiness=false. Elapsed: 3.586258ms
    Jun 22 10:35:37.256: INFO: Pod "pod-8s9dh": Phase="Running", Reason="", readiness=true. Elapsed: 2.009354596s
    Jun 22 10:35:37.256: INFO: Pod "pod-8s9dh" satisfied condition "running"
    STEP: patching /status 06/22/23 10:35:37.257
    Jun 22 10:35:37.267: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:35:37.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3922" for this suite. 06/22/23 10:35:37.273
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:35:37.283
Jun 22 10:35:37.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename pods 06/22/23 10:35:37.284
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:35:37.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:35:37.305
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 06/22/23 10:35:37.309
STEP: submitting the pod to kubernetes 06/22/23 10:35:37.309
Jun 22 10:35:37.319: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6" in namespace "pods-4758" to be "running and ready"
Jun 22 10:35:37.322: INFO: Pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.338032ms
Jun 22 10:35:37.323: INFO: The phase of Pod pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:35:39.328: INFO: Pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.008646995s
Jun 22 10:35:39.328: INFO: The phase of Pod pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6 is Running (Ready = true)
Jun 22 10:35:39.328: INFO: Pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 06/22/23 10:35:39.332
STEP: updating the pod 06/22/23 10:35:39.337
Jun 22 10:35:39.855: INFO: Successfully updated pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6"
Jun 22 10:35:39.856: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6" in namespace "pods-4758" to be "terminated with reason DeadlineExceeded"
Jun 22 10:35:39.860: INFO: Pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6": Phase="Running", Reason="", readiness=true. Elapsed: 4.737826ms
Jun 22 10:35:41.867: INFO: Pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.01117708s
Jun 22 10:35:43.867: INFO: Pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6": Phase="Running", Reason="", readiness=false. Elapsed: 4.011523202s
Jun 22 10:35:45.867: INFO: Pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.011650304s
Jun 22 10:35:45.867: INFO: Pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun 22 10:35:45.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4758" for this suite. 06/22/23 10:35:45.875
------------------------------
• [SLOW TEST] [8.603 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:35:37.283
    Jun 22 10:35:37.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename pods 06/22/23 10:35:37.284
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:35:37.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:35:37.305
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 06/22/23 10:35:37.309
    STEP: submitting the pod to kubernetes 06/22/23 10:35:37.309
    Jun 22 10:35:37.319: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6" in namespace "pods-4758" to be "running and ready"
    Jun 22 10:35:37.322: INFO: Pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.338032ms
    Jun 22 10:35:37.323: INFO: The phase of Pod pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:35:39.328: INFO: Pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.008646995s
    Jun 22 10:35:39.328: INFO: The phase of Pod pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6 is Running (Ready = true)
    Jun 22 10:35:39.328: INFO: Pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 06/22/23 10:35:39.332
    STEP: updating the pod 06/22/23 10:35:39.337
    Jun 22 10:35:39.855: INFO: Successfully updated pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6"
    Jun 22 10:35:39.856: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6" in namespace "pods-4758" to be "terminated with reason DeadlineExceeded"
    Jun 22 10:35:39.860: INFO: Pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6": Phase="Running", Reason="", readiness=true. Elapsed: 4.737826ms
    Jun 22 10:35:41.867: INFO: Pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.01117708s
    Jun 22 10:35:43.867: INFO: Pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6": Phase="Running", Reason="", readiness=false. Elapsed: 4.011523202s
    Jun 22 10:35:45.867: INFO: Pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.011650304s
    Jun 22 10:35:45.867: INFO: Pod "pod-update-activedeadlineseconds-370d31af-5fa8-4146-9b5f-ff8db50020f6" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:35:45.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4758" for this suite. 06/22/23 10:35:45.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:35:45.888
Jun 22 10:35:45.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename statefulset 06/22/23 10:35:45.89
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:35:45.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:35:45.925
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4043 06/22/23 10:35:45.931
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-4043 06/22/23 10:35:45.938
W0622 10:35:45.950252      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4043 06/22/23 10:35:45.95
Jun 22 10:35:45.955: INFO: Found 0 stateful pods, waiting for 1
Jun 22 10:35:55.961: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 06/22/23 10:35:55.961
Jun 22 10:35:55.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4043 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 10:35:56.156: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 10:35:56.157: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 10:35:56.157: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 10:35:56.161: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 22 10:36:06.169: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 10:36:06.169: INFO: Waiting for statefulset status.replicas updated to 0
W0622 10:36:06.184392      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:36:06.188: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jun 22 10:36:06.188: INFO: ss-0  wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:35:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:35:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:35:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:35:45 +0000 UTC  }]
Jun 22 10:36:06.188: INFO: 
Jun 22 10:36:06.188: INFO: StatefulSet ss has not reached scale 3, at 1
Jun 22 10:36:07.195: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99527462s
Jun 22 10:36:08.201: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988820039s
Jun 22 10:36:09.209: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982008935s
Jun 22 10:36:10.214: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974972344s
Jun 22 10:36:11.220: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.969452545s
Jun 22 10:36:12.226: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963780981s
Jun 22 10:36:13.232: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957459336s
Jun 22 10:36:14.238: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.951542728s
Jun 22 10:36:15.244: INFO: Verifying statefulset ss doesn't scale past 3 for another 945.714175ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4043 06/22/23 10:36:16.244
Jun 22 10:36:16.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4043 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 10:36:16.427: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 10:36:16.427: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 10:36:16.427: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 10:36:16.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4043 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 10:36:16.609: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 22 10:36:16.609: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 10:36:16.609: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 10:36:16.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4043 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 10:36:16.788: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 22 10:36:16.788: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 10:36:16.788: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 10:36:16.794: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jun 22 10:36:26.803: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 10:36:26.803: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 10:36:26.803: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 06/22/23 10:36:26.803
Jun 22 10:36:26.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4043 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 10:36:26.981: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 10:36:26.982: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 10:36:26.982: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 10:36:26.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4043 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 10:36:27.166: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 10:36:27.166: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 10:36:27.166: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 10:36:27.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4043 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 10:36:27.337: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 10:36:27.337: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 10:36:27.337: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 10:36:27.337: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 10:36:27.343: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun 22 10:36:37.358: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 10:36:37.358: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 10:36:37.358: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
W0622 10:36:37.372874      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:36:37.378: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jun 22 10:36:37.378: INFO: ss-0  wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:35:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:35:45 +0000 UTC  }]
Jun 22 10:36:37.378: INFO: ss-1  wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:06 +0000 UTC  }]
Jun 22 10:36:37.378: INFO: ss-2  wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:06 +0000 UTC  }]
Jun 22 10:36:37.378: INFO: 
Jun 22 10:36:37.378: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 22 10:36:38.385: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jun 22 10:36:38.385: INFO: ss-0  wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:35:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:35:45 +0000 UTC  }]
Jun 22 10:36:38.385: INFO: ss-1  wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:06 +0000 UTC  }]
Jun 22 10:36:38.385: INFO: 
Jun 22 10:36:38.385: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 10:36:39.391: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.98603525s
Jun 22 10:36:40.397: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.981005211s
Jun 22 10:36:41.403: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.975334477s
Jun 22 10:36:42.408: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.969439331s
Jun 22 10:36:43.413: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.964309673s
Jun 22 10:36:44.419: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.958260942s
Jun 22 10:36:45.424: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.95301332s
Jun 22 10:36:46.430: INFO: Verifying statefulset ss doesn't scale past 0 for another 947.816773ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4043 06/22/23 10:36:47.43
Jun 22 10:36:47.436: INFO: Scaling statefulset ss to 0
W0622 10:36:47.447994      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:36:47.452: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun 22 10:36:47.456: INFO: Deleting all statefulset in ns statefulset-4043
Jun 22 10:36:47.461: INFO: Scaling statefulset ss to 0
W0622 10:36:47.471368      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:36:47.475: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 10:36:47.479: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun 22 10:36:47.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4043" for this suite. 06/22/23 10:36:47.503
------------------------------
• [SLOW TEST] [61.624 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:35:45.888
    Jun 22 10:35:45.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename statefulset 06/22/23 10:35:45.89
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:35:45.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:35:45.925
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4043 06/22/23 10:35:45.931
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-4043 06/22/23 10:35:45.938
    W0622 10:35:45.950252      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4043 06/22/23 10:35:45.95
    Jun 22 10:35:45.955: INFO: Found 0 stateful pods, waiting for 1
    Jun 22 10:35:55.961: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 06/22/23 10:35:55.961
    Jun 22 10:35:55.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4043 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 22 10:35:56.156: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 22 10:35:56.157: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 22 10:35:56.157: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 22 10:35:56.161: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jun 22 10:36:06.169: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun 22 10:36:06.169: INFO: Waiting for statefulset status.replicas updated to 0
    W0622 10:36:06.184392      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:36:06.188: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
    Jun 22 10:36:06.188: INFO: ss-0  wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:35:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:35:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:35:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:35:45 +0000 UTC  }]
    Jun 22 10:36:06.188: INFO: 
    Jun 22 10:36:06.188: INFO: StatefulSet ss has not reached scale 3, at 1
    Jun 22 10:36:07.195: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99527462s
    Jun 22 10:36:08.201: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988820039s
    Jun 22 10:36:09.209: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982008935s
    Jun 22 10:36:10.214: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974972344s
    Jun 22 10:36:11.220: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.969452545s
    Jun 22 10:36:12.226: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963780981s
    Jun 22 10:36:13.232: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957459336s
    Jun 22 10:36:14.238: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.951542728s
    Jun 22 10:36:15.244: INFO: Verifying statefulset ss doesn't scale past 3 for another 945.714175ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4043 06/22/23 10:36:16.244
    Jun 22 10:36:16.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4043 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 22 10:36:16.427: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 22 10:36:16.427: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 22 10:36:16.427: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 22 10:36:16.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4043 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 22 10:36:16.609: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jun 22 10:36:16.609: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 22 10:36:16.609: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 22 10:36:16.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4043 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 22 10:36:16.788: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jun 22 10:36:16.788: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 22 10:36:16.788: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 22 10:36:16.794: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jun 22 10:36:26.803: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 22 10:36:26.803: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 22 10:36:26.803: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 06/22/23 10:36:26.803
    Jun 22 10:36:26.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4043 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 22 10:36:26.981: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 22 10:36:26.982: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 22 10:36:26.982: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 22 10:36:26.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4043 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 22 10:36:27.166: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 22 10:36:27.166: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 22 10:36:27.166: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 22 10:36:27.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4043 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 22 10:36:27.337: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 22 10:36:27.337: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 22 10:36:27.337: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 22 10:36:27.337: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 22 10:36:27.343: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jun 22 10:36:37.358: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun 22 10:36:37.358: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jun 22 10:36:37.358: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    W0622 10:36:37.372874      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:36:37.378: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
    Jun 22 10:36:37.378: INFO: ss-0  wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:35:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:35:45 +0000 UTC  }]
    Jun 22 10:36:37.378: INFO: ss-1  wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:06 +0000 UTC  }]
    Jun 22 10:36:37.378: INFO: ss-2  wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:06 +0000 UTC  }]
    Jun 22 10:36:37.378: INFO: 
    Jun 22 10:36:37.378: INFO: StatefulSet ss has not reached scale 0, at 3
    Jun 22 10:36:38.385: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
    Jun 22 10:36:38.385: INFO: ss-0  wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:35:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:35:45 +0000 UTC  }]
    Jun 22 10:36:38.385: INFO: ss-1  wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 10:36:06 +0000 UTC  }]
    Jun 22 10:36:38.385: INFO: 
    Jun 22 10:36:38.385: INFO: StatefulSet ss has not reached scale 0, at 2
    Jun 22 10:36:39.391: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.98603525s
    Jun 22 10:36:40.397: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.981005211s
    Jun 22 10:36:41.403: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.975334477s
    Jun 22 10:36:42.408: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.969439331s
    Jun 22 10:36:43.413: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.964309673s
    Jun 22 10:36:44.419: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.958260942s
    Jun 22 10:36:45.424: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.95301332s
    Jun 22 10:36:46.430: INFO: Verifying statefulset ss doesn't scale past 0 for another 947.816773ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4043 06/22/23 10:36:47.43
    Jun 22 10:36:47.436: INFO: Scaling statefulset ss to 0
    W0622 10:36:47.447994      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:36:47.452: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun 22 10:36:47.456: INFO: Deleting all statefulset in ns statefulset-4043
    Jun 22 10:36:47.461: INFO: Scaling statefulset ss to 0
    W0622 10:36:47.471368      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:36:47.475: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 22 10:36:47.479: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:36:47.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4043" for this suite. 06/22/23 10:36:47.503
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:36:47.513
Jun 22 10:36:47.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename dns 06/22/23 10:36:47.515
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:36:47.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:36:47.541
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 06/22/23 10:36:47.545
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4033 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4033;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4033 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4033;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4033.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4033.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4033.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4033.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4033.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4033.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4033.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4033.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4033.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4033.svc;check="$$(dig +notcp +noall +answer +search 43.139.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.139.43_udp@PTR;check="$$(dig +tcp +noall +answer +search 43.139.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.139.43_tcp@PTR;sleep 1; done
 06/22/23 10:36:47.569
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4033 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4033;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4033 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4033;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4033.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4033.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4033.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4033.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4033.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4033.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4033.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4033.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4033.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4033.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4033.svc;check="$$(dig +notcp +noall +answer +search 43.139.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.139.43_udp@PTR;check="$$(dig +tcp +noall +answer +search 43.139.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.139.43_tcp@PTR;sleep 1; done
 06/22/23 10:36:47.57
STEP: creating a pod to probe DNS 06/22/23 10:36:47.57
STEP: submitting the pod to kubernetes 06/22/23 10:36:47.57
W0622 10:36:47.583924      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:36:47.584: INFO: Waiting up to 15m0s for pod "dns-test-de7ba60f-a839-4441-9969-691c6db788be" in namespace "dns-4033" to be "running"
Jun 22 10:36:47.588: INFO: Pod "dns-test-de7ba60f-a839-4441-9969-691c6db788be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046244ms
Jun 22 10:36:49.594: INFO: Pod "dns-test-de7ba60f-a839-4441-9969-691c6db788be": Phase="Running", Reason="", readiness=true. Elapsed: 2.010245968s
Jun 22 10:36:49.594: INFO: Pod "dns-test-de7ba60f-a839-4441-9969-691c6db788be" satisfied condition "running"
STEP: retrieving the pod 06/22/23 10:36:49.594
STEP: looking for the results for each expected name from probers 06/22/23 10:36:49.599
Jun 22 10:36:49.609: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:49.615: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:49.621: INFO: Unable to read wheezy_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:49.627: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:49.643: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:49.648: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:49.673: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:49.677: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:49.682: INFO: Unable to read jessie_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:49.688: INFO: Unable to read jessie_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:49.704: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:49.709: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:49.730: INFO: Lookups using dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4033 wheezy_tcp@dns-test-service.dns-4033 wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4033 jessie_tcp@dns-test-service.dns-4033 jessie_udp@_http._tcp.dns-test-service.dns-4033.svc jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc]

Jun 22 10:36:54.738: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:54.743: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:54.748: INFO: Unable to read wheezy_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:54.754: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:54.769: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:54.774: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:54.798: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:54.802: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:54.806: INFO: Unable to read jessie_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:54.810: INFO: Unable to read jessie_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:54.823: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:54.828: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:54.847: INFO: Lookups using dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4033 wheezy_tcp@dns-test-service.dns-4033 wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4033 jessie_tcp@dns-test-service.dns-4033 jessie_udp@_http._tcp.dns-test-service.dns-4033.svc jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc]

Jun 22 10:36:59.739: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:59.744: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:59.750: INFO: Unable to read wheezy_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:59.755: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:59.770: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:59.775: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:59.801: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:59.806: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:59.811: INFO: Unable to read jessie_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:59.817: INFO: Unable to read jessie_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:59.831: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:59.835: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:36:59.854: INFO: Lookups using dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4033 wheezy_tcp@dns-test-service.dns-4033 wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4033 jessie_tcp@dns-test-service.dns-4033 jessie_udp@_http._tcp.dns-test-service.dns-4033.svc jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc]

Jun 22 10:37:04.737: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:04.743: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:04.748: INFO: Unable to read wheezy_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:04.754: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:04.768: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:04.773: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:04.797: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:04.801: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:04.806: INFO: Unable to read jessie_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:04.810: INFO: Unable to read jessie_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:04.823: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:04.829: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:04.848: INFO: Lookups using dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4033 wheezy_tcp@dns-test-service.dns-4033 wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4033 jessie_tcp@dns-test-service.dns-4033 jessie_udp@_http._tcp.dns-test-service.dns-4033.svc jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc]

Jun 22 10:37:09.738: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:09.744: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:09.749: INFO: Unable to read wheezy_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:09.757: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:09.775: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:09.780: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:09.806: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:09.811: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:09.816: INFO: Unable to read jessie_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:09.821: INFO: Unable to read jessie_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:09.835: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:09.839: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:09.859: INFO: Lookups using dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4033 wheezy_tcp@dns-test-service.dns-4033 wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4033 jessie_tcp@dns-test-service.dns-4033 jessie_udp@_http._tcp.dns-test-service.dns-4033.svc jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc]

Jun 22 10:37:14.739: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:14.744: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:14.750: INFO: Unable to read wheezy_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:14.755: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:14.770: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:14.774: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:14.796: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:14.801: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:14.805: INFO: Unable to read jessie_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:14.809: INFO: Unable to read jessie_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:14.823: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:14.828: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:14.846: INFO: Lookups using dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4033 wheezy_tcp@dns-test-service.dns-4033 wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4033 jessie_tcp@dns-test-service.dns-4033 jessie_udp@_http._tcp.dns-test-service.dns-4033.svc jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc]

Jun 22 10:37:19.775: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:19.781: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
Jun 22 10:37:19.856: INFO: Lookups using dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc]

Jun 22 10:37:24.860: INFO: DNS probes using dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be succeeded

STEP: deleting the pod 06/22/23 10:37:24.86
STEP: deleting the test service 06/22/23 10:37:24.879
STEP: deleting the test headless service 06/22/23 10:37:24.911
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun 22 10:37:24.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4033" for this suite. 06/22/23 10:37:24.932
------------------------------
• [SLOW TEST] [37.430 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:36:47.513
    Jun 22 10:36:47.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename dns 06/22/23 10:36:47.515
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:36:47.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:36:47.541
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 06/22/23 10:36:47.545
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4033 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4033;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4033 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4033;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4033.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4033.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4033.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4033.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4033.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4033.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4033.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4033.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4033.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4033.svc;check="$$(dig +notcp +noall +answer +search 43.139.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.139.43_udp@PTR;check="$$(dig +tcp +noall +answer +search 43.139.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.139.43_tcp@PTR;sleep 1; done
     06/22/23 10:36:47.569
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4033 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4033;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4033 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4033;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4033.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4033.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4033.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4033.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4033.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4033.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4033.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4033.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4033.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4033.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4033.svc;check="$$(dig +notcp +noall +answer +search 43.139.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.139.43_udp@PTR;check="$$(dig +tcp +noall +answer +search 43.139.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.139.43_tcp@PTR;sleep 1; done
     06/22/23 10:36:47.57
    STEP: creating a pod to probe DNS 06/22/23 10:36:47.57
    STEP: submitting the pod to kubernetes 06/22/23 10:36:47.57
    W0622 10:36:47.583924      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:36:47.584: INFO: Waiting up to 15m0s for pod "dns-test-de7ba60f-a839-4441-9969-691c6db788be" in namespace "dns-4033" to be "running"
    Jun 22 10:36:47.588: INFO: Pod "dns-test-de7ba60f-a839-4441-9969-691c6db788be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046244ms
    Jun 22 10:36:49.594: INFO: Pod "dns-test-de7ba60f-a839-4441-9969-691c6db788be": Phase="Running", Reason="", readiness=true. Elapsed: 2.010245968s
    Jun 22 10:36:49.594: INFO: Pod "dns-test-de7ba60f-a839-4441-9969-691c6db788be" satisfied condition "running"
    STEP: retrieving the pod 06/22/23 10:36:49.594
    STEP: looking for the results for each expected name from probers 06/22/23 10:36:49.599
    Jun 22 10:36:49.609: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:49.615: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:49.621: INFO: Unable to read wheezy_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:49.627: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:49.643: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:49.648: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:49.673: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:49.677: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:49.682: INFO: Unable to read jessie_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:49.688: INFO: Unable to read jessie_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:49.704: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:49.709: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:49.730: INFO: Lookups using dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4033 wheezy_tcp@dns-test-service.dns-4033 wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4033 jessie_tcp@dns-test-service.dns-4033 jessie_udp@_http._tcp.dns-test-service.dns-4033.svc jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc]

    Jun 22 10:36:54.738: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:54.743: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:54.748: INFO: Unable to read wheezy_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:54.754: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:54.769: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:54.774: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:54.798: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:54.802: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:54.806: INFO: Unable to read jessie_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:54.810: INFO: Unable to read jessie_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:54.823: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:54.828: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:54.847: INFO: Lookups using dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4033 wheezy_tcp@dns-test-service.dns-4033 wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4033 jessie_tcp@dns-test-service.dns-4033 jessie_udp@_http._tcp.dns-test-service.dns-4033.svc jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc]

    Jun 22 10:36:59.739: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:59.744: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:59.750: INFO: Unable to read wheezy_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:59.755: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:59.770: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:59.775: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:59.801: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:59.806: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:59.811: INFO: Unable to read jessie_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:59.817: INFO: Unable to read jessie_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:59.831: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:59.835: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:36:59.854: INFO: Lookups using dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4033 wheezy_tcp@dns-test-service.dns-4033 wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4033 jessie_tcp@dns-test-service.dns-4033 jessie_udp@_http._tcp.dns-test-service.dns-4033.svc jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc]

    Jun 22 10:37:04.737: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:04.743: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:04.748: INFO: Unable to read wheezy_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:04.754: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:04.768: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:04.773: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:04.797: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:04.801: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:04.806: INFO: Unable to read jessie_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:04.810: INFO: Unable to read jessie_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:04.823: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:04.829: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:04.848: INFO: Lookups using dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4033 wheezy_tcp@dns-test-service.dns-4033 wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4033 jessie_tcp@dns-test-service.dns-4033 jessie_udp@_http._tcp.dns-test-service.dns-4033.svc jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc]

    Jun 22 10:37:09.738: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:09.744: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:09.749: INFO: Unable to read wheezy_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:09.757: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:09.775: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:09.780: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:09.806: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:09.811: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:09.816: INFO: Unable to read jessie_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:09.821: INFO: Unable to read jessie_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:09.835: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:09.839: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:09.859: INFO: Lookups using dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4033 wheezy_tcp@dns-test-service.dns-4033 wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4033 jessie_tcp@dns-test-service.dns-4033 jessie_udp@_http._tcp.dns-test-service.dns-4033.svc jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc]

    Jun 22 10:37:14.739: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:14.744: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:14.750: INFO: Unable to read wheezy_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:14.755: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:14.770: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:14.774: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:14.796: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:14.801: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:14.805: INFO: Unable to read jessie_udp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:14.809: INFO: Unable to read jessie_tcp@dns-test-service.dns-4033 from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:14.823: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:14.828: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:14.846: INFO: Lookups using dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4033 wheezy_tcp@dns-test-service.dns-4033 wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4033 jessie_tcp@dns-test-service.dns-4033 jessie_udp@_http._tcp.dns-test-service.dns-4033.svc jessie_tcp@_http._tcp.dns-test-service.dns-4033.svc]

    Jun 22 10:37:19.775: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:19.781: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc from pod dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be: the server could not find the requested resource (get pods dns-test-de7ba60f-a839-4441-9969-691c6db788be)
    Jun 22 10:37:19.856: INFO: Lookups using dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-4033.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4033.svc]

    Jun 22 10:37:24.860: INFO: DNS probes using dns-4033/dns-test-de7ba60f-a839-4441-9969-691c6db788be succeeded

    STEP: deleting the pod 06/22/23 10:37:24.86
    STEP: deleting the test service 06/22/23 10:37:24.879
    STEP: deleting the test headless service 06/22/23 10:37:24.911
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:37:24.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4033" for this suite. 06/22/23 10:37:24.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:37:24.948
Jun 22 10:37:24.948: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename webhook 06/22/23 10:37:24.949
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:37:24.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:37:24.983
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/22/23 10:37:25.007
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 10:37:25.262
STEP: Deploying the webhook pod 06/22/23 10:37:25.274
W0622 10:37:25.291861      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 10:37:25.291
Jun 22 10:37:25.302: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/22/23 10:37:27.315
STEP: Verifying the service has paired with the endpoint 06/22/23 10:37:27.332
Jun 22 10:37:28.333: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Jun 22 10:37:28.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2715-crds.webhook.example.com via the AdmissionRegistration API 06/22/23 10:37:28.857
STEP: Creating a custom resource that should be mutated by the webhook 06/22/23 10:37:28.881
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:37:31.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-770" for this suite. 06/22/23 10:37:31.535
STEP: Destroying namespace "webhook-770-markers" for this suite. 06/22/23 10:37:31.543
------------------------------
• [SLOW TEST] [6.603 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:37:24.948
    Jun 22 10:37:24.948: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename webhook 06/22/23 10:37:24.949
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:37:24.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:37:24.983
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/22/23 10:37:25.007
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 10:37:25.262
    STEP: Deploying the webhook pod 06/22/23 10:37:25.274
    W0622 10:37:25.291861      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 10:37:25.291
    Jun 22 10:37:25.302: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/22/23 10:37:27.315
    STEP: Verifying the service has paired with the endpoint 06/22/23 10:37:27.332
    Jun 22 10:37:28.333: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Jun 22 10:37:28.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2715-crds.webhook.example.com via the AdmissionRegistration API 06/22/23 10:37:28.857
    STEP: Creating a custom resource that should be mutated by the webhook 06/22/23 10:37:28.881
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:37:31.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-770" for this suite. 06/22/23 10:37:31.535
    STEP: Destroying namespace "webhook-770-markers" for this suite. 06/22/23 10:37:31.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:37:31.551
Jun 22 10:37:31.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename configmap 06/22/23 10:37:31.552
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:37:31.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:37:31.577
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-3973/configmap-test-023ad3f7-831c-4002-84cf-4b0ba7dd3e15 06/22/23 10:37:31.582
STEP: Creating a pod to test consume configMaps 06/22/23 10:37:31.586
W0622 10:37:31.597936      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "env-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "env-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "env-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "env-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:37:31.598: INFO: Waiting up to 5m0s for pod "pod-configmaps-3c641e2e-a028-4d51-9b1e-5afb03f66a75" in namespace "configmap-3973" to be "Succeeded or Failed"
Jun 22 10:37:31.602: INFO: Pod "pod-configmaps-3c641e2e-a028-4d51-9b1e-5afb03f66a75": Phase="Pending", Reason="", readiness=false. Elapsed: 4.139164ms
Jun 22 10:37:33.608: INFO: Pod "pod-configmaps-3c641e2e-a028-4d51-9b1e-5afb03f66a75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010698382s
Jun 22 10:37:35.607: INFO: Pod "pod-configmaps-3c641e2e-a028-4d51-9b1e-5afb03f66a75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009672284s
STEP: Saw pod success 06/22/23 10:37:35.607
Jun 22 10:37:35.608: INFO: Pod "pod-configmaps-3c641e2e-a028-4d51-9b1e-5afb03f66a75" satisfied condition "Succeeded or Failed"
Jun 22 10:37:35.612: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-configmaps-3c641e2e-a028-4d51-9b1e-5afb03f66a75 container env-test: <nil>
STEP: delete the pod 06/22/23 10:37:35.629
Jun 22 10:37:35.643: INFO: Waiting for pod pod-configmaps-3c641e2e-a028-4d51-9b1e-5afb03f66a75 to disappear
Jun 22 10:37:35.647: INFO: Pod pod-configmaps-3c641e2e-a028-4d51-9b1e-5afb03f66a75 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun 22 10:37:35.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3973" for this suite. 06/22/23 10:37:35.653
------------------------------
• [4.110 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:37:31.551
    Jun 22 10:37:31.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename configmap 06/22/23 10:37:31.552
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:37:31.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:37:31.577
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-3973/configmap-test-023ad3f7-831c-4002-84cf-4b0ba7dd3e15 06/22/23 10:37:31.582
    STEP: Creating a pod to test consume configMaps 06/22/23 10:37:31.586
    W0622 10:37:31.597936      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "env-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "env-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "env-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "env-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:37:31.598: INFO: Waiting up to 5m0s for pod "pod-configmaps-3c641e2e-a028-4d51-9b1e-5afb03f66a75" in namespace "configmap-3973" to be "Succeeded or Failed"
    Jun 22 10:37:31.602: INFO: Pod "pod-configmaps-3c641e2e-a028-4d51-9b1e-5afb03f66a75": Phase="Pending", Reason="", readiness=false. Elapsed: 4.139164ms
    Jun 22 10:37:33.608: INFO: Pod "pod-configmaps-3c641e2e-a028-4d51-9b1e-5afb03f66a75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010698382s
    Jun 22 10:37:35.607: INFO: Pod "pod-configmaps-3c641e2e-a028-4d51-9b1e-5afb03f66a75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009672284s
    STEP: Saw pod success 06/22/23 10:37:35.607
    Jun 22 10:37:35.608: INFO: Pod "pod-configmaps-3c641e2e-a028-4d51-9b1e-5afb03f66a75" satisfied condition "Succeeded or Failed"
    Jun 22 10:37:35.612: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-configmaps-3c641e2e-a028-4d51-9b1e-5afb03f66a75 container env-test: <nil>
    STEP: delete the pod 06/22/23 10:37:35.629
    Jun 22 10:37:35.643: INFO: Waiting for pod pod-configmaps-3c641e2e-a028-4d51-9b1e-5afb03f66a75 to disappear
    Jun 22 10:37:35.647: INFO: Pod pod-configmaps-3c641e2e-a028-4d51-9b1e-5afb03f66a75 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:37:35.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3973" for this suite. 06/22/23 10:37:35.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:37:35.667
Jun 22 10:37:35.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename deployment 06/22/23 10:37:35.668
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:37:35.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:37:35.701
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 06/22/23 10:37:35.71
W0622 10:37:35.717930      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-deployment" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-deployment" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-deployment" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-deployment" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: waiting for Deployment to be created 06/22/23 10:37:35.718
STEP: waiting for all Replicas to be Ready 06/22/23 10:37:35.719
Jun 22 10:37:35.721: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 22 10:37:35.722: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 22 10:37:35.736: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 22 10:37:35.736: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 22 10:37:35.750: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 22 10:37:35.750: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 22 10:37:35.771: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 22 10:37:35.771: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 22 10:37:36.582: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun 22 10:37:36.582: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun 22 10:37:37.661: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 06/22/23 10:37:37.662
W0622 10:37:37.682027      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
W0622 10:37:37.682172      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-deployment" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-deployment" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-deployment" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-deployment" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:37:37.687: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 06/22/23 10:37:37.687
Jun 22 10:37:37.692: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0
Jun 22 10:37:37.692: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0
Jun 22 10:37:37.692: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0
Jun 22 10:37:37.692: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0
Jun 22 10:37:37.693: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0
Jun 22 10:37:37.693: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0
Jun 22 10:37:37.693: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0
Jun 22 10:37:37.693: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0
Jun 22 10:37:37.693: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
Jun 22 10:37:37.693: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
Jun 22 10:37:37.694: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
Jun 22 10:37:37.694: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
Jun 22 10:37:37.694: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
Jun 22 10:37:37.694: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
Jun 22 10:37:37.716: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
Jun 22 10:37:37.716: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
Jun 22 10:37:37.746: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
Jun 22 10:37:37.746: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
Jun 22 10:37:37.805: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
Jun 22 10:37:37.805: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
Jun 22 10:37:37.829: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
Jun 22 10:37:37.829: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
Jun 22 10:37:38.598: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
Jun 22 10:37:38.598: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
Jun 22 10:37:38.662: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
STEP: listing Deployments 06/22/23 10:37:38.662
Jun 22 10:37:38.671: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 06/22/23 10:37:38.671
W0622 10:37:38.696051      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-deployment" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-deployment" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-deployment" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-deployment" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:37:38.700: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 06/22/23 10:37:38.7
Jun 22 10:37:38.711: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 22 10:37:38.723: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 22 10:37:38.755: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 22 10:37:38.783: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 22 10:37:38.795: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 22 10:37:39.544: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 22 10:37:39.627: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jun 22 10:37:39.670: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 22 10:37:39.696: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 22 10:37:40.567: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 06/22/23 10:37:40.603
STEP: fetching the DeploymentStatus 06/22/23 10:37:40.613
Jun 22 10:37:40.622: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
Jun 22 10:37:40.622: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
Jun 22 10:37:40.622: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
Jun 22 10:37:40.623: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
Jun 22 10:37:40.623: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
Jun 22 10:37:40.623: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
Jun 22 10:37:40.623: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 3
Jun 22 10:37:40.623: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
Jun 22 10:37:40.623: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
Jun 22 10:37:40.624: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 3
STEP: deleting the Deployment 06/22/23 10:37:40.624
Jun 22 10:37:40.638: INFO: observed event type MODIFIED
Jun 22 10:37:40.638: INFO: observed event type MODIFIED
Jun 22 10:37:40.638: INFO: observed event type MODIFIED
Jun 22 10:37:40.638: INFO: observed event type MODIFIED
Jun 22 10:37:40.638: INFO: observed event type MODIFIED
Jun 22 10:37:40.638: INFO: observed event type MODIFIED
Jun 22 10:37:40.639: INFO: observed event type MODIFIED
Jun 22 10:37:40.639: INFO: observed event type MODIFIED
Jun 22 10:37:40.639: INFO: observed event type MODIFIED
Jun 22 10:37:40.639: INFO: observed event type MODIFIED
Jun 22 10:37:40.639: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 22 10:37:40.644: INFO: Log out all the ReplicaSets if there is no deployment created
Jun 22 10:37:40.651: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-7377  6cad0e79-71b7-467a-9a7f-fd261118aa3f 135170 2 2023-06-22 10:37:38 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e29651de-c54a-4a76-8b34-5ae085659043 0xc0030733d7 0xc0030733d8}] [] [{kube-controller-manager Update apps/v1 2023-06-22 10:37:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e29651de-c54a-4a76-8b34-5ae085659043\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:37:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003073460 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jun 22 10:37:40.660: INFO: pod: "test-deployment-7b7876f9d6-fjdcz":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-fjdcz test-deployment-7b7876f9d6- deployment-7377  d94f9aaf-33a9-46ac-8d07-6a05786a5194 135169 0 2023-06-22 10:37:39 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 6cad0e79-71b7-467a-9a7f-fd261118aa3f 0xc0030738f7 0xc0030738f8}] [] [{kube-controller-manager Update v1 2023-06-22 10:37:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cad0e79-71b7-467a-9a7f-fd261118aa3f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:37:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xjx24,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xjx24,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.62,PodIP:100.96.2.203,StartTime:2023-06-22 10:37:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:37:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d5115320f7c713b8ddb9598a366ee38b8c1f2ab97eb75a72f9f960fa0068f1ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 22 10:37:40.661: INFO: pod: "test-deployment-7b7876f9d6-frftp":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-frftp test-deployment-7b7876f9d6- deployment-7377  aff082ac-cd41-4768-a451-5417b82b3a9a 135134 0 2023-06-22 10:37:38 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 6cad0e79-71b7-467a-9a7f-fd261118aa3f 0xc003073ad7 0xc003073ad8}] [] [{kube-controller-manager Update v1 2023-06-22 10:37:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cad0e79-71b7-467a-9a7f-fd261118aa3f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:37:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lkbgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lkbgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.38,StartTime:2023-06-22 10:37:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:37:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://18b3f276e590bf83c70b73a5d4df2ffe1bb4db5f7c453657769c32bda56f9912,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.38,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 22 10:37:40.661: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-7377  c4570655-eb1d-4d89-9fd9-ba0bd9f527e0 135178 4 2023-06-22 10:37:37 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment e29651de-c54a-4a76-8b34-5ae085659043 0xc0030734c7 0xc0030734c8}] [] [{kube-controller-manager Update apps/v1 2023-06-22 10:37:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e29651de-c54a-4a76-8b34-5ae085659043\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:37:40 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003073550 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jun 22 10:37:40.670: INFO: pod: "test-deployment-7df74c55ff-vzxjm":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-vzxjm test-deployment-7df74c55ff- deployment-7377  df49d99b-fd7a-4366-ba49-38edce890876 135143 0 2023-06-22 10:37:37 +0000 UTC 2023-06-22 10:37:40 +0000 UTC 0xc00405e0c8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff c4570655-eb1d-4d89-9fd9-ba0bd9f527e0 0xc00405e0f7 0xc00405e0f8}] [] [{kube-controller-manager Update v1 2023-06-22 10:37:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4570655-eb1d-4d89-9fd9-ba0bd9f527e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:37:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q7vvw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q7vvw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.37,StartTime:2023-06-22 10:37:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:37:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://9b2d3dee453927cd695661c29e92de3892474f55cbf9e925a0fd9f7ddacf1d40,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 22 10:37:40.671: INFO: pod: "test-deployment-7df74c55ff-zd9pc":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-zd9pc test-deployment-7df74c55ff- deployment-7377  e0711623-e0df-419e-9716-a067db815e25 135175 0 2023-06-22 10:37:38 +0000 UTC 2023-06-22 10:37:41 +0000 UTC 0xc00405e2b0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff c4570655-eb1d-4d89-9fd9-ba0bd9f527e0 0xc00405e947 0xc00405e948}] [] [{kube-controller-manager Update v1 2023-06-22 10:37:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4570655-eb1d-4d89-9fd9-ba0bd9f527e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:37:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fk8ln,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fk8ln,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.62,PodIP:100.96.2.202,StartTime:2023-06-22 10:37:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:37:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://ca7653bc4559580afd1eed583f9982f3d86b5bb9181efd98dd855a795e6d1fd1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 22 10:37:40.671: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-7377  e4595dd1-43d8-4c8f-ab8f-2a34408c10a4 135092 3 2023-06-22 10:37:35 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment e29651de-c54a-4a76-8b34-5ae085659043 0xc0030735b7 0xc0030735b8}] [] [{kube-controller-manager Update apps/v1 2023-06-22 10:37:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e29651de-c54a-4a76-8b34-5ae085659043\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:37:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003073640 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun 22 10:37:40.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7377" for this suite. 06/22/23 10:37:40.689
------------------------------
• [SLOW TEST] [5.029 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:37:35.667
    Jun 22 10:37:35.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename deployment 06/22/23 10:37:35.668
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:37:35.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:37:35.701
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 06/22/23 10:37:35.71
    W0622 10:37:35.717930      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-deployment" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-deployment" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-deployment" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-deployment" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: waiting for Deployment to be created 06/22/23 10:37:35.718
    STEP: waiting for all Replicas to be Ready 06/22/23 10:37:35.719
    Jun 22 10:37:35.721: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 22 10:37:35.722: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 22 10:37:35.736: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 22 10:37:35.736: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 22 10:37:35.750: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 22 10:37:35.750: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 22 10:37:35.771: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 22 10:37:35.771: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 22 10:37:36.582: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jun 22 10:37:36.582: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jun 22 10:37:37.661: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 06/22/23 10:37:37.662
    W0622 10:37:37.682027      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    W0622 10:37:37.682172      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-deployment" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-deployment" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-deployment" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-deployment" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:37:37.687: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 06/22/23 10:37:37.687
    Jun 22 10:37:37.692: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0
    Jun 22 10:37:37.692: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0
    Jun 22 10:37:37.692: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0
    Jun 22 10:37:37.692: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0
    Jun 22 10:37:37.693: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0
    Jun 22 10:37:37.693: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0
    Jun 22 10:37:37.693: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0
    Jun 22 10:37:37.693: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 0
    Jun 22 10:37:37.693: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
    Jun 22 10:37:37.693: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
    Jun 22 10:37:37.694: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
    Jun 22 10:37:37.694: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
    Jun 22 10:37:37.694: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
    Jun 22 10:37:37.694: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
    Jun 22 10:37:37.716: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
    Jun 22 10:37:37.716: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
    Jun 22 10:37:37.746: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
    Jun 22 10:37:37.746: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
    Jun 22 10:37:37.805: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
    Jun 22 10:37:37.805: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
    Jun 22 10:37:37.829: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
    Jun 22 10:37:37.829: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
    Jun 22 10:37:38.598: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
    Jun 22 10:37:38.598: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
    Jun 22 10:37:38.662: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
    STEP: listing Deployments 06/22/23 10:37:38.662
    Jun 22 10:37:38.671: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 06/22/23 10:37:38.671
    W0622 10:37:38.696051      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-deployment" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-deployment" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-deployment" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-deployment" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:37:38.700: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 06/22/23 10:37:38.7
    Jun 22 10:37:38.711: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 22 10:37:38.723: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 22 10:37:38.755: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 22 10:37:38.783: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 22 10:37:38.795: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 22 10:37:39.544: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 22 10:37:39.627: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 22 10:37:39.670: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 22 10:37:39.696: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 22 10:37:40.567: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 06/22/23 10:37:40.603
    STEP: fetching the DeploymentStatus 06/22/23 10:37:40.613
    Jun 22 10:37:40.622: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
    Jun 22 10:37:40.622: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
    Jun 22 10:37:40.622: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
    Jun 22 10:37:40.623: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
    Jun 22 10:37:40.623: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 1
    Jun 22 10:37:40.623: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
    Jun 22 10:37:40.623: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 3
    Jun 22 10:37:40.623: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
    Jun 22 10:37:40.623: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 2
    Jun 22 10:37:40.624: INFO: observed Deployment test-deployment in namespace deployment-7377 with ReadyReplicas 3
    STEP: deleting the Deployment 06/22/23 10:37:40.624
    Jun 22 10:37:40.638: INFO: observed event type MODIFIED
    Jun 22 10:37:40.638: INFO: observed event type MODIFIED
    Jun 22 10:37:40.638: INFO: observed event type MODIFIED
    Jun 22 10:37:40.638: INFO: observed event type MODIFIED
    Jun 22 10:37:40.638: INFO: observed event type MODIFIED
    Jun 22 10:37:40.638: INFO: observed event type MODIFIED
    Jun 22 10:37:40.639: INFO: observed event type MODIFIED
    Jun 22 10:37:40.639: INFO: observed event type MODIFIED
    Jun 22 10:37:40.639: INFO: observed event type MODIFIED
    Jun 22 10:37:40.639: INFO: observed event type MODIFIED
    Jun 22 10:37:40.639: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 22 10:37:40.644: INFO: Log out all the ReplicaSets if there is no deployment created
    Jun 22 10:37:40.651: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-7377  6cad0e79-71b7-467a-9a7f-fd261118aa3f 135170 2 2023-06-22 10:37:38 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment e29651de-c54a-4a76-8b34-5ae085659043 0xc0030733d7 0xc0030733d8}] [] [{kube-controller-manager Update apps/v1 2023-06-22 10:37:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e29651de-c54a-4a76-8b34-5ae085659043\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:37:40 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003073460 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jun 22 10:37:40.660: INFO: pod: "test-deployment-7b7876f9d6-fjdcz":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-fjdcz test-deployment-7b7876f9d6- deployment-7377  d94f9aaf-33a9-46ac-8d07-6a05786a5194 135169 0 2023-06-22 10:37:39 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 6cad0e79-71b7-467a-9a7f-fd261118aa3f 0xc0030738f7 0xc0030738f8}] [] [{kube-controller-manager Update v1 2023-06-22 10:37:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cad0e79-71b7-467a-9a7f-fd261118aa3f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:37:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xjx24,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xjx24,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.62,PodIP:100.96.2.203,StartTime:2023-06-22 10:37:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:37:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d5115320f7c713b8ddb9598a366ee38b8c1f2ab97eb75a72f9f960fa0068f1ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jun 22 10:37:40.661: INFO: pod: "test-deployment-7b7876f9d6-frftp":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-frftp test-deployment-7b7876f9d6- deployment-7377  aff082ac-cd41-4768-a451-5417b82b3a9a 135134 0 2023-06-22 10:37:38 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 6cad0e79-71b7-467a-9a7f-fd261118aa3f 0xc003073ad7 0xc003073ad8}] [] [{kube-controller-manager Update v1 2023-06-22 10:37:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6cad0e79-71b7-467a-9a7f-fd261118aa3f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:37:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lkbgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lkbgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.38,StartTime:2023-06-22 10:37:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:37:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://18b3f276e590bf83c70b73a5d4df2ffe1bb4db5f7c453657769c32bda56f9912,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.38,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jun 22 10:37:40.661: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-7377  c4570655-eb1d-4d89-9fd9-ba0bd9f527e0 135178 4 2023-06-22 10:37:37 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment e29651de-c54a-4a76-8b34-5ae085659043 0xc0030734c7 0xc0030734c8}] [] [{kube-controller-manager Update apps/v1 2023-06-22 10:37:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e29651de-c54a-4a76-8b34-5ae085659043\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:37:40 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003073550 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jun 22 10:37:40.670: INFO: pod: "test-deployment-7df74c55ff-vzxjm":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-vzxjm test-deployment-7df74c55ff- deployment-7377  df49d99b-fd7a-4366-ba49-38edce890876 135143 0 2023-06-22 10:37:37 +0000 UTC 2023-06-22 10:37:40 +0000 UTC 0xc00405e0c8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff c4570655-eb1d-4d89-9fd9-ba0bd9f527e0 0xc00405e0f7 0xc00405e0f8}] [] [{kube-controller-manager Update v1 2023-06-22 10:37:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4570655-eb1d-4d89-9fd9-ba0bd9f527e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:37:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q7vvw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q7vvw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.37,StartTime:2023-06-22 10:37:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:37:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://9b2d3dee453927cd695661c29e92de3892474f55cbf9e925a0fd9f7ddacf1d40,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jun 22 10:37:40.671: INFO: pod: "test-deployment-7df74c55ff-zd9pc":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-zd9pc test-deployment-7df74c55ff- deployment-7377  e0711623-e0df-419e-9716-a067db815e25 135175 0 2023-06-22 10:37:38 +0000 UTC 2023-06-22 10:37:41 +0000 UTC 0xc00405e2b0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff c4570655-eb1d-4d89-9fd9-ba0bd9f527e0 0xc00405e947 0xc00405e948}] [] [{kube-controller-manager Update v1 2023-06-22 10:37:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c4570655-eb1d-4d89-9fd9-ba0bd9f527e0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:37:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.2.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fk8ln,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fk8ln,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:37:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.224.62,PodIP:100.96.2.202,StartTime:2023-06-22 10:37:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:37:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://ca7653bc4559580afd1eed583f9982f3d86b5bb9181efd98dd855a795e6d1fd1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.2.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jun 22 10:37:40.671: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-7377  e4595dd1-43d8-4c8f-ab8f-2a34408c10a4 135092 3 2023-06-22 10:37:35 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment e29651de-c54a-4a76-8b34-5ae085659043 0xc0030735b7 0xc0030735b8}] [] [{kube-controller-manager Update apps/v1 2023-06-22 10:37:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e29651de-c54a-4a76-8b34-5ae085659043\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:37:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003073640 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:37:40.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7377" for this suite. 06/22/23 10:37:40.689
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:37:40.703
Jun 22 10:37:40.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename certificates 06/22/23 10:37:40.709
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:37:40.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:37:40.739
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 06/22/23 10:37:41.366
STEP: getting /apis/certificates.k8s.io 06/22/23 10:37:41.372
STEP: getting /apis/certificates.k8s.io/v1 06/22/23 10:37:41.374
STEP: creating 06/22/23 10:37:41.376
STEP: getting 06/22/23 10:37:41.401
STEP: listing 06/22/23 10:37:41.406
STEP: watching 06/22/23 10:37:41.411
Jun 22 10:37:41.411: INFO: starting watch
STEP: patching 06/22/23 10:37:41.413
STEP: updating 06/22/23 10:37:41.42
Jun 22 10:37:41.428: INFO: waiting for watch events with expected annotations
Jun 22 10:37:41.428: INFO: saw patched and updated annotations
STEP: getting /approval 06/22/23 10:37:41.428
STEP: patching /approval 06/22/23 10:37:41.432
STEP: updating /approval 06/22/23 10:37:41.44
STEP: getting /status 06/22/23 10:37:41.449
STEP: patching /status 06/22/23 10:37:41.453
STEP: updating /status 06/22/23 10:37:41.463
STEP: deleting 06/22/23 10:37:41.473
STEP: deleting a collection 06/22/23 10:37:41.49
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:37:41.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-5590" for this suite. 06/22/23 10:37:41.517
------------------------------
• [0.821 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:37:40.703
    Jun 22 10:37:40.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename certificates 06/22/23 10:37:40.709
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:37:40.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:37:40.739
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 06/22/23 10:37:41.366
    STEP: getting /apis/certificates.k8s.io 06/22/23 10:37:41.372
    STEP: getting /apis/certificates.k8s.io/v1 06/22/23 10:37:41.374
    STEP: creating 06/22/23 10:37:41.376
    STEP: getting 06/22/23 10:37:41.401
    STEP: listing 06/22/23 10:37:41.406
    STEP: watching 06/22/23 10:37:41.411
    Jun 22 10:37:41.411: INFO: starting watch
    STEP: patching 06/22/23 10:37:41.413
    STEP: updating 06/22/23 10:37:41.42
    Jun 22 10:37:41.428: INFO: waiting for watch events with expected annotations
    Jun 22 10:37:41.428: INFO: saw patched and updated annotations
    STEP: getting /approval 06/22/23 10:37:41.428
    STEP: patching /approval 06/22/23 10:37:41.432
    STEP: updating /approval 06/22/23 10:37:41.44
    STEP: getting /status 06/22/23 10:37:41.449
    STEP: patching /status 06/22/23 10:37:41.453
    STEP: updating /status 06/22/23 10:37:41.463
    STEP: deleting 06/22/23 10:37:41.473
    STEP: deleting a collection 06/22/23 10:37:41.49
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:37:41.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-5590" for this suite. 06/22/23 10:37:41.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:37:41.527
Jun 22 10:37:41.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename container-probe 06/22/23 10:37:41.529
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:37:41.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:37:41.554
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-cdc4c72a-ea97-4e81-8060-71834a60c444 in namespace container-probe-8967 06/22/23 10:37:41.558
W0622 10:37:41.568671      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:37:41.568: INFO: Waiting up to 5m0s for pod "liveness-cdc4c72a-ea97-4e81-8060-71834a60c444" in namespace "container-probe-8967" to be "not pending"
Jun 22 10:37:41.572: INFO: Pod "liveness-cdc4c72a-ea97-4e81-8060-71834a60c444": Phase="Pending", Reason="", readiness=false. Elapsed: 3.707365ms
Jun 22 10:37:43.578: INFO: Pod "liveness-cdc4c72a-ea97-4e81-8060-71834a60c444": Phase="Running", Reason="", readiness=true. Elapsed: 2.009715418s
Jun 22 10:37:43.578: INFO: Pod "liveness-cdc4c72a-ea97-4e81-8060-71834a60c444" satisfied condition "not pending"
Jun 22 10:37:43.578: INFO: Started pod liveness-cdc4c72a-ea97-4e81-8060-71834a60c444 in namespace container-probe-8967
STEP: checking the pod's current state and verifying that restartCount is present 06/22/23 10:37:43.578
Jun 22 10:37:43.583: INFO: Initial restart count of pod liveness-cdc4c72a-ea97-4e81-8060-71834a60c444 is 0
STEP: deleting the pod 06/22/23 10:41:44.357
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun 22 10:41:44.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8967" for this suite. 06/22/23 10:41:44.389
------------------------------
• [SLOW TEST] [242.873 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:37:41.527
    Jun 22 10:37:41.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename container-probe 06/22/23 10:37:41.529
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:37:41.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:37:41.554
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-cdc4c72a-ea97-4e81-8060-71834a60c444 in namespace container-probe-8967 06/22/23 10:37:41.558
    W0622 10:37:41.568671      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:37:41.568: INFO: Waiting up to 5m0s for pod "liveness-cdc4c72a-ea97-4e81-8060-71834a60c444" in namespace "container-probe-8967" to be "not pending"
    Jun 22 10:37:41.572: INFO: Pod "liveness-cdc4c72a-ea97-4e81-8060-71834a60c444": Phase="Pending", Reason="", readiness=false. Elapsed: 3.707365ms
    Jun 22 10:37:43.578: INFO: Pod "liveness-cdc4c72a-ea97-4e81-8060-71834a60c444": Phase="Running", Reason="", readiness=true. Elapsed: 2.009715418s
    Jun 22 10:37:43.578: INFO: Pod "liveness-cdc4c72a-ea97-4e81-8060-71834a60c444" satisfied condition "not pending"
    Jun 22 10:37:43.578: INFO: Started pod liveness-cdc4c72a-ea97-4e81-8060-71834a60c444 in namespace container-probe-8967
    STEP: checking the pod's current state and verifying that restartCount is present 06/22/23 10:37:43.578
    Jun 22 10:37:43.583: INFO: Initial restart count of pod liveness-cdc4c72a-ea97-4e81-8060-71834a60c444 is 0
    STEP: deleting the pod 06/22/23 10:41:44.357
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:41:44.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8967" for this suite. 06/22/23 10:41:44.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:41:44.402
Jun 22 10:41:44.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubelet-test 06/22/23 10:41:44.406
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:41:44.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:41:44.446
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
W0622 10:41:44.468176      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting for pod completion 06/22/23 10:41:44.468
Jun 22 10:41:44.468: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesd8c95ab2-6839-4ff4-be6a-084c21db271a" in namespace "kubelet-test-1057" to be "completed"
Jun 22 10:41:44.474: INFO: Pod "agnhost-host-aliasesd8c95ab2-6839-4ff4-be6a-084c21db271a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.942691ms
Jun 22 10:41:46.479: INFO: Pod "agnhost-host-aliasesd8c95ab2-6839-4ff4-be6a-084c21db271a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011123499s
Jun 22 10:41:48.480: INFO: Pod "agnhost-host-aliasesd8c95ab2-6839-4ff4-be6a-084c21db271a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012091863s
Jun 22 10:41:48.480: INFO: Pod "agnhost-host-aliasesd8c95ab2-6839-4ff4-be6a-084c21db271a" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jun 22 10:41:48.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1057" for this suite. 06/22/23 10:41:48.517
------------------------------
• [4.123 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:41:44.402
    Jun 22 10:41:44.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubelet-test 06/22/23 10:41:44.406
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:41:44.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:41:44.446
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    W0622 10:41:44.468176      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Waiting for pod completion 06/22/23 10:41:44.468
    Jun 22 10:41:44.468: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesd8c95ab2-6839-4ff4-be6a-084c21db271a" in namespace "kubelet-test-1057" to be "completed"
    Jun 22 10:41:44.474: INFO: Pod "agnhost-host-aliasesd8c95ab2-6839-4ff4-be6a-084c21db271a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.942691ms
    Jun 22 10:41:46.479: INFO: Pod "agnhost-host-aliasesd8c95ab2-6839-4ff4-be6a-084c21db271a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011123499s
    Jun 22 10:41:48.480: INFO: Pod "agnhost-host-aliasesd8c95ab2-6839-4ff4-be6a-084c21db271a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012091863s
    Jun 22 10:41:48.480: INFO: Pod "agnhost-host-aliasesd8c95ab2-6839-4ff4-be6a-084c21db271a" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:41:48.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1057" for this suite. 06/22/23 10:41:48.517
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:41:48.526
Jun 22 10:41:48.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename csistoragecapacity 06/22/23 10:41:48.527
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:41:48.55
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:41:48.555
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 06/22/23 10:41:48.56
STEP: getting /apis/storage.k8s.io 06/22/23 10:41:48.566
STEP: getting /apis/storage.k8s.io/v1 06/22/23 10:41:48.568
STEP: creating 06/22/23 10:41:48.571
STEP: watching 06/22/23 10:41:48.595
Jun 22 10:41:48.596: INFO: starting watch
STEP: getting 06/22/23 10:41:48.605
STEP: listing in namespace 06/22/23 10:41:48.61
STEP: listing across namespaces 06/22/23 10:41:48.615
STEP: patching 06/22/23 10:41:48.62
STEP: updating 06/22/23 10:41:48.628
Jun 22 10:41:48.634: INFO: waiting for watch events with expected annotations in namespace
Jun 22 10:41:48.634: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 06/22/23 10:41:48.634
STEP: deleting a collection 06/22/23 10:41:48.65
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Jun 22 10:41:48.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-802" for this suite. 06/22/23 10:41:48.676
------------------------------
• [0.157 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:41:48.526
    Jun 22 10:41:48.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename csistoragecapacity 06/22/23 10:41:48.527
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:41:48.55
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:41:48.555
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 06/22/23 10:41:48.56
    STEP: getting /apis/storage.k8s.io 06/22/23 10:41:48.566
    STEP: getting /apis/storage.k8s.io/v1 06/22/23 10:41:48.568
    STEP: creating 06/22/23 10:41:48.571
    STEP: watching 06/22/23 10:41:48.595
    Jun 22 10:41:48.596: INFO: starting watch
    STEP: getting 06/22/23 10:41:48.605
    STEP: listing in namespace 06/22/23 10:41:48.61
    STEP: listing across namespaces 06/22/23 10:41:48.615
    STEP: patching 06/22/23 10:41:48.62
    STEP: updating 06/22/23 10:41:48.628
    Jun 22 10:41:48.634: INFO: waiting for watch events with expected annotations in namespace
    Jun 22 10:41:48.634: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 06/22/23 10:41:48.634
    STEP: deleting a collection 06/22/23 10:41:48.65
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:41:48.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-802" for this suite. 06/22/23 10:41:48.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:41:48.685
Jun 22 10:41:48.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubectl 06/22/23 10:41:48.687
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:41:48.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:41:48.712
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/22/23 10:41:48.717
Jun 22 10:41:48.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-993 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun 22 10:41:48.867: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"e2e-test-httpd-pod\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"e2e-test-httpd-pod\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"e2e-test-httpd-pod\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"e2e-test-httpd-pod\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Jun 22 10:41:48.867: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 06/22/23 10:41:48.867
STEP: verifying the pod e2e-test-httpd-pod was created 06/22/23 10:41:53.921
Jun 22 10:41:53.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-993 get pod e2e-test-httpd-pod -o json'
Jun 22 10:41:54.009: INFO: stderr: ""
Jun 22 10:41:54.009: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-06-22T10:41:48Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-993\",\n        \"resourceVersion\": \"136356\",\n        \"uid\": \"c00ed544-50f3-4a3b-a2e2-3fbaa97bb06a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-q5bg4\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-q5bg4\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-22T10:41:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-22T10:41:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-22T10:41:50Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-22T10:41:48Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://c6ee715916c3d3f4a35e7b02c268c1689546b9ca2f255eab8efabc59c5606e82\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-06-22T10:41:49Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.92.226.162\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.3.41\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.96.3.41\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-06-22T10:41:48Z\"\n    }\n}\n"
STEP: replace the image in the pod 06/22/23 10:41:54.01
Jun 22 10:41:54.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-993 replace -f -'
Jun 22 10:41:55.644: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"e2e-test-httpd-pod\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"e2e-test-httpd-pod\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"e2e-test-httpd-pod\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"e2e-test-httpd-pod\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Jun 22 10:41:55.644: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 06/22/23 10:41:55.644
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Jun 22 10:41:55.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-993 delete pods e2e-test-httpd-pod'
Jun 22 10:41:57.242: INFO: stderr: ""
Jun 22 10:41:57.243: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun 22 10:41:57.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-993" for this suite. 06/22/23 10:41:57.252
------------------------------
• [SLOW TEST] [8.574 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:41:48.685
    Jun 22 10:41:48.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubectl 06/22/23 10:41:48.687
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:41:48.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:41:48.712
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 06/22/23 10:41:48.717
    Jun 22 10:41:48.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-993 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jun 22 10:41:48.867: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"e2e-test-httpd-pod\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"e2e-test-httpd-pod\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"e2e-test-httpd-pod\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"e2e-test-httpd-pod\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
    Jun 22 10:41:48.867: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 06/22/23 10:41:48.867
    STEP: verifying the pod e2e-test-httpd-pod was created 06/22/23 10:41:53.921
    Jun 22 10:41:53.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-993 get pod e2e-test-httpd-pod -o json'
    Jun 22 10:41:54.009: INFO: stderr: ""
    Jun 22 10:41:54.009: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-06-22T10:41:48Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-993\",\n        \"resourceVersion\": \"136356\",\n        \"uid\": \"c00ed544-50f3-4a3b-a2e2-3fbaa97bb06a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-q5bg4\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-q5bg4\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-22T10:41:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-22T10:41:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-22T10:41:50Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-22T10:41:48Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://c6ee715916c3d3f4a35e7b02c268c1689546b9ca2f255eab8efabc59c5606e82\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-06-22T10:41:49Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.92.226.162\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.3.41\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.96.3.41\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-06-22T10:41:48Z\"\n    }\n}\n"
    STEP: replace the image in the pod 06/22/23 10:41:54.01
    Jun 22 10:41:54.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-993 replace -f -'
    Jun 22 10:41:55.644: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"e2e-test-httpd-pod\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"e2e-test-httpd-pod\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"e2e-test-httpd-pod\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"e2e-test-httpd-pod\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
    Jun 22 10:41:55.644: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 06/22/23 10:41:55.644
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Jun 22 10:41:55.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-993 delete pods e2e-test-httpd-pod'
    Jun 22 10:41:57.242: INFO: stderr: ""
    Jun 22 10:41:57.243: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:41:57.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-993" for this suite. 06/22/23 10:41:57.252
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:41:57.26
Jun 22 10:41:57.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename job 06/22/23 10:41:57.261
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:41:57.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:41:57.287
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 06/22/23 10:41:57.292
W0622 10:41:57.306751      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensure pods equal to parallelism count is attached to the job 06/22/23 10:41:57.306
STEP: patching /status 06/22/23 10:41:59.315
STEP: updating /status 06/22/23 10:41:59.351
STEP: get /status 06/22/23 10:41:59.366
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jun 22 10:41:59.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3877" for this suite. 06/22/23 10:41:59.38
------------------------------
• [2.166 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:41:57.26
    Jun 22 10:41:57.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename job 06/22/23 10:41:57.261
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:41:57.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:41:57.287
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 06/22/23 10:41:57.292
    W0622 10:41:57.306751      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensure pods equal to parallelism count is attached to the job 06/22/23 10:41:57.306
    STEP: patching /status 06/22/23 10:41:59.315
    STEP: updating /status 06/22/23 10:41:59.351
    STEP: get /status 06/22/23 10:41:59.366
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:41:59.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3877" for this suite. 06/22/23 10:41:59.38
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:41:59.428
Jun 22 10:41:59.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename security-context-test 06/22/23 10:41:59.429
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:41:59.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:41:59.513
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
W0622 10:41:59.529859      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "busybox-readonly-false-a1ddd532-8f34-43eb-b346-784a34d5c638" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-readonly-false-a1ddd532-8f34-43eb-b346-784a34d5c638" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-readonly-false-a1ddd532-8f34-43eb-b346-784a34d5c638" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-readonly-false-a1ddd532-8f34-43eb-b346-784a34d5c638" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:41:59.530: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-a1ddd532-8f34-43eb-b346-784a34d5c638" in namespace "security-context-test-5439" to be "Succeeded or Failed"
Jun 22 10:41:59.534: INFO: Pod "busybox-readonly-false-a1ddd532-8f34-43eb-b346-784a34d5c638": Phase="Pending", Reason="", readiness=false. Elapsed: 4.852174ms
Jun 22 10:42:01.541: INFO: Pod "busybox-readonly-false-a1ddd532-8f34-43eb-b346-784a34d5c638": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011706077s
Jun 22 10:42:03.542: INFO: Pod "busybox-readonly-false-a1ddd532-8f34-43eb-b346-784a34d5c638": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01285265s
Jun 22 10:42:03.542: INFO: Pod "busybox-readonly-false-a1ddd532-8f34-43eb-b346-784a34d5c638" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jun 22 10:42:03.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-5439" for this suite. 06/22/23 10:42:03.552
------------------------------
• [4.132 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:41:59.428
    Jun 22 10:41:59.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename security-context-test 06/22/23 10:41:59.429
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:41:59.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:41:59.513
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    W0622 10:41:59.529859      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "busybox-readonly-false-a1ddd532-8f34-43eb-b346-784a34d5c638" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-readonly-false-a1ddd532-8f34-43eb-b346-784a34d5c638" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-readonly-false-a1ddd532-8f34-43eb-b346-784a34d5c638" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-readonly-false-a1ddd532-8f34-43eb-b346-784a34d5c638" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:41:59.530: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-a1ddd532-8f34-43eb-b346-784a34d5c638" in namespace "security-context-test-5439" to be "Succeeded or Failed"
    Jun 22 10:41:59.534: INFO: Pod "busybox-readonly-false-a1ddd532-8f34-43eb-b346-784a34d5c638": Phase="Pending", Reason="", readiness=false. Elapsed: 4.852174ms
    Jun 22 10:42:01.541: INFO: Pod "busybox-readonly-false-a1ddd532-8f34-43eb-b346-784a34d5c638": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011706077s
    Jun 22 10:42:03.542: INFO: Pod "busybox-readonly-false-a1ddd532-8f34-43eb-b346-784a34d5c638": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01285265s
    Jun 22 10:42:03.542: INFO: Pod "busybox-readonly-false-a1ddd532-8f34-43eb-b346-784a34d5c638" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:42:03.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-5439" for this suite. 06/22/23 10:42:03.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:42:03.562
Jun 22 10:42:03.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename configmap 06/22/23 10:42:03.563
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:42:03.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:42:03.59
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-0979f526-abfd-4ba7-bf36-c58521d9a85e 06/22/23 10:42:03.604
STEP: Creating configMap with name cm-test-opt-upd-253d8b78-866a-4b50-a76c-f28b552eb3cb 06/22/23 10:42:03.611
STEP: Creating the pod 06/22/23 10:42:03.617
W0622 10:42:03.631194      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:42:03.631: INFO: Waiting up to 5m0s for pod "pod-configmaps-2a254f94-085f-47b8-831e-cbae5f7f8684" in namespace "configmap-1206" to be "running and ready"
Jun 22 10:42:03.636: INFO: Pod "pod-configmaps-2a254f94-085f-47b8-831e-cbae5f7f8684": Phase="Pending", Reason="", readiness=false. Elapsed: 4.615881ms
Jun 22 10:42:03.636: INFO: The phase of Pod pod-configmaps-2a254f94-085f-47b8-831e-cbae5f7f8684 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:42:05.642: INFO: Pod "pod-configmaps-2a254f94-085f-47b8-831e-cbae5f7f8684": Phase="Running", Reason="", readiness=true. Elapsed: 2.010976118s
Jun 22 10:42:05.642: INFO: The phase of Pod pod-configmaps-2a254f94-085f-47b8-831e-cbae5f7f8684 is Running (Ready = true)
Jun 22 10:42:05.642: INFO: Pod "pod-configmaps-2a254f94-085f-47b8-831e-cbae5f7f8684" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-0979f526-abfd-4ba7-bf36-c58521d9a85e 06/22/23 10:42:05.688
STEP: Updating configmap cm-test-opt-upd-253d8b78-866a-4b50-a76c-f28b552eb3cb 06/22/23 10:42:05.695
STEP: Creating configMap with name cm-test-opt-create-bc1b7049-a8c5-4912-bb47-3f6ea845db5d 06/22/23 10:42:05.702
STEP: waiting to observe update in volume 06/22/23 10:42:05.707
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun 22 10:42:07.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1206" for this suite. 06/22/23 10:42:07.75
------------------------------
• [4.197 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:42:03.562
    Jun 22 10:42:03.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename configmap 06/22/23 10:42:03.563
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:42:03.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:42:03.59
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-0979f526-abfd-4ba7-bf36-c58521d9a85e 06/22/23 10:42:03.604
    STEP: Creating configMap with name cm-test-opt-upd-253d8b78-866a-4b50-a76c-f28b552eb3cb 06/22/23 10:42:03.611
    STEP: Creating the pod 06/22/23 10:42:03.617
    W0622 10:42:03.631194      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:42:03.631: INFO: Waiting up to 5m0s for pod "pod-configmaps-2a254f94-085f-47b8-831e-cbae5f7f8684" in namespace "configmap-1206" to be "running and ready"
    Jun 22 10:42:03.636: INFO: Pod "pod-configmaps-2a254f94-085f-47b8-831e-cbae5f7f8684": Phase="Pending", Reason="", readiness=false. Elapsed: 4.615881ms
    Jun 22 10:42:03.636: INFO: The phase of Pod pod-configmaps-2a254f94-085f-47b8-831e-cbae5f7f8684 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:42:05.642: INFO: Pod "pod-configmaps-2a254f94-085f-47b8-831e-cbae5f7f8684": Phase="Running", Reason="", readiness=true. Elapsed: 2.010976118s
    Jun 22 10:42:05.642: INFO: The phase of Pod pod-configmaps-2a254f94-085f-47b8-831e-cbae5f7f8684 is Running (Ready = true)
    Jun 22 10:42:05.642: INFO: Pod "pod-configmaps-2a254f94-085f-47b8-831e-cbae5f7f8684" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-0979f526-abfd-4ba7-bf36-c58521d9a85e 06/22/23 10:42:05.688
    STEP: Updating configmap cm-test-opt-upd-253d8b78-866a-4b50-a76c-f28b552eb3cb 06/22/23 10:42:05.695
    STEP: Creating configMap with name cm-test-opt-create-bc1b7049-a8c5-4912-bb47-3f6ea845db5d 06/22/23 10:42:05.702
    STEP: waiting to observe update in volume 06/22/23 10:42:05.707
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:42:07.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1206" for this suite. 06/22/23 10:42:07.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:42:07.761
Jun 22 10:42:07.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename conformance-tests 06/22/23 10:42:07.762
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:42:07.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:42:07.789
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 06/22/23 10:42:07.793
Jun 22 10:42:07.793: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Jun 22 10:42:07.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-6791" for this suite. 06/22/23 10:42:07.815
------------------------------
• [0.062 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:42:07.761
    Jun 22 10:42:07.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename conformance-tests 06/22/23 10:42:07.762
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:42:07.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:42:07.789
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 06/22/23 10:42:07.793
    Jun 22 10:42:07.793: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:42:07.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-6791" for this suite. 06/22/23 10:42:07.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:42:07.823
Jun 22 10:42:07.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename cronjob 06/22/23 10:42:07.824
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:42:07.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:42:07.851
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 06/22/23 10:42:07.855
W0622 10:42:07.863250      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring no jobs are scheduled 06/22/23 10:42:07.863
STEP: Ensuring no job exists by listing jobs explicitly 06/22/23 10:47:07.874
STEP: Removing cronjob 06/22/23 10:47:07.88
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jun 22 10:47:07.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-6510" for this suite. 06/22/23 10:47:07.899
------------------------------
• [SLOW TEST] [300.084 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:42:07.823
    Jun 22 10:42:07.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename cronjob 06/22/23 10:42:07.824
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:42:07.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:42:07.851
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 06/22/23 10:42:07.855
    W0622 10:42:07.863250      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring no jobs are scheduled 06/22/23 10:42:07.863
    STEP: Ensuring no job exists by listing jobs explicitly 06/22/23 10:47:07.874
    STEP: Removing cronjob 06/22/23 10:47:07.88
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:47:07.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-6510" for this suite. 06/22/23 10:47:07.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:47:07.913
Jun 22 10:47:07.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename deployment 06/22/23 10:47:07.92
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:47:07.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:47:07.947
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
W0622 10:47:07.959029      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:47:07.963: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 22 10:47:12.970: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/22/23 10:47:12.97
Jun 22 10:47:12.970: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 22 10:47:14.976: INFO: Creating deployment "test-rollover-deployment"
W0622 10:47:14.984209      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "redis-slave" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "redis-slave" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "redis-slave" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "redis-slave" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:47:14.988: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 22 10:47:16.997: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 22 10:47:17.007: INFO: Ensure that both replica sets have 1 created replica
Jun 22 10:47:17.015: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
W0622 10:47:17.029837      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:47:17.029: INFO: Updating deployment test-rollover-deployment
Jun 22 10:47:17.030: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 22 10:47:19.039: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 22 10:47:19.047: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 22 10:47:19.054: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 10:47:19.055: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 10:47:21.064: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 10:47:21.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 10:47:23.067: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 10:47:23.067: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 10:47:25.066: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 10:47:25.066: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 10:47:27.066: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 10:47:27.066: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 10:47:29.065: INFO: 
Jun 22 10:47:29.065: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 22 10:47:29.079: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7610  c3999d6f-cf65-4080-ba85-fdfdfa6264ab 137914 2 2023-06-22 10:47:14 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-22 10:47:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:47:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00063bfa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-22 10:47:15 +0000 UTC,LastTransitionTime:2023-06-22 10:47:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-06-22 10:47:28 +0000 UTC,LastTransitionTime:2023-06-22 10:47:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 10:47:29.084: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-7610  ecdbe465-075b-43b6-b3ce-7c4b6ecf035b 137904 2 2023-06-22 10:47:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment c3999d6f-cf65-4080-ba85-fdfdfa6264ab 0xc0003fafb7 0xc0003fafb8}] [] [{kube-controller-manager Update apps/v1 2023-06-22 10:47:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3999d6f-cf65-4080-ba85-fdfdfa6264ab\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:47:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0003fb098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 22 10:47:29.084: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 22 10:47:29.084: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7610  3eb2e3e9-7c77-4a48-8cc6-eed84c9a5a45 137913 2 2023-06-22 10:47:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment c3999d6f-cf65-4080-ba85-fdfdfa6264ab 0xc0003fae87 0xc0003fae88}] [] [{e2e.test Update apps/v1 2023-06-22 10:47:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:47:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3999d6f-cf65-4080-ba85-fdfdfa6264ab\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:47:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0003faf48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 10:47:29.084: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-7610  b0d50b7a-2bdc-4166-a3c0-3e195c1239a2 137856 2 2023-06-22 10:47:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment c3999d6f-cf65-4080-ba85-fdfdfa6264ab 0xc0003fb1d7 0xc0003fb1d8}] [] [{kube-controller-manager Update apps/v1 2023-06-22 10:47:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3999d6f-cf65-4080-ba85-fdfdfa6264ab\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:47:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0003fb288 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 10:47:29.090: INFO: Pod "test-rollover-deployment-6c6df9974f-btl8d" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-btl8d test-rollover-deployment-6c6df9974f- deployment-7610  35702744-4365-474e-ac82-2a5a6b8ae4dd 137864 0 2023-06-22 10:47:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f ecdbe465-075b-43b6-b3ce-7c4b6ecf035b 0xc0007ce8a7 0xc0007ce8a8}] [] [{kube-controller-manager Update v1 2023-06-22 10:47:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ecdbe465-075b-43b6-b3ce-7c4b6ecf035b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:47:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.46\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kwttg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kwttg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:47:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:47:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:47:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:47:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.46,StartTime:2023-06-22 10:47:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:47:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://12ab96c6f5bcd00f40a2688dbc45929410e74b61d4a16e40ece8ecc22e70708d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.46,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun 22 10:47:29.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7610" for this suite. 06/22/23 10:47:29.099
------------------------------
• [SLOW TEST] [21.195 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:47:07.913
    Jun 22 10:47:07.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename deployment 06/22/23 10:47:07.92
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:47:07.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:47:07.947
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    W0622 10:47:07.959029      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:47:07.963: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jun 22 10:47:12.970: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/22/23 10:47:12.97
    Jun 22 10:47:12.970: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jun 22 10:47:14.976: INFO: Creating deployment "test-rollover-deployment"
    W0622 10:47:14.984209      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "redis-slave" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "redis-slave" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "redis-slave" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "redis-slave" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:47:14.988: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jun 22 10:47:16.997: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jun 22 10:47:17.007: INFO: Ensure that both replica sets have 1 created replica
    Jun 22 10:47:17.015: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    W0622 10:47:17.029837      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:47:17.029: INFO: Updating deployment test-rollover-deployment
    Jun 22 10:47:17.030: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jun 22 10:47:19.039: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jun 22 10:47:19.047: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jun 22 10:47:19.054: INFO: all replica sets need to contain the pod-template-hash label
    Jun 22 10:47:19.055: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 22 10:47:21.064: INFO: all replica sets need to contain the pod-template-hash label
    Jun 22 10:47:21.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 22 10:47:23.067: INFO: all replica sets need to contain the pod-template-hash label
    Jun 22 10:47:23.067: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 22 10:47:25.066: INFO: all replica sets need to contain the pod-template-hash label
    Jun 22 10:47:25.066: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 22 10:47:27.066: INFO: all replica sets need to contain the pod-template-hash label
    Jun 22 10:47:27.066: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 10, 47, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 10, 47, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 22 10:47:29.065: INFO: 
    Jun 22 10:47:29.065: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 22 10:47:29.079: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-7610  c3999d6f-cf65-4080-ba85-fdfdfa6264ab 137914 2 2023-06-22 10:47:14 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-22 10:47:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:47:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00063bfa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-22 10:47:15 +0000 UTC,LastTransitionTime:2023-06-22 10:47:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-06-22 10:47:28 +0000 UTC,LastTransitionTime:2023-06-22 10:47:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun 22 10:47:29.084: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-7610  ecdbe465-075b-43b6-b3ce-7c4b6ecf035b 137904 2 2023-06-22 10:47:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment c3999d6f-cf65-4080-ba85-fdfdfa6264ab 0xc0003fafb7 0xc0003fafb8}] [] [{kube-controller-manager Update apps/v1 2023-06-22 10:47:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3999d6f-cf65-4080-ba85-fdfdfa6264ab\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:47:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0003fb098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 22 10:47:29.084: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jun 22 10:47:29.084: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7610  3eb2e3e9-7c77-4a48-8cc6-eed84c9a5a45 137913 2 2023-06-22 10:47:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment c3999d6f-cf65-4080-ba85-fdfdfa6264ab 0xc0003fae87 0xc0003fae88}] [] [{e2e.test Update apps/v1 2023-06-22 10:47:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:47:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3999d6f-cf65-4080-ba85-fdfdfa6264ab\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:47:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0003faf48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 22 10:47:29.084: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-7610  b0d50b7a-2bdc-4166-a3c0-3e195c1239a2 137856 2 2023-06-22 10:47:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment c3999d6f-cf65-4080-ba85-fdfdfa6264ab 0xc0003fb1d7 0xc0003fb1d8}] [] [{kube-controller-manager Update apps/v1 2023-06-22 10:47:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c3999d6f-cf65-4080-ba85-fdfdfa6264ab\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:47:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0003fb288 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 22 10:47:29.090: INFO: Pod "test-rollover-deployment-6c6df9974f-btl8d" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-btl8d test-rollover-deployment-6c6df9974f- deployment-7610  35702744-4365-474e-ac82-2a5a6b8ae4dd 137864 0 2023-06-22 10:47:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f ecdbe465-075b-43b6-b3ce-7c4b6ecf035b 0xc0007ce8a7 0xc0007ce8a8}] [] [{kube-controller-manager Update v1 2023-06-22 10:47:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ecdbe465-075b-43b6-b3ce-7c4b6ecf035b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:47:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.46\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kwttg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kwttg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:47:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:47:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:47:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:47:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.46,StartTime:2023-06-22 10:47:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:47:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://12ab96c6f5bcd00f40a2688dbc45929410e74b61d4a16e40ece8ecc22e70708d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.46,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:47:29.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7610" for this suite. 06/22/23 10:47:29.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:47:29.111
Jun 22 10:47:29.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename runtimeclass 06/22/23 10:47:29.112
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:47:29.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:47:29.139
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 06/22/23 10:47:29.143
STEP: getting /apis/node.k8s.io 06/22/23 10:47:29.148
STEP: getting /apis/node.k8s.io/v1 06/22/23 10:47:29.15
STEP: creating 06/22/23 10:47:29.152
STEP: watching 06/22/23 10:47:29.171
Jun 22 10:47:29.171: INFO: starting watch
STEP: getting 06/22/23 10:47:29.178
STEP: listing 06/22/23 10:47:29.182
STEP: patching 06/22/23 10:47:29.186
STEP: updating 06/22/23 10:47:29.192
Jun 22 10:47:29.198: INFO: waiting for watch events with expected annotations
STEP: deleting 06/22/23 10:47:29.198
STEP: deleting a collection 06/22/23 10:47:29.212
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jun 22 10:47:29.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8302" for this suite. 06/22/23 10:47:29.238
------------------------------
• [0.134 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:47:29.111
    Jun 22 10:47:29.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename runtimeclass 06/22/23 10:47:29.112
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:47:29.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:47:29.139
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 06/22/23 10:47:29.143
    STEP: getting /apis/node.k8s.io 06/22/23 10:47:29.148
    STEP: getting /apis/node.k8s.io/v1 06/22/23 10:47:29.15
    STEP: creating 06/22/23 10:47:29.152
    STEP: watching 06/22/23 10:47:29.171
    Jun 22 10:47:29.171: INFO: starting watch
    STEP: getting 06/22/23 10:47:29.178
    STEP: listing 06/22/23 10:47:29.182
    STEP: patching 06/22/23 10:47:29.186
    STEP: updating 06/22/23 10:47:29.192
    Jun 22 10:47:29.198: INFO: waiting for watch events with expected annotations
    STEP: deleting 06/22/23 10:47:29.198
    STEP: deleting a collection 06/22/23 10:47:29.212
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:47:29.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8302" for this suite. 06/22/23 10:47:29.238
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:47:29.247
Jun 22 10:47:29.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubectl 06/22/23 10:47:29.248
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:47:29.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:47:29.271
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 06/22/23 10:47:29.275
Jun 22 10:47:29.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-6807 api-versions'
Jun 22 10:47:29.445: INFO: stderr: ""
Jun 22 10:47:29.445: INFO: stdout: "admissionregistration.k8s.io/v1\nako.vmware.com/v1alpha1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncns.vmware.com/v1alpha1\ncontrolplane.antrea.io/v1beta2\ncoordination.k8s.io/v1\ncore.tanzu.vmware.com/v1alpha2\ncrd.antrea.io/v1alpha1\ncrd.antrea.io/v1alpha2\ncrd.antrea.io/v1alpha3\ncrd.antrea.io/v1beta1\ncrd.antrea.tanzu.vmware.com/v1alpha1\ndata.packaging.carvel.dev/v1alpha1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\ninternal.packaging.carvel.dev/v1alpha1\nkappctrl.k14s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.x-k8s.io/v1alpha1\nnode.k8s.io/v1\npackaging.carvel.dev/v1alpha1\npolicy/v1\nrbac.authorization.k8s.io/v1\nrun.tanzu.vmware.com/v1alpha1\nscheduling.k8s.io/v1\nsecretgen.carvel.dev/v1alpha1\nsecretgen.k14s.io/v1alpha1\nstats.antrea.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nsystem.antrea.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun 22 10:47:29.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6807" for this suite. 06/22/23 10:47:29.453
------------------------------
• [0.213 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:47:29.247
    Jun 22 10:47:29.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubectl 06/22/23 10:47:29.248
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:47:29.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:47:29.271
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 06/22/23 10:47:29.275
    Jun 22 10:47:29.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-6807 api-versions'
    Jun 22 10:47:29.445: INFO: stderr: ""
    Jun 22 10:47:29.445: INFO: stdout: "admissionregistration.k8s.io/v1\nako.vmware.com/v1alpha1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncns.vmware.com/v1alpha1\ncontrolplane.antrea.io/v1beta2\ncoordination.k8s.io/v1\ncore.tanzu.vmware.com/v1alpha2\ncrd.antrea.io/v1alpha1\ncrd.antrea.io/v1alpha2\ncrd.antrea.io/v1alpha3\ncrd.antrea.io/v1beta1\ncrd.antrea.tanzu.vmware.com/v1alpha1\ndata.packaging.carvel.dev/v1alpha1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\ninternal.packaging.carvel.dev/v1alpha1\nkappctrl.k14s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.x-k8s.io/v1alpha1\nnode.k8s.io/v1\npackaging.carvel.dev/v1alpha1\npolicy/v1\nrbac.authorization.k8s.io/v1\nrun.tanzu.vmware.com/v1alpha1\nscheduling.k8s.io/v1\nsecretgen.carvel.dev/v1alpha1\nsecretgen.k14s.io/v1alpha1\nstats.antrea.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nsystem.antrea.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:47:29.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6807" for this suite. 06/22/23 10:47:29.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:47:29.462
Jun 22 10:47:29.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename services 06/22/23 10:47:29.463
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:47:29.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:47:29.484
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 06/22/23 10:47:29.492
STEP: waiting for available Endpoint 06/22/23 10:47:29.497
STEP: listing all Endpoints 06/22/23 10:47:29.5
STEP: updating the Endpoint 06/22/23 10:47:29.504
STEP: fetching the Endpoint 06/22/23 10:47:29.511
STEP: patching the Endpoint 06/22/23 10:47:29.515
STEP: fetching the Endpoint 06/22/23 10:47:29.522
STEP: deleting the Endpoint by Collection 06/22/23 10:47:29.526
STEP: waiting for Endpoint deletion 06/22/23 10:47:29.536
STEP: fetching the Endpoint 06/22/23 10:47:29.539
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun 22 10:47:29.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8216" for this suite. 06/22/23 10:47:29.549
------------------------------
• [0.095 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:47:29.462
    Jun 22 10:47:29.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename services 06/22/23 10:47:29.463
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:47:29.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:47:29.484
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 06/22/23 10:47:29.492
    STEP: waiting for available Endpoint 06/22/23 10:47:29.497
    STEP: listing all Endpoints 06/22/23 10:47:29.5
    STEP: updating the Endpoint 06/22/23 10:47:29.504
    STEP: fetching the Endpoint 06/22/23 10:47:29.511
    STEP: patching the Endpoint 06/22/23 10:47:29.515
    STEP: fetching the Endpoint 06/22/23 10:47:29.522
    STEP: deleting the Endpoint by Collection 06/22/23 10:47:29.526
    STEP: waiting for Endpoint deletion 06/22/23 10:47:29.536
    STEP: fetching the Endpoint 06/22/23 10:47:29.539
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:47:29.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8216" for this suite. 06/22/23 10:47:29.549
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:47:29.561
Jun 22 10:47:29.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename events 06/22/23 10:47:29.562
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:47:29.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:47:29.586
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 06/22/23 10:47:29.589
STEP: listing events in all namespaces 06/22/23 10:47:29.599
STEP: listing events in test namespace 06/22/23 10:47:29.608
STEP: listing events with field selection filtering on source 06/22/23 10:47:29.611
STEP: listing events with field selection filtering on reportingController 06/22/23 10:47:29.615
STEP: getting the test event 06/22/23 10:47:29.619
STEP: patching the test event 06/22/23 10:47:29.622
STEP: getting the test event 06/22/23 10:47:29.631
STEP: updating the test event 06/22/23 10:47:29.635
STEP: getting the test event 06/22/23 10:47:29.644
STEP: deleting the test event 06/22/23 10:47:29.648
STEP: listing events in all namespaces 06/22/23 10:47:29.655
STEP: listing events in test namespace 06/22/23 10:47:29.663
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jun 22 10:47:29.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1912" for this suite. 06/22/23 10:47:29.674
------------------------------
• [0.120 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:47:29.561
    Jun 22 10:47:29.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename events 06/22/23 10:47:29.562
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:47:29.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:47:29.586
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 06/22/23 10:47:29.589
    STEP: listing events in all namespaces 06/22/23 10:47:29.599
    STEP: listing events in test namespace 06/22/23 10:47:29.608
    STEP: listing events with field selection filtering on source 06/22/23 10:47:29.611
    STEP: listing events with field selection filtering on reportingController 06/22/23 10:47:29.615
    STEP: getting the test event 06/22/23 10:47:29.619
    STEP: patching the test event 06/22/23 10:47:29.622
    STEP: getting the test event 06/22/23 10:47:29.631
    STEP: updating the test event 06/22/23 10:47:29.635
    STEP: getting the test event 06/22/23 10:47:29.644
    STEP: deleting the test event 06/22/23 10:47:29.648
    STEP: listing events in all namespaces 06/22/23 10:47:29.655
    STEP: listing events in test namespace 06/22/23 10:47:29.663
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:47:29.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1912" for this suite. 06/22/23 10:47:29.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:47:29.685
Jun 22 10:47:29.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename endpointslice 06/22/23 10:47:29.686
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:47:29.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:47:29.709
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Jun 22 10:47:29.725: INFO: Endpoints addresses: [10.92.226.47 10.92.228.172 10.92.228.224] , ports: [6443]
Jun 22 10:47:29.725: INFO: EndpointSlices addresses: [10.92.226.47 10.92.228.172 10.92.228.224] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jun 22 10:47:29.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5699" for this suite. 06/22/23 10:47:29.732
------------------------------
• [0.054 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:47:29.685
    Jun 22 10:47:29.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename endpointslice 06/22/23 10:47:29.686
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:47:29.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:47:29.709
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Jun 22 10:47:29.725: INFO: Endpoints addresses: [10.92.226.47 10.92.228.172 10.92.228.224] , ports: [6443]
    Jun 22 10:47:29.725: INFO: EndpointSlices addresses: [10.92.226.47 10.92.228.172 10.92.228.224] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:47:29.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5699" for this suite. 06/22/23 10:47:29.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:47:29.742
Jun 22 10:47:29.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename svcaccounts 06/22/23 10:47:29.744
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:47:29.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:47:29.777
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
W0622 10:47:29.803587      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:47:29.803: INFO: created pod pod-service-account-defaultsa
Jun 22 10:47:29.803: INFO: pod pod-service-account-defaultsa service account token volume mount: true
W0622 10:47:29.811013      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:47:29.811: INFO: created pod pod-service-account-mountsa
Jun 22 10:47:29.811: INFO: pod pod-service-account-mountsa service account token volume mount: true
W0622 10:47:29.820438      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:47:29.820: INFO: created pod pod-service-account-nomountsa
Jun 22 10:47:29.820: INFO: pod pod-service-account-nomountsa service account token volume mount: false
W0622 10:47:29.827243      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:47:29.827: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 22 10:47:29.827: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
W0622 10:47:29.837391      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:47:29.837: INFO: created pod pod-service-account-mountsa-mountspec
Jun 22 10:47:29.837: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
W0622 10:47:29.844393      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:47:29.844: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 22 10:47:29.844: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
W0622 10:47:29.852413      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:47:29.852: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 22 10:47:29.852: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
W0622 10:47:29.861626      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:47:29.861: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 22 10:47:29.861: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
W0622 10:47:29.868652      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:47:29.868: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 22 10:47:29.868: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun 22 10:47:29.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1711" for this suite. 06/22/23 10:47:29.875
------------------------------
• [0.142 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:47:29.742
    Jun 22 10:47:29.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename svcaccounts 06/22/23 10:47:29.744
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:47:29.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:47:29.777
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    W0622 10:47:29.803587      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:47:29.803: INFO: created pod pod-service-account-defaultsa
    Jun 22 10:47:29.803: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    W0622 10:47:29.811013      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:47:29.811: INFO: created pod pod-service-account-mountsa
    Jun 22 10:47:29.811: INFO: pod pod-service-account-mountsa service account token volume mount: true
    W0622 10:47:29.820438      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:47:29.820: INFO: created pod pod-service-account-nomountsa
    Jun 22 10:47:29.820: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    W0622 10:47:29.827243      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:47:29.827: INFO: created pod pod-service-account-defaultsa-mountspec
    Jun 22 10:47:29.827: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    W0622 10:47:29.837391      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:47:29.837: INFO: created pod pod-service-account-mountsa-mountspec
    Jun 22 10:47:29.837: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    W0622 10:47:29.844393      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:47:29.844: INFO: created pod pod-service-account-nomountsa-mountspec
    Jun 22 10:47:29.844: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    W0622 10:47:29.852413      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:47:29.852: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jun 22 10:47:29.852: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    W0622 10:47:29.861626      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:47:29.861: INFO: created pod pod-service-account-mountsa-nomountspec
    Jun 22 10:47:29.861: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    W0622 10:47:29.868652      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:47:29.868: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jun 22 10:47:29.868: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:47:29.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1711" for this suite. 06/22/23 10:47:29.875
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:47:29.885
Jun 22 10:47:29.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename replicaset 06/22/23 10:47:29.886
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:47:29.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:47:29.962
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 06/22/23 10:47:29.976
W0622 10:47:29.982627      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Verify that the required pods have come up. 06/22/23 10:47:29.982
Jun 22 10:47:29.986: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 22 10:47:34.992: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/22/23 10:47:34.992
STEP: Getting /status 06/22/23 10:47:34.992
Jun 22 10:47:34.997: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 06/22/23 10:47:34.997
Jun 22 10:47:35.007: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 06/22/23 10:47:35.008
Jun 22 10:47:35.010: INFO: Observed &ReplicaSet event: ADDED
Jun 22 10:47:35.010: INFO: Observed &ReplicaSet event: MODIFIED
Jun 22 10:47:35.011: INFO: Observed &ReplicaSet event: MODIFIED
Jun 22 10:47:35.011: INFO: Observed &ReplicaSet event: MODIFIED
Jun 22 10:47:35.011: INFO: Found replicaset test-rs in namespace replicaset-507 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 22 10:47:35.011: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 06/22/23 10:47:35.011
Jun 22 10:47:35.011: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun 22 10:47:35.019: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 06/22/23 10:47:35.019
Jun 22 10:47:35.022: INFO: Observed &ReplicaSet event: ADDED
Jun 22 10:47:35.022: INFO: Observed &ReplicaSet event: MODIFIED
Jun 22 10:47:35.022: INFO: Observed &ReplicaSet event: MODIFIED
Jun 22 10:47:35.023: INFO: Observed &ReplicaSet event: MODIFIED
Jun 22 10:47:35.023: INFO: Observed replicaset test-rs in namespace replicaset-507 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 22 10:47:35.023: INFO: Observed &ReplicaSet event: MODIFIED
Jun 22 10:47:35.023: INFO: Found replicaset test-rs in namespace replicaset-507 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jun 22 10:47:35.023: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jun 22 10:47:35.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-507" for this suite. 06/22/23 10:47:35.03
------------------------------
• [SLOW TEST] [5.152 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:47:29.885
    Jun 22 10:47:29.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename replicaset 06/22/23 10:47:29.886
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:47:29.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:47:29.962
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 06/22/23 10:47:29.976
    W0622 10:47:29.982627      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Verify that the required pods have come up. 06/22/23 10:47:29.982
    Jun 22 10:47:29.986: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun 22 10:47:34.992: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/22/23 10:47:34.992
    STEP: Getting /status 06/22/23 10:47:34.992
    Jun 22 10:47:34.997: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 06/22/23 10:47:34.997
    Jun 22 10:47:35.007: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 06/22/23 10:47:35.008
    Jun 22 10:47:35.010: INFO: Observed &ReplicaSet event: ADDED
    Jun 22 10:47:35.010: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 22 10:47:35.011: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 22 10:47:35.011: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 22 10:47:35.011: INFO: Found replicaset test-rs in namespace replicaset-507 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun 22 10:47:35.011: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 06/22/23 10:47:35.011
    Jun 22 10:47:35.011: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jun 22 10:47:35.019: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 06/22/23 10:47:35.019
    Jun 22 10:47:35.022: INFO: Observed &ReplicaSet event: ADDED
    Jun 22 10:47:35.022: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 22 10:47:35.022: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 22 10:47:35.023: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 22 10:47:35.023: INFO: Observed replicaset test-rs in namespace replicaset-507 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun 22 10:47:35.023: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 22 10:47:35.023: INFO: Found replicaset test-rs in namespace replicaset-507 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jun 22 10:47:35.023: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:47:35.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-507" for this suite. 06/22/23 10:47:35.03
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:47:35.038
Jun 22 10:47:35.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename svcaccounts 06/22/23 10:47:35.039
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:47:35.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:47:35.062
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
W0622 10:47:35.088614      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "oidc-discovery-validator" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "oidc-discovery-validator" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "oidc-discovery-validator" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "oidc-discovery-validator" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:47:35.088: INFO: created pod
Jun 22 10:47:35.088: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4905" to be "Succeeded or Failed"
Jun 22 10:47:35.095: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.284968ms
Jun 22 10:47:37.102: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014047251s
Jun 22 10:47:39.101: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012484947s
STEP: Saw pod success 06/22/23 10:47:39.101
Jun 22 10:47:39.101: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jun 22 10:48:09.101: INFO: polling logs
Jun 22 10:48:09.127: INFO: Pod logs: 
I0622 10:47:35.848505       1 log.go:198] OK: Got token
I0622 10:47:35.848674       1 log.go:198] validating with in-cluster discovery
I0622 10:47:35.850167       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0622 10:47:35.850228       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4905:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1687431455, NotBefore:1687430855, IssuedAt:1687430855, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4905", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"97f5f9cf-6043-435d-baee-850d4d1dfda9"}}}
I0622 10:47:35.905425       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0622 10:47:35.914175       1 log.go:198] OK: Validated signature on JWT
I0622 10:47:35.914278       1 log.go:198] OK: Got valid claims from token!
I0622 10:47:35.914299       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4905:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1687431455, NotBefore:1687430855, IssuedAt:1687430855, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4905", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"97f5f9cf-6043-435d-baee-850d4d1dfda9"}}}

Jun 22 10:48:09.127: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun 22 10:48:09.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4905" for this suite. 06/22/23 10:48:09.142
------------------------------
• [SLOW TEST] [34.113 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:47:35.038
    Jun 22 10:47:35.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename svcaccounts 06/22/23 10:47:35.039
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:47:35.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:47:35.062
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    W0622 10:47:35.088614      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "oidc-discovery-validator" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "oidc-discovery-validator" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "oidc-discovery-validator" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "oidc-discovery-validator" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:47:35.088: INFO: created pod
    Jun 22 10:47:35.088: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-4905" to be "Succeeded or Failed"
    Jun 22 10:47:35.095: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.284968ms
    Jun 22 10:47:37.102: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014047251s
    Jun 22 10:47:39.101: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012484947s
    STEP: Saw pod success 06/22/23 10:47:39.101
    Jun 22 10:47:39.101: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jun 22 10:48:09.101: INFO: polling logs
    Jun 22 10:48:09.127: INFO: Pod logs: 
    I0622 10:47:35.848505       1 log.go:198] OK: Got token
    I0622 10:47:35.848674       1 log.go:198] validating with in-cluster discovery
    I0622 10:47:35.850167       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0622 10:47:35.850228       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4905:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1687431455, NotBefore:1687430855, IssuedAt:1687430855, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4905", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"97f5f9cf-6043-435d-baee-850d4d1dfda9"}}}
    I0622 10:47:35.905425       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0622 10:47:35.914175       1 log.go:198] OK: Validated signature on JWT
    I0622 10:47:35.914278       1 log.go:198] OK: Got valid claims from token!
    I0622 10:47:35.914299       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4905:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1687431455, NotBefore:1687430855, IssuedAt:1687430855, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4905", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"97f5f9cf-6043-435d-baee-850d4d1dfda9"}}}

    Jun 22 10:48:09.127: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:48:09.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4905" for this suite. 06/22/23 10:48:09.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:48:09.152
Jun 22 10:48:09.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename podtemplate 06/22/23 10:48:09.154
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:48:09.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:48:09.179
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
W0622 10:48:09.194808      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0622 10:48:09.205960      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jun 22 10:48:09.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-2740" for this suite. 06/22/23 10:48:09.226
------------------------------
• [0.081 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:48:09.152
    Jun 22 10:48:09.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename podtemplate 06/22/23 10:48:09.154
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:48:09.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:48:09.179
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    W0622 10:48:09.194808      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0622 10:48:09.205960      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:48:09.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-2740" for this suite. 06/22/23 10:48:09.226
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:48:09.235
Jun 22 10:48:09.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename deployment 06/22/23 10:48:09.236
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:48:09.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:48:09.26
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jun 22 10:48:09.264: INFO: Creating simple deployment test-new-deployment
W0622 10:48:09.270086      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:48:09.277: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 06/22/23 10:48:11.294
STEP: updating a scale subresource 06/22/23 10:48:11.298
STEP: verifying the deployment Spec.Replicas was modified 06/22/23 10:48:11.309
STEP: Patch a scale subresource 06/22/23 10:48:11.313
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 22 10:48:11.331: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9663  21527f88-7f44-4fc2-b241-19eb75631b34 138335 3 2023-06-22 10:48:09 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-06-22 10:48:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:48:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bc43f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-22 10:48:10 +0000 UTC,LastTransitionTime:2023-06-22 10:48:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-06-22 10:48:10 +0000 UTC,LastTransitionTime:2023-06-22 10:48:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 10:48:11.339: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9663  802791e4-fd4e-4fe4-9fff-ce2b9293ff33 138334 2 2023-06-22 10:48:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 21527f88-7f44-4fc2-b241-19eb75631b34 0xc004bc4817 0xc004bc4818}] [] [{kube-controller-manager Update apps/v1 2023-06-22 10:48:10 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-22 10:48:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"21527f88-7f44-4fc2-b241-19eb75631b34\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bc48a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 22 10:48:11.344: INFO: Pod "test-new-deployment-7f5969cbc7-2k5pk" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-2k5pk test-new-deployment-7f5969cbc7- deployment-9663  bc9b2340-73b6-42fe-9c47-bd99d84374be 138325 0 2023-06-22 10:48:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 802791e4-fd4e-4fe4-9fff-ce2b9293ff33 0xc004bc4c37 0xc004bc4c38}] [] [{kube-controller-manager Update v1 2023-06-22 10:48:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"802791e4-fd4e-4fe4-9fff-ce2b9293ff33\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:48:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bzf2g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bzf2g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:48:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:48:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:48:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:48:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.50,StartTime:2023-06-22 10:48:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:48:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e1f0c6c0a1db9b97e518768c08e9181a184c8c0d1b42b7d0d6d15091c32363c7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 10:48:11.345: INFO: Pod "test-new-deployment-7f5969cbc7-rtd9w" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-rtd9w test-new-deployment-7f5969cbc7- deployment-9663  8cf465f7-3ac2-46fd-b93a-5c27ed1dd70b 138338 0 2023-06-22 10:48:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 802791e4-fd4e-4fe4-9fff-ce2b9293ff33 0xc004bc4e17 0xc004bc4e18}] [] [{kube-controller-manager Update v1 2023-06-22 10:48:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"802791e4-fd4e-4fe4-9fff-ce2b9293ff33\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njgpw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njgpw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:48:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun 22 10:48:11.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9663" for this suite. 06/22/23 10:48:11.354
------------------------------
• [2.131 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:48:09.235
    Jun 22 10:48:09.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename deployment 06/22/23 10:48:09.236
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:48:09.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:48:09.26
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jun 22 10:48:09.264: INFO: Creating simple deployment test-new-deployment
    W0622 10:48:09.270086      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:48:09.277: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 06/22/23 10:48:11.294
    STEP: updating a scale subresource 06/22/23 10:48:11.298
    STEP: verifying the deployment Spec.Replicas was modified 06/22/23 10:48:11.309
    STEP: Patch a scale subresource 06/22/23 10:48:11.313
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 22 10:48:11.331: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-9663  21527f88-7f44-4fc2-b241-19eb75631b34 138335 3 2023-06-22 10:48:09 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-06-22 10:48:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:48:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bc43f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-22 10:48:10 +0000 UTC,LastTransitionTime:2023-06-22 10:48:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-06-22 10:48:10 +0000 UTC,LastTransitionTime:2023-06-22 10:48:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun 22 10:48:11.339: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9663  802791e4-fd4e-4fe4-9fff-ce2b9293ff33 138334 2 2023-06-22 10:48:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 21527f88-7f44-4fc2-b241-19eb75631b34 0xc004bc4817 0xc004bc4818}] [] [{kube-controller-manager Update apps/v1 2023-06-22 10:48:10 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-22 10:48:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"21527f88-7f44-4fc2-b241-19eb75631b34\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bc48a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 22 10:48:11.344: INFO: Pod "test-new-deployment-7f5969cbc7-2k5pk" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-2k5pk test-new-deployment-7f5969cbc7- deployment-9663  bc9b2340-73b6-42fe-9c47-bd99d84374be 138325 0 2023-06-22 10:48:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 802791e4-fd4e-4fe4-9fff-ce2b9293ff33 0xc004bc4c37 0xc004bc4c38}] [] [{kube-controller-manager Update v1 2023-06-22 10:48:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"802791e4-fd4e-4fe4-9fff-ce2b9293ff33\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:48:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bzf2g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bzf2g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:48:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:48:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:48:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:48:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.50,StartTime:2023-06-22 10:48:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:48:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e1f0c6c0a1db9b97e518768c08e9181a184c8c0d1b42b7d0d6d15091c32363c7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 10:48:11.345: INFO: Pod "test-new-deployment-7f5969cbc7-rtd9w" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-rtd9w test-new-deployment-7f5969cbc7- deployment-9663  8cf465f7-3ac2-46fd-b93a-5c27ed1dd70b 138338 0 2023-06-22 10:48:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 802791e4-fd4e-4fe4-9fff-ce2b9293ff33 0xc004bc4e17 0xc004bc4e18}] [] [{kube-controller-manager Update v1 2023-06-22 10:48:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"802791e4-fd4e-4fe4-9fff-ce2b9293ff33\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-njgpw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-njgpw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:48:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:48:11.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9663" for this suite. 06/22/23 10:48:11.354
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:48:11.366
Jun 22 10:48:11.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename pod-network-test 06/22/23 10:48:11.367
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:48:11.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:48:11.4
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-2235 06/22/23 10:48:11.404
STEP: creating a selector 06/22/23 10:48:11.405
STEP: Creating the service pods in kubernetes 06/22/23 10:48:11.405
Jun 22 10:48:11.405: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
W0622 10:48:11.428160      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0622 10:48:11.436940      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0622 10:48:11.443963      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:48:11.444: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2235" to be "running and ready"
Jun 22 10:48:11.450: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.595564ms
Jun 22 10:48:11.450: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:48:13.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011684386s
Jun 22 10:48:13.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 10:48:15.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014850605s
Jun 22 10:48:15.459: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 10:48:17.464: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.020566901s
Jun 22 10:48:17.464: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 10:48:19.456: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012722118s
Jun 22 10:48:19.456: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 10:48:21.456: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.012529195s
Jun 22 10:48:21.456: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 10:48:23.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.013827778s
Jun 22 10:48:23.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 10:48:25.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013161114s
Jun 22 10:48:25.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 10:48:27.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014243265s
Jun 22 10:48:27.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 10:48:29.456: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012282492s
Jun 22 10:48:29.456: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 10:48:31.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013385165s
Jun 22 10:48:31.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 10:48:33.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.01335278s
Jun 22 10:48:33.457: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun 22 10:48:33.457: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun 22 10:48:33.461: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2235" to be "running and ready"
Jun 22 10:48:33.464: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.608347ms
Jun 22 10:48:33.465: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun 22 10:48:33.465: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun 22 10:48:33.468: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2235" to be "running and ready"
Jun 22 10:48:33.472: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.096341ms
Jun 22 10:48:33.472: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun 22 10:48:33.472: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/22/23 10:48:33.476
W0622 10:48:33.483029      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0622 10:48:33.492758      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": host namespaces (hostNetwork=true), allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:48:33.492: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2235" to be "running"
Jun 22 10:48:33.496: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.757428ms
Jun 22 10:48:35.502: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009239664s
Jun 22 10:48:35.502: INFO: Pod "test-container-pod" satisfied condition "running"
Jun 22 10:48:35.505: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2235" to be "running"
Jun 22 10:48:35.509: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.190265ms
Jun 22 10:48:35.509: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jun 22 10:48:35.512: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 22 10:48:35.512: INFO: Going to poll 100.96.4.200 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun 22 10:48:35.516: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.4.200:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2235 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:48:35.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:48:35.516: INFO: ExecWithOptions: Clientset creation
Jun 22 10:48:35.517: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-2235/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.96.4.200%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 22 10:48:35.643: INFO: Found all 1 expected endpoints: [netserver-0]
Jun 22 10:48:35.643: INFO: Going to poll 100.96.3.51 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun 22 10:48:35.648: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.3.51:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2235 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:48:35.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:48:35.648: INFO: ExecWithOptions: Clientset creation
Jun 22 10:48:35.648: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-2235/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.96.3.51%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 22 10:48:35.750: INFO: Found all 1 expected endpoints: [netserver-1]
Jun 22 10:48:35.750: INFO: Going to poll 100.96.2.205 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun 22 10:48:35.756: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.2.205:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2235 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:48:35.756: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:48:35.757: INFO: ExecWithOptions: Clientset creation
Jun 22 10:48:35.757: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-2235/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.96.2.205%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 22 10:48:35.863: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jun 22 10:48:35.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2235" for this suite. 06/22/23 10:48:35.871
------------------------------
• [SLOW TEST] [24.512 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:48:11.366
    Jun 22 10:48:11.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename pod-network-test 06/22/23 10:48:11.367
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:48:11.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:48:11.4
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-2235 06/22/23 10:48:11.404
    STEP: creating a selector 06/22/23 10:48:11.405
    STEP: Creating the service pods in kubernetes 06/22/23 10:48:11.405
    Jun 22 10:48:11.405: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    W0622 10:48:11.428160      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0622 10:48:11.436940      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0622 10:48:11.443963      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:48:11.444: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2235" to be "running and ready"
    Jun 22 10:48:11.450: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.595564ms
    Jun 22 10:48:11.450: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:48:13.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011684386s
    Jun 22 10:48:13.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 10:48:15.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014850605s
    Jun 22 10:48:15.459: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 10:48:17.464: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.020566901s
    Jun 22 10:48:17.464: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 10:48:19.456: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012722118s
    Jun 22 10:48:19.456: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 10:48:21.456: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.012529195s
    Jun 22 10:48:21.456: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 10:48:23.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.013827778s
    Jun 22 10:48:23.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 10:48:25.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013161114s
    Jun 22 10:48:25.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 10:48:27.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.014243265s
    Jun 22 10:48:27.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 10:48:29.456: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012282492s
    Jun 22 10:48:29.456: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 10:48:31.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013385165s
    Jun 22 10:48:31.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 10:48:33.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.01335278s
    Jun 22 10:48:33.457: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun 22 10:48:33.457: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun 22 10:48:33.461: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2235" to be "running and ready"
    Jun 22 10:48:33.464: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.608347ms
    Jun 22 10:48:33.465: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun 22 10:48:33.465: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun 22 10:48:33.468: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-2235" to be "running and ready"
    Jun 22 10:48:33.472: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.096341ms
    Jun 22 10:48:33.472: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun 22 10:48:33.472: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/22/23 10:48:33.476
    W0622 10:48:33.483029      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0622 10:48:33.492758      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": host namespaces (hostNetwork=true), allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:48:33.492: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2235" to be "running"
    Jun 22 10:48:33.496: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.757428ms
    Jun 22 10:48:35.502: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009239664s
    Jun 22 10:48:35.502: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun 22 10:48:35.505: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2235" to be "running"
    Jun 22 10:48:35.509: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.190265ms
    Jun 22 10:48:35.509: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jun 22 10:48:35.512: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun 22 10:48:35.512: INFO: Going to poll 100.96.4.200 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jun 22 10:48:35.516: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.4.200:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2235 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:48:35.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:48:35.516: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:48:35.517: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-2235/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.96.4.200%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 22 10:48:35.643: INFO: Found all 1 expected endpoints: [netserver-0]
    Jun 22 10:48:35.643: INFO: Going to poll 100.96.3.51 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jun 22 10:48:35.648: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.3.51:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2235 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:48:35.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:48:35.648: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:48:35.648: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-2235/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.96.3.51%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 22 10:48:35.750: INFO: Found all 1 expected endpoints: [netserver-1]
    Jun 22 10:48:35.750: INFO: Going to poll 100.96.2.205 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jun 22 10:48:35.756: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.2.205:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2235 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:48:35.756: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:48:35.757: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:48:35.757: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-2235/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F100.96.2.205%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 22 10:48:35.863: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:48:35.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2235" for this suite. 06/22/23 10:48:35.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:48:35.88
Jun 22 10:48:35.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename crd-publish-openapi 06/22/23 10:48:35.881
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:48:35.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:48:35.905
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Jun 22 10:48:35.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/22/23 10:48:39.538
Jun 22 10:48:39.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-3992 --namespace=crd-publish-openapi-3992 create -f -'
Jun 22 10:48:40.863: INFO: stderr: ""
Jun 22 10:48:40.863: INFO: stdout: "e2e-test-crd-publish-openapi-761-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 22 10:48:40.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-3992 --namespace=crd-publish-openapi-3992 delete e2e-test-crd-publish-openapi-761-crds test-cr'
Jun 22 10:48:40.991: INFO: stderr: ""
Jun 22 10:48:40.991: INFO: stdout: "e2e-test-crd-publish-openapi-761-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun 22 10:48:40.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-3992 --namespace=crd-publish-openapi-3992 apply -f -'
Jun 22 10:48:42.534: INFO: stderr: ""
Jun 22 10:48:42.534: INFO: stdout: "e2e-test-crd-publish-openapi-761-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 22 10:48:42.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-3992 --namespace=crd-publish-openapi-3992 delete e2e-test-crd-publish-openapi-761-crds test-cr'
Jun 22 10:48:42.632: INFO: stderr: ""
Jun 22 10:48:42.632: INFO: stdout: "e2e-test-crd-publish-openapi-761-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 06/22/23 10:48:42.632
Jun 22 10:48:42.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-3992 explain e2e-test-crd-publish-openapi-761-crds'
Jun 22 10:48:42.943: INFO: stderr: ""
Jun 22 10:48:42.943: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-761-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:48:45.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3992" for this suite. 06/22/23 10:48:45.581
------------------------------
• [SLOW TEST] [9.710 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:48:35.88
    Jun 22 10:48:35.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename crd-publish-openapi 06/22/23 10:48:35.881
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:48:35.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:48:35.905
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Jun 22 10:48:35.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/22/23 10:48:39.538
    Jun 22 10:48:39.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-3992 --namespace=crd-publish-openapi-3992 create -f -'
    Jun 22 10:48:40.863: INFO: stderr: ""
    Jun 22 10:48:40.863: INFO: stdout: "e2e-test-crd-publish-openapi-761-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jun 22 10:48:40.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-3992 --namespace=crd-publish-openapi-3992 delete e2e-test-crd-publish-openapi-761-crds test-cr'
    Jun 22 10:48:40.991: INFO: stderr: ""
    Jun 22 10:48:40.991: INFO: stdout: "e2e-test-crd-publish-openapi-761-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jun 22 10:48:40.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-3992 --namespace=crd-publish-openapi-3992 apply -f -'
    Jun 22 10:48:42.534: INFO: stderr: ""
    Jun 22 10:48:42.534: INFO: stdout: "e2e-test-crd-publish-openapi-761-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jun 22 10:48:42.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-3992 --namespace=crd-publish-openapi-3992 delete e2e-test-crd-publish-openapi-761-crds test-cr'
    Jun 22 10:48:42.632: INFO: stderr: ""
    Jun 22 10:48:42.632: INFO: stdout: "e2e-test-crd-publish-openapi-761-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 06/22/23 10:48:42.632
    Jun 22 10:48:42.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-3992 explain e2e-test-crd-publish-openapi-761-crds'
    Jun 22 10:48:42.943: INFO: stderr: ""
    Jun 22 10:48:42.943: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-761-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:48:45.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3992" for this suite. 06/22/23 10:48:45.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:48:45.59
Jun 22 10:48:45.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename deployment 06/22/23 10:48:45.591
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:48:45.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:48:45.612
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
W0622 10:48:45.622621      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:48:45.626: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 22 10:48:50.632: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/22/23 10:48:50.632
Jun 22 10:48:50.632: INFO: Creating deployment test-cleanup-deployment
W0622 10:48:50.644962      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 06/22/23 10:48:50.645
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 22 10:48:50.656: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-789  4ec1367e-4108-4d63-b16c-29b801f61862 138725 1 2023-06-22 10:48:50 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-06-22 10:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043b4e58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jun 22 10:48:50.659: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jun 22 10:48:50.659: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jun 22 10:48:50.659: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-789  b6168e74-9735-4a99-879e-a7daed47cf79 138726 1 2023-06-22 10:48:45 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 4ec1367e-4108-4d63-b16c-29b801f61862 0xc0043b58c7 0xc0043b58c8}] [] [{e2e.test Update apps/v1 2023-06-22 10:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:48:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-22 10:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"4ec1367e-4108-4d63-b16c-29b801f61862\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0043b5988 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 22 10:48:50.664: INFO: Pod "test-cleanup-controller-mjgbg" is available:
&Pod{ObjectMeta:{test-cleanup-controller-mjgbg test-cleanup-controller- deployment-789  6298e46f-9c47-46c8-8442-709597900940 138706 0 2023-06-22 10:48:45 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller b6168e74-9735-4a99-879e-a7daed47cf79 0xc0043b5cb7 0xc0043b5cb8}] [] [{kube-controller-manager Update v1 2023-06-22 10:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6168e74-9735-4a99-879e-a7daed47cf79\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:48:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4tt9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4tt9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:48:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.53,StartTime:2023-06-22 10:48:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:48:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9bdd7b8f7942bb17254d9445bb4d03c77c3685cefb17ba5ee673ea585b78d90a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun 22 10:48:50.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-789" for this suite. 06/22/23 10:48:50.673
------------------------------
• [SLOW TEST] [5.092 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:48:45.59
    Jun 22 10:48:45.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename deployment 06/22/23 10:48:45.591
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:48:45.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:48:45.612
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    W0622 10:48:45.622621      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:48:45.626: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jun 22 10:48:50.632: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/22/23 10:48:50.632
    Jun 22 10:48:50.632: INFO: Creating deployment test-cleanup-deployment
    W0622 10:48:50.644962      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 06/22/23 10:48:50.645
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 22 10:48:50.656: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-789  4ec1367e-4108-4d63-b16c-29b801f61862 138725 1 2023-06-22 10:48:50 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-06-22 10:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043b4e58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jun 22 10:48:50.659: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Jun 22 10:48:50.659: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jun 22 10:48:50.659: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-789  b6168e74-9735-4a99-879e-a7daed47cf79 138726 1 2023-06-22 10:48:45 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 4ec1367e-4108-4d63-b16c-29b801f61862 0xc0043b58c7 0xc0043b58c8}] [] [{e2e.test Update apps/v1 2023-06-22 10:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:48:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-22 10:48:50 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"4ec1367e-4108-4d63-b16c-29b801f61862\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0043b5988 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 22 10:48:50.664: INFO: Pod "test-cleanup-controller-mjgbg" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-mjgbg test-cleanup-controller- deployment-789  6298e46f-9c47-46c8-8442-709597900940 138706 0 2023-06-22 10:48:45 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller b6168e74-9735-4a99-879e-a7daed47cf79 0xc0043b5cb7 0xc0043b5cb8}] [] [{kube-controller-manager Update v1 2023-06-22 10:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6168e74-9735-4a99-879e-a7daed47cf79\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:48:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4tt9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4tt9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:48:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.53,StartTime:2023-06-22 10:48:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:48:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9bdd7b8f7942bb17254d9445bb4d03c77c3685cefb17ba5ee673ea585b78d90a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:48:50.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-789" for this suite. 06/22/23 10:48:50.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:48:50.684
Jun 22 10:48:50.684: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename init-container 06/22/23 10:48:50.684
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:48:50.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:48:50.711
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 06/22/23 10:48:50.715
Jun 22 10:48:50.715: INFO: PodSpec: initContainers in spec.initContainers
W0622 10:48:50.727486      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "init1", "init2", "run1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "init1", "init2", "run1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "init1", "init2", "run1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "init1", "init2", "run1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:48:54.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-6569" for this suite. 06/22/23 10:48:54.424
------------------------------
• [3.748 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:48:50.684
    Jun 22 10:48:50.684: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename init-container 06/22/23 10:48:50.684
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:48:50.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:48:50.711
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 06/22/23 10:48:50.715
    Jun 22 10:48:50.715: INFO: PodSpec: initContainers in spec.initContainers
    W0622 10:48:50.727486      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "init1", "init2", "run1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "init1", "init2", "run1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "init1", "init2", "run1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "init1", "init2", "run1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:48:54.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-6569" for this suite. 06/22/23 10:48:54.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:48:54.432
Jun 22 10:48:54.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename containers 06/22/23 10:48:54.433
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:48:54.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:48:54.456
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 06/22/23 10:48:54.46
W0622 10:48:54.469639      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:48:54.469: INFO: Waiting up to 5m0s for pod "client-containers-9e456528-0a91-4d75-813f-0a5a5455bd4c" in namespace "containers-3102" to be "Succeeded or Failed"
Jun 22 10:48:54.473: INFO: Pod "client-containers-9e456528-0a91-4d75-813f-0a5a5455bd4c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.825722ms
Jun 22 10:48:56.479: INFO: Pod "client-containers-9e456528-0a91-4d75-813f-0a5a5455bd4c": Phase="Running", Reason="", readiness=false. Elapsed: 2.009697662s
Jun 22 10:48:58.481: INFO: Pod "client-containers-9e456528-0a91-4d75-813f-0a5a5455bd4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011557303s
STEP: Saw pod success 06/22/23 10:48:58.481
Jun 22 10:48:58.481: INFO: Pod "client-containers-9e456528-0a91-4d75-813f-0a5a5455bd4c" satisfied condition "Succeeded or Failed"
Jun 22 10:48:58.485: INFO: Trying to get logs from node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k pod client-containers-9e456528-0a91-4d75-813f-0a5a5455bd4c container agnhost-container: <nil>
STEP: delete the pod 06/22/23 10:48:58.505
Jun 22 10:48:58.518: INFO: Waiting for pod client-containers-9e456528-0a91-4d75-813f-0a5a5455bd4c to disappear
Jun 22 10:48:58.522: INFO: Pod client-containers-9e456528-0a91-4d75-813f-0a5a5455bd4c no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jun 22 10:48:58.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3102" for this suite. 06/22/23 10:48:58.528
------------------------------
• [4.103 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:48:54.432
    Jun 22 10:48:54.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename containers 06/22/23 10:48:54.433
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:48:54.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:48:54.456
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 06/22/23 10:48:54.46
    W0622 10:48:54.469639      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:48:54.469: INFO: Waiting up to 5m0s for pod "client-containers-9e456528-0a91-4d75-813f-0a5a5455bd4c" in namespace "containers-3102" to be "Succeeded or Failed"
    Jun 22 10:48:54.473: INFO: Pod "client-containers-9e456528-0a91-4d75-813f-0a5a5455bd4c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.825722ms
    Jun 22 10:48:56.479: INFO: Pod "client-containers-9e456528-0a91-4d75-813f-0a5a5455bd4c": Phase="Running", Reason="", readiness=false. Elapsed: 2.009697662s
    Jun 22 10:48:58.481: INFO: Pod "client-containers-9e456528-0a91-4d75-813f-0a5a5455bd4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011557303s
    STEP: Saw pod success 06/22/23 10:48:58.481
    Jun 22 10:48:58.481: INFO: Pod "client-containers-9e456528-0a91-4d75-813f-0a5a5455bd4c" satisfied condition "Succeeded or Failed"
    Jun 22 10:48:58.485: INFO: Trying to get logs from node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k pod client-containers-9e456528-0a91-4d75-813f-0a5a5455bd4c container agnhost-container: <nil>
    STEP: delete the pod 06/22/23 10:48:58.505
    Jun 22 10:48:58.518: INFO: Waiting for pod client-containers-9e456528-0a91-4d75-813f-0a5a5455bd4c to disappear
    Jun 22 10:48:58.522: INFO: Pod client-containers-9e456528-0a91-4d75-813f-0a5a5455bd4c no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:48:58.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3102" for this suite. 06/22/23 10:48:58.528
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:48:58.537
Jun 22 10:48:58.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename discovery 06/22/23 10:48:58.538
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:48:58.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:48:58.569
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 06/22/23 10:48:58.574
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jun 22 10:48:59.226: INFO: Checking APIGroup: apiregistration.k8s.io
Jun 22 10:48:59.228: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jun 22 10:48:59.228: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jun 22 10:48:59.228: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jun 22 10:48:59.228: INFO: Checking APIGroup: apps
Jun 22 10:48:59.229: INFO: PreferredVersion.GroupVersion: apps/v1
Jun 22 10:48:59.229: INFO: Versions found [{apps/v1 v1}]
Jun 22 10:48:59.229: INFO: apps/v1 matches apps/v1
Jun 22 10:48:59.229: INFO: Checking APIGroup: events.k8s.io
Jun 22 10:48:59.231: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jun 22 10:48:59.231: INFO: Versions found [{events.k8s.io/v1 v1}]
Jun 22 10:48:59.231: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jun 22 10:48:59.231: INFO: Checking APIGroup: authentication.k8s.io
Jun 22 10:48:59.232: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jun 22 10:48:59.232: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jun 22 10:48:59.232: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jun 22 10:48:59.232: INFO: Checking APIGroup: authorization.k8s.io
Jun 22 10:48:59.234: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jun 22 10:48:59.234: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jun 22 10:48:59.234: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jun 22 10:48:59.234: INFO: Checking APIGroup: autoscaling
Jun 22 10:48:59.235: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jun 22 10:48:59.235: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Jun 22 10:48:59.235: INFO: autoscaling/v2 matches autoscaling/v2
Jun 22 10:48:59.235: INFO: Checking APIGroup: batch
Jun 22 10:48:59.237: INFO: PreferredVersion.GroupVersion: batch/v1
Jun 22 10:48:59.237: INFO: Versions found [{batch/v1 v1}]
Jun 22 10:48:59.237: INFO: batch/v1 matches batch/v1
Jun 22 10:48:59.237: INFO: Checking APIGroup: certificates.k8s.io
Jun 22 10:48:59.238: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jun 22 10:48:59.238: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jun 22 10:48:59.239: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jun 22 10:48:59.239: INFO: Checking APIGroup: networking.k8s.io
Jun 22 10:48:59.240: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jun 22 10:48:59.240: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jun 22 10:48:59.240: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jun 22 10:48:59.240: INFO: Checking APIGroup: policy
Jun 22 10:48:59.242: INFO: PreferredVersion.GroupVersion: policy/v1
Jun 22 10:48:59.242: INFO: Versions found [{policy/v1 v1}]
Jun 22 10:48:59.242: INFO: policy/v1 matches policy/v1
Jun 22 10:48:59.242: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jun 22 10:48:59.243: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jun 22 10:48:59.243: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jun 22 10:48:59.243: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jun 22 10:48:59.243: INFO: Checking APIGroup: storage.k8s.io
Jun 22 10:48:59.244: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jun 22 10:48:59.244: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jun 22 10:48:59.244: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jun 22 10:48:59.244: INFO: Checking APIGroup: admissionregistration.k8s.io
Jun 22 10:48:59.246: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jun 22 10:48:59.246: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jun 22 10:48:59.246: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jun 22 10:48:59.246: INFO: Checking APIGroup: apiextensions.k8s.io
Jun 22 10:48:59.247: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jun 22 10:48:59.247: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jun 22 10:48:59.247: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jun 22 10:48:59.247: INFO: Checking APIGroup: scheduling.k8s.io
Jun 22 10:48:59.248: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jun 22 10:48:59.248: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jun 22 10:48:59.248: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jun 22 10:48:59.248: INFO: Checking APIGroup: coordination.k8s.io
Jun 22 10:48:59.250: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jun 22 10:48:59.250: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jun 22 10:48:59.250: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jun 22 10:48:59.250: INFO: Checking APIGroup: node.k8s.io
Jun 22 10:48:59.252: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jun 22 10:48:59.252: INFO: Versions found [{node.k8s.io/v1 v1}]
Jun 22 10:48:59.252: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jun 22 10:48:59.252: INFO: Checking APIGroup: discovery.k8s.io
Jun 22 10:48:59.253: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jun 22 10:48:59.253: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jun 22 10:48:59.253: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jun 22 10:48:59.253: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jun 22 10:48:59.254: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Jun 22 10:48:59.254: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Jun 22 10:48:59.254: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Jun 22 10:48:59.254: INFO: Checking APIGroup: ako.vmware.com
Jun 22 10:48:59.257: INFO: PreferredVersion.GroupVersion: ako.vmware.com/v1alpha1
Jun 22 10:48:59.257: INFO: Versions found [{ako.vmware.com/v1alpha1 v1alpha1}]
Jun 22 10:48:59.257: INFO: ako.vmware.com/v1alpha1 matches ako.vmware.com/v1alpha1
Jun 22 10:48:59.257: INFO: Checking APIGroup: cns.vmware.com
Jun 22 10:48:59.259: INFO: PreferredVersion.GroupVersion: cns.vmware.com/v1alpha1
Jun 22 10:48:59.259: INFO: Versions found [{cns.vmware.com/v1alpha1 v1alpha1}]
Jun 22 10:48:59.259: INFO: cns.vmware.com/v1alpha1 matches cns.vmware.com/v1alpha1
Jun 22 10:48:59.259: INFO: Checking APIGroup: crd.antrea.io
Jun 22 10:48:59.260: INFO: PreferredVersion.GroupVersion: crd.antrea.io/v1beta1
Jun 22 10:48:59.260: INFO: Versions found [{crd.antrea.io/v1beta1 v1beta1} {crd.antrea.io/v1alpha3 v1alpha3} {crd.antrea.io/v1alpha2 v1alpha2} {crd.antrea.io/v1alpha1 v1alpha1}]
Jun 22 10:48:59.260: INFO: crd.antrea.io/v1beta1 matches crd.antrea.io/v1beta1
Jun 22 10:48:59.260: INFO: Checking APIGroup: crd.antrea.tanzu.vmware.com
Jun 22 10:48:59.261: INFO: PreferredVersion.GroupVersion: crd.antrea.tanzu.vmware.com/v1alpha1
Jun 22 10:48:59.261: INFO: Versions found [{crd.antrea.tanzu.vmware.com/v1alpha1 v1alpha1}]
Jun 22 10:48:59.261: INFO: crd.antrea.tanzu.vmware.com/v1alpha1 matches crd.antrea.tanzu.vmware.com/v1alpha1
Jun 22 10:48:59.261: INFO: Checking APIGroup: internal.packaging.carvel.dev
Jun 22 10:48:59.263: INFO: PreferredVersion.GroupVersion: internal.packaging.carvel.dev/v1alpha1
Jun 22 10:48:59.263: INFO: Versions found [{internal.packaging.carvel.dev/v1alpha1 v1alpha1}]
Jun 22 10:48:59.263: INFO: internal.packaging.carvel.dev/v1alpha1 matches internal.packaging.carvel.dev/v1alpha1
Jun 22 10:48:59.263: INFO: Checking APIGroup: kappctrl.k14s.io
Jun 22 10:48:59.264: INFO: PreferredVersion.GroupVersion: kappctrl.k14s.io/v1alpha1
Jun 22 10:48:59.264: INFO: Versions found [{kappctrl.k14s.io/v1alpha1 v1alpha1}]
Jun 22 10:48:59.264: INFO: kappctrl.k14s.io/v1alpha1 matches kappctrl.k14s.io/v1alpha1
Jun 22 10:48:59.264: INFO: Checking APIGroup: networking.x-k8s.io
Jun 22 10:48:59.266: INFO: PreferredVersion.GroupVersion: networking.x-k8s.io/v1alpha1
Jun 22 10:48:59.266: INFO: Versions found [{networking.x-k8s.io/v1alpha1 v1alpha1}]
Jun 22 10:48:59.266: INFO: networking.x-k8s.io/v1alpha1 matches networking.x-k8s.io/v1alpha1
Jun 22 10:48:59.266: INFO: Checking APIGroup: packaging.carvel.dev
Jun 22 10:48:59.268: INFO: PreferredVersion.GroupVersion: packaging.carvel.dev/v1alpha1
Jun 22 10:48:59.268: INFO: Versions found [{packaging.carvel.dev/v1alpha1 v1alpha1}]
Jun 22 10:48:59.268: INFO: packaging.carvel.dev/v1alpha1 matches packaging.carvel.dev/v1alpha1
Jun 22 10:48:59.268: INFO: Checking APIGroup: run.tanzu.vmware.com
Jun 22 10:48:59.269: INFO: PreferredVersion.GroupVersion: run.tanzu.vmware.com/v1alpha1
Jun 22 10:48:59.269: INFO: Versions found [{run.tanzu.vmware.com/v1alpha1 v1alpha1}]
Jun 22 10:48:59.269: INFO: run.tanzu.vmware.com/v1alpha1 matches run.tanzu.vmware.com/v1alpha1
Jun 22 10:48:59.269: INFO: Checking APIGroup: secretgen.carvel.dev
Jun 22 10:48:59.270: INFO: PreferredVersion.GroupVersion: secretgen.carvel.dev/v1alpha1
Jun 22 10:48:59.270: INFO: Versions found [{secretgen.carvel.dev/v1alpha1 v1alpha1}]
Jun 22 10:48:59.270: INFO: secretgen.carvel.dev/v1alpha1 matches secretgen.carvel.dev/v1alpha1
Jun 22 10:48:59.270: INFO: Checking APIGroup: secretgen.k14s.io
Jun 22 10:48:59.272: INFO: PreferredVersion.GroupVersion: secretgen.k14s.io/v1alpha1
Jun 22 10:48:59.272: INFO: Versions found [{secretgen.k14s.io/v1alpha1 v1alpha1}]
Jun 22 10:48:59.272: INFO: secretgen.k14s.io/v1alpha1 matches secretgen.k14s.io/v1alpha1
Jun 22 10:48:59.272: INFO: Checking APIGroup: core.tanzu.vmware.com
Jun 22 10:48:59.273: INFO: PreferredVersion.GroupVersion: core.tanzu.vmware.com/v1alpha2
Jun 22 10:48:59.273: INFO: Versions found [{core.tanzu.vmware.com/v1alpha2 v1alpha2}]
Jun 22 10:48:59.273: INFO: core.tanzu.vmware.com/v1alpha2 matches core.tanzu.vmware.com/v1alpha2
Jun 22 10:48:59.273: INFO: Checking APIGroup: data.packaging.carvel.dev
Jun 22 10:48:59.274: INFO: PreferredVersion.GroupVersion: data.packaging.carvel.dev/v1alpha1
Jun 22 10:48:59.274: INFO: Versions found [{data.packaging.carvel.dev/v1alpha1 v1alpha1}]
Jun 22 10:48:59.274: INFO: data.packaging.carvel.dev/v1alpha1 matches data.packaging.carvel.dev/v1alpha1
Jun 22 10:48:59.274: INFO: Checking APIGroup: stats.antrea.io
Jun 22 10:48:59.276: INFO: PreferredVersion.GroupVersion: stats.antrea.io/v1alpha1
Jun 22 10:48:59.276: INFO: Versions found [{stats.antrea.io/v1alpha1 v1alpha1}]
Jun 22 10:48:59.276: INFO: stats.antrea.io/v1alpha1 matches stats.antrea.io/v1alpha1
Jun 22 10:48:59.276: INFO: Checking APIGroup: metrics.k8s.io
Jun 22 10:48:59.277: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jun 22 10:48:59.277: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jun 22 10:48:59.278: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
Jun 22 10:48:59.278: INFO: Checking APIGroup: system.antrea.io
Jun 22 10:48:59.279: INFO: PreferredVersion.GroupVersion: system.antrea.io/v1beta1
Jun 22 10:48:59.279: INFO: Versions found [{system.antrea.io/v1beta1 v1beta1}]
Jun 22 10:48:59.279: INFO: system.antrea.io/v1beta1 matches system.antrea.io/v1beta1
Jun 22 10:48:59.279: INFO: Checking APIGroup: controlplane.antrea.io
Jun 22 10:48:59.280: INFO: PreferredVersion.GroupVersion: controlplane.antrea.io/v1beta2
Jun 22 10:48:59.280: INFO: Versions found [{controlplane.antrea.io/v1beta2 v1beta2}]
Jun 22 10:48:59.280: INFO: controlplane.antrea.io/v1beta2 matches controlplane.antrea.io/v1beta2
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Jun 22 10:48:59.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-2118" for this suite. 06/22/23 10:48:59.287
------------------------------
• [0.759 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:48:58.537
    Jun 22 10:48:58.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename discovery 06/22/23 10:48:58.538
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:48:58.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:48:58.569
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 06/22/23 10:48:58.574
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jun 22 10:48:59.226: INFO: Checking APIGroup: apiregistration.k8s.io
    Jun 22 10:48:59.228: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jun 22 10:48:59.228: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jun 22 10:48:59.228: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jun 22 10:48:59.228: INFO: Checking APIGroup: apps
    Jun 22 10:48:59.229: INFO: PreferredVersion.GroupVersion: apps/v1
    Jun 22 10:48:59.229: INFO: Versions found [{apps/v1 v1}]
    Jun 22 10:48:59.229: INFO: apps/v1 matches apps/v1
    Jun 22 10:48:59.229: INFO: Checking APIGroup: events.k8s.io
    Jun 22 10:48:59.231: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jun 22 10:48:59.231: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jun 22 10:48:59.231: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jun 22 10:48:59.231: INFO: Checking APIGroup: authentication.k8s.io
    Jun 22 10:48:59.232: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jun 22 10:48:59.232: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jun 22 10:48:59.232: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jun 22 10:48:59.232: INFO: Checking APIGroup: authorization.k8s.io
    Jun 22 10:48:59.234: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jun 22 10:48:59.234: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jun 22 10:48:59.234: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jun 22 10:48:59.234: INFO: Checking APIGroup: autoscaling
    Jun 22 10:48:59.235: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jun 22 10:48:59.235: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Jun 22 10:48:59.235: INFO: autoscaling/v2 matches autoscaling/v2
    Jun 22 10:48:59.235: INFO: Checking APIGroup: batch
    Jun 22 10:48:59.237: INFO: PreferredVersion.GroupVersion: batch/v1
    Jun 22 10:48:59.237: INFO: Versions found [{batch/v1 v1}]
    Jun 22 10:48:59.237: INFO: batch/v1 matches batch/v1
    Jun 22 10:48:59.237: INFO: Checking APIGroup: certificates.k8s.io
    Jun 22 10:48:59.238: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jun 22 10:48:59.238: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jun 22 10:48:59.239: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jun 22 10:48:59.239: INFO: Checking APIGroup: networking.k8s.io
    Jun 22 10:48:59.240: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jun 22 10:48:59.240: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jun 22 10:48:59.240: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jun 22 10:48:59.240: INFO: Checking APIGroup: policy
    Jun 22 10:48:59.242: INFO: PreferredVersion.GroupVersion: policy/v1
    Jun 22 10:48:59.242: INFO: Versions found [{policy/v1 v1}]
    Jun 22 10:48:59.242: INFO: policy/v1 matches policy/v1
    Jun 22 10:48:59.242: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jun 22 10:48:59.243: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jun 22 10:48:59.243: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jun 22 10:48:59.243: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jun 22 10:48:59.243: INFO: Checking APIGroup: storage.k8s.io
    Jun 22 10:48:59.244: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jun 22 10:48:59.244: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jun 22 10:48:59.244: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jun 22 10:48:59.244: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jun 22 10:48:59.246: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jun 22 10:48:59.246: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jun 22 10:48:59.246: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jun 22 10:48:59.246: INFO: Checking APIGroup: apiextensions.k8s.io
    Jun 22 10:48:59.247: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jun 22 10:48:59.247: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jun 22 10:48:59.247: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jun 22 10:48:59.247: INFO: Checking APIGroup: scheduling.k8s.io
    Jun 22 10:48:59.248: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jun 22 10:48:59.248: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jun 22 10:48:59.248: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jun 22 10:48:59.248: INFO: Checking APIGroup: coordination.k8s.io
    Jun 22 10:48:59.250: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jun 22 10:48:59.250: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jun 22 10:48:59.250: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jun 22 10:48:59.250: INFO: Checking APIGroup: node.k8s.io
    Jun 22 10:48:59.252: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jun 22 10:48:59.252: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jun 22 10:48:59.252: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jun 22 10:48:59.252: INFO: Checking APIGroup: discovery.k8s.io
    Jun 22 10:48:59.253: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jun 22 10:48:59.253: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jun 22 10:48:59.253: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jun 22 10:48:59.253: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jun 22 10:48:59.254: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Jun 22 10:48:59.254: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Jun 22 10:48:59.254: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Jun 22 10:48:59.254: INFO: Checking APIGroup: ako.vmware.com
    Jun 22 10:48:59.257: INFO: PreferredVersion.GroupVersion: ako.vmware.com/v1alpha1
    Jun 22 10:48:59.257: INFO: Versions found [{ako.vmware.com/v1alpha1 v1alpha1}]
    Jun 22 10:48:59.257: INFO: ako.vmware.com/v1alpha1 matches ako.vmware.com/v1alpha1
    Jun 22 10:48:59.257: INFO: Checking APIGroup: cns.vmware.com
    Jun 22 10:48:59.259: INFO: PreferredVersion.GroupVersion: cns.vmware.com/v1alpha1
    Jun 22 10:48:59.259: INFO: Versions found [{cns.vmware.com/v1alpha1 v1alpha1}]
    Jun 22 10:48:59.259: INFO: cns.vmware.com/v1alpha1 matches cns.vmware.com/v1alpha1
    Jun 22 10:48:59.259: INFO: Checking APIGroup: crd.antrea.io
    Jun 22 10:48:59.260: INFO: PreferredVersion.GroupVersion: crd.antrea.io/v1beta1
    Jun 22 10:48:59.260: INFO: Versions found [{crd.antrea.io/v1beta1 v1beta1} {crd.antrea.io/v1alpha3 v1alpha3} {crd.antrea.io/v1alpha2 v1alpha2} {crd.antrea.io/v1alpha1 v1alpha1}]
    Jun 22 10:48:59.260: INFO: crd.antrea.io/v1beta1 matches crd.antrea.io/v1beta1
    Jun 22 10:48:59.260: INFO: Checking APIGroup: crd.antrea.tanzu.vmware.com
    Jun 22 10:48:59.261: INFO: PreferredVersion.GroupVersion: crd.antrea.tanzu.vmware.com/v1alpha1
    Jun 22 10:48:59.261: INFO: Versions found [{crd.antrea.tanzu.vmware.com/v1alpha1 v1alpha1}]
    Jun 22 10:48:59.261: INFO: crd.antrea.tanzu.vmware.com/v1alpha1 matches crd.antrea.tanzu.vmware.com/v1alpha1
    Jun 22 10:48:59.261: INFO: Checking APIGroup: internal.packaging.carvel.dev
    Jun 22 10:48:59.263: INFO: PreferredVersion.GroupVersion: internal.packaging.carvel.dev/v1alpha1
    Jun 22 10:48:59.263: INFO: Versions found [{internal.packaging.carvel.dev/v1alpha1 v1alpha1}]
    Jun 22 10:48:59.263: INFO: internal.packaging.carvel.dev/v1alpha1 matches internal.packaging.carvel.dev/v1alpha1
    Jun 22 10:48:59.263: INFO: Checking APIGroup: kappctrl.k14s.io
    Jun 22 10:48:59.264: INFO: PreferredVersion.GroupVersion: kappctrl.k14s.io/v1alpha1
    Jun 22 10:48:59.264: INFO: Versions found [{kappctrl.k14s.io/v1alpha1 v1alpha1}]
    Jun 22 10:48:59.264: INFO: kappctrl.k14s.io/v1alpha1 matches kappctrl.k14s.io/v1alpha1
    Jun 22 10:48:59.264: INFO: Checking APIGroup: networking.x-k8s.io
    Jun 22 10:48:59.266: INFO: PreferredVersion.GroupVersion: networking.x-k8s.io/v1alpha1
    Jun 22 10:48:59.266: INFO: Versions found [{networking.x-k8s.io/v1alpha1 v1alpha1}]
    Jun 22 10:48:59.266: INFO: networking.x-k8s.io/v1alpha1 matches networking.x-k8s.io/v1alpha1
    Jun 22 10:48:59.266: INFO: Checking APIGroup: packaging.carvel.dev
    Jun 22 10:48:59.268: INFO: PreferredVersion.GroupVersion: packaging.carvel.dev/v1alpha1
    Jun 22 10:48:59.268: INFO: Versions found [{packaging.carvel.dev/v1alpha1 v1alpha1}]
    Jun 22 10:48:59.268: INFO: packaging.carvel.dev/v1alpha1 matches packaging.carvel.dev/v1alpha1
    Jun 22 10:48:59.268: INFO: Checking APIGroup: run.tanzu.vmware.com
    Jun 22 10:48:59.269: INFO: PreferredVersion.GroupVersion: run.tanzu.vmware.com/v1alpha1
    Jun 22 10:48:59.269: INFO: Versions found [{run.tanzu.vmware.com/v1alpha1 v1alpha1}]
    Jun 22 10:48:59.269: INFO: run.tanzu.vmware.com/v1alpha1 matches run.tanzu.vmware.com/v1alpha1
    Jun 22 10:48:59.269: INFO: Checking APIGroup: secretgen.carvel.dev
    Jun 22 10:48:59.270: INFO: PreferredVersion.GroupVersion: secretgen.carvel.dev/v1alpha1
    Jun 22 10:48:59.270: INFO: Versions found [{secretgen.carvel.dev/v1alpha1 v1alpha1}]
    Jun 22 10:48:59.270: INFO: secretgen.carvel.dev/v1alpha1 matches secretgen.carvel.dev/v1alpha1
    Jun 22 10:48:59.270: INFO: Checking APIGroup: secretgen.k14s.io
    Jun 22 10:48:59.272: INFO: PreferredVersion.GroupVersion: secretgen.k14s.io/v1alpha1
    Jun 22 10:48:59.272: INFO: Versions found [{secretgen.k14s.io/v1alpha1 v1alpha1}]
    Jun 22 10:48:59.272: INFO: secretgen.k14s.io/v1alpha1 matches secretgen.k14s.io/v1alpha1
    Jun 22 10:48:59.272: INFO: Checking APIGroup: core.tanzu.vmware.com
    Jun 22 10:48:59.273: INFO: PreferredVersion.GroupVersion: core.tanzu.vmware.com/v1alpha2
    Jun 22 10:48:59.273: INFO: Versions found [{core.tanzu.vmware.com/v1alpha2 v1alpha2}]
    Jun 22 10:48:59.273: INFO: core.tanzu.vmware.com/v1alpha2 matches core.tanzu.vmware.com/v1alpha2
    Jun 22 10:48:59.273: INFO: Checking APIGroup: data.packaging.carvel.dev
    Jun 22 10:48:59.274: INFO: PreferredVersion.GroupVersion: data.packaging.carvel.dev/v1alpha1
    Jun 22 10:48:59.274: INFO: Versions found [{data.packaging.carvel.dev/v1alpha1 v1alpha1}]
    Jun 22 10:48:59.274: INFO: data.packaging.carvel.dev/v1alpha1 matches data.packaging.carvel.dev/v1alpha1
    Jun 22 10:48:59.274: INFO: Checking APIGroup: stats.antrea.io
    Jun 22 10:48:59.276: INFO: PreferredVersion.GroupVersion: stats.antrea.io/v1alpha1
    Jun 22 10:48:59.276: INFO: Versions found [{stats.antrea.io/v1alpha1 v1alpha1}]
    Jun 22 10:48:59.276: INFO: stats.antrea.io/v1alpha1 matches stats.antrea.io/v1alpha1
    Jun 22 10:48:59.276: INFO: Checking APIGroup: metrics.k8s.io
    Jun 22 10:48:59.277: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Jun 22 10:48:59.277: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Jun 22 10:48:59.278: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    Jun 22 10:48:59.278: INFO: Checking APIGroup: system.antrea.io
    Jun 22 10:48:59.279: INFO: PreferredVersion.GroupVersion: system.antrea.io/v1beta1
    Jun 22 10:48:59.279: INFO: Versions found [{system.antrea.io/v1beta1 v1beta1}]
    Jun 22 10:48:59.279: INFO: system.antrea.io/v1beta1 matches system.antrea.io/v1beta1
    Jun 22 10:48:59.279: INFO: Checking APIGroup: controlplane.antrea.io
    Jun 22 10:48:59.280: INFO: PreferredVersion.GroupVersion: controlplane.antrea.io/v1beta2
    Jun 22 10:48:59.280: INFO: Versions found [{controlplane.antrea.io/v1beta2 v1beta2}]
    Jun 22 10:48:59.280: INFO: controlplane.antrea.io/v1beta2 matches controlplane.antrea.io/v1beta2
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:48:59.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-2118" for this suite. 06/22/23 10:48:59.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:48:59.297
Jun 22 10:48:59.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename configmap 06/22/23 10:48:59.298
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:48:59.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:48:59.322
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-51c6c18e-6119-4af1-966a-e03c3fe0a9e8 06/22/23 10:48:59.326
STEP: Creating a pod to test consume configMaps 06/22/23 10:48:59.331
W0622 10:48:59.340355      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:48:59.340: INFO: Waiting up to 5m0s for pod "pod-configmaps-ca86c749-dabb-4857-b320-be0b01130435" in namespace "configmap-956" to be "Succeeded or Failed"
Jun 22 10:48:59.345: INFO: Pod "pod-configmaps-ca86c749-dabb-4857-b320-be0b01130435": Phase="Pending", Reason="", readiness=false. Elapsed: 5.023879ms
Jun 22 10:49:01.350: INFO: Pod "pod-configmaps-ca86c749-dabb-4857-b320-be0b01130435": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010349229s
Jun 22 10:49:03.350: INFO: Pod "pod-configmaps-ca86c749-dabb-4857-b320-be0b01130435": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010293573s
STEP: Saw pod success 06/22/23 10:49:03.35
Jun 22 10:49:03.351: INFO: Pod "pod-configmaps-ca86c749-dabb-4857-b320-be0b01130435" satisfied condition "Succeeded or Failed"
Jun 22 10:49:03.355: INFO: Trying to get logs from node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k pod pod-configmaps-ca86c749-dabb-4857-b320-be0b01130435 container agnhost-container: <nil>
STEP: delete the pod 06/22/23 10:49:03.362
Jun 22 10:49:03.377: INFO: Waiting for pod pod-configmaps-ca86c749-dabb-4857-b320-be0b01130435 to disappear
Jun 22 10:49:03.382: INFO: Pod pod-configmaps-ca86c749-dabb-4857-b320-be0b01130435 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun 22 10:49:03.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-956" for this suite. 06/22/23 10:49:03.387
------------------------------
• [4.096 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:48:59.297
    Jun 22 10:48:59.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename configmap 06/22/23 10:48:59.298
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:48:59.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:48:59.322
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-51c6c18e-6119-4af1-966a-e03c3fe0a9e8 06/22/23 10:48:59.326
    STEP: Creating a pod to test consume configMaps 06/22/23 10:48:59.331
    W0622 10:48:59.340355      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:48:59.340: INFO: Waiting up to 5m0s for pod "pod-configmaps-ca86c749-dabb-4857-b320-be0b01130435" in namespace "configmap-956" to be "Succeeded or Failed"
    Jun 22 10:48:59.345: INFO: Pod "pod-configmaps-ca86c749-dabb-4857-b320-be0b01130435": Phase="Pending", Reason="", readiness=false. Elapsed: 5.023879ms
    Jun 22 10:49:01.350: INFO: Pod "pod-configmaps-ca86c749-dabb-4857-b320-be0b01130435": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010349229s
    Jun 22 10:49:03.350: INFO: Pod "pod-configmaps-ca86c749-dabb-4857-b320-be0b01130435": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010293573s
    STEP: Saw pod success 06/22/23 10:49:03.35
    Jun 22 10:49:03.351: INFO: Pod "pod-configmaps-ca86c749-dabb-4857-b320-be0b01130435" satisfied condition "Succeeded or Failed"
    Jun 22 10:49:03.355: INFO: Trying to get logs from node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k pod pod-configmaps-ca86c749-dabb-4857-b320-be0b01130435 container agnhost-container: <nil>
    STEP: delete the pod 06/22/23 10:49:03.362
    Jun 22 10:49:03.377: INFO: Waiting for pod pod-configmaps-ca86c749-dabb-4857-b320-be0b01130435 to disappear
    Jun 22 10:49:03.382: INFO: Pod pod-configmaps-ca86c749-dabb-4857-b320-be0b01130435 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:49:03.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-956" for this suite. 06/22/23 10:49:03.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:49:03.396
Jun 22 10:49:03.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubectl 06/22/23 10:49:03.397
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:49:03.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:49:03.422
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 06/22/23 10:49:03.427
Jun 22 10:49:03.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-1469 create -f -'
Jun 22 10:49:04.382: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"agnhost-primary\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"agnhost-primary\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"agnhost-primary\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"agnhost-primary\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Jun 22 10:49:04.382: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 06/22/23 10:49:04.382
Jun 22 10:49:05.388: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 10:49:05.388: INFO: Found 0 / 1
Jun 22 10:49:06.389: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 10:49:06.389: INFO: Found 1 / 1
Jun 22 10:49:06.389: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 06/22/23 10:49:06.389
Jun 22 10:49:06.393: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 10:49:06.393: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 10:49:06.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-1469 patch pod agnhost-primary-gzsjw -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 22 10:49:06.485: INFO: stderr: ""
Jun 22 10:49:06.485: INFO: stdout: "pod/agnhost-primary-gzsjw patched\n"
STEP: checking annotations 06/22/23 10:49:06.485
Jun 22 10:49:06.489: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 10:49:06.489: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun 22 10:49:06.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1469" for this suite. 06/22/23 10:49:06.495
------------------------------
• [3.105 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:49:03.396
    Jun 22 10:49:03.396: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubectl 06/22/23 10:49:03.397
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:49:03.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:49:03.422
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 06/22/23 10:49:03.427
    Jun 22 10:49:03.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-1469 create -f -'
    Jun 22 10:49:04.382: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"agnhost-primary\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"agnhost-primary\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"agnhost-primary\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"agnhost-primary\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
    Jun 22 10:49:04.382: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 06/22/23 10:49:04.382
    Jun 22 10:49:05.388: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 22 10:49:05.388: INFO: Found 0 / 1
    Jun 22 10:49:06.389: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 22 10:49:06.389: INFO: Found 1 / 1
    Jun 22 10:49:06.389: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 06/22/23 10:49:06.389
    Jun 22 10:49:06.393: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 22 10:49:06.393: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jun 22 10:49:06.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-1469 patch pod agnhost-primary-gzsjw -p {"metadata":{"annotations":{"x":"y"}}}'
    Jun 22 10:49:06.485: INFO: stderr: ""
    Jun 22 10:49:06.485: INFO: stdout: "pod/agnhost-primary-gzsjw patched\n"
    STEP: checking annotations 06/22/23 10:49:06.485
    Jun 22 10:49:06.489: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 22 10:49:06.489: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:49:06.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1469" for this suite. 06/22/23 10:49:06.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:49:06.502
Jun 22 10:49:06.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 10:49:06.503
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:49:06.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:49:06.526
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-7235dde8-c285-4a00-9576-df9132b6e594 06/22/23 10:49:06.535
STEP: Creating the pod 06/22/23 10:49:06.543
W0622 10:49:06.554887      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:49:06.554: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-386f5df4-ab3c-45d5-8b9c-399fec5dbbb9" in namespace "projected-2872" to be "running and ready"
Jun 22 10:49:06.558: INFO: Pod "pod-projected-configmaps-386f5df4-ab3c-45d5-8b9c-399fec5dbbb9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.552704ms
Jun 22 10:49:06.558: INFO: The phase of Pod pod-projected-configmaps-386f5df4-ab3c-45d5-8b9c-399fec5dbbb9 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:49:08.563: INFO: Pod "pod-projected-configmaps-386f5df4-ab3c-45d5-8b9c-399fec5dbbb9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008893622s
Jun 22 10:49:08.563: INFO: The phase of Pod pod-projected-configmaps-386f5df4-ab3c-45d5-8b9c-399fec5dbbb9 is Running (Ready = true)
Jun 22 10:49:08.563: INFO: Pod "pod-projected-configmaps-386f5df4-ab3c-45d5-8b9c-399fec5dbbb9" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-7235dde8-c285-4a00-9576-df9132b6e594 06/22/23 10:49:08.579
STEP: waiting to observe update in volume 06/22/23 10:49:08.586
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun 22 10:49:12.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2872" for this suite. 06/22/23 10:49:12.62
------------------------------
• [SLOW TEST] [6.125 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:49:06.502
    Jun 22 10:49:06.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 10:49:06.503
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:49:06.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:49:06.526
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-7235dde8-c285-4a00-9576-df9132b6e594 06/22/23 10:49:06.535
    STEP: Creating the pod 06/22/23 10:49:06.543
    W0622 10:49:06.554887      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:49:06.554: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-386f5df4-ab3c-45d5-8b9c-399fec5dbbb9" in namespace "projected-2872" to be "running and ready"
    Jun 22 10:49:06.558: INFO: Pod "pod-projected-configmaps-386f5df4-ab3c-45d5-8b9c-399fec5dbbb9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.552704ms
    Jun 22 10:49:06.558: INFO: The phase of Pod pod-projected-configmaps-386f5df4-ab3c-45d5-8b9c-399fec5dbbb9 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:49:08.563: INFO: Pod "pod-projected-configmaps-386f5df4-ab3c-45d5-8b9c-399fec5dbbb9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008893622s
    Jun 22 10:49:08.563: INFO: The phase of Pod pod-projected-configmaps-386f5df4-ab3c-45d5-8b9c-399fec5dbbb9 is Running (Ready = true)
    Jun 22 10:49:08.563: INFO: Pod "pod-projected-configmaps-386f5df4-ab3c-45d5-8b9c-399fec5dbbb9" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-7235dde8-c285-4a00-9576-df9132b6e594 06/22/23 10:49:08.579
    STEP: waiting to observe update in volume 06/22/23 10:49:08.586
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:49:12.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2872" for this suite. 06/22/23 10:49:12.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:49:12.628
Jun 22 10:49:12.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename disruption 06/22/23 10:49:12.629
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:49:12.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:49:12.652
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 06/22/23 10:49:12.663
STEP: Updating PodDisruptionBudget status 06/22/23 10:49:14.671
W0622 10:49:14.682470      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "donothing" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "donothing" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "donothing" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "donothing" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting for all pods to be running 06/22/23 10:49:14.682
Jun 22 10:49:14.686: INFO: running pods: 0 < 1
STEP: locating a running pod 06/22/23 10:49:16.692
STEP: Waiting for the pdb to be processed 06/22/23 10:49:16.708
STEP: Patching PodDisruptionBudget status 06/22/23 10:49:16.715
STEP: Waiting for the pdb to be processed 06/22/23 10:49:16.727
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jun 22 10:49:16.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2869" for this suite. 06/22/23 10:49:16.739
------------------------------
• [4.120 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:49:12.628
    Jun 22 10:49:12.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename disruption 06/22/23 10:49:12.629
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:49:12.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:49:12.652
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 06/22/23 10:49:12.663
    STEP: Updating PodDisruptionBudget status 06/22/23 10:49:14.671
    W0622 10:49:14.682470      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "donothing" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "donothing" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "donothing" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "donothing" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Waiting for all pods to be running 06/22/23 10:49:14.682
    Jun 22 10:49:14.686: INFO: running pods: 0 < 1
    STEP: locating a running pod 06/22/23 10:49:16.692
    STEP: Waiting for the pdb to be processed 06/22/23 10:49:16.708
    STEP: Patching PodDisruptionBudget status 06/22/23 10:49:16.715
    STEP: Waiting for the pdb to be processed 06/22/23 10:49:16.727
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:49:16.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2869" for this suite. 06/22/23 10:49:16.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:49:16.75
Jun 22 10:49:16.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename services 06/22/23 10:49:16.751
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:49:16.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:49:16.774
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-3061 06/22/23 10:49:16.777
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3061 to expose endpoints map[] 06/22/23 10:49:16.796
Jun 22 10:49:16.809: INFO: successfully validated that service multi-endpoint-test in namespace services-3061 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3061 06/22/23 10:49:16.809
W0622 10:49:16.824780      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:49:16.824: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3061" to be "running and ready"
Jun 22 10:49:16.828: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.907555ms
Jun 22 10:49:16.828: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:49:18.833: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00870272s
Jun 22 10:49:18.833: INFO: The phase of Pod pod1 is Running (Ready = true)
Jun 22 10:49:18.833: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3061 to expose endpoints map[pod1:[100]] 06/22/23 10:49:18.837
Jun 22 10:49:18.852: INFO: successfully validated that service multi-endpoint-test in namespace services-3061 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3061 06/22/23 10:49:18.852
W0622 10:49:18.858171      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:49:18.858: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3061" to be "running and ready"
Jun 22 10:49:18.861: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.381281ms
Jun 22 10:49:18.861: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:49:20.866: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008699861s
Jun 22 10:49:20.867: INFO: The phase of Pod pod2 is Running (Ready = true)
Jun 22 10:49:20.867: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3061 to expose endpoints map[pod1:[100] pod2:[101]] 06/22/23 10:49:20.871
Jun 22 10:49:20.891: INFO: successfully validated that service multi-endpoint-test in namespace services-3061 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 06/22/23 10:49:20.891
Jun 22 10:49:20.892: INFO: Creating new exec pod
W0622 10:49:20.899112      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:49:20.899: INFO: Waiting up to 5m0s for pod "execpodhj6g4" in namespace "services-3061" to be "running"
Jun 22 10:49:20.902: INFO: Pod "execpodhj6g4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.619174ms
Jun 22 10:49:22.908: INFO: Pod "execpodhj6g4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008956645s
Jun 22 10:49:22.908: INFO: Pod "execpodhj6g4" satisfied condition "running"
Jun 22 10:49:23.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-3061 exec execpodhj6g4 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Jun 22 10:49:24.098: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jun 22 10:49:24.098: INFO: stdout: ""
Jun 22 10:49:24.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-3061 exec execpodhj6g4 -- /bin/sh -x -c nc -v -z -w 2 100.69.224.76 80'
Jun 22 10:49:24.265: INFO: stderr: "+ nc -v -z -w 2 100.69.224.76 80\nConnection to 100.69.224.76 80 port [tcp/http] succeeded!\n"
Jun 22 10:49:24.265: INFO: stdout: ""
Jun 22 10:49:24.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-3061 exec execpodhj6g4 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Jun 22 10:49:24.436: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jun 22 10:49:24.436: INFO: stdout: ""
Jun 22 10:49:24.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-3061 exec execpodhj6g4 -- /bin/sh -x -c nc -v -z -w 2 100.69.224.76 81'
Jun 22 10:49:24.608: INFO: stderr: "+ nc -v -z -w 2 100.69.224.76 81\nConnection to 100.69.224.76 81 port [tcp/*] succeeded!\n"
Jun 22 10:49:24.608: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-3061 06/22/23 10:49:24.609
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3061 to expose endpoints map[pod2:[101]] 06/22/23 10:49:24.621
Jun 22 10:49:24.636: INFO: successfully validated that service multi-endpoint-test in namespace services-3061 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3061 06/22/23 10:49:24.636
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3061 to expose endpoints map[] 06/22/23 10:49:24.654
Jun 22 10:49:24.668: INFO: successfully validated that service multi-endpoint-test in namespace services-3061 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun 22 10:49:24.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3061" for this suite. 06/22/23 10:49:24.7
------------------------------
• [SLOW TEST] [7.957 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:49:16.75
    Jun 22 10:49:16.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename services 06/22/23 10:49:16.751
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:49:16.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:49:16.774
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-3061 06/22/23 10:49:16.777
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3061 to expose endpoints map[] 06/22/23 10:49:16.796
    Jun 22 10:49:16.809: INFO: successfully validated that service multi-endpoint-test in namespace services-3061 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3061 06/22/23 10:49:16.809
    W0622 10:49:16.824780      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:49:16.824: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3061" to be "running and ready"
    Jun 22 10:49:16.828: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.907555ms
    Jun 22 10:49:16.828: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:49:18.833: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00870272s
    Jun 22 10:49:18.833: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jun 22 10:49:18.833: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3061 to expose endpoints map[pod1:[100]] 06/22/23 10:49:18.837
    Jun 22 10:49:18.852: INFO: successfully validated that service multi-endpoint-test in namespace services-3061 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-3061 06/22/23 10:49:18.852
    W0622 10:49:18.858171      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:49:18.858: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3061" to be "running and ready"
    Jun 22 10:49:18.861: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.381281ms
    Jun 22 10:49:18.861: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:49:20.866: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008699861s
    Jun 22 10:49:20.867: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jun 22 10:49:20.867: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3061 to expose endpoints map[pod1:[100] pod2:[101]] 06/22/23 10:49:20.871
    Jun 22 10:49:20.891: INFO: successfully validated that service multi-endpoint-test in namespace services-3061 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 06/22/23 10:49:20.891
    Jun 22 10:49:20.892: INFO: Creating new exec pod
    W0622 10:49:20.899112      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:49:20.899: INFO: Waiting up to 5m0s for pod "execpodhj6g4" in namespace "services-3061" to be "running"
    Jun 22 10:49:20.902: INFO: Pod "execpodhj6g4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.619174ms
    Jun 22 10:49:22.908: INFO: Pod "execpodhj6g4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008956645s
    Jun 22 10:49:22.908: INFO: Pod "execpodhj6g4" satisfied condition "running"
    Jun 22 10:49:23.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-3061 exec execpodhj6g4 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Jun 22 10:49:24.098: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jun 22 10:49:24.098: INFO: stdout: ""
    Jun 22 10:49:24.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-3061 exec execpodhj6g4 -- /bin/sh -x -c nc -v -z -w 2 100.69.224.76 80'
    Jun 22 10:49:24.265: INFO: stderr: "+ nc -v -z -w 2 100.69.224.76 80\nConnection to 100.69.224.76 80 port [tcp/http] succeeded!\n"
    Jun 22 10:49:24.265: INFO: stdout: ""
    Jun 22 10:49:24.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-3061 exec execpodhj6g4 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Jun 22 10:49:24.436: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jun 22 10:49:24.436: INFO: stdout: ""
    Jun 22 10:49:24.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-3061 exec execpodhj6g4 -- /bin/sh -x -c nc -v -z -w 2 100.69.224.76 81'
    Jun 22 10:49:24.608: INFO: stderr: "+ nc -v -z -w 2 100.69.224.76 81\nConnection to 100.69.224.76 81 port [tcp/*] succeeded!\n"
    Jun 22 10:49:24.608: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-3061 06/22/23 10:49:24.609
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3061 to expose endpoints map[pod2:[101]] 06/22/23 10:49:24.621
    Jun 22 10:49:24.636: INFO: successfully validated that service multi-endpoint-test in namespace services-3061 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-3061 06/22/23 10:49:24.636
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3061 to expose endpoints map[] 06/22/23 10:49:24.654
    Jun 22 10:49:24.668: INFO: successfully validated that service multi-endpoint-test in namespace services-3061 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:49:24.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3061" for this suite. 06/22/23 10:49:24.7
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:49:24.708
Jun 22 10:49:24.708: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename resourcequota 06/22/23 10:49:24.709
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:49:24.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:49:24.733
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 06/22/23 10:49:24.737
STEP: Creating a ResourceQuota 06/22/23 10:49:29.742
STEP: Ensuring resource quota status is calculated 06/22/23 10:49:29.749
STEP: Creating a ReplicationController 06/22/23 10:49:31.757
W0622 10:49:31.775558      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring resource quota status captures replication controller creation 06/22/23 10:49:31.775
STEP: Deleting a ReplicationController 06/22/23 10:49:33.781
STEP: Ensuring resource quota status released usage 06/22/23 10:49:33.788
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun 22 10:49:35.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9479" for this suite. 06/22/23 10:49:35.8
------------------------------
• [SLOW TEST] [11.102 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:49:24.708
    Jun 22 10:49:24.708: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename resourcequota 06/22/23 10:49:24.709
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:49:24.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:49:24.733
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 06/22/23 10:49:24.737
    STEP: Creating a ResourceQuota 06/22/23 10:49:29.742
    STEP: Ensuring resource quota status is calculated 06/22/23 10:49:29.749
    STEP: Creating a ReplicationController 06/22/23 10:49:31.757
    W0622 10:49:31.775558      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring resource quota status captures replication controller creation 06/22/23 10:49:31.775
    STEP: Deleting a ReplicationController 06/22/23 10:49:33.781
    STEP: Ensuring resource quota status released usage 06/22/23 10:49:33.788
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:49:35.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9479" for this suite. 06/22/23 10:49:35.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:49:35.811
Jun 22 10:49:35.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename replicaset 06/22/23 10:49:35.812
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:49:35.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:49:35.835
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
W0622 10:49:35.851749      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:49:35.855: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 22 10:49:40.874: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/22/23 10:49:40.874
STEP: Scaling up "test-rs" replicaset  06/22/23 10:49:40.877
W0622 10:49:40.891253      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:49:40.891: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 06/22/23 10:49:40.891
W0622 10:49:40.906064      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
W0622 10:49:40.906135      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "test-rs", "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "test-rs", "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "test-rs", "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "test-rs", "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:49:40.911: INFO: observed ReplicaSet test-rs in namespace replicaset-9035 with ReadyReplicas 1, AvailableReplicas 1
Jun 22 10:49:40.956: INFO: observed ReplicaSet test-rs in namespace replicaset-9035 with ReadyReplicas 1, AvailableReplicas 1
Jun 22 10:49:40.979: INFO: observed ReplicaSet test-rs in namespace replicaset-9035 with ReadyReplicas 1, AvailableReplicas 1
Jun 22 10:49:41.007: INFO: observed ReplicaSet test-rs in namespace replicaset-9035 with ReadyReplicas 1, AvailableReplicas 1
Jun 22 10:49:42.458: INFO: observed ReplicaSet test-rs in namespace replicaset-9035 with ReadyReplicas 2, AvailableReplicas 2
Jun 22 10:49:42.472: INFO: observed Replicaset test-rs in namespace replicaset-9035 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jun 22 10:49:42.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9035" for this suite. 06/22/23 10:49:42.486
------------------------------
• [SLOW TEST] [6.686 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:49:35.811
    Jun 22 10:49:35.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename replicaset 06/22/23 10:49:35.812
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:49:35.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:49:35.835
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    W0622 10:49:35.851749      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:49:35.855: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun 22 10:49:40.874: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/22/23 10:49:40.874
    STEP: Scaling up "test-rs" replicaset  06/22/23 10:49:40.877
    W0622 10:49:40.891253      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:49:40.891: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 06/22/23 10:49:40.891
    W0622 10:49:40.906064      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    W0622 10:49:40.906135      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "test-rs", "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "test-rs", "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "test-rs", "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "test-rs", "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:49:40.911: INFO: observed ReplicaSet test-rs in namespace replicaset-9035 with ReadyReplicas 1, AvailableReplicas 1
    Jun 22 10:49:40.956: INFO: observed ReplicaSet test-rs in namespace replicaset-9035 with ReadyReplicas 1, AvailableReplicas 1
    Jun 22 10:49:40.979: INFO: observed ReplicaSet test-rs in namespace replicaset-9035 with ReadyReplicas 1, AvailableReplicas 1
    Jun 22 10:49:41.007: INFO: observed ReplicaSet test-rs in namespace replicaset-9035 with ReadyReplicas 1, AvailableReplicas 1
    Jun 22 10:49:42.458: INFO: observed ReplicaSet test-rs in namespace replicaset-9035 with ReadyReplicas 2, AvailableReplicas 2
    Jun 22 10:49:42.472: INFO: observed Replicaset test-rs in namespace replicaset-9035 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:49:42.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9035" for this suite. 06/22/23 10:49:42.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:49:42.504
Jun 22 10:49:42.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename statefulset 06/22/23 10:49:42.515
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:49:42.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:49:42.542
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2248 06/22/23 10:49:42.547
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 06/22/23 10:49:42.556
W0622 10:49:42.564428      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:49:42.569: INFO: Found 0 stateful pods, waiting for 3
Jun 22 10:49:52.577: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 10:49:52.577: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 10:49:52.577: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 06/22/23 10:49:52.592
W0622 10:49:52.618039      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:49:52.618: INFO: Updating stateful set ss2
STEP: Creating a new revision 06/22/23 10:49:52.618
STEP: Not applying an update when the partition is greater than the number of replicas 06/22/23 10:50:02.642
STEP: Performing a canary update 06/22/23 10:50:02.642
W0622 10:50:02.668882      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:50:02.669: INFO: Updating stateful set ss2
Jun 22 10:50:02.683: INFO: Waiting for Pod statefulset-2248/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 06/22/23 10:50:12.699
Jun 22 10:50:12.728: INFO: Found 1 stateful pods, waiting for 3
Jun 22 10:50:22.735: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 10:50:22.735: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 10:50:22.735: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 06/22/23 10:50:22.744
W0622 10:50:22.765565      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:50:22.765: INFO: Updating stateful set ss2
Jun 22 10:50:22.775: INFO: Waiting for Pod statefulset-2248/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
W0622 10:50:32.815138      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:50:32.815: INFO: Updating stateful set ss2
Jun 22 10:50:32.824: INFO: Waiting for StatefulSet statefulset-2248/ss2 to complete update
Jun 22 10:50:32.824: INFO: Waiting for Pod statefulset-2248/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun 22 10:50:42.840: INFO: Deleting all statefulset in ns statefulset-2248
Jun 22 10:50:42.846: INFO: Scaling statefulset ss2 to 0
W0622 10:50:42.862344      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:50:52.876: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 10:50:52.880: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun 22 10:50:52.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2248" for this suite. 06/22/23 10:50:52.905
------------------------------
• [SLOW TEST] [70.411 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:49:42.504
    Jun 22 10:49:42.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename statefulset 06/22/23 10:49:42.515
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:49:42.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:49:42.542
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2248 06/22/23 10:49:42.547
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 06/22/23 10:49:42.556
    W0622 10:49:42.564428      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:49:42.569: INFO: Found 0 stateful pods, waiting for 3
    Jun 22 10:49:52.577: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 22 10:49:52.577: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 22 10:49:52.577: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 06/22/23 10:49:52.592
    W0622 10:49:52.618039      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:49:52.618: INFO: Updating stateful set ss2
    STEP: Creating a new revision 06/22/23 10:49:52.618
    STEP: Not applying an update when the partition is greater than the number of replicas 06/22/23 10:50:02.642
    STEP: Performing a canary update 06/22/23 10:50:02.642
    W0622 10:50:02.668882      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:50:02.669: INFO: Updating stateful set ss2
    Jun 22 10:50:02.683: INFO: Waiting for Pod statefulset-2248/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 06/22/23 10:50:12.699
    Jun 22 10:50:12.728: INFO: Found 1 stateful pods, waiting for 3
    Jun 22 10:50:22.735: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 22 10:50:22.735: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 22 10:50:22.735: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 06/22/23 10:50:22.744
    W0622 10:50:22.765565      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:50:22.765: INFO: Updating stateful set ss2
    Jun 22 10:50:22.775: INFO: Waiting for Pod statefulset-2248/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    W0622 10:50:32.815138      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:50:32.815: INFO: Updating stateful set ss2
    Jun 22 10:50:32.824: INFO: Waiting for StatefulSet statefulset-2248/ss2 to complete update
    Jun 22 10:50:32.824: INFO: Waiting for Pod statefulset-2248/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun 22 10:50:42.840: INFO: Deleting all statefulset in ns statefulset-2248
    Jun 22 10:50:42.846: INFO: Scaling statefulset ss2 to 0
    W0622 10:50:42.862344      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:50:52.876: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 22 10:50:52.880: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:50:52.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2248" for this suite. 06/22/23 10:50:52.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:50:52.916
Jun 22 10:50:52.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 10:50:52.917
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:50:52.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:50:52.947
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-fa149d6d-0a1d-4ad9-b85b-53e18e9daa08 06/22/23 10:50:52.951
STEP: Creating a pod to test consume configMaps 06/22/23 10:50:52.959
W0622 10:50:52.970660      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:50:52.970: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2028193f-5c38-487f-a774-d73118d46783" in namespace "projected-5789" to be "Succeeded or Failed"
Jun 22 10:50:52.974: INFO: Pod "pod-projected-configmaps-2028193f-5c38-487f-a774-d73118d46783": Phase="Pending", Reason="", readiness=false. Elapsed: 3.774684ms
Jun 22 10:50:54.981: INFO: Pod "pod-projected-configmaps-2028193f-5c38-487f-a774-d73118d46783": Phase="Running", Reason="", readiness=false. Elapsed: 2.01019486s
Jun 22 10:50:56.985: INFO: Pod "pod-projected-configmaps-2028193f-5c38-487f-a774-d73118d46783": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015014459s
STEP: Saw pod success 06/22/23 10:50:56.985
Jun 22 10:50:56.986: INFO: Pod "pod-projected-configmaps-2028193f-5c38-487f-a774-d73118d46783" satisfied condition "Succeeded or Failed"
Jun 22 10:50:56.991: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-configmaps-2028193f-5c38-487f-a774-d73118d46783 container agnhost-container: <nil>
STEP: delete the pod 06/22/23 10:50:57.008
Jun 22 10:50:57.023: INFO: Waiting for pod pod-projected-configmaps-2028193f-5c38-487f-a774-d73118d46783 to disappear
Jun 22 10:50:57.029: INFO: Pod pod-projected-configmaps-2028193f-5c38-487f-a774-d73118d46783 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun 22 10:50:57.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5789" for this suite. 06/22/23 10:50:57.037
------------------------------
• [4.127 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:50:52.916
    Jun 22 10:50:52.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 10:50:52.917
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:50:52.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:50:52.947
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-fa149d6d-0a1d-4ad9-b85b-53e18e9daa08 06/22/23 10:50:52.951
    STEP: Creating a pod to test consume configMaps 06/22/23 10:50:52.959
    W0622 10:50:52.970660      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:50:52.970: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2028193f-5c38-487f-a774-d73118d46783" in namespace "projected-5789" to be "Succeeded or Failed"
    Jun 22 10:50:52.974: INFO: Pod "pod-projected-configmaps-2028193f-5c38-487f-a774-d73118d46783": Phase="Pending", Reason="", readiness=false. Elapsed: 3.774684ms
    Jun 22 10:50:54.981: INFO: Pod "pod-projected-configmaps-2028193f-5c38-487f-a774-d73118d46783": Phase="Running", Reason="", readiness=false. Elapsed: 2.01019486s
    Jun 22 10:50:56.985: INFO: Pod "pod-projected-configmaps-2028193f-5c38-487f-a774-d73118d46783": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015014459s
    STEP: Saw pod success 06/22/23 10:50:56.985
    Jun 22 10:50:56.986: INFO: Pod "pod-projected-configmaps-2028193f-5c38-487f-a774-d73118d46783" satisfied condition "Succeeded or Failed"
    Jun 22 10:50:56.991: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-configmaps-2028193f-5c38-487f-a774-d73118d46783 container agnhost-container: <nil>
    STEP: delete the pod 06/22/23 10:50:57.008
    Jun 22 10:50:57.023: INFO: Waiting for pod pod-projected-configmaps-2028193f-5c38-487f-a774-d73118d46783 to disappear
    Jun 22 10:50:57.029: INFO: Pod pod-projected-configmaps-2028193f-5c38-487f-a774-d73118d46783 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:50:57.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5789" for this suite. 06/22/23 10:50:57.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:50:57.06
Jun 22 10:50:57.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename resourcequota 06/22/23 10:50:57.061
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:50:57.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:50:57.086
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 06/22/23 10:50:57.09
STEP: Creating a ResourceQuota 06/22/23 10:51:02.096
STEP: Ensuring resource quota status is calculated 06/22/23 10:51:02.107
STEP: Creating a Service 06/22/23 10:51:04.114
STEP: Creating a NodePort Service 06/22/23 10:51:04.139
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 06/22/23 10:51:04.166
STEP: Ensuring resource quota status captures service creation 06/22/23 10:51:04.196
STEP: Deleting Services 06/22/23 10:51:06.202
STEP: Ensuring resource quota status released usage 06/22/23 10:51:06.251
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun 22 10:51:08.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7335" for this suite. 06/22/23 10:51:08.265
------------------------------
• [SLOW TEST] [11.213 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:50:57.06
    Jun 22 10:50:57.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename resourcequota 06/22/23 10:50:57.061
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:50:57.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:50:57.086
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 06/22/23 10:50:57.09
    STEP: Creating a ResourceQuota 06/22/23 10:51:02.096
    STEP: Ensuring resource quota status is calculated 06/22/23 10:51:02.107
    STEP: Creating a Service 06/22/23 10:51:04.114
    STEP: Creating a NodePort Service 06/22/23 10:51:04.139
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 06/22/23 10:51:04.166
    STEP: Ensuring resource quota status captures service creation 06/22/23 10:51:04.196
    STEP: Deleting Services 06/22/23 10:51:06.202
    STEP: Ensuring resource quota status released usage 06/22/23 10:51:06.251
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:51:08.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7335" for this suite. 06/22/23 10:51:08.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:51:08.277
Jun 22 10:51:08.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename pods 06/22/23 10:51:08.278
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:51:08.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:51:08.302
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 06/22/23 10:51:08.307
STEP: submitting the pod to kubernetes 06/22/23 10:51:08.308
Jun 22 10:51:08.325: INFO: Waiting up to 5m0s for pod "pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff" in namespace "pods-9456" to be "running and ready"
Jun 22 10:51:08.330: INFO: Pod "pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079371ms
Jun 22 10:51:08.330: INFO: The phase of Pod pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:51:10.335: INFO: Pod "pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff": Phase="Running", Reason="", readiness=true. Elapsed: 2.009920892s
Jun 22 10:51:10.335: INFO: The phase of Pod pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff is Running (Ready = true)
Jun 22 10:51:10.336: INFO: Pod "pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 06/22/23 10:51:10.34
STEP: updating the pod 06/22/23 10:51:10.345
Jun 22 10:51:10.865: INFO: Successfully updated pod "pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff"
Jun 22 10:51:10.865: INFO: Waiting up to 5m0s for pod "pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff" in namespace "pods-9456" to be "running"
Jun 22 10:51:10.869: INFO: Pod "pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff": Phase="Running", Reason="", readiness=true. Elapsed: 4.086853ms
Jun 22 10:51:10.869: INFO: Pod "pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 06/22/23 10:51:10.869
Jun 22 10:51:10.875: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun 22 10:51:10.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9456" for this suite. 06/22/23 10:51:10.882
------------------------------
• [2.612 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:51:08.277
    Jun 22 10:51:08.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename pods 06/22/23 10:51:08.278
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:51:08.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:51:08.302
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 06/22/23 10:51:08.307
    STEP: submitting the pod to kubernetes 06/22/23 10:51:08.308
    Jun 22 10:51:08.325: INFO: Waiting up to 5m0s for pod "pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff" in namespace "pods-9456" to be "running and ready"
    Jun 22 10:51:08.330: INFO: Pod "pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079371ms
    Jun 22 10:51:08.330: INFO: The phase of Pod pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:51:10.335: INFO: Pod "pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff": Phase="Running", Reason="", readiness=true. Elapsed: 2.009920892s
    Jun 22 10:51:10.335: INFO: The phase of Pod pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff is Running (Ready = true)
    Jun 22 10:51:10.336: INFO: Pod "pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 06/22/23 10:51:10.34
    STEP: updating the pod 06/22/23 10:51:10.345
    Jun 22 10:51:10.865: INFO: Successfully updated pod "pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff"
    Jun 22 10:51:10.865: INFO: Waiting up to 5m0s for pod "pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff" in namespace "pods-9456" to be "running"
    Jun 22 10:51:10.869: INFO: Pod "pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff": Phase="Running", Reason="", readiness=true. Elapsed: 4.086853ms
    Jun 22 10:51:10.869: INFO: Pod "pod-update-435b0c55-dfa3-438f-a20f-401b7ee8beff" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 06/22/23 10:51:10.869
    Jun 22 10:51:10.875: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:51:10.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9456" for this suite. 06/22/23 10:51:10.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:51:10.892
Jun 22 10:51:10.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename custom-resource-definition 06/22/23 10:51:10.894
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:51:10.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:51:10.922
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jun 22 10:51:10.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:51:11.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9246" for this suite. 06/22/23 10:51:11.965
------------------------------
• [1.080 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:51:10.892
    Jun 22 10:51:10.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename custom-resource-definition 06/22/23 10:51:10.894
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:51:10.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:51:10.922
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jun 22 10:51:10.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:51:11.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9246" for this suite. 06/22/23 10:51:11.965
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:51:11.974
Jun 22 10:51:11.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename var-expansion 06/22/23 10:51:11.975
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:51:11.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:51:11.998
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 06/22/23 10:51:12.002
W0622 10:51:12.022949      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: waiting for pod running 06/22/23 10:51:12.023
Jun 22 10:51:12.023: INFO: Waiting up to 2m0s for pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18" in namespace "var-expansion-3418" to be "running"
Jun 22 10:51:12.027: INFO: Pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18": Phase="Pending", Reason="", readiness=false. Elapsed: 4.33114ms
Jun 22 10:51:14.033: INFO: Pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18": Phase="Running", Reason="", readiness=true. Elapsed: 2.010150895s
Jun 22 10:51:14.033: INFO: Pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18" satisfied condition "running"
STEP: creating a file in subpath 06/22/23 10:51:14.033
Jun 22 10:51:14.039: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3418 PodName:var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:51:14.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:51:14.039: INFO: ExecWithOptions: Clientset creation
Jun 22 10:51:14.040: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/var-expansion-3418/pods/var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 06/22/23 10:51:14.145
Jun 22 10:51:14.151: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3418 PodName:var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:51:14.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:51:14.151: INFO: ExecWithOptions: Clientset creation
Jun 22 10:51:14.152: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/var-expansion-3418/pods/var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 06/22/23 10:51:14.249
Jun 22 10:51:14.768: INFO: Successfully updated pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18"
STEP: waiting for annotated pod running 06/22/23 10:51:14.768
Jun 22 10:51:14.769: INFO: Waiting up to 2m0s for pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18" in namespace "var-expansion-3418" to be "running"
Jun 22 10:51:14.773: INFO: Pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18": Phase="Running", Reason="", readiness=true. Elapsed: 4.353041ms
Jun 22 10:51:14.773: INFO: Pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18" satisfied condition "running"
STEP: deleting the pod gracefully 06/22/23 10:51:14.773
Jun 22 10:51:14.773: INFO: Deleting pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18" in namespace "var-expansion-3418"
Jun 22 10:51:14.783: INFO: Wait up to 5m0s for pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun 22 10:51:48.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3418" for this suite. 06/22/23 10:51:48.804
------------------------------
• [SLOW TEST] [36.841 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:51:11.974
    Jun 22 10:51:11.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename var-expansion 06/22/23 10:51:11.975
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:51:11.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:51:11.998
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 06/22/23 10:51:12.002
    W0622 10:51:12.022949      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: waiting for pod running 06/22/23 10:51:12.023
    Jun 22 10:51:12.023: INFO: Waiting up to 2m0s for pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18" in namespace "var-expansion-3418" to be "running"
    Jun 22 10:51:12.027: INFO: Pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18": Phase="Pending", Reason="", readiness=false. Elapsed: 4.33114ms
    Jun 22 10:51:14.033: INFO: Pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18": Phase="Running", Reason="", readiness=true. Elapsed: 2.010150895s
    Jun 22 10:51:14.033: INFO: Pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18" satisfied condition "running"
    STEP: creating a file in subpath 06/22/23 10:51:14.033
    Jun 22 10:51:14.039: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3418 PodName:var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:51:14.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:51:14.039: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:51:14.040: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/var-expansion-3418/pods/var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 06/22/23 10:51:14.145
    Jun 22 10:51:14.151: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3418 PodName:var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:51:14.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:51:14.151: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:51:14.152: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/var-expansion-3418/pods/var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 06/22/23 10:51:14.249
    Jun 22 10:51:14.768: INFO: Successfully updated pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18"
    STEP: waiting for annotated pod running 06/22/23 10:51:14.768
    Jun 22 10:51:14.769: INFO: Waiting up to 2m0s for pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18" in namespace "var-expansion-3418" to be "running"
    Jun 22 10:51:14.773: INFO: Pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18": Phase="Running", Reason="", readiness=true. Elapsed: 4.353041ms
    Jun 22 10:51:14.773: INFO: Pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18" satisfied condition "running"
    STEP: deleting the pod gracefully 06/22/23 10:51:14.773
    Jun 22 10:51:14.773: INFO: Deleting pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18" in namespace "var-expansion-3418"
    Jun 22 10:51:14.783: INFO: Wait up to 5m0s for pod "var-expansion-4ee52006-2e0b-4845-a5f3-5b6170aa4f18" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:51:48.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3418" for this suite. 06/22/23 10:51:48.804
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:51:48.816
Jun 22 10:51:48.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename resourcequota 06/22/23 10:51:48.817
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:51:48.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:51:48.85
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 06/22/23 10:51:48.856
STEP: Creating a ResourceQuota 06/22/23 10:51:53.862
STEP: Ensuring resource quota status is calculated 06/22/23 10:51:53.883
STEP: Creating a ReplicaSet 06/22/23 10:51:55.891
W0622 10:51:55.926324      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-rs" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-rs" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-rs" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-rs" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring resource quota status captures replicaset creation 06/22/23 10:51:55.926
STEP: Deleting a ReplicaSet 06/22/23 10:51:57.935
STEP: Ensuring resource quota status released usage 06/22/23 10:51:57.948
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun 22 10:51:59.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8424" for this suite. 06/22/23 10:51:59.969
------------------------------
• [SLOW TEST] [11.166 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:51:48.816
    Jun 22 10:51:48.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename resourcequota 06/22/23 10:51:48.817
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:51:48.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:51:48.85
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 06/22/23 10:51:48.856
    STEP: Creating a ResourceQuota 06/22/23 10:51:53.862
    STEP: Ensuring resource quota status is calculated 06/22/23 10:51:53.883
    STEP: Creating a ReplicaSet 06/22/23 10:51:55.891
    W0622 10:51:55.926324      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-rs" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-rs" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-rs" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-rs" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring resource quota status captures replicaset creation 06/22/23 10:51:55.926
    STEP: Deleting a ReplicaSet 06/22/23 10:51:57.935
    STEP: Ensuring resource quota status released usage 06/22/23 10:51:57.948
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:51:59.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8424" for this suite. 06/22/23 10:51:59.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:51:59.984
Jun 22 10:51:59.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename subpath 06/22/23 10:51:59.986
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:52:00.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:52:00.019
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/22/23 10:52:00.026
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-2wzd 06/22/23 10:52:00.043
STEP: Creating a pod to test atomic-volume-subpath 06/22/23 10:52:00.043
W0622 10:52:00.076450      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container-subpath-projected-2wzd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-subpath-projected-2wzd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-subpath-projected-2wzd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-subpath-projected-2wzd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:52:00.076: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-2wzd" in namespace "subpath-6889" to be "Succeeded or Failed"
Jun 22 10:52:00.088: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.242947ms
Jun 22 10:52:02.095: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 2.019089025s
Jun 22 10:52:04.095: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 4.018444677s
Jun 22 10:52:06.096: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 6.019630017s
Jun 22 10:52:08.096: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 8.019752357s
Jun 22 10:52:10.095: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 10.018633607s
Jun 22 10:52:12.094: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 12.017869182s
Jun 22 10:52:14.094: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 14.01786266s
Jun 22 10:52:16.095: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 16.018769195s
Jun 22 10:52:18.095: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 18.019102688s
Jun 22 10:52:20.094: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 20.017680378s
Jun 22 10:52:22.096: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=false. Elapsed: 22.020021716s
Jun 22 10:52:24.093: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.017163698s
STEP: Saw pod success 06/22/23 10:52:24.094
Jun 22 10:52:24.094: INFO: Pod "pod-subpath-test-projected-2wzd" satisfied condition "Succeeded or Failed"
Jun 22 10:52:24.099: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-subpath-test-projected-2wzd container test-container-subpath-projected-2wzd: <nil>
STEP: delete the pod 06/22/23 10:52:24.109
Jun 22 10:52:24.128: INFO: Waiting for pod pod-subpath-test-projected-2wzd to disappear
Jun 22 10:52:24.133: INFO: Pod pod-subpath-test-projected-2wzd no longer exists
STEP: Deleting pod pod-subpath-test-projected-2wzd 06/22/23 10:52:24.133
Jun 22 10:52:24.133: INFO: Deleting pod "pod-subpath-test-projected-2wzd" in namespace "subpath-6889"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jun 22 10:52:24.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6889" for this suite. 06/22/23 10:52:24.148
------------------------------
• [SLOW TEST] [24.172 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:51:59.984
    Jun 22 10:51:59.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename subpath 06/22/23 10:51:59.986
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:52:00.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:52:00.019
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/22/23 10:52:00.026
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-2wzd 06/22/23 10:52:00.043
    STEP: Creating a pod to test atomic-volume-subpath 06/22/23 10:52:00.043
    W0622 10:52:00.076450      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container-subpath-projected-2wzd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-subpath-projected-2wzd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-subpath-projected-2wzd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-subpath-projected-2wzd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:52:00.076: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-2wzd" in namespace "subpath-6889" to be "Succeeded or Failed"
    Jun 22 10:52:00.088: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.242947ms
    Jun 22 10:52:02.095: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 2.019089025s
    Jun 22 10:52:04.095: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 4.018444677s
    Jun 22 10:52:06.096: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 6.019630017s
    Jun 22 10:52:08.096: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 8.019752357s
    Jun 22 10:52:10.095: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 10.018633607s
    Jun 22 10:52:12.094: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 12.017869182s
    Jun 22 10:52:14.094: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 14.01786266s
    Jun 22 10:52:16.095: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 16.018769195s
    Jun 22 10:52:18.095: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 18.019102688s
    Jun 22 10:52:20.094: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=true. Elapsed: 20.017680378s
    Jun 22 10:52:22.096: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Running", Reason="", readiness=false. Elapsed: 22.020021716s
    Jun 22 10:52:24.093: INFO: Pod "pod-subpath-test-projected-2wzd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.017163698s
    STEP: Saw pod success 06/22/23 10:52:24.094
    Jun 22 10:52:24.094: INFO: Pod "pod-subpath-test-projected-2wzd" satisfied condition "Succeeded or Failed"
    Jun 22 10:52:24.099: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-subpath-test-projected-2wzd container test-container-subpath-projected-2wzd: <nil>
    STEP: delete the pod 06/22/23 10:52:24.109
    Jun 22 10:52:24.128: INFO: Waiting for pod pod-subpath-test-projected-2wzd to disappear
    Jun 22 10:52:24.133: INFO: Pod pod-subpath-test-projected-2wzd no longer exists
    STEP: Deleting pod pod-subpath-test-projected-2wzd 06/22/23 10:52:24.133
    Jun 22 10:52:24.133: INFO: Deleting pod "pod-subpath-test-projected-2wzd" in namespace "subpath-6889"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:52:24.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6889" for this suite. 06/22/23 10:52:24.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:52:24.158
Jun 22 10:52:24.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename crd-publish-openapi 06/22/23 10:52:24.16
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:52:24.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:52:24.187
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 06/22/23 10:52:24.192
Jun 22 10:52:24.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 06/22/23 10:52:33.653
Jun 22 10:52:33.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:52:36.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:52:46.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6839" for this suite. 06/22/23 10:52:46.368
------------------------------
• [SLOW TEST] [22.226 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:52:24.158
    Jun 22 10:52:24.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename crd-publish-openapi 06/22/23 10:52:24.16
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:52:24.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:52:24.187
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 06/22/23 10:52:24.192
    Jun 22 10:52:24.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 06/22/23 10:52:33.653
    Jun 22 10:52:33.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:52:36.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:52:46.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6839" for this suite. 06/22/23 10:52:46.368
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:52:46.384
Jun 22 10:52:46.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename watch 06/22/23 10:52:46.385
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:52:46.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:52:46.412
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 06/22/23 10:52:46.416
STEP: creating a new configmap 06/22/23 10:52:46.418
STEP: modifying the configmap once 06/22/23 10:52:46.425
STEP: changing the label value of the configmap 06/22/23 10:52:46.437
STEP: Expecting to observe a delete notification for the watched object 06/22/23 10:52:46.45
Jun 22 10:52:46.450: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2241  f9f3f7e8-91b0-4299-b8dd-09d341db29d3 140568 0 2023-06-22 10:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-22 10:52:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 10:52:46.451: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2241  f9f3f7e8-91b0-4299-b8dd-09d341db29d3 140570 0 2023-06-22 10:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-22 10:52:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 10:52:46.451: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2241  f9f3f7e8-91b0-4299-b8dd-09d341db29d3 140571 0 2023-06-22 10:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-22 10:52:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 06/22/23 10:52:46.451
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 06/22/23 10:52:46.464
STEP: changing the label value of the configmap back 06/22/23 10:52:56.464
STEP: modifying the configmap a third time 06/22/23 10:52:56.478
STEP: deleting the configmap 06/22/23 10:52:56.49
STEP: Expecting to observe an add notification for the watched object when the label value was restored 06/22/23 10:52:56.499
Jun 22 10:52:56.500: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2241  f9f3f7e8-91b0-4299-b8dd-09d341db29d3 140612 0 2023-06-22 10:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-22 10:52:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 10:52:56.500: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2241  f9f3f7e8-91b0-4299-b8dd-09d341db29d3 140613 0 2023-06-22 10:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-22 10:52:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 10:52:56.500: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2241  f9f3f7e8-91b0-4299-b8dd-09d341db29d3 140614 0 2023-06-22 10:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-22 10:52:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jun 22 10:52:56.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2241" for this suite. 06/22/23 10:52:56.507
------------------------------
• [SLOW TEST] [10.134 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:52:46.384
    Jun 22 10:52:46.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename watch 06/22/23 10:52:46.385
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:52:46.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:52:46.412
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 06/22/23 10:52:46.416
    STEP: creating a new configmap 06/22/23 10:52:46.418
    STEP: modifying the configmap once 06/22/23 10:52:46.425
    STEP: changing the label value of the configmap 06/22/23 10:52:46.437
    STEP: Expecting to observe a delete notification for the watched object 06/22/23 10:52:46.45
    Jun 22 10:52:46.450: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2241  f9f3f7e8-91b0-4299-b8dd-09d341db29d3 140568 0 2023-06-22 10:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-22 10:52:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 22 10:52:46.451: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2241  f9f3f7e8-91b0-4299-b8dd-09d341db29d3 140570 0 2023-06-22 10:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-22 10:52:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 22 10:52:46.451: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2241  f9f3f7e8-91b0-4299-b8dd-09d341db29d3 140571 0 2023-06-22 10:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-22 10:52:46 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 06/22/23 10:52:46.451
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 06/22/23 10:52:46.464
    STEP: changing the label value of the configmap back 06/22/23 10:52:56.464
    STEP: modifying the configmap a third time 06/22/23 10:52:56.478
    STEP: deleting the configmap 06/22/23 10:52:56.49
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 06/22/23 10:52:56.499
    Jun 22 10:52:56.500: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2241  f9f3f7e8-91b0-4299-b8dd-09d341db29d3 140612 0 2023-06-22 10:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-22 10:52:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 22 10:52:56.500: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2241  f9f3f7e8-91b0-4299-b8dd-09d341db29d3 140613 0 2023-06-22 10:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-22 10:52:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 22 10:52:56.500: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2241  f9f3f7e8-91b0-4299-b8dd-09d341db29d3 140614 0 2023-06-22 10:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-22 10:52:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:52:56.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2241" for this suite. 06/22/23 10:52:56.507
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:52:56.519
Jun 22 10:52:56.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 10:52:56.519
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:52:56.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:52:56.548
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-00a7b4c7-805f-46db-ae32-a05fb5175152 06/22/23 10:52:56.553
STEP: Creating a pod to test consume secrets 06/22/23 10:52:56.563
W0622 10:52:56.581303      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:52:56.581: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d0ac325a-2a17-4a77-a962-31eadf1acf09" in namespace "projected-5512" to be "Succeeded or Failed"
Jun 22 10:52:56.588: INFO: Pod "pod-projected-secrets-d0ac325a-2a17-4a77-a962-31eadf1acf09": Phase="Pending", Reason="", readiness=false. Elapsed: 7.011309ms
Jun 22 10:52:58.596: INFO: Pod "pod-projected-secrets-d0ac325a-2a17-4a77-a962-31eadf1acf09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014702936s
Jun 22 10:53:00.596: INFO: Pod "pod-projected-secrets-d0ac325a-2a17-4a77-a962-31eadf1acf09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015241016s
STEP: Saw pod success 06/22/23 10:53:00.596
Jun 22 10:53:00.597: INFO: Pod "pod-projected-secrets-d0ac325a-2a17-4a77-a962-31eadf1acf09" satisfied condition "Succeeded or Failed"
Jun 22 10:53:00.602: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-secrets-d0ac325a-2a17-4a77-a962-31eadf1acf09 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/22/23 10:53:00.623
Jun 22 10:53:00.644: INFO: Waiting for pod pod-projected-secrets-d0ac325a-2a17-4a77-a962-31eadf1acf09 to disappear
Jun 22 10:53:00.649: INFO: Pod pod-projected-secrets-d0ac325a-2a17-4a77-a962-31eadf1acf09 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun 22 10:53:00.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5512" for this suite. 06/22/23 10:53:00.658
------------------------------
• [4.152 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:52:56.519
    Jun 22 10:52:56.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 10:52:56.519
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:52:56.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:52:56.548
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-00a7b4c7-805f-46db-ae32-a05fb5175152 06/22/23 10:52:56.553
    STEP: Creating a pod to test consume secrets 06/22/23 10:52:56.563
    W0622 10:52:56.581303      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:52:56.581: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d0ac325a-2a17-4a77-a962-31eadf1acf09" in namespace "projected-5512" to be "Succeeded or Failed"
    Jun 22 10:52:56.588: INFO: Pod "pod-projected-secrets-d0ac325a-2a17-4a77-a962-31eadf1acf09": Phase="Pending", Reason="", readiness=false. Elapsed: 7.011309ms
    Jun 22 10:52:58.596: INFO: Pod "pod-projected-secrets-d0ac325a-2a17-4a77-a962-31eadf1acf09": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014702936s
    Jun 22 10:53:00.596: INFO: Pod "pod-projected-secrets-d0ac325a-2a17-4a77-a962-31eadf1acf09": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015241016s
    STEP: Saw pod success 06/22/23 10:53:00.596
    Jun 22 10:53:00.597: INFO: Pod "pod-projected-secrets-d0ac325a-2a17-4a77-a962-31eadf1acf09" satisfied condition "Succeeded or Failed"
    Jun 22 10:53:00.602: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-secrets-d0ac325a-2a17-4a77-a962-31eadf1acf09 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/22/23 10:53:00.623
    Jun 22 10:53:00.644: INFO: Waiting for pod pod-projected-secrets-d0ac325a-2a17-4a77-a962-31eadf1acf09 to disappear
    Jun 22 10:53:00.649: INFO: Pod pod-projected-secrets-d0ac325a-2a17-4a77-a962-31eadf1acf09 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:53:00.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5512" for this suite. 06/22/23 10:53:00.658
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:53:00.671
Jun 22 10:53:00.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename deployment 06/22/23 10:53:00.672
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:00.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:00.702
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jun 22 10:53:00.706: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
W0622 10:53:00.716416      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:53:00.722: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 22 10:53:05.729: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/22/23 10:53:05.729
Jun 22 10:53:05.729: INFO: Creating deployment "test-rolling-update-deployment"
W0622 10:53:05.738541      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:53:05.738: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 22 10:53:05.752: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 22 10:53:07.765: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 22 10:53:07.770: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 22 10:53:07.786: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1124  12b993be-0381-4fac-bce1-03bdd1d13d59 140728 1 2023-06-22 10:53:05 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-06-22 10:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f69ce8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-22 10:53:05 +0000 UTC,LastTransitionTime:2023-06-22 10:53:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-06-22 10:53:07 +0000 UTC,LastTransitionTime:2023-06-22 10:53:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 10:53:07.791: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1124  7e5521a3-6cd0-4f24-ba8d-19c8fd883425 140718 1 2023-06-22 10:53:05 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 12b993be-0381-4fac-bce1-03bdd1d13d59 0xc00650a767 0xc00650a768}] [] [{kube-controller-manager Update apps/v1 2023-06-22 10:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b993be-0381-4fac-bce1-03bdd1d13d59\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:53:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00650a818 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 22 10:53:07.791: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 22 10:53:07.791: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1124  46d792b9-8b31-4e15-bb19-6d27d731fde9 140727 2 2023-06-22 10:53:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 12b993be-0381-4fac-bce1-03bdd1d13d59 0xc00650a637 0xc00650a638}] [] [{e2e.test Update apps/v1 2023-06-22 10:53:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b993be-0381-4fac-bce1-03bdd1d13d59\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:53:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00650a6f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 10:53:07.798: INFO: Pod "test-rolling-update-deployment-7549d9f46d-4cmt8" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-4cmt8 test-rolling-update-deployment-7549d9f46d- deployment-1124  e6056b92-f724-4c8d-9312-6a8665f5c3ec 140717 0 2023-06-22 10:53:05 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 7e5521a3-6cd0-4f24-ba8d-19c8fd883425 0xc00650ac77 0xc00650ac78}] [] [{kube-controller-manager Update v1 2023-06-22 10:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5521a3-6cd0-4f24-ba8d-19c8fd883425\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:53:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p8jpd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p8jpd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:53:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:53:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:53:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:53:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.69,StartTime:2023-06-22 10:53:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:53:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://9a84398a98beab3229a42cdd715319679155acc35ae1d38f51e90742d15f7fd7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun 22 10:53:07.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1124" for this suite. 06/22/23 10:53:07.806
------------------------------
• [SLOW TEST] [7.146 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:53:00.671
    Jun 22 10:53:00.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename deployment 06/22/23 10:53:00.672
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:00.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:00.702
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jun 22 10:53:00.706: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    W0622 10:53:00.716416      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:53:00.722: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun 22 10:53:05.729: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/22/23 10:53:05.729
    Jun 22 10:53:05.729: INFO: Creating deployment "test-rolling-update-deployment"
    W0622 10:53:05.738541      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:53:05.738: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jun 22 10:53:05.752: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jun 22 10:53:07.765: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jun 22 10:53:07.770: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 22 10:53:07.786: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1124  12b993be-0381-4fac-bce1-03bdd1d13d59 140728 1 2023-06-22 10:53:05 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-06-22 10:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f69ce8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-22 10:53:05 +0000 UTC,LastTransitionTime:2023-06-22 10:53:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-06-22 10:53:07 +0000 UTC,LastTransitionTime:2023-06-22 10:53:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun 22 10:53:07.791: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1124  7e5521a3-6cd0-4f24-ba8d-19c8fd883425 140718 1 2023-06-22 10:53:05 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 12b993be-0381-4fac-bce1-03bdd1d13d59 0xc00650a767 0xc00650a768}] [] [{kube-controller-manager Update apps/v1 2023-06-22 10:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b993be-0381-4fac-bce1-03bdd1d13d59\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:53:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00650a818 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 22 10:53:07.791: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jun 22 10:53:07.791: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1124  46d792b9-8b31-4e15-bb19-6d27d731fde9 140727 2 2023-06-22 10:53:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 12b993be-0381-4fac-bce1-03bdd1d13d59 0xc00650a637 0xc00650a638}] [] [{e2e.test Update apps/v1 2023-06-22 10:53:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:53:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12b993be-0381-4fac-bce1-03bdd1d13d59\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-22 10:53:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00650a6f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 22 10:53:07.798: INFO: Pod "test-rolling-update-deployment-7549d9f46d-4cmt8" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-4cmt8 test-rolling-update-deployment-7549d9f46d- deployment-1124  e6056b92-f724-4c8d-9312-6a8665f5c3ec 140717 0 2023-06-22 10:53:05 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 7e5521a3-6cd0-4f24-ba8d-19c8fd883425 0xc00650ac77 0xc00650ac78}] [] [{kube-controller-manager Update v1 2023-06-22 10:53:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e5521a3-6cd0-4f24-ba8d-19c8fd883425\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 10:53:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p8jpd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p8jpd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:53:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:53:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:53:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 10:53:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.69,StartTime:2023-06-22 10:53:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 10:53:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://9a84398a98beab3229a42cdd715319679155acc35ae1d38f51e90742d15f7fd7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:53:07.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1124" for this suite. 06/22/23 10:53:07.806
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:53:07.818
Jun 22 10:53:07.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename downward-api 06/22/23 10:53:07.819
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:07.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:07.85
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 06/22/23 10:53:07.853
W0622 10:53:07.868329      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:53:07.868: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc7c2273-459a-43be-8f63-bdd8f2174e88" in namespace "downward-api-5877" to be "Succeeded or Failed"
Jun 22 10:53:07.876: INFO: Pod "downwardapi-volume-bc7c2273-459a-43be-8f63-bdd8f2174e88": Phase="Pending", Reason="", readiness=false. Elapsed: 7.556459ms
Jun 22 10:53:09.883: INFO: Pod "downwardapi-volume-bc7c2273-459a-43be-8f63-bdd8f2174e88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015220737s
Jun 22 10:53:11.883: INFO: Pod "downwardapi-volume-bc7c2273-459a-43be-8f63-bdd8f2174e88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014772476s
STEP: Saw pod success 06/22/23 10:53:11.883
Jun 22 10:53:11.883: INFO: Pod "downwardapi-volume-bc7c2273-459a-43be-8f63-bdd8f2174e88" satisfied condition "Succeeded or Failed"
Jun 22 10:53:11.888: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-bc7c2273-459a-43be-8f63-bdd8f2174e88 container client-container: <nil>
STEP: delete the pod 06/22/23 10:53:11.903
Jun 22 10:53:11.921: INFO: Waiting for pod downwardapi-volume-bc7c2273-459a-43be-8f63-bdd8f2174e88 to disappear
Jun 22 10:53:11.925: INFO: Pod downwardapi-volume-bc7c2273-459a-43be-8f63-bdd8f2174e88 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun 22 10:53:11.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5877" for this suite. 06/22/23 10:53:11.932
------------------------------
• [4.123 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:53:07.818
    Jun 22 10:53:07.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename downward-api 06/22/23 10:53:07.819
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:07.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:07.85
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 06/22/23 10:53:07.853
    W0622 10:53:07.868329      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:53:07.868: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc7c2273-459a-43be-8f63-bdd8f2174e88" in namespace "downward-api-5877" to be "Succeeded or Failed"
    Jun 22 10:53:07.876: INFO: Pod "downwardapi-volume-bc7c2273-459a-43be-8f63-bdd8f2174e88": Phase="Pending", Reason="", readiness=false. Elapsed: 7.556459ms
    Jun 22 10:53:09.883: INFO: Pod "downwardapi-volume-bc7c2273-459a-43be-8f63-bdd8f2174e88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015220737s
    Jun 22 10:53:11.883: INFO: Pod "downwardapi-volume-bc7c2273-459a-43be-8f63-bdd8f2174e88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014772476s
    STEP: Saw pod success 06/22/23 10:53:11.883
    Jun 22 10:53:11.883: INFO: Pod "downwardapi-volume-bc7c2273-459a-43be-8f63-bdd8f2174e88" satisfied condition "Succeeded or Failed"
    Jun 22 10:53:11.888: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-bc7c2273-459a-43be-8f63-bdd8f2174e88 container client-container: <nil>
    STEP: delete the pod 06/22/23 10:53:11.903
    Jun 22 10:53:11.921: INFO: Waiting for pod downwardapi-volume-bc7c2273-459a-43be-8f63-bdd8f2174e88 to disappear
    Jun 22 10:53:11.925: INFO: Pod downwardapi-volume-bc7c2273-459a-43be-8f63-bdd8f2174e88 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:53:11.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5877" for this suite. 06/22/23 10:53:11.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:53:11.942
Jun 22 10:53:11.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename proxy 06/22/23 10:53:11.943
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:11.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:11.97
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jun 22 10:53:11.975: INFO: Creating pod...
W0622 10:53:11.988365      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:53:11.988: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9786" to be "running"
Jun 22 10:53:11.996: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.681069ms
Jun 22 10:53:14.003: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.015437053s
Jun 22 10:53:14.004: INFO: Pod "agnhost" satisfied condition "running"
Jun 22 10:53:14.004: INFO: Creating service...
Jun 22 10:53:14.029: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/pods/agnhost/proxy?method=DELETE
Jun 22 10:53:14.049: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 22 10:53:14.049: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/pods/agnhost/proxy?method=OPTIONS
Jun 22 10:53:14.058: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 22 10:53:14.058: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/pods/agnhost/proxy?method=PATCH
Jun 22 10:53:14.063: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 22 10:53:14.064: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/pods/agnhost/proxy?method=POST
Jun 22 10:53:14.070: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 22 10:53:14.070: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/pods/agnhost/proxy?method=PUT
Jun 22 10:53:14.077: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun 22 10:53:14.077: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/services/e2e-proxy-test-service/proxy?method=DELETE
Jun 22 10:53:14.086: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 22 10:53:14.086: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jun 22 10:53:14.095: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 22 10:53:14.096: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/services/e2e-proxy-test-service/proxy?method=PATCH
Jun 22 10:53:14.108: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 22 10:53:14.108: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/services/e2e-proxy-test-service/proxy?method=POST
Jun 22 10:53:14.119: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 22 10:53:14.119: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/services/e2e-proxy-test-service/proxy?method=PUT
Jun 22 10:53:14.129: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun 22 10:53:14.129: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/pods/agnhost/proxy?method=GET
Jun 22 10:53:14.135: INFO: http.Client request:GET StatusCode:301
Jun 22 10:53:14.136: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/services/e2e-proxy-test-service/proxy?method=GET
Jun 22 10:53:14.147: INFO: http.Client request:GET StatusCode:301
Jun 22 10:53:14.147: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/pods/agnhost/proxy?method=HEAD
Jun 22 10:53:14.152: INFO: http.Client request:HEAD StatusCode:301
Jun 22 10:53:14.152: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/services/e2e-proxy-test-service/proxy?method=HEAD
Jun 22 10:53:14.161: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jun 22 10:53:14.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-9786" for this suite. 06/22/23 10:53:14.169
------------------------------
• [2.238 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:53:11.942
    Jun 22 10:53:11.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename proxy 06/22/23 10:53:11.943
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:11.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:11.97
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jun 22 10:53:11.975: INFO: Creating pod...
    W0622 10:53:11.988365      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:53:11.988: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9786" to be "running"
    Jun 22 10:53:11.996: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 7.681069ms
    Jun 22 10:53:14.003: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.015437053s
    Jun 22 10:53:14.004: INFO: Pod "agnhost" satisfied condition "running"
    Jun 22 10:53:14.004: INFO: Creating service...
    Jun 22 10:53:14.029: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/pods/agnhost/proxy?method=DELETE
    Jun 22 10:53:14.049: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun 22 10:53:14.049: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/pods/agnhost/proxy?method=OPTIONS
    Jun 22 10:53:14.058: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun 22 10:53:14.058: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/pods/agnhost/proxy?method=PATCH
    Jun 22 10:53:14.063: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun 22 10:53:14.064: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/pods/agnhost/proxy?method=POST
    Jun 22 10:53:14.070: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun 22 10:53:14.070: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/pods/agnhost/proxy?method=PUT
    Jun 22 10:53:14.077: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jun 22 10:53:14.077: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/services/e2e-proxy-test-service/proxy?method=DELETE
    Jun 22 10:53:14.086: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun 22 10:53:14.086: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jun 22 10:53:14.095: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun 22 10:53:14.096: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/services/e2e-proxy-test-service/proxy?method=PATCH
    Jun 22 10:53:14.108: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun 22 10:53:14.108: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/services/e2e-proxy-test-service/proxy?method=POST
    Jun 22 10:53:14.119: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun 22 10:53:14.119: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/services/e2e-proxy-test-service/proxy?method=PUT
    Jun 22 10:53:14.129: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jun 22 10:53:14.129: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/pods/agnhost/proxy?method=GET
    Jun 22 10:53:14.135: INFO: http.Client request:GET StatusCode:301
    Jun 22 10:53:14.136: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/services/e2e-proxy-test-service/proxy?method=GET
    Jun 22 10:53:14.147: INFO: http.Client request:GET StatusCode:301
    Jun 22 10:53:14.147: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/pods/agnhost/proxy?method=HEAD
    Jun 22 10:53:14.152: INFO: http.Client request:HEAD StatusCode:301
    Jun 22 10:53:14.152: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-9786/services/e2e-proxy-test-service/proxy?method=HEAD
    Jun 22 10:53:14.161: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:53:14.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-9786" for this suite. 06/22/23 10:53:14.169
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:53:14.182
Jun 22 10:53:14.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename pods 06/22/23 10:53:14.183
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:14.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:14.216
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Jun 22 10:53:14.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: creating the pod 06/22/23 10:53:14.221
STEP: submitting the pod to kubernetes 06/22/23 10:53:14.221
Jun 22 10:53:14.238: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-e1f6ecf4-41de-4f31-8e05-aeab0b628e99" in namespace "pods-7958" to be "running and ready"
Jun 22 10:53:14.244: INFO: Pod "pod-exec-websocket-e1f6ecf4-41de-4f31-8e05-aeab0b628e99": Phase="Pending", Reason="", readiness=false. Elapsed: 6.526182ms
Jun 22 10:53:14.244: INFO: The phase of Pod pod-exec-websocket-e1f6ecf4-41de-4f31-8e05-aeab0b628e99 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:53:16.253: INFO: Pod "pod-exec-websocket-e1f6ecf4-41de-4f31-8e05-aeab0b628e99": Phase="Running", Reason="", readiness=true. Elapsed: 2.015202055s
Jun 22 10:53:16.253: INFO: The phase of Pod pod-exec-websocket-e1f6ecf4-41de-4f31-8e05-aeab0b628e99 is Running (Ready = true)
Jun 22 10:53:16.253: INFO: Pod "pod-exec-websocket-e1f6ecf4-41de-4f31-8e05-aeab0b628e99" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun 22 10:53:16.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7958" for this suite. 06/22/23 10:53:16.376
------------------------------
• [2.203 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:53:14.182
    Jun 22 10:53:14.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename pods 06/22/23 10:53:14.183
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:14.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:14.216
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Jun 22 10:53:14.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: creating the pod 06/22/23 10:53:14.221
    STEP: submitting the pod to kubernetes 06/22/23 10:53:14.221
    Jun 22 10:53:14.238: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-e1f6ecf4-41de-4f31-8e05-aeab0b628e99" in namespace "pods-7958" to be "running and ready"
    Jun 22 10:53:14.244: INFO: Pod "pod-exec-websocket-e1f6ecf4-41de-4f31-8e05-aeab0b628e99": Phase="Pending", Reason="", readiness=false. Elapsed: 6.526182ms
    Jun 22 10:53:14.244: INFO: The phase of Pod pod-exec-websocket-e1f6ecf4-41de-4f31-8e05-aeab0b628e99 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:53:16.253: INFO: Pod "pod-exec-websocket-e1f6ecf4-41de-4f31-8e05-aeab0b628e99": Phase="Running", Reason="", readiness=true. Elapsed: 2.015202055s
    Jun 22 10:53:16.253: INFO: The phase of Pod pod-exec-websocket-e1f6ecf4-41de-4f31-8e05-aeab0b628e99 is Running (Ready = true)
    Jun 22 10:53:16.253: INFO: Pod "pod-exec-websocket-e1f6ecf4-41de-4f31-8e05-aeab0b628e99" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:53:16.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7958" for this suite. 06/22/23 10:53:16.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:53:16.387
Jun 22 10:53:16.387: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename hostport 06/22/23 10:53:16.388
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:16.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:16.414
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 06/22/23 10:53:16.428
W0622 10:53:16.441371      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": hostPort (container "agnhost" uses hostPort 54323), allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:53:16.441: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-6963" to be "running and ready"
Jun 22 10:53:16.446: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.236483ms
Jun 22 10:53:16.446: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:53:18.454: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013174219s
Jun 22 10:53:18.454: INFO: The phase of Pod pod1 is Running (Ready = true)
Jun 22 10:53:18.454: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.92.224.179 on the node which pod1 resides and expect scheduled 06/22/23 10:53:18.454
W0622 10:53:18.464650      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": hostPort (container "agnhost" uses hostPort 54323), allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:53:18.464: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-6963" to be "running and ready"
Jun 22 10:53:18.471: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.842993ms
Jun 22 10:53:18.471: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:53:20.478: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013424019s
Jun 22 10:53:20.478: INFO: The phase of Pod pod2 is Running (Ready = true)
Jun 22 10:53:20.478: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.92.224.179 but use UDP protocol on the node which pod2 resides 06/22/23 10:53:20.478
W0622 10:53:20.488528      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": hostPort (container "agnhost" uses hostPort 54323), allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:53:20.489: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-6963" to be "running and ready"
Jun 22 10:53:20.493: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.41178ms
Jun 22 10:53:20.493: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:53:22.525: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.036197482s
Jun 22 10:53:22.525: INFO: The phase of Pod pod3 is Running (Ready = true)
Jun 22 10:53:22.525: INFO: Pod "pod3" satisfied condition "running and ready"
W0622 10:53:22.535083      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": host namespaces (hostNetwork=true), allowPrivilegeEscalation != false (container "e2e-host-exec" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-host-exec" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-host-exec" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-host-exec" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:53:22.535: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-6963" to be "running and ready"
Jun 22 10:53:22.540: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.904462ms
Jun 22 10:53:22.540: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:53:24.545: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.01047786s
Jun 22 10:53:24.545: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jun 22 10:53:24.545: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 06/22/23 10:53:24.55
Jun 22 10:53:24.551: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.92.224.179 http://127.0.0.1:54323/hostname] Namespace:hostport-6963 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:53:24.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:53:24.551: INFO: ExecWithOptions: Clientset creation
Jun 22 10:53:24.551: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/hostport-6963/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.92.224.179+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.92.224.179, port: 54323 06/22/23 10:53:24.672
Jun 22 10:53:24.672: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.92.224.179:54323/hostname] Namespace:hostport-6963 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:53:24.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:53:24.673: INFO: ExecWithOptions: Clientset creation
Jun 22 10:53:24.673: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/hostport-6963/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.92.224.179%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.92.224.179, port: 54323 UDP 06/22/23 10:53:24.763
Jun 22 10:53:24.763: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.92.224.179 54323] Namespace:hostport-6963 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:53:24.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:53:24.763: INFO: ExecWithOptions: Clientset creation
Jun 22 10:53:24.763: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/hostport-6963/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.92.224.179+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Jun 22 10:53:29.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-6963" for this suite. 06/22/23 10:53:29.867
------------------------------
• [SLOW TEST] [13.489 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:53:16.387
    Jun 22 10:53:16.387: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename hostport 06/22/23 10:53:16.388
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:16.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:16.414
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 06/22/23 10:53:16.428
    W0622 10:53:16.441371      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": hostPort (container "agnhost" uses hostPort 54323), allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:53:16.441: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-6963" to be "running and ready"
    Jun 22 10:53:16.446: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.236483ms
    Jun 22 10:53:16.446: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:53:18.454: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013174219s
    Jun 22 10:53:18.454: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jun 22 10:53:18.454: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.92.224.179 on the node which pod1 resides and expect scheduled 06/22/23 10:53:18.454
    W0622 10:53:18.464650      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": hostPort (container "agnhost" uses hostPort 54323), allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:53:18.464: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-6963" to be "running and ready"
    Jun 22 10:53:18.471: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.842993ms
    Jun 22 10:53:18.471: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:53:20.478: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013424019s
    Jun 22 10:53:20.478: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jun 22 10:53:20.478: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.92.224.179 but use UDP protocol on the node which pod2 resides 06/22/23 10:53:20.478
    W0622 10:53:20.488528      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": hostPort (container "agnhost" uses hostPort 54323), allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:53:20.489: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-6963" to be "running and ready"
    Jun 22 10:53:20.493: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.41178ms
    Jun 22 10:53:20.493: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:53:22.525: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.036197482s
    Jun 22 10:53:22.525: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jun 22 10:53:22.525: INFO: Pod "pod3" satisfied condition "running and ready"
    W0622 10:53:22.535083      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": host namespaces (hostNetwork=true), allowPrivilegeEscalation != false (container "e2e-host-exec" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-host-exec" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-host-exec" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-host-exec" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:53:22.535: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-6963" to be "running and ready"
    Jun 22 10:53:22.540: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.904462ms
    Jun 22 10:53:22.540: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:53:24.545: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.01047786s
    Jun 22 10:53:24.545: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jun 22 10:53:24.545: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 06/22/23 10:53:24.55
    Jun 22 10:53:24.551: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.92.224.179 http://127.0.0.1:54323/hostname] Namespace:hostport-6963 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:53:24.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:53:24.551: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:53:24.551: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/hostport-6963/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.92.224.179+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.92.224.179, port: 54323 06/22/23 10:53:24.672
    Jun 22 10:53:24.672: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.92.224.179:54323/hostname] Namespace:hostport-6963 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:53:24.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:53:24.673: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:53:24.673: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/hostport-6963/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.92.224.179%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.92.224.179, port: 54323 UDP 06/22/23 10:53:24.763
    Jun 22 10:53:24.763: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.92.224.179 54323] Namespace:hostport-6963 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:53:24.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:53:24.763: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:53:24.763: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/hostport-6963/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.92.224.179+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:53:29.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-6963" for this suite. 06/22/23 10:53:29.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:53:29.877
Jun 22 10:53:29.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename downward-api 06/22/23 10:53:29.878
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:29.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:29.903
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 06/22/23 10:53:29.907
W0622 10:53:29.920150      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:53:29.920: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9faf1752-84ee-4de5-a685-ac2bbb8d0661" in namespace "downward-api-1361" to be "Succeeded or Failed"
Jun 22 10:53:29.927: INFO: Pod "downwardapi-volume-9faf1752-84ee-4de5-a685-ac2bbb8d0661": Phase="Pending", Reason="", readiness=false. Elapsed: 7.157828ms
Jun 22 10:53:31.934: INFO: Pod "downwardapi-volume-9faf1752-84ee-4de5-a685-ac2bbb8d0661": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014167134s
Jun 22 10:53:33.935: INFO: Pod "downwardapi-volume-9faf1752-84ee-4de5-a685-ac2bbb8d0661": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015534228s
STEP: Saw pod success 06/22/23 10:53:33.935
Jun 22 10:53:33.935: INFO: Pod "downwardapi-volume-9faf1752-84ee-4de5-a685-ac2bbb8d0661" satisfied condition "Succeeded or Failed"
Jun 22 10:53:33.941: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-9faf1752-84ee-4de5-a685-ac2bbb8d0661 container client-container: <nil>
STEP: delete the pod 06/22/23 10:53:33.951
Jun 22 10:53:33.975: INFO: Waiting for pod downwardapi-volume-9faf1752-84ee-4de5-a685-ac2bbb8d0661 to disappear
Jun 22 10:53:33.980: INFO: Pod downwardapi-volume-9faf1752-84ee-4de5-a685-ac2bbb8d0661 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun 22 10:53:33.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1361" for this suite. 06/22/23 10:53:33.987
------------------------------
• [4.119 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:53:29.877
    Jun 22 10:53:29.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename downward-api 06/22/23 10:53:29.878
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:29.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:29.903
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 06/22/23 10:53:29.907
    W0622 10:53:29.920150      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:53:29.920: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9faf1752-84ee-4de5-a685-ac2bbb8d0661" in namespace "downward-api-1361" to be "Succeeded or Failed"
    Jun 22 10:53:29.927: INFO: Pod "downwardapi-volume-9faf1752-84ee-4de5-a685-ac2bbb8d0661": Phase="Pending", Reason="", readiness=false. Elapsed: 7.157828ms
    Jun 22 10:53:31.934: INFO: Pod "downwardapi-volume-9faf1752-84ee-4de5-a685-ac2bbb8d0661": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014167134s
    Jun 22 10:53:33.935: INFO: Pod "downwardapi-volume-9faf1752-84ee-4de5-a685-ac2bbb8d0661": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015534228s
    STEP: Saw pod success 06/22/23 10:53:33.935
    Jun 22 10:53:33.935: INFO: Pod "downwardapi-volume-9faf1752-84ee-4de5-a685-ac2bbb8d0661" satisfied condition "Succeeded or Failed"
    Jun 22 10:53:33.941: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-9faf1752-84ee-4de5-a685-ac2bbb8d0661 container client-container: <nil>
    STEP: delete the pod 06/22/23 10:53:33.951
    Jun 22 10:53:33.975: INFO: Waiting for pod downwardapi-volume-9faf1752-84ee-4de5-a685-ac2bbb8d0661 to disappear
    Jun 22 10:53:33.980: INFO: Pod downwardapi-volume-9faf1752-84ee-4de5-a685-ac2bbb8d0661 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:53:33.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1361" for this suite. 06/22/23 10:53:33.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:53:33.998
Jun 22 10:53:33.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename namespaces 06/22/23 10:53:33.998
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:34.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:34.023
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-xhvxs" 06/22/23 10:53:34.027
Jun 22 10:53:34.048: INFO: Namespace "e2e-ns-xhvxs-8242" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-xhvxs-8242" 06/22/23 10:53:34.048
Jun 22 10:53:34.059: INFO: Namespace "e2e-ns-xhvxs-8242" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-xhvxs-8242" 06/22/23 10:53:34.059
Jun 22 10:53:34.070: INFO: Namespace "e2e-ns-xhvxs-8242" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:53:34.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3031" for this suite. 06/22/23 10:53:34.077
STEP: Destroying namespace "e2e-ns-xhvxs-8242" for this suite. 06/22/23 10:53:34.085
------------------------------
• [0.096 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:53:33.998
    Jun 22 10:53:33.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename namespaces 06/22/23 10:53:33.998
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:34.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:34.023
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-xhvxs" 06/22/23 10:53:34.027
    Jun 22 10:53:34.048: INFO: Namespace "e2e-ns-xhvxs-8242" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-xhvxs-8242" 06/22/23 10:53:34.048
    Jun 22 10:53:34.059: INFO: Namespace "e2e-ns-xhvxs-8242" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-xhvxs-8242" 06/22/23 10:53:34.059
    Jun 22 10:53:34.070: INFO: Namespace "e2e-ns-xhvxs-8242" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:53:34.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3031" for this suite. 06/22/23 10:53:34.077
    STEP: Destroying namespace "e2e-ns-xhvxs-8242" for this suite. 06/22/23 10:53:34.085
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:53:34.094
Jun 22 10:53:34.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename ephemeral-containers-test 06/22/23 10:53:34.094
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:34.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:34.121
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 06/22/23 10:53:34.124
W0622 10:53:34.135394      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container-1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:53:34.135: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5827" to be "running and ready"
Jun 22 10:53:34.140: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.537404ms
Jun 22 10:53:34.140: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 22 10:53:36.145: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010394402s
Jun 22 10:53:36.146: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jun 22 10:53:36.146: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 06/22/23 10:53:36.153
W0622 10:53:36.173938      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "test-container-1", "debugger" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "test-container-1", "debugger" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "test-container-1", "debugger" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "test-container-1", "debugger" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:53:36.174: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5827" to be "container debugger running"
Jun 22 10:53:36.178: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.115616ms
Jun 22 10:53:38.185: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010847813s
Jun 22 10:53:40.185: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011179828s
Jun 22 10:53:40.185: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 06/22/23 10:53:40.185
Jun 22 10:53:40.186: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5827 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 10:53:40.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 10:53:40.186: INFO: ExecWithOptions: Clientset creation
Jun 22 10:53:40.186: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/ephemeral-containers-test-5827/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jun 22 10:53:40.290: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:53:40.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-5827" for this suite. 06/22/23 10:53:40.308
------------------------------
• [SLOW TEST] [6.225 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:53:34.094
    Jun 22 10:53:34.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename ephemeral-containers-test 06/22/23 10:53:34.094
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:34.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:34.121
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 06/22/23 10:53:34.124
    W0622 10:53:34.135394      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container-1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:53:34.135: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5827" to be "running and ready"
    Jun 22 10:53:34.140: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.537404ms
    Jun 22 10:53:34.140: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 10:53:36.145: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010394402s
    Jun 22 10:53:36.146: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jun 22 10:53:36.146: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 06/22/23 10:53:36.153
    W0622 10:53:36.173938      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "test-container-1", "debugger" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "test-container-1", "debugger" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "test-container-1", "debugger" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "test-container-1", "debugger" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:53:36.174: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5827" to be "container debugger running"
    Jun 22 10:53:36.178: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.115616ms
    Jun 22 10:53:38.185: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010847813s
    Jun 22 10:53:40.185: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.011179828s
    Jun 22 10:53:40.185: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 06/22/23 10:53:40.185
    Jun 22 10:53:40.186: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5827 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 10:53:40.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 10:53:40.186: INFO: ExecWithOptions: Clientset creation
    Jun 22 10:53:40.186: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/ephemeral-containers-test-5827/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jun 22 10:53:40.290: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:53:40.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-5827" for this suite. 06/22/23 10:53:40.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:53:40.319
Jun 22 10:53:40.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename watch 06/22/23 10:53:40.32
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:40.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:40.346
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 06/22/23 10:53:40.349
STEP: modifying the configmap once 06/22/23 10:53:40.358
STEP: modifying the configmap a second time 06/22/23 10:53:40.368
STEP: deleting the configmap 06/22/23 10:53:40.379
STEP: creating a watch on configmaps from the resource version returned by the first update 06/22/23 10:53:40.386
STEP: Expecting to observe notifications for all changes to the configmap after the first update 06/22/23 10:53:40.388
Jun 22 10:53:40.388: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1918  e9b73f45-aabd-40ca-9857-ae6a526694a6 141096 0 2023-06-22 10:53:40 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-22 10:53:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 10:53:40.388: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1918  e9b73f45-aabd-40ca-9857-ae6a526694a6 141097 0 2023-06-22 10:53:40 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-22 10:53:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jun 22 10:53:40.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1918" for this suite. 06/22/23 10:53:40.396
------------------------------
• [0.084 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:53:40.319
    Jun 22 10:53:40.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename watch 06/22/23 10:53:40.32
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:40.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:40.346
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 06/22/23 10:53:40.349
    STEP: modifying the configmap once 06/22/23 10:53:40.358
    STEP: modifying the configmap a second time 06/22/23 10:53:40.368
    STEP: deleting the configmap 06/22/23 10:53:40.379
    STEP: creating a watch on configmaps from the resource version returned by the first update 06/22/23 10:53:40.386
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 06/22/23 10:53:40.388
    Jun 22 10:53:40.388: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1918  e9b73f45-aabd-40ca-9857-ae6a526694a6 141096 0 2023-06-22 10:53:40 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-22 10:53:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 22 10:53:40.388: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1918  e9b73f45-aabd-40ca-9857-ae6a526694a6 141097 0 2023-06-22 10:53:40 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-22 10:53:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:53:40.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1918" for this suite. 06/22/23 10:53:40.396
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:53:40.406
Jun 22 10:53:40.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename container-probe 06/22/23 10:53:40.407
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:40.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:40.433
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-e42704ea-46f9-4bc4-902d-4048c656875b in namespace container-probe-3631 06/22/23 10:53:40.437
W0622 10:53:40.448882      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:53:40.448: INFO: Waiting up to 5m0s for pod "liveness-e42704ea-46f9-4bc4-902d-4048c656875b" in namespace "container-probe-3631" to be "not pending"
Jun 22 10:53:40.453: INFO: Pod "liveness-e42704ea-46f9-4bc4-902d-4048c656875b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.526811ms
Jun 22 10:53:42.460: INFO: Pod "liveness-e42704ea-46f9-4bc4-902d-4048c656875b": Phase="Running", Reason="", readiness=true. Elapsed: 2.011275711s
Jun 22 10:53:42.460: INFO: Pod "liveness-e42704ea-46f9-4bc4-902d-4048c656875b" satisfied condition "not pending"
Jun 22 10:53:42.460: INFO: Started pod liveness-e42704ea-46f9-4bc4-902d-4048c656875b in namespace container-probe-3631
STEP: checking the pod's current state and verifying that restartCount is present 06/22/23 10:53:42.46
Jun 22 10:53:42.466: INFO: Initial restart count of pod liveness-e42704ea-46f9-4bc4-902d-4048c656875b is 0
Jun 22 10:54:02.544: INFO: Restart count of pod container-probe-3631/liveness-e42704ea-46f9-4bc4-902d-4048c656875b is now 1 (20.07839367s elapsed)
Jun 22 10:54:22.613: INFO: Restart count of pod container-probe-3631/liveness-e42704ea-46f9-4bc4-902d-4048c656875b is now 2 (40.147556111s elapsed)
Jun 22 10:54:42.682: INFO: Restart count of pod container-probe-3631/liveness-e42704ea-46f9-4bc4-902d-4048c656875b is now 3 (1m0.216609741s elapsed)
Jun 22 10:55:02.753: INFO: Restart count of pod container-probe-3631/liveness-e42704ea-46f9-4bc4-902d-4048c656875b is now 4 (1m20.286919541s elapsed)
Jun 22 10:56:10.990: INFO: Restart count of pod container-probe-3631/liveness-e42704ea-46f9-4bc4-902d-4048c656875b is now 5 (2m28.524782727s elapsed)
STEP: deleting the pod 06/22/23 10:56:10.991
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun 22 10:56:11.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3631" for this suite. 06/22/23 10:56:11.013
------------------------------
• [SLOW TEST] [150.619 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:53:40.406
    Jun 22 10:53:40.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename container-probe 06/22/23 10:53:40.407
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:53:40.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:53:40.433
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-e42704ea-46f9-4bc4-902d-4048c656875b in namespace container-probe-3631 06/22/23 10:53:40.437
    W0622 10:53:40.448882      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:53:40.448: INFO: Waiting up to 5m0s for pod "liveness-e42704ea-46f9-4bc4-902d-4048c656875b" in namespace "container-probe-3631" to be "not pending"
    Jun 22 10:53:40.453: INFO: Pod "liveness-e42704ea-46f9-4bc4-902d-4048c656875b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.526811ms
    Jun 22 10:53:42.460: INFO: Pod "liveness-e42704ea-46f9-4bc4-902d-4048c656875b": Phase="Running", Reason="", readiness=true. Elapsed: 2.011275711s
    Jun 22 10:53:42.460: INFO: Pod "liveness-e42704ea-46f9-4bc4-902d-4048c656875b" satisfied condition "not pending"
    Jun 22 10:53:42.460: INFO: Started pod liveness-e42704ea-46f9-4bc4-902d-4048c656875b in namespace container-probe-3631
    STEP: checking the pod's current state and verifying that restartCount is present 06/22/23 10:53:42.46
    Jun 22 10:53:42.466: INFO: Initial restart count of pod liveness-e42704ea-46f9-4bc4-902d-4048c656875b is 0
    Jun 22 10:54:02.544: INFO: Restart count of pod container-probe-3631/liveness-e42704ea-46f9-4bc4-902d-4048c656875b is now 1 (20.07839367s elapsed)
    Jun 22 10:54:22.613: INFO: Restart count of pod container-probe-3631/liveness-e42704ea-46f9-4bc4-902d-4048c656875b is now 2 (40.147556111s elapsed)
    Jun 22 10:54:42.682: INFO: Restart count of pod container-probe-3631/liveness-e42704ea-46f9-4bc4-902d-4048c656875b is now 3 (1m0.216609741s elapsed)
    Jun 22 10:55:02.753: INFO: Restart count of pod container-probe-3631/liveness-e42704ea-46f9-4bc4-902d-4048c656875b is now 4 (1m20.286919541s elapsed)
    Jun 22 10:56:10.990: INFO: Restart count of pod container-probe-3631/liveness-e42704ea-46f9-4bc4-902d-4048c656875b is now 5 (2m28.524782727s elapsed)
    STEP: deleting the pod 06/22/23 10:56:10.991
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:56:11.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3631" for this suite. 06/22/23 10:56:11.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:56:11.026
Jun 22 10:56:11.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename crd-webhook 06/22/23 10:56:11.027
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:11.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:11.052
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 06/22/23 10:56:11.056
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/22/23 10:56:11.673
STEP: Deploying the custom resource conversion webhook pod 06/22/23 10:56:11.686
W0622 10:56:11.705088      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-crd-conversion-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-crd-conversion-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-crd-conversion-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-crd-conversion-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 10:56:11.705
Jun 22 10:56:11.718: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/22/23 10:56:13.738
STEP: Verifying the service has paired with the endpoint 06/22/23 10:56:13.761
Jun 22 10:56:14.762: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jun 22 10:56:14.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Creating a v1 custom resource 06/22/23 10:56:17.371
STEP: v2 custom resource should be converted 06/22/23 10:56:17.38
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:56:17.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-9055" for this suite. 06/22/23 10:56:17.976
------------------------------
• [SLOW TEST] [6.963 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:56:11.026
    Jun 22 10:56:11.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename crd-webhook 06/22/23 10:56:11.027
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:11.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:11.052
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 06/22/23 10:56:11.056
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/22/23 10:56:11.673
    STEP: Deploying the custom resource conversion webhook pod 06/22/23 10:56:11.686
    W0622 10:56:11.705088      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-crd-conversion-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-crd-conversion-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-crd-conversion-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-crd-conversion-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 10:56:11.705
    Jun 22 10:56:11.718: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/22/23 10:56:13.738
    STEP: Verifying the service has paired with the endpoint 06/22/23 10:56:13.761
    Jun 22 10:56:14.762: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jun 22 10:56:14.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Creating a v1 custom resource 06/22/23 10:56:17.371
    STEP: v2 custom resource should be converted 06/22/23 10:56:17.38
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:56:17.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-9055" for this suite. 06/22/23 10:56:17.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:56:17.993
Jun 22 10:56:17.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename emptydir 06/22/23 10:56:17.994
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:18.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:18.028
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 06/22/23 10:56:18.033
W0622 10:56:18.046627      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:56:18.046: INFO: Waiting up to 5m0s for pod "pod-638e8ae5-0712-4cc5-97f1-a6d782c8e330" in namespace "emptydir-2356" to be "Succeeded or Failed"
Jun 22 10:56:18.051: INFO: Pod "pod-638e8ae5-0712-4cc5-97f1-a6d782c8e330": Phase="Pending", Reason="", readiness=false. Elapsed: 4.390045ms
Jun 22 10:56:20.057: INFO: Pod "pod-638e8ae5-0712-4cc5-97f1-a6d782c8e330": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010765458s
Jun 22 10:56:22.058: INFO: Pod "pod-638e8ae5-0712-4cc5-97f1-a6d782c8e330": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011449948s
STEP: Saw pod success 06/22/23 10:56:22.058
Jun 22 10:56:22.058: INFO: Pod "pod-638e8ae5-0712-4cc5-97f1-a6d782c8e330" satisfied condition "Succeeded or Failed"
Jun 22 10:56:22.063: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-638e8ae5-0712-4cc5-97f1-a6d782c8e330 container test-container: <nil>
STEP: delete the pod 06/22/23 10:56:22.081
Jun 22 10:56:22.099: INFO: Waiting for pod pod-638e8ae5-0712-4cc5-97f1-a6d782c8e330 to disappear
Jun 22 10:56:22.104: INFO: Pod pod-638e8ae5-0712-4cc5-97f1-a6d782c8e330 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun 22 10:56:22.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2356" for this suite. 06/22/23 10:56:22.111
------------------------------
• [4.129 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:56:17.993
    Jun 22 10:56:17.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename emptydir 06/22/23 10:56:17.994
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:18.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:18.028
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 06/22/23 10:56:18.033
    W0622 10:56:18.046627      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:56:18.046: INFO: Waiting up to 5m0s for pod "pod-638e8ae5-0712-4cc5-97f1-a6d782c8e330" in namespace "emptydir-2356" to be "Succeeded or Failed"
    Jun 22 10:56:18.051: INFO: Pod "pod-638e8ae5-0712-4cc5-97f1-a6d782c8e330": Phase="Pending", Reason="", readiness=false. Elapsed: 4.390045ms
    Jun 22 10:56:20.057: INFO: Pod "pod-638e8ae5-0712-4cc5-97f1-a6d782c8e330": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010765458s
    Jun 22 10:56:22.058: INFO: Pod "pod-638e8ae5-0712-4cc5-97f1-a6d782c8e330": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011449948s
    STEP: Saw pod success 06/22/23 10:56:22.058
    Jun 22 10:56:22.058: INFO: Pod "pod-638e8ae5-0712-4cc5-97f1-a6d782c8e330" satisfied condition "Succeeded or Failed"
    Jun 22 10:56:22.063: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-638e8ae5-0712-4cc5-97f1-a6d782c8e330 container test-container: <nil>
    STEP: delete the pod 06/22/23 10:56:22.081
    Jun 22 10:56:22.099: INFO: Waiting for pod pod-638e8ae5-0712-4cc5-97f1-a6d782c8e330 to disappear
    Jun 22 10:56:22.104: INFO: Pod pod-638e8ae5-0712-4cc5-97f1-a6d782c8e330 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:56:22.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2356" for this suite. 06/22/23 10:56:22.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:56:22.124
Jun 22 10:56:22.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename webhook 06/22/23 10:56:22.125
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:22.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:22.152
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/22/23 10:56:22.174
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 10:56:22.768
STEP: Deploying the webhook pod 06/22/23 10:56:22.777
W0622 10:56:22.794343      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 10:56:22.794
Jun 22 10:56:22.806: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/22/23 10:56:24.823
STEP: Verifying the service has paired with the endpoint 06/22/23 10:56:24.842
Jun 22 10:56:25.843: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Jun 22 10:56:25.851: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Registering the custom resource webhook via the AdmissionRegistration API 06/22/23 10:56:26.365
STEP: Creating a custom resource that should be denied by the webhook 06/22/23 10:56:26.388
STEP: Creating a custom resource whose deletion would be denied by the webhook 06/22/23 10:56:28.425
STEP: Updating the custom resource with disallowed data should be denied 06/22/23 10:56:28.434
STEP: Deleting the custom resource should be denied 06/22/23 10:56:28.448
STEP: Remove the offending key and value from the custom resource data 06/22/23 10:56:28.459
STEP: Deleting the updated custom resource should be successful 06/22/23 10:56:28.479
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:56:29.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2595" for this suite. 06/22/23 10:56:29.082
STEP: Destroying namespace "webhook-2595-markers" for this suite. 06/22/23 10:56:29.092
------------------------------
• [SLOW TEST] [6.981 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:56:22.124
    Jun 22 10:56:22.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename webhook 06/22/23 10:56:22.125
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:22.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:22.152
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/22/23 10:56:22.174
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 10:56:22.768
    STEP: Deploying the webhook pod 06/22/23 10:56:22.777
    W0622 10:56:22.794343      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 10:56:22.794
    Jun 22 10:56:22.806: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/22/23 10:56:24.823
    STEP: Verifying the service has paired with the endpoint 06/22/23 10:56:24.842
    Jun 22 10:56:25.843: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Jun 22 10:56:25.851: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 06/22/23 10:56:26.365
    STEP: Creating a custom resource that should be denied by the webhook 06/22/23 10:56:26.388
    STEP: Creating a custom resource whose deletion would be denied by the webhook 06/22/23 10:56:28.425
    STEP: Updating the custom resource with disallowed data should be denied 06/22/23 10:56:28.434
    STEP: Deleting the custom resource should be denied 06/22/23 10:56:28.448
    STEP: Remove the offending key and value from the custom resource data 06/22/23 10:56:28.459
    STEP: Deleting the updated custom resource should be successful 06/22/23 10:56:28.479
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:56:29.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2595" for this suite. 06/22/23 10:56:29.082
    STEP: Destroying namespace "webhook-2595-markers" for this suite. 06/22/23 10:56:29.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:56:29.11
Jun 22 10:56:29.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename containers 06/22/23 10:56:29.111
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:29.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:29.138
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 06/22/23 10:56:29.142
W0622 10:56:29.154891      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:56:29.155: INFO: Waiting up to 5m0s for pod "client-containers-00c03992-50ac-408d-9ae8-3f42718ed915" in namespace "containers-4718" to be "Succeeded or Failed"
Jun 22 10:56:29.160: INFO: Pod "client-containers-00c03992-50ac-408d-9ae8-3f42718ed915": Phase="Pending", Reason="", readiness=false. Elapsed: 5.505981ms
Jun 22 10:56:31.166: INFO: Pod "client-containers-00c03992-50ac-408d-9ae8-3f42718ed915": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011725601s
Jun 22 10:56:33.168: INFO: Pod "client-containers-00c03992-50ac-408d-9ae8-3f42718ed915": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013376728s
STEP: Saw pod success 06/22/23 10:56:33.168
Jun 22 10:56:33.168: INFO: Pod "client-containers-00c03992-50ac-408d-9ae8-3f42718ed915" satisfied condition "Succeeded or Failed"
Jun 22 10:56:33.174: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod client-containers-00c03992-50ac-408d-9ae8-3f42718ed915 container agnhost-container: <nil>
STEP: delete the pod 06/22/23 10:56:33.184
Jun 22 10:56:33.201: INFO: Waiting for pod client-containers-00c03992-50ac-408d-9ae8-3f42718ed915 to disappear
Jun 22 10:56:33.206: INFO: Pod client-containers-00c03992-50ac-408d-9ae8-3f42718ed915 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jun 22 10:56:33.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4718" for this suite. 06/22/23 10:56:33.212
------------------------------
• [4.111 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:56:29.11
    Jun 22 10:56:29.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename containers 06/22/23 10:56:29.111
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:29.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:29.138
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 06/22/23 10:56:29.142
    W0622 10:56:29.154891      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:56:29.155: INFO: Waiting up to 5m0s for pod "client-containers-00c03992-50ac-408d-9ae8-3f42718ed915" in namespace "containers-4718" to be "Succeeded or Failed"
    Jun 22 10:56:29.160: INFO: Pod "client-containers-00c03992-50ac-408d-9ae8-3f42718ed915": Phase="Pending", Reason="", readiness=false. Elapsed: 5.505981ms
    Jun 22 10:56:31.166: INFO: Pod "client-containers-00c03992-50ac-408d-9ae8-3f42718ed915": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011725601s
    Jun 22 10:56:33.168: INFO: Pod "client-containers-00c03992-50ac-408d-9ae8-3f42718ed915": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013376728s
    STEP: Saw pod success 06/22/23 10:56:33.168
    Jun 22 10:56:33.168: INFO: Pod "client-containers-00c03992-50ac-408d-9ae8-3f42718ed915" satisfied condition "Succeeded or Failed"
    Jun 22 10:56:33.174: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod client-containers-00c03992-50ac-408d-9ae8-3f42718ed915 container agnhost-container: <nil>
    STEP: delete the pod 06/22/23 10:56:33.184
    Jun 22 10:56:33.201: INFO: Waiting for pod client-containers-00c03992-50ac-408d-9ae8-3f42718ed915 to disappear
    Jun 22 10:56:33.206: INFO: Pod client-containers-00c03992-50ac-408d-9ae8-3f42718ed915 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:56:33.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4718" for this suite. 06/22/23 10:56:33.212
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:56:33.226
Jun 22 10:56:33.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename emptydir 06/22/23 10:56:33.227
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:33.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:33.254
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 06/22/23 10:56:33.258
W0622 10:56:33.271630      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:56:33.271: INFO: Waiting up to 5m0s for pod "pod-400b9fce-1617-4b37-b657-dd1edc31b941" in namespace "emptydir-3834" to be "Succeeded or Failed"
Jun 22 10:56:33.276: INFO: Pod "pod-400b9fce-1617-4b37-b657-dd1edc31b941": Phase="Pending", Reason="", readiness=false. Elapsed: 5.11176ms
Jun 22 10:56:35.283: INFO: Pod "pod-400b9fce-1617-4b37-b657-dd1edc31b941": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012097245s
Jun 22 10:56:37.316: INFO: Pod "pod-400b9fce-1617-4b37-b657-dd1edc31b941": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04424051s
STEP: Saw pod success 06/22/23 10:56:37.317
Jun 22 10:56:37.323: INFO: Pod "pod-400b9fce-1617-4b37-b657-dd1edc31b941" satisfied condition "Succeeded or Failed"
Jun 22 10:56:37.332: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-400b9fce-1617-4b37-b657-dd1edc31b941 container test-container: <nil>
STEP: delete the pod 06/22/23 10:56:37.344
Jun 22 10:56:37.364: INFO: Waiting for pod pod-400b9fce-1617-4b37-b657-dd1edc31b941 to disappear
Jun 22 10:56:37.371: INFO: Pod pod-400b9fce-1617-4b37-b657-dd1edc31b941 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun 22 10:56:37.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3834" for this suite. 06/22/23 10:56:37.391
------------------------------
• [4.176 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:56:33.226
    Jun 22 10:56:33.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename emptydir 06/22/23 10:56:33.227
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:33.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:33.254
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 06/22/23 10:56:33.258
    W0622 10:56:33.271630      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:56:33.271: INFO: Waiting up to 5m0s for pod "pod-400b9fce-1617-4b37-b657-dd1edc31b941" in namespace "emptydir-3834" to be "Succeeded or Failed"
    Jun 22 10:56:33.276: INFO: Pod "pod-400b9fce-1617-4b37-b657-dd1edc31b941": Phase="Pending", Reason="", readiness=false. Elapsed: 5.11176ms
    Jun 22 10:56:35.283: INFO: Pod "pod-400b9fce-1617-4b37-b657-dd1edc31b941": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012097245s
    Jun 22 10:56:37.316: INFO: Pod "pod-400b9fce-1617-4b37-b657-dd1edc31b941": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04424051s
    STEP: Saw pod success 06/22/23 10:56:37.317
    Jun 22 10:56:37.323: INFO: Pod "pod-400b9fce-1617-4b37-b657-dd1edc31b941" satisfied condition "Succeeded or Failed"
    Jun 22 10:56:37.332: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-400b9fce-1617-4b37-b657-dd1edc31b941 container test-container: <nil>
    STEP: delete the pod 06/22/23 10:56:37.344
    Jun 22 10:56:37.364: INFO: Waiting for pod pod-400b9fce-1617-4b37-b657-dd1edc31b941 to disappear
    Jun 22 10:56:37.371: INFO: Pod pod-400b9fce-1617-4b37-b657-dd1edc31b941 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:56:37.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3834" for this suite. 06/22/23 10:56:37.391
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:56:37.409
Jun 22 10:56:37.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename runtimeclass 06/22/23 10:56:37.424
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:37.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:37.5
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jun 22 10:56:37.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7266" for this suite. 06/22/23 10:56:37.532
------------------------------
• [0.135 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:56:37.409
    Jun 22 10:56:37.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename runtimeclass 06/22/23 10:56:37.424
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:37.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:37.5
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:56:37.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7266" for this suite. 06/22/23 10:56:37.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:56:37.56
Jun 22 10:56:37.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename proxy 06/22/23 10:56:37.563
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:37.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:37.596
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jun 22 10:56:37.601: INFO: Creating pod...
W0622 10:56:37.614418      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:56:37.614: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7272" to be "running"
Jun 22 10:56:37.619: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.639763ms
Jun 22 10:56:39.631: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.016299586s
Jun 22 10:56:39.631: INFO: Pod "agnhost" satisfied condition "running"
Jun 22 10:56:39.631: INFO: Creating service...
Jun 22 10:56:39.653: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/pods/agnhost/proxy/some/path/with/DELETE
Jun 22 10:56:39.665: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 22 10:56:39.665: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/pods/agnhost/proxy/some/path/with/GET
Jun 22 10:56:39.672: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun 22 10:56:39.672: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/pods/agnhost/proxy/some/path/with/HEAD
Jun 22 10:56:39.683: INFO: http.Client request:HEAD | StatusCode:200
Jun 22 10:56:39.684: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/pods/agnhost/proxy/some/path/with/OPTIONS
Jun 22 10:56:39.690: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 22 10:56:39.690: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/pods/agnhost/proxy/some/path/with/PATCH
Jun 22 10:56:39.695: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 22 10:56:39.695: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/pods/agnhost/proxy/some/path/with/POST
Jun 22 10:56:39.703: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 22 10:56:39.703: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/pods/agnhost/proxy/some/path/with/PUT
Jun 22 10:56:39.709: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun 22 10:56:39.709: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/services/test-service/proxy/some/path/with/DELETE
Jun 22 10:56:39.718: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 22 10:56:39.718: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/services/test-service/proxy/some/path/with/GET
Jun 22 10:56:39.728: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun 22 10:56:39.728: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/services/test-service/proxy/some/path/with/HEAD
Jun 22 10:56:39.736: INFO: http.Client request:HEAD | StatusCode:200
Jun 22 10:56:39.736: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/services/test-service/proxy/some/path/with/OPTIONS
Jun 22 10:56:39.746: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 22 10:56:39.746: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/services/test-service/proxy/some/path/with/PATCH
Jun 22 10:56:39.755: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 22 10:56:39.755: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/services/test-service/proxy/some/path/with/POST
Jun 22 10:56:39.763: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 22 10:56:39.763: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/services/test-service/proxy/some/path/with/PUT
Jun 22 10:56:39.773: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jun 22 10:56:39.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-7272" for this suite. 06/22/23 10:56:39.788
------------------------------
• [2.239 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:56:37.56
    Jun 22 10:56:37.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename proxy 06/22/23 10:56:37.563
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:37.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:37.596
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jun 22 10:56:37.601: INFO: Creating pod...
    W0622 10:56:37.614418      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:56:37.614: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7272" to be "running"
    Jun 22 10:56:37.619: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.639763ms
    Jun 22 10:56:39.631: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.016299586s
    Jun 22 10:56:39.631: INFO: Pod "agnhost" satisfied condition "running"
    Jun 22 10:56:39.631: INFO: Creating service...
    Jun 22 10:56:39.653: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/pods/agnhost/proxy/some/path/with/DELETE
    Jun 22 10:56:39.665: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun 22 10:56:39.665: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/pods/agnhost/proxy/some/path/with/GET
    Jun 22 10:56:39.672: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jun 22 10:56:39.672: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/pods/agnhost/proxy/some/path/with/HEAD
    Jun 22 10:56:39.683: INFO: http.Client request:HEAD | StatusCode:200
    Jun 22 10:56:39.684: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/pods/agnhost/proxy/some/path/with/OPTIONS
    Jun 22 10:56:39.690: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun 22 10:56:39.690: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/pods/agnhost/proxy/some/path/with/PATCH
    Jun 22 10:56:39.695: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun 22 10:56:39.695: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/pods/agnhost/proxy/some/path/with/POST
    Jun 22 10:56:39.703: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun 22 10:56:39.703: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/pods/agnhost/proxy/some/path/with/PUT
    Jun 22 10:56:39.709: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jun 22 10:56:39.709: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/services/test-service/proxy/some/path/with/DELETE
    Jun 22 10:56:39.718: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun 22 10:56:39.718: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/services/test-service/proxy/some/path/with/GET
    Jun 22 10:56:39.728: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jun 22 10:56:39.728: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/services/test-service/proxy/some/path/with/HEAD
    Jun 22 10:56:39.736: INFO: http.Client request:HEAD | StatusCode:200
    Jun 22 10:56:39.736: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/services/test-service/proxy/some/path/with/OPTIONS
    Jun 22 10:56:39.746: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun 22 10:56:39.746: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/services/test-service/proxy/some/path/with/PATCH
    Jun 22 10:56:39.755: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun 22 10:56:39.755: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/services/test-service/proxy/some/path/with/POST
    Jun 22 10:56:39.763: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun 22 10:56:39.763: INFO: Starting http.Client for https://100.64.0.1:443/api/v1/namespaces/proxy-7272/services/test-service/proxy/some/path/with/PUT
    Jun 22 10:56:39.773: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:56:39.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-7272" for this suite. 06/22/23 10:56:39.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:56:39.805
Jun 22 10:56:39.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 10:56:39.806
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:39.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:39.838
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-8944a5bc-39d2-4d53-b776-de9a97901ad5 06/22/23 10:56:39.843
STEP: Creating a pod to test consume configMaps 06/22/23 10:56:39.854
W0622 10:56:39.867222      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "projected-configmap-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-configmap-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-configmap-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-configmap-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:56:39.867: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-614b70ce-ad8f-414b-aa1f-89d5c1d64460" in namespace "projected-4599" to be "Succeeded or Failed"
Jun 22 10:56:39.873: INFO: Pod "pod-projected-configmaps-614b70ce-ad8f-414b-aa1f-89d5c1d64460": Phase="Pending", Reason="", readiness=false. Elapsed: 5.894331ms
Jun 22 10:56:41.881: INFO: Pod "pod-projected-configmaps-614b70ce-ad8f-414b-aa1f-89d5c1d64460": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013715176s
Jun 22 10:56:43.880: INFO: Pod "pod-projected-configmaps-614b70ce-ad8f-414b-aa1f-89d5c1d64460": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013208669s
STEP: Saw pod success 06/22/23 10:56:43.88
Jun 22 10:56:43.881: INFO: Pod "pod-projected-configmaps-614b70ce-ad8f-414b-aa1f-89d5c1d64460" satisfied condition "Succeeded or Failed"
Jun 22 10:56:43.886: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-configmaps-614b70ce-ad8f-414b-aa1f-89d5c1d64460 container projected-configmap-volume-test: <nil>
STEP: delete the pod 06/22/23 10:56:43.9
Jun 22 10:56:43.917: INFO: Waiting for pod pod-projected-configmaps-614b70ce-ad8f-414b-aa1f-89d5c1d64460 to disappear
Jun 22 10:56:43.921: INFO: Pod pod-projected-configmaps-614b70ce-ad8f-414b-aa1f-89d5c1d64460 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun 22 10:56:43.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4599" for this suite. 06/22/23 10:56:43.93
------------------------------
• [4.135 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:56:39.805
    Jun 22 10:56:39.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 10:56:39.806
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:39.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:39.838
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-8944a5bc-39d2-4d53-b776-de9a97901ad5 06/22/23 10:56:39.843
    STEP: Creating a pod to test consume configMaps 06/22/23 10:56:39.854
    W0622 10:56:39.867222      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "projected-configmap-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-configmap-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-configmap-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-configmap-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:56:39.867: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-614b70ce-ad8f-414b-aa1f-89d5c1d64460" in namespace "projected-4599" to be "Succeeded or Failed"
    Jun 22 10:56:39.873: INFO: Pod "pod-projected-configmaps-614b70ce-ad8f-414b-aa1f-89d5c1d64460": Phase="Pending", Reason="", readiness=false. Elapsed: 5.894331ms
    Jun 22 10:56:41.881: INFO: Pod "pod-projected-configmaps-614b70ce-ad8f-414b-aa1f-89d5c1d64460": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013715176s
    Jun 22 10:56:43.880: INFO: Pod "pod-projected-configmaps-614b70ce-ad8f-414b-aa1f-89d5c1d64460": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013208669s
    STEP: Saw pod success 06/22/23 10:56:43.88
    Jun 22 10:56:43.881: INFO: Pod "pod-projected-configmaps-614b70ce-ad8f-414b-aa1f-89d5c1d64460" satisfied condition "Succeeded or Failed"
    Jun 22 10:56:43.886: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-configmaps-614b70ce-ad8f-414b-aa1f-89d5c1d64460 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 06/22/23 10:56:43.9
    Jun 22 10:56:43.917: INFO: Waiting for pod pod-projected-configmaps-614b70ce-ad8f-414b-aa1f-89d5c1d64460 to disappear
    Jun 22 10:56:43.921: INFO: Pod pod-projected-configmaps-614b70ce-ad8f-414b-aa1f-89d5c1d64460 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:56:43.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4599" for this suite. 06/22/23 10:56:43.93
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:56:43.943
Jun 22 10:56:43.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename runtimeclass 06/22/23 10:56:43.945
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:43.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:43.971
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
W0622 10:56:43.995283      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:56:43.995: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8792 to be scheduled
Jun 22 10:56:44.000: INFO: 1 pods are not scheduled: [runtimeclass-8792/test-runtimeclass-runtimeclass-8792-preconfigured-handler-j2ldb(29259c47-34d7-40e9-a76e-1b5145a1f736)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jun 22 10:56:46.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8792" for this suite. 06/22/23 10:56:46.028
------------------------------
• [2.094 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:56:43.943
    Jun 22 10:56:43.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename runtimeclass 06/22/23 10:56:43.945
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:43.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:43.971
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    W0622 10:56:43.995283      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:56:43.995: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8792 to be scheduled
    Jun 22 10:56:44.000: INFO: 1 pods are not scheduled: [runtimeclass-8792/test-runtimeclass-runtimeclass-8792-preconfigured-handler-j2ldb(29259c47-34d7-40e9-a76e-1b5145a1f736)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:56:46.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8792" for this suite. 06/22/23 10:56:46.028
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:56:46.038
Jun 22 10:56:46.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 10:56:46.04
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:46.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:46.07
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 06/22/23 10:56:46.075
W0622 10:56:46.089450      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:56:46.089: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e06ca36-fa57-489c-9bfd-95e1696abc5c" in namespace "projected-8668" to be "Succeeded or Failed"
Jun 22 10:56:46.094: INFO: Pod "downwardapi-volume-1e06ca36-fa57-489c-9bfd-95e1696abc5c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.917569ms
Jun 22 10:56:48.102: INFO: Pod "downwardapi-volume-1e06ca36-fa57-489c-9bfd-95e1696abc5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013184162s
Jun 22 10:56:50.102: INFO: Pod "downwardapi-volume-1e06ca36-fa57-489c-9bfd-95e1696abc5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012352274s
STEP: Saw pod success 06/22/23 10:56:50.102
Jun 22 10:56:50.102: INFO: Pod "downwardapi-volume-1e06ca36-fa57-489c-9bfd-95e1696abc5c" satisfied condition "Succeeded or Failed"
Jun 22 10:56:50.107: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-1e06ca36-fa57-489c-9bfd-95e1696abc5c container client-container: <nil>
STEP: delete the pod 06/22/23 10:56:50.118
Jun 22 10:56:50.134: INFO: Waiting for pod downwardapi-volume-1e06ca36-fa57-489c-9bfd-95e1696abc5c to disappear
Jun 22 10:56:50.138: INFO: Pod downwardapi-volume-1e06ca36-fa57-489c-9bfd-95e1696abc5c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun 22 10:56:50.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8668" for this suite. 06/22/23 10:56:50.148
------------------------------
• [4.120 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:56:46.038
    Jun 22 10:56:46.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 10:56:46.04
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:46.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:46.07
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 06/22/23 10:56:46.075
    W0622 10:56:46.089450      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:56:46.089: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e06ca36-fa57-489c-9bfd-95e1696abc5c" in namespace "projected-8668" to be "Succeeded or Failed"
    Jun 22 10:56:46.094: INFO: Pod "downwardapi-volume-1e06ca36-fa57-489c-9bfd-95e1696abc5c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.917569ms
    Jun 22 10:56:48.102: INFO: Pod "downwardapi-volume-1e06ca36-fa57-489c-9bfd-95e1696abc5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013184162s
    Jun 22 10:56:50.102: INFO: Pod "downwardapi-volume-1e06ca36-fa57-489c-9bfd-95e1696abc5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012352274s
    STEP: Saw pod success 06/22/23 10:56:50.102
    Jun 22 10:56:50.102: INFO: Pod "downwardapi-volume-1e06ca36-fa57-489c-9bfd-95e1696abc5c" satisfied condition "Succeeded or Failed"
    Jun 22 10:56:50.107: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-1e06ca36-fa57-489c-9bfd-95e1696abc5c container client-container: <nil>
    STEP: delete the pod 06/22/23 10:56:50.118
    Jun 22 10:56:50.134: INFO: Waiting for pod downwardapi-volume-1e06ca36-fa57-489c-9bfd-95e1696abc5c to disappear
    Jun 22 10:56:50.138: INFO: Pod downwardapi-volume-1e06ca36-fa57-489c-9bfd-95e1696abc5c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:56:50.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8668" for this suite. 06/22/23 10:56:50.148
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:56:50.159
Jun 22 10:56:50.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename security-context-test 06/22/23 10:56:50.161
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:50.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:50.188
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
W0622 10:56:50.206477      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "busybox-user-65534-38cec6f3-bb49-4ab6-b8b2-69ecd70fb4b1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-user-65534-38cec6f3-bb49-4ab6-b8b2-69ecd70fb4b1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-user-65534-38cec6f3-bb49-4ab6-b8b2-69ecd70fb4b1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-user-65534-38cec6f3-bb49-4ab6-b8b2-69ecd70fb4b1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:56:50.206: INFO: Waiting up to 5m0s for pod "busybox-user-65534-38cec6f3-bb49-4ab6-b8b2-69ecd70fb4b1" in namespace "security-context-test-7757" to be "Succeeded or Failed"
Jun 22 10:56:50.212: INFO: Pod "busybox-user-65534-38cec6f3-bb49-4ab6-b8b2-69ecd70fb4b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.751384ms
Jun 22 10:56:52.220: INFO: Pod "busybox-user-65534-38cec6f3-bb49-4ab6-b8b2-69ecd70fb4b1": Phase="Running", Reason="", readiness=false. Elapsed: 2.013543536s
Jun 22 10:56:54.220: INFO: Pod "busybox-user-65534-38cec6f3-bb49-4ab6-b8b2-69ecd70fb4b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013954578s
Jun 22 10:56:54.220: INFO: Pod "busybox-user-65534-38cec6f3-bb49-4ab6-b8b2-69ecd70fb4b1" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jun 22 10:56:54.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7757" for this suite. 06/22/23 10:56:54.23
------------------------------
• [4.081 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:56:50.159
    Jun 22 10:56:50.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename security-context-test 06/22/23 10:56:50.161
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:50.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:50.188
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    W0622 10:56:50.206477      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "busybox-user-65534-38cec6f3-bb49-4ab6-b8b2-69ecd70fb4b1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-user-65534-38cec6f3-bb49-4ab6-b8b2-69ecd70fb4b1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-user-65534-38cec6f3-bb49-4ab6-b8b2-69ecd70fb4b1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-user-65534-38cec6f3-bb49-4ab6-b8b2-69ecd70fb4b1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:56:50.206: INFO: Waiting up to 5m0s for pod "busybox-user-65534-38cec6f3-bb49-4ab6-b8b2-69ecd70fb4b1" in namespace "security-context-test-7757" to be "Succeeded or Failed"
    Jun 22 10:56:50.212: INFO: Pod "busybox-user-65534-38cec6f3-bb49-4ab6-b8b2-69ecd70fb4b1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.751384ms
    Jun 22 10:56:52.220: INFO: Pod "busybox-user-65534-38cec6f3-bb49-4ab6-b8b2-69ecd70fb4b1": Phase="Running", Reason="", readiness=false. Elapsed: 2.013543536s
    Jun 22 10:56:54.220: INFO: Pod "busybox-user-65534-38cec6f3-bb49-4ab6-b8b2-69ecd70fb4b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013954578s
    Jun 22 10:56:54.220: INFO: Pod "busybox-user-65534-38cec6f3-bb49-4ab6-b8b2-69ecd70fb4b1" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:56:54.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7757" for this suite. 06/22/23 10:56:54.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:56:54.246
Jun 22 10:56:54.246: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename emptydir 06/22/23 10:56:54.247
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:54.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:54.273
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 06/22/23 10:56:54.277
W0622 10:56:54.290696      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:56:54.290: INFO: Waiting up to 5m0s for pod "pod-575939ce-60a0-49da-951d-c28aefcd3ab3" in namespace "emptydir-3156" to be "Succeeded or Failed"
Jun 22 10:56:54.296: INFO: Pod "pod-575939ce-60a0-49da-951d-c28aefcd3ab3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.292053ms
Jun 22 10:56:56.304: INFO: Pod "pod-575939ce-60a0-49da-951d-c28aefcd3ab3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013565679s
Jun 22 10:56:58.305: INFO: Pod "pod-575939ce-60a0-49da-951d-c28aefcd3ab3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014035392s
STEP: Saw pod success 06/22/23 10:56:58.305
Jun 22 10:56:58.305: INFO: Pod "pod-575939ce-60a0-49da-951d-c28aefcd3ab3" satisfied condition "Succeeded or Failed"
Jun 22 10:56:58.311: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-575939ce-60a0-49da-951d-c28aefcd3ab3 container test-container: <nil>
STEP: delete the pod 06/22/23 10:56:58.321
Jun 22 10:56:58.341: INFO: Waiting for pod pod-575939ce-60a0-49da-951d-c28aefcd3ab3 to disappear
Jun 22 10:56:58.345: INFO: Pod pod-575939ce-60a0-49da-951d-c28aefcd3ab3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun 22 10:56:58.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3156" for this suite. 06/22/23 10:56:58.361
------------------------------
• [4.127 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:56:54.246
    Jun 22 10:56:54.246: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename emptydir 06/22/23 10:56:54.247
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:54.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:54.273
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 06/22/23 10:56:54.277
    W0622 10:56:54.290696      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:56:54.290: INFO: Waiting up to 5m0s for pod "pod-575939ce-60a0-49da-951d-c28aefcd3ab3" in namespace "emptydir-3156" to be "Succeeded or Failed"
    Jun 22 10:56:54.296: INFO: Pod "pod-575939ce-60a0-49da-951d-c28aefcd3ab3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.292053ms
    Jun 22 10:56:56.304: INFO: Pod "pod-575939ce-60a0-49da-951d-c28aefcd3ab3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013565679s
    Jun 22 10:56:58.305: INFO: Pod "pod-575939ce-60a0-49da-951d-c28aefcd3ab3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014035392s
    STEP: Saw pod success 06/22/23 10:56:58.305
    Jun 22 10:56:58.305: INFO: Pod "pod-575939ce-60a0-49da-951d-c28aefcd3ab3" satisfied condition "Succeeded or Failed"
    Jun 22 10:56:58.311: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-575939ce-60a0-49da-951d-c28aefcd3ab3 container test-container: <nil>
    STEP: delete the pod 06/22/23 10:56:58.321
    Jun 22 10:56:58.341: INFO: Waiting for pod pod-575939ce-60a0-49da-951d-c28aefcd3ab3 to disappear
    Jun 22 10:56:58.345: INFO: Pod pod-575939ce-60a0-49da-951d-c28aefcd3ab3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:56:58.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3156" for this suite. 06/22/23 10:56:58.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:56:58.373
Jun 22 10:56:58.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 10:56:58.375
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:58.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:58.402
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-b68b5acd-ca62-498a-b9e9-65b4920a809e 06/22/23 10:56:58.406
STEP: Creating a pod to test consume configMaps 06/22/23 10:56:58.413
W0622 10:56:58.428383      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:56:58.428: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-997af3a0-020d-4265-ad80-794c240cf5ab" in namespace "projected-8306" to be "Succeeded or Failed"
Jun 22 10:56:58.434: INFO: Pod "pod-projected-configmaps-997af3a0-020d-4265-ad80-794c240cf5ab": Phase="Pending", Reason="", readiness=false. Elapsed: 5.615565ms
Jun 22 10:57:00.441: INFO: Pod "pod-projected-configmaps-997af3a0-020d-4265-ad80-794c240cf5ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012522415s
Jun 22 10:57:02.446: INFO: Pod "pod-projected-configmaps-997af3a0-020d-4265-ad80-794c240cf5ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017790631s
STEP: Saw pod success 06/22/23 10:57:02.446
Jun 22 10:57:02.446: INFO: Pod "pod-projected-configmaps-997af3a0-020d-4265-ad80-794c240cf5ab" satisfied condition "Succeeded or Failed"
Jun 22 10:57:02.452: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-configmaps-997af3a0-020d-4265-ad80-794c240cf5ab container agnhost-container: <nil>
STEP: delete the pod 06/22/23 10:57:02.464
Jun 22 10:57:02.480: INFO: Waiting for pod pod-projected-configmaps-997af3a0-020d-4265-ad80-794c240cf5ab to disappear
Jun 22 10:57:02.487: INFO: Pod pod-projected-configmaps-997af3a0-020d-4265-ad80-794c240cf5ab no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun 22 10:57:02.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8306" for this suite. 06/22/23 10:57:02.495
------------------------------
• [4.137 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:56:58.373
    Jun 22 10:56:58.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 10:56:58.375
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:56:58.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:56:58.402
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-b68b5acd-ca62-498a-b9e9-65b4920a809e 06/22/23 10:56:58.406
    STEP: Creating a pod to test consume configMaps 06/22/23 10:56:58.413
    W0622 10:56:58.428383      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:56:58.428: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-997af3a0-020d-4265-ad80-794c240cf5ab" in namespace "projected-8306" to be "Succeeded or Failed"
    Jun 22 10:56:58.434: INFO: Pod "pod-projected-configmaps-997af3a0-020d-4265-ad80-794c240cf5ab": Phase="Pending", Reason="", readiness=false. Elapsed: 5.615565ms
    Jun 22 10:57:00.441: INFO: Pod "pod-projected-configmaps-997af3a0-020d-4265-ad80-794c240cf5ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012522415s
    Jun 22 10:57:02.446: INFO: Pod "pod-projected-configmaps-997af3a0-020d-4265-ad80-794c240cf5ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017790631s
    STEP: Saw pod success 06/22/23 10:57:02.446
    Jun 22 10:57:02.446: INFO: Pod "pod-projected-configmaps-997af3a0-020d-4265-ad80-794c240cf5ab" satisfied condition "Succeeded or Failed"
    Jun 22 10:57:02.452: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-configmaps-997af3a0-020d-4265-ad80-794c240cf5ab container agnhost-container: <nil>
    STEP: delete the pod 06/22/23 10:57:02.464
    Jun 22 10:57:02.480: INFO: Waiting for pod pod-projected-configmaps-997af3a0-020d-4265-ad80-794c240cf5ab to disappear
    Jun 22 10:57:02.487: INFO: Pod pod-projected-configmaps-997af3a0-020d-4265-ad80-794c240cf5ab no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:57:02.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8306" for this suite. 06/22/23 10:57:02.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:57:02.513
Jun 22 10:57:02.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename custom-resource-definition 06/22/23 10:57:02.514
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:57:02.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:57:02.546
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jun 22 10:57:02.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:57:05.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-2100" for this suite. 06/22/23 10:57:05.735
------------------------------
• [3.233 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:57:02.513
    Jun 22 10:57:02.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename custom-resource-definition 06/22/23 10:57:02.514
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:57:02.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:57:02.546
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jun 22 10:57:02.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:57:05.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-2100" for this suite. 06/22/23 10:57:05.735
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:57:05.748
Jun 22 10:57:05.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename limitrange 06/22/23 10:57:05.749
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:57:05.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:57:05.775
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 06/22/23 10:57:05.78
STEP: Setting up watch 06/22/23 10:57:05.78
STEP: Submitting a LimitRange 06/22/23 10:57:05.885
STEP: Verifying LimitRange creation was observed 06/22/23 10:57:05.894
STEP: Fetching the LimitRange to ensure it has proper values 06/22/23 10:57:05.895
Jun 22 10:57:05.909: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 22 10:57:05.910: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 06/22/23 10:57:05.91
W0622 10:57:05.975802      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring Pod has resource requirements applied from LimitRange 06/22/23 10:57:05.975
Jun 22 10:57:05.982: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 22 10:57:05.982: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 06/22/23 10:57:05.982
W0622 10:57:05.996178      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 06/22/23 10:57:05.996
Jun 22 10:57:06.001: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jun 22 10:57:06.001: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 06/22/23 10:57:06.001
STEP: Failing to create a Pod with more than max resources 06/22/23 10:57:06.004
STEP: Updating a LimitRange 06/22/23 10:57:06.007
STEP: Verifying LimitRange updating is effective 06/22/23 10:57:06.025
STEP: Creating a Pod with less than former min resources 06/22/23 10:57:08.032
W0622 10:57:08.040784      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Failing to create a Pod with more than max resources 06/22/23 10:57:08.04
STEP: Deleting a LimitRange 06/22/23 10:57:08.044
STEP: Verifying the LimitRange was deleted 06/22/23 10:57:08.055
Jun 22 10:57:13.061: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 06/22/23 10:57:13.061
W0622 10:57:13.074143      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jun 22 10:57:13.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-9803" for this suite. 06/22/23 10:57:13.081
------------------------------
• [SLOW TEST] [7.343 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:57:05.748
    Jun 22 10:57:05.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename limitrange 06/22/23 10:57:05.749
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:57:05.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:57:05.775
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 06/22/23 10:57:05.78
    STEP: Setting up watch 06/22/23 10:57:05.78
    STEP: Submitting a LimitRange 06/22/23 10:57:05.885
    STEP: Verifying LimitRange creation was observed 06/22/23 10:57:05.894
    STEP: Fetching the LimitRange to ensure it has proper values 06/22/23 10:57:05.895
    Jun 22 10:57:05.909: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jun 22 10:57:05.910: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 06/22/23 10:57:05.91
    W0622 10:57:05.975802      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring Pod has resource requirements applied from LimitRange 06/22/23 10:57:05.975
    Jun 22 10:57:05.982: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jun 22 10:57:05.982: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 06/22/23 10:57:05.982
    W0622 10:57:05.996178      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 06/22/23 10:57:05.996
    Jun 22 10:57:06.001: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jun 22 10:57:06.001: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 06/22/23 10:57:06.001
    STEP: Failing to create a Pod with more than max resources 06/22/23 10:57:06.004
    STEP: Updating a LimitRange 06/22/23 10:57:06.007
    STEP: Verifying LimitRange updating is effective 06/22/23 10:57:06.025
    STEP: Creating a Pod with less than former min resources 06/22/23 10:57:08.032
    W0622 10:57:08.040784      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Failing to create a Pod with more than max resources 06/22/23 10:57:08.04
    STEP: Deleting a LimitRange 06/22/23 10:57:08.044
    STEP: Verifying the LimitRange was deleted 06/22/23 10:57:08.055
    Jun 22 10:57:13.061: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 06/22/23 10:57:13.061
    W0622 10:57:13.074143      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:57:13.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-9803" for this suite. 06/22/23 10:57:13.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:57:13.101
Jun 22 10:57:13.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename daemonsets 06/22/23 10:57:13.103
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:57:13.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:57:13.129
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Jun 22 10:57:13.161: INFO: Creating daemon "daemon-set" with a node selector
W0622 10:57:13.169900      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Initially, daemon pods should not be running on any nodes. 06/22/23 10:57:13.17
Jun 22 10:57:13.174: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 10:57:13.174: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 06/22/23 10:57:13.174
Jun 22 10:57:13.204: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 10:57:13.204: INFO: Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw is running 0 daemon pod, expected 1
Jun 22 10:57:14.211: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 22 10:57:14.211: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 06/22/23 10:57:14.217
Jun 22 10:57:14.249: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 22 10:57:14.249: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jun 22 10:57:15.255: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 10:57:15.255: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 06/22/23 10:57:15.255
W0622 10:57:15.266283      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:57:15.273: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 10:57:15.273: INFO: Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw is running 0 daemon pod, expected 1
Jun 22 10:57:16.281: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 10:57:16.281: INFO: Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw is running 0 daemon pod, expected 1
Jun 22 10:57:17.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 10:57:17.280: INFO: Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw is running 0 daemon pod, expected 1
Jun 22 10:57:18.281: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 22 10:57:18.282: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 06/22/23 10:57:18.295
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4697, will wait for the garbage collector to delete the pods 06/22/23 10:57:18.295
Jun 22 10:57:18.362: INFO: Deleting DaemonSet.extensions daemon-set took: 10.133623ms
Jun 22 10:57:18.463: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.114134ms
Jun 22 10:57:21.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 10:57:21.070: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 22 10:57:21.075: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"142540"},"items":null}

Jun 22 10:57:21.080: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"142540"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:57:21.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4697" for this suite. 06/22/23 10:57:21.131
------------------------------
• [SLOW TEST] [8.039 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:57:13.101
    Jun 22 10:57:13.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename daemonsets 06/22/23 10:57:13.103
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:57:13.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:57:13.129
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Jun 22 10:57:13.161: INFO: Creating daemon "daemon-set" with a node selector
    W0622 10:57:13.169900      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Initially, daemon pods should not be running on any nodes. 06/22/23 10:57:13.17
    Jun 22 10:57:13.174: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 10:57:13.174: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 06/22/23 10:57:13.174
    Jun 22 10:57:13.204: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 10:57:13.204: INFO: Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw is running 0 daemon pod, expected 1
    Jun 22 10:57:14.211: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 22 10:57:14.211: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 06/22/23 10:57:14.217
    Jun 22 10:57:14.249: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 22 10:57:14.249: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jun 22 10:57:15.255: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 10:57:15.255: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 06/22/23 10:57:15.255
    W0622 10:57:15.266283      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:57:15.273: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 10:57:15.273: INFO: Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw is running 0 daemon pod, expected 1
    Jun 22 10:57:16.281: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 10:57:16.281: INFO: Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw is running 0 daemon pod, expected 1
    Jun 22 10:57:17.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 10:57:17.280: INFO: Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw is running 0 daemon pod, expected 1
    Jun 22 10:57:18.281: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 22 10:57:18.282: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 06/22/23 10:57:18.295
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4697, will wait for the garbage collector to delete the pods 06/22/23 10:57:18.295
    Jun 22 10:57:18.362: INFO: Deleting DaemonSet.extensions daemon-set took: 10.133623ms
    Jun 22 10:57:18.463: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.114134ms
    Jun 22 10:57:21.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 10:57:21.070: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 22 10:57:21.075: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"142540"},"items":null}

    Jun 22 10:57:21.080: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"142540"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:57:21.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4697" for this suite. 06/22/23 10:57:21.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:57:21.142
Jun 22 10:57:21.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename security-context 06/22/23 10:57:21.143
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:57:21.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:57:21.169
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/22/23 10:57:21.174
W0622 10:57:21.187452      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 10:57:21.187: INFO: Waiting up to 5m0s for pod "security-context-fbd7b800-537e-428b-99b5-9d5aa8aed523" in namespace "security-context-4181" to be "Succeeded or Failed"
Jun 22 10:57:21.194: INFO: Pod "security-context-fbd7b800-537e-428b-99b5-9d5aa8aed523": Phase="Pending", Reason="", readiness=false. Elapsed: 7.309475ms
Jun 22 10:57:23.202: INFO: Pod "security-context-fbd7b800-537e-428b-99b5-9d5aa8aed523": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014500311s
Jun 22 10:57:25.201: INFO: Pod "security-context-fbd7b800-537e-428b-99b5-9d5aa8aed523": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013852857s
STEP: Saw pod success 06/22/23 10:57:25.201
Jun 22 10:57:25.201: INFO: Pod "security-context-fbd7b800-537e-428b-99b5-9d5aa8aed523" satisfied condition "Succeeded or Failed"
Jun 22 10:57:25.206: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod security-context-fbd7b800-537e-428b-99b5-9d5aa8aed523 container test-container: <nil>
STEP: delete the pod 06/22/23 10:57:25.215
Jun 22 10:57:25.234: INFO: Waiting for pod security-context-fbd7b800-537e-428b-99b5-9d5aa8aed523 to disappear
Jun 22 10:57:25.239: INFO: Pod security-context-fbd7b800-537e-428b-99b5-9d5aa8aed523 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jun 22 10:57:25.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-4181" for this suite. 06/22/23 10:57:25.247
------------------------------
• [4.115 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:57:21.142
    Jun 22 10:57:21.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename security-context 06/22/23 10:57:21.143
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:57:21.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:57:21.169
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/22/23 10:57:21.174
    W0622 10:57:21.187452      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 10:57:21.187: INFO: Waiting up to 5m0s for pod "security-context-fbd7b800-537e-428b-99b5-9d5aa8aed523" in namespace "security-context-4181" to be "Succeeded or Failed"
    Jun 22 10:57:21.194: INFO: Pod "security-context-fbd7b800-537e-428b-99b5-9d5aa8aed523": Phase="Pending", Reason="", readiness=false. Elapsed: 7.309475ms
    Jun 22 10:57:23.202: INFO: Pod "security-context-fbd7b800-537e-428b-99b5-9d5aa8aed523": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014500311s
    Jun 22 10:57:25.201: INFO: Pod "security-context-fbd7b800-537e-428b-99b5-9d5aa8aed523": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013852857s
    STEP: Saw pod success 06/22/23 10:57:25.201
    Jun 22 10:57:25.201: INFO: Pod "security-context-fbd7b800-537e-428b-99b5-9d5aa8aed523" satisfied condition "Succeeded or Failed"
    Jun 22 10:57:25.206: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod security-context-fbd7b800-537e-428b-99b5-9d5aa8aed523 container test-container: <nil>
    STEP: delete the pod 06/22/23 10:57:25.215
    Jun 22 10:57:25.234: INFO: Waiting for pod security-context-fbd7b800-537e-428b-99b5-9d5aa8aed523 to disappear
    Jun 22 10:57:25.239: INFO: Pod security-context-fbd7b800-537e-428b-99b5-9d5aa8aed523 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:57:25.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-4181" for this suite. 06/22/23 10:57:25.247
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:57:25.264
Jun 22 10:57:25.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename crd-publish-openapi 06/22/23 10:57:25.265
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:57:25.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:57:25.291
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Jun 22 10:57:25.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/22/23 10:57:28.436
Jun 22 10:57:28.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-1172 --namespace=crd-publish-openapi-1172 create -f -'
Jun 22 10:57:30.131: INFO: stderr: ""
Jun 22 10:57:30.131: INFO: stdout: "e2e-test-crd-publish-openapi-5382-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 22 10:57:30.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-1172 --namespace=crd-publish-openapi-1172 delete e2e-test-crd-publish-openapi-5382-crds test-cr'
Jun 22 10:57:30.251: INFO: stderr: ""
Jun 22 10:57:30.251: INFO: stdout: "e2e-test-crd-publish-openapi-5382-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun 22 10:57:30.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-1172 --namespace=crd-publish-openapi-1172 apply -f -'
Jun 22 10:57:30.642: INFO: stderr: ""
Jun 22 10:57:30.642: INFO: stdout: "e2e-test-crd-publish-openapi-5382-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 22 10:57:30.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-1172 --namespace=crd-publish-openapi-1172 delete e2e-test-crd-publish-openapi-5382-crds test-cr'
Jun 22 10:57:30.762: INFO: stderr: ""
Jun 22 10:57:30.762: INFO: stdout: "e2e-test-crd-publish-openapi-5382-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 06/22/23 10:57:30.762
Jun 22 10:57:30.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-1172 explain e2e-test-crd-publish-openapi-5382-crds'
Jun 22 10:57:31.091: INFO: stderr: ""
Jun 22 10:57:31.091: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5382-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 10:57:33.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1172" for this suite. 06/22/23 10:57:33.579
------------------------------
• [SLOW TEST] [8.322 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:57:25.264
    Jun 22 10:57:25.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename crd-publish-openapi 06/22/23 10:57:25.265
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:57:25.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:57:25.291
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Jun 22 10:57:25.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/22/23 10:57:28.436
    Jun 22 10:57:28.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-1172 --namespace=crd-publish-openapi-1172 create -f -'
    Jun 22 10:57:30.131: INFO: stderr: ""
    Jun 22 10:57:30.131: INFO: stdout: "e2e-test-crd-publish-openapi-5382-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jun 22 10:57:30.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-1172 --namespace=crd-publish-openapi-1172 delete e2e-test-crd-publish-openapi-5382-crds test-cr'
    Jun 22 10:57:30.251: INFO: stderr: ""
    Jun 22 10:57:30.251: INFO: stdout: "e2e-test-crd-publish-openapi-5382-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jun 22 10:57:30.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-1172 --namespace=crd-publish-openapi-1172 apply -f -'
    Jun 22 10:57:30.642: INFO: stderr: ""
    Jun 22 10:57:30.642: INFO: stdout: "e2e-test-crd-publish-openapi-5382-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jun 22 10:57:30.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-1172 --namespace=crd-publish-openapi-1172 delete e2e-test-crd-publish-openapi-5382-crds test-cr'
    Jun 22 10:57:30.762: INFO: stderr: ""
    Jun 22 10:57:30.762: INFO: stdout: "e2e-test-crd-publish-openapi-5382-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 06/22/23 10:57:30.762
    Jun 22 10:57:30.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-1172 explain e2e-test-crd-publish-openapi-5382-crds'
    Jun 22 10:57:31.091: INFO: stderr: ""
    Jun 22 10:57:31.091: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5382-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 10:57:33.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1172" for this suite. 06/22/23 10:57:33.579
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 10:57:33.587
Jun 22 10:57:33.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename cronjob 06/22/23 10:57:33.588
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:57:33.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:57:33.607
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 06/22/23 10:57:33.61
W0622 10:57:33.618885      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a job is scheduled 06/22/23 10:57:33.619
STEP: Ensuring exactly one is scheduled 06/22/23 10:58:01.627
STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/22/23 10:58:01.633
STEP: Ensuring no more jobs are scheduled 06/22/23 10:58:01.637
STEP: Removing cronjob 06/22/23 11:03:01.65
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jun 22 11:03:01.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8930" for this suite. 06/22/23 11:03:01.675
------------------------------
• [SLOW TEST] [328.105 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 10:57:33.587
    Jun 22 10:57:33.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename cronjob 06/22/23 10:57:33.588
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 10:57:33.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 10:57:33.607
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 06/22/23 10:57:33.61
    W0622 10:57:33.618885      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring a job is scheduled 06/22/23 10:57:33.619
    STEP: Ensuring exactly one is scheduled 06/22/23 10:58:01.627
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/22/23 10:58:01.633
    STEP: Ensuring no more jobs are scheduled 06/22/23 10:58:01.637
    STEP: Removing cronjob 06/22/23 11:03:01.65
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:03:01.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8930" for this suite. 06/22/23 11:03:01.675
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:03:01.696
Jun 22 11:03:01.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename init-container 06/22/23 11:03:01.703
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:03:01.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:03:01.732
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 06/22/23 11:03:01.738
Jun 22 11:03:01.738: INFO: PodSpec: initContainers in spec.initContainers
W0622 11:03:01.753568      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "init1", "init2", "run1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "init1", "init2", "run1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "init1", "init2", "run1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "init1", "init2", "run1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:03:47.004: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c1f419c8-d7bc-4b70-ab2c-45d45be33b57", GenerateName:"", Namespace:"init-container-7957", SelfLink:"", UID:"a7f80b44-d1e4-4e41-9071-40e48aeb6aa6", ResourceVersion:"144213", Generation:0, CreationTimestamp:time.Date(2023, time.June, 22, 11, 3, 1, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"738632424"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 22, 11, 3, 1, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00643a078), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 22, 11, 3, 46, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00643a0a8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-h9vvl", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00003c020), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-h9vvl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-h9vvl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-h9vvl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0040821d0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000c7c000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004082250)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004082270)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004082278), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00408227c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0008e20a0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 22, 11, 3, 1, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 22, 11, 3, 1, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 22, 11, 3, 1, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 22, 11, 3, 1, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.92.226.162", PodIP:"100.96.3.91", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.96.3.91"}}, StartTime:time.Date(2023, time.June, 22, 11, 3, 1, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c7c0e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c7c150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://a6f377bc3ae6536cedfa2d257da7e875583f5b94b39ed79097277fce6961547f", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00003c700), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00003c5c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0040822ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:03:47.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7957" for this suite. 06/22/23 11:03:47.016
------------------------------
• [SLOW TEST] [45.329 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:03:01.696
    Jun 22 11:03:01.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename init-container 06/22/23 11:03:01.703
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:03:01.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:03:01.732
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 06/22/23 11:03:01.738
    Jun 22 11:03:01.738: INFO: PodSpec: initContainers in spec.initContainers
    W0622 11:03:01.753568      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "init1", "init2", "run1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "init1", "init2", "run1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "init1", "init2", "run1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "init1", "init2", "run1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:03:47.004: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c1f419c8-d7bc-4b70-ab2c-45d45be33b57", GenerateName:"", Namespace:"init-container-7957", SelfLink:"", UID:"a7f80b44-d1e4-4e41-9071-40e48aeb6aa6", ResourceVersion:"144213", Generation:0, CreationTimestamp:time.Date(2023, time.June, 22, 11, 3, 1, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"738632424"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 22, 11, 3, 1, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00643a078), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 22, 11, 3, 46, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00643a0a8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-h9vvl", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00003c020), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-h9vvl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-h9vvl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-h9vvl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0040821d0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000c7c000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004082250)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004082270)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004082278), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00408227c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0008e20a0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 22, 11, 3, 1, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 22, 11, 3, 1, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 22, 11, 3, 1, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 22, 11, 3, 1, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.92.226.162", PodIP:"100.96.3.91", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.96.3.91"}}, StartTime:time.Date(2023, time.June, 22, 11, 3, 1, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c7c0e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c7c150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://a6f377bc3ae6536cedfa2d257da7e875583f5b94b39ed79097277fce6961547f", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00003c700), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00003c5c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0040822ff)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:03:47.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7957" for this suite. 06/22/23 11:03:47.016
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:03:47.027
Jun 22 11:03:47.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename replication-controller 06/22/23 11:03:47.029
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:03:47.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:03:47.051
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c 06/22/23 11:03:47.055
W0622 11:03:47.064260      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:03:47.068: INFO: Pod name my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c: Found 0 pods out of 1
Jun 22 11:03:52.076: INFO: Pod name my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c: Found 1 pods out of 1
Jun 22 11:03:52.076: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c" are running
Jun 22 11:03:52.076: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c-bsmtr" in namespace "replication-controller-2264" to be "running"
Jun 22 11:03:52.080: INFO: Pod "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c-bsmtr": Phase="Running", Reason="", readiness=true. Elapsed: 3.646576ms
Jun 22 11:03:52.080: INFO: Pod "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c-bsmtr" satisfied condition "running"
Jun 22 11:03:52.080: INFO: Pod "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c-bsmtr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-22 11:03:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-22 11:03:48 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-22 11:03:48 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-22 11:03:47 +0000 UTC Reason: Message:}])
Jun 22 11:03:52.080: INFO: Trying to dial the pod
Jun 22 11:03:57.101: INFO: Controller my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c: Got expected result from replica 1 [my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c-bsmtr]: "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c-bsmtr", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jun 22 11:03:57.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2264" for this suite. 06/22/23 11:03:57.108
------------------------------
• [SLOW TEST] [10.088 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:03:47.027
    Jun 22 11:03:47.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename replication-controller 06/22/23 11:03:47.029
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:03:47.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:03:47.051
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c 06/22/23 11:03:47.055
    W0622 11:03:47.064260      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:03:47.068: INFO: Pod name my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c: Found 0 pods out of 1
    Jun 22 11:03:52.076: INFO: Pod name my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c: Found 1 pods out of 1
    Jun 22 11:03:52.076: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c" are running
    Jun 22 11:03:52.076: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c-bsmtr" in namespace "replication-controller-2264" to be "running"
    Jun 22 11:03:52.080: INFO: Pod "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c-bsmtr": Phase="Running", Reason="", readiness=true. Elapsed: 3.646576ms
    Jun 22 11:03:52.080: INFO: Pod "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c-bsmtr" satisfied condition "running"
    Jun 22 11:03:52.080: INFO: Pod "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c-bsmtr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-22 11:03:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-22 11:03:48 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-22 11:03:48 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-22 11:03:47 +0000 UTC Reason: Message:}])
    Jun 22 11:03:52.080: INFO: Trying to dial the pod
    Jun 22 11:03:57.101: INFO: Controller my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c: Got expected result from replica 1 [my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c-bsmtr]: "my-hostname-basic-4f2ea2b1-7dd6-4e4b-91e3-d9660270d04c-bsmtr", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:03:57.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2264" for this suite. 06/22/23 11:03:57.108
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:03:57.116
Jun 22 11:03:57.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename webhook 06/22/23 11:03:57.117
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:03:57.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:03:57.137
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/22/23 11:03:57.155
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:03:57.742
STEP: Deploying the webhook pod 06/22/23 11:03:57.758
W0622 11:03:57.777146      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 11:03:57.777
Jun 22 11:03:57.793: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/22/23 11:03:59.81
STEP: Verifying the service has paired with the endpoint 06/22/23 11:03:59.835
Jun 22 11:04:00.835: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Jun 22 11:04:00.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1844-crds.webhook.example.com via the AdmissionRegistration API 06/22/23 11:04:01.359
STEP: Creating a custom resource while v1 is storage version 06/22/23 11:04:01.386
STEP: Patching Custom Resource Definition to set v2 as storage 06/22/23 11:04:03.467
STEP: Patching the custom resource while v2 is storage version 06/22/23 11:04:03.476
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:04:04.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7446" for this suite. 06/22/23 11:04:04.117
STEP: Destroying namespace "webhook-7446-markers" for this suite. 06/22/23 11:04:04.13
------------------------------
• [SLOW TEST] [7.024 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:03:57.116
    Jun 22 11:03:57.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename webhook 06/22/23 11:03:57.117
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:03:57.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:03:57.137
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/22/23 11:03:57.155
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:03:57.742
    STEP: Deploying the webhook pod 06/22/23 11:03:57.758
    W0622 11:03:57.777146      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 11:03:57.777
    Jun 22 11:03:57.793: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/22/23 11:03:59.81
    STEP: Verifying the service has paired with the endpoint 06/22/23 11:03:59.835
    Jun 22 11:04:00.835: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Jun 22 11:04:00.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1844-crds.webhook.example.com via the AdmissionRegistration API 06/22/23 11:04:01.359
    STEP: Creating a custom resource while v1 is storage version 06/22/23 11:04:01.386
    STEP: Patching Custom Resource Definition to set v2 as storage 06/22/23 11:04:03.467
    STEP: Patching the custom resource while v2 is storage version 06/22/23 11:04:03.476
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:04:04.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7446" for this suite. 06/22/23 11:04:04.117
    STEP: Destroying namespace "webhook-7446-markers" for this suite. 06/22/23 11:04:04.13
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:04:04.145
Jun 22 11:04:04.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename runtimeclass 06/22/23 11:04:04.146
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:04:04.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:04:04.175
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
W0622 11:04:04.202531      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:04:04.202: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7276 to be scheduled
Jun 22 11:04:04.208: INFO: 1 pods are not scheduled: [runtimeclass-7276/test-runtimeclass-runtimeclass-7276-preconfigured-handler-5cndk(f55ad29b-bc2f-4943-bc27-ac343fc09a23)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jun 22 11:04:06.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7276" for this suite. 06/22/23 11:04:06.231
------------------------------
• [2.095 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:04:04.145
    Jun 22 11:04:04.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename runtimeclass 06/22/23 11:04:04.146
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:04:04.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:04:04.175
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    W0622 11:04:04.202531      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:04:04.202: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7276 to be scheduled
    Jun 22 11:04:04.208: INFO: 1 pods are not scheduled: [runtimeclass-7276/test-runtimeclass-runtimeclass-7276-preconfigured-handler-5cndk(f55ad29b-bc2f-4943-bc27-ac343fc09a23)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:04:06.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7276" for this suite. 06/22/23 11:04:06.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:04:06.242
Jun 22 11:04:06.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename custom-resource-definition 06/22/23 11:04:06.243
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:04:06.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:04:06.264
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jun 22 11:04:06.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:04:06.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7930" for this suite. 06/22/23 11:04:06.832
------------------------------
• [0.598 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:04:06.242
    Jun 22 11:04:06.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename custom-resource-definition 06/22/23 11:04:06.243
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:04:06.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:04:06.264
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jun 22 11:04:06.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:04:06.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7930" for this suite. 06/22/23 11:04:06.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:04:06.841
Jun 22 11:04:06.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename resourcequota 06/22/23 11:04:06.843
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:04:06.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:04:06.868
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-g8mg2" 06/22/23 11:04:06.876
Jun 22 11:04:06.888: INFO: Resource quota "e2e-rq-status-g8mg2" reports spec: hard cpu limit of 500m
Jun 22 11:04:06.888: INFO: Resource quota "e2e-rq-status-g8mg2" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-g8mg2" /status 06/22/23 11:04:06.888
STEP: Confirm /status for "e2e-rq-status-g8mg2" resourceQuota via watch 06/22/23 11:04:06.899
Jun 22 11:04:06.902: INFO: observed resourceQuota "e2e-rq-status-g8mg2" in namespace "resourcequota-4455" with hard status: v1.ResourceList(nil)
Jun 22 11:04:06.902: INFO: Found resourceQuota "e2e-rq-status-g8mg2" in namespace "resourcequota-4455" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jun 22 11:04:06.902: INFO: ResourceQuota "e2e-rq-status-g8mg2" /status was updated
STEP: Patching hard spec values for cpu & memory 06/22/23 11:04:06.906
Jun 22 11:04:06.915: INFO: Resource quota "e2e-rq-status-g8mg2" reports spec: hard cpu limit of 1
Jun 22 11:04:06.915: INFO: Resource quota "e2e-rq-status-g8mg2" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-g8mg2" /status 06/22/23 11:04:06.915
STEP: Confirm /status for "e2e-rq-status-g8mg2" resourceQuota via watch 06/22/23 11:04:06.925
Jun 22 11:04:06.928: INFO: observed resourceQuota "e2e-rq-status-g8mg2" in namespace "resourcequota-4455" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jun 22 11:04:06.928: INFO: Found resourceQuota "e2e-rq-status-g8mg2" in namespace "resourcequota-4455" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Jun 22 11:04:06.928: INFO: ResourceQuota "e2e-rq-status-g8mg2" /status was patched
STEP: Get "e2e-rq-status-g8mg2" /status 06/22/23 11:04:06.928
Jun 22 11:04:06.934: INFO: Resourcequota "e2e-rq-status-g8mg2" reports status: hard cpu of 1
Jun 22 11:04:06.934: INFO: Resourcequota "e2e-rq-status-g8mg2" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-g8mg2" /status before checking Spec is unchanged 06/22/23 11:04:06.938
Jun 22 11:04:06.944: INFO: Resourcequota "e2e-rq-status-g8mg2" reports status: hard cpu of 2
Jun 22 11:04:06.944: INFO: Resourcequota "e2e-rq-status-g8mg2" reports status: hard memory of 2Gi
Jun 22 11:04:06.946: INFO: Found resourceQuota "e2e-rq-status-g8mg2" in namespace "resourcequota-4455" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Jun 22 11:06:11.958: INFO: ResourceQuota "e2e-rq-status-g8mg2" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun 22 11:06:11.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4455" for this suite. 06/22/23 11:06:11.965
------------------------------
• [SLOW TEST] [125.132 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:04:06.841
    Jun 22 11:04:06.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename resourcequota 06/22/23 11:04:06.843
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:04:06.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:04:06.868
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-g8mg2" 06/22/23 11:04:06.876
    Jun 22 11:04:06.888: INFO: Resource quota "e2e-rq-status-g8mg2" reports spec: hard cpu limit of 500m
    Jun 22 11:04:06.888: INFO: Resource quota "e2e-rq-status-g8mg2" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-g8mg2" /status 06/22/23 11:04:06.888
    STEP: Confirm /status for "e2e-rq-status-g8mg2" resourceQuota via watch 06/22/23 11:04:06.899
    Jun 22 11:04:06.902: INFO: observed resourceQuota "e2e-rq-status-g8mg2" in namespace "resourcequota-4455" with hard status: v1.ResourceList(nil)
    Jun 22 11:04:06.902: INFO: Found resourceQuota "e2e-rq-status-g8mg2" in namespace "resourcequota-4455" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jun 22 11:04:06.902: INFO: ResourceQuota "e2e-rq-status-g8mg2" /status was updated
    STEP: Patching hard spec values for cpu & memory 06/22/23 11:04:06.906
    Jun 22 11:04:06.915: INFO: Resource quota "e2e-rq-status-g8mg2" reports spec: hard cpu limit of 1
    Jun 22 11:04:06.915: INFO: Resource quota "e2e-rq-status-g8mg2" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-g8mg2" /status 06/22/23 11:04:06.915
    STEP: Confirm /status for "e2e-rq-status-g8mg2" resourceQuota via watch 06/22/23 11:04:06.925
    Jun 22 11:04:06.928: INFO: observed resourceQuota "e2e-rq-status-g8mg2" in namespace "resourcequota-4455" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jun 22 11:04:06.928: INFO: Found resourceQuota "e2e-rq-status-g8mg2" in namespace "resourcequota-4455" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Jun 22 11:04:06.928: INFO: ResourceQuota "e2e-rq-status-g8mg2" /status was patched
    STEP: Get "e2e-rq-status-g8mg2" /status 06/22/23 11:04:06.928
    Jun 22 11:04:06.934: INFO: Resourcequota "e2e-rq-status-g8mg2" reports status: hard cpu of 1
    Jun 22 11:04:06.934: INFO: Resourcequota "e2e-rq-status-g8mg2" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-g8mg2" /status before checking Spec is unchanged 06/22/23 11:04:06.938
    Jun 22 11:04:06.944: INFO: Resourcequota "e2e-rq-status-g8mg2" reports status: hard cpu of 2
    Jun 22 11:04:06.944: INFO: Resourcequota "e2e-rq-status-g8mg2" reports status: hard memory of 2Gi
    Jun 22 11:04:06.946: INFO: Found resourceQuota "e2e-rq-status-g8mg2" in namespace "resourcequota-4455" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Jun 22 11:06:11.958: INFO: ResourceQuota "e2e-rq-status-g8mg2" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:06:11.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4455" for this suite. 06/22/23 11:06:11.965
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:06:11.974
Jun 22 11:06:11.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename webhook 06/22/23 11:06:11.976
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:06:11.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:06:11.994
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/22/23 11:06:12.011
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:06:12.426
STEP: Deploying the webhook pod 06/22/23 11:06:12.436
W0622 11:06:12.454956      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 11:06:12.455
Jun 22 11:06:12.467: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/22/23 11:06:14.482
STEP: Verifying the service has paired with the endpoint 06/22/23 11:06:14.502
Jun 22 11:06:15.502: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/22/23 11:06:15.509
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/22/23 11:06:15.53
STEP: Creating a dummy validating-webhook-configuration object 06/22/23 11:06:15.546
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 06/22/23 11:06:15.558
STEP: Creating a dummy mutating-webhook-configuration object 06/22/23 11:06:15.567
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 06/22/23 11:06:15.579
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:06:15.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3575" for this suite. 06/22/23 11:06:15.676
STEP: Destroying namespace "webhook-3575-markers" for this suite. 06/22/23 11:06:15.688
------------------------------
• [3.722 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:06:11.974
    Jun 22 11:06:11.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename webhook 06/22/23 11:06:11.976
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:06:11.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:06:11.994
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/22/23 11:06:12.011
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:06:12.426
    STEP: Deploying the webhook pod 06/22/23 11:06:12.436
    W0622 11:06:12.454956      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 11:06:12.455
    Jun 22 11:06:12.467: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/22/23 11:06:14.482
    STEP: Verifying the service has paired with the endpoint 06/22/23 11:06:14.502
    Jun 22 11:06:15.502: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/22/23 11:06:15.509
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/22/23 11:06:15.53
    STEP: Creating a dummy validating-webhook-configuration object 06/22/23 11:06:15.546
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 06/22/23 11:06:15.558
    STEP: Creating a dummy mutating-webhook-configuration object 06/22/23 11:06:15.567
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 06/22/23 11:06:15.579
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:06:15.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3575" for this suite. 06/22/23 11:06:15.676
    STEP: Destroying namespace "webhook-3575-markers" for this suite. 06/22/23 11:06:15.688
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:06:15.697
Jun 22 11:06:15.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename services 06/22/23 11:06:15.698
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:06:15.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:06:15.721
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3101 06/22/23 11:06:15.724
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/22/23 11:06:15.747
STEP: creating service externalsvc in namespace services-3101 06/22/23 11:06:15.748
STEP: creating replication controller externalsvc in namespace services-3101 06/22/23 11:06:15.772
W0622 11:06:15.781399      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "externalsvc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "externalsvc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "externalsvc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "externalsvc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0622 11:06:15.781623      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3101, replica count: 2
I0622 11:06:18.833376      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 06/22/23 11:06:18.838
Jun 22 11:06:18.867: INFO: Creating new exec pod
W0622 11:06:18.877099      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:06:18.877: INFO: Waiting up to 5m0s for pod "execpod87tqp" in namespace "services-3101" to be "running"
Jun 22 11:06:18.881: INFO: Pod "execpod87tqp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281757ms
Jun 22 11:06:20.887: INFO: Pod "execpod87tqp": Phase="Running", Reason="", readiness=true. Elapsed: 2.010358611s
Jun 22 11:06:20.887: INFO: Pod "execpod87tqp" satisfied condition "running"
Jun 22 11:06:20.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-3101 exec execpod87tqp -- /bin/sh -x -c nslookup nodeport-service.services-3101.svc.cluster.local'
Jun 22 11:06:21.247: INFO: stderr: "+ nslookup nodeport-service.services-3101.svc.cluster.local\n"
Jun 22 11:06:21.247: INFO: stdout: "Server:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nnodeport-service.services-3101.svc.cluster.local\tcanonical name = externalsvc.services-3101.svc.cluster.local.\nName:\texternalsvc.services-3101.svc.cluster.local\nAddress: 100.67.116.67\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3101, will wait for the garbage collector to delete the pods 06/22/23 11:06:21.247
Jun 22 11:06:21.310: INFO: Deleting ReplicationController externalsvc took: 7.51483ms
Jun 22 11:06:21.411: INFO: Terminating ReplicationController externalsvc pods took: 100.513495ms
Jun 22 11:06:23.547: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun 22 11:06:23.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3101" for this suite. 06/22/23 11:06:23.57
------------------------------
• [SLOW TEST] [7.884 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:06:15.697
    Jun 22 11:06:15.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename services 06/22/23 11:06:15.698
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:06:15.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:06:15.721
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-3101 06/22/23 11:06:15.724
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/22/23 11:06:15.747
    STEP: creating service externalsvc in namespace services-3101 06/22/23 11:06:15.748
    STEP: creating replication controller externalsvc in namespace services-3101 06/22/23 11:06:15.772
    W0622 11:06:15.781399      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "externalsvc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "externalsvc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "externalsvc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "externalsvc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0622 11:06:15.781623      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3101, replica count: 2
    I0622 11:06:18.833376      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 06/22/23 11:06:18.838
    Jun 22 11:06:18.867: INFO: Creating new exec pod
    W0622 11:06:18.877099      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:06:18.877: INFO: Waiting up to 5m0s for pod "execpod87tqp" in namespace "services-3101" to be "running"
    Jun 22 11:06:18.881: INFO: Pod "execpod87tqp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.281757ms
    Jun 22 11:06:20.887: INFO: Pod "execpod87tqp": Phase="Running", Reason="", readiness=true. Elapsed: 2.010358611s
    Jun 22 11:06:20.887: INFO: Pod "execpod87tqp" satisfied condition "running"
    Jun 22 11:06:20.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-3101 exec execpod87tqp -- /bin/sh -x -c nslookup nodeport-service.services-3101.svc.cluster.local'
    Jun 22 11:06:21.247: INFO: stderr: "+ nslookup nodeport-service.services-3101.svc.cluster.local\n"
    Jun 22 11:06:21.247: INFO: stdout: "Server:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nnodeport-service.services-3101.svc.cluster.local\tcanonical name = externalsvc.services-3101.svc.cluster.local.\nName:\texternalsvc.services-3101.svc.cluster.local\nAddress: 100.67.116.67\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3101, will wait for the garbage collector to delete the pods 06/22/23 11:06:21.247
    Jun 22 11:06:21.310: INFO: Deleting ReplicationController externalsvc took: 7.51483ms
    Jun 22 11:06:21.411: INFO: Terminating ReplicationController externalsvc pods took: 100.513495ms
    Jun 22 11:06:23.547: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:06:23.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3101" for this suite. 06/22/23 11:06:23.57
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:06:23.582
Jun 22 11:06:23.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename downward-api 06/22/23 11:06:23.583
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:06:23.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:06:23.602
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 06/22/23 11:06:23.605
W0622 11:06:23.618163      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:06:23.618: INFO: Waiting up to 5m0s for pod "labelsupdate89a5b285-7e5a-4b72-9afb-09455165d71b" in namespace "downward-api-6393" to be "running and ready"
Jun 22 11:06:23.623: INFO: Pod "labelsupdate89a5b285-7e5a-4b72-9afb-09455165d71b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.081008ms
Jun 22 11:06:23.623: INFO: The phase of Pod labelsupdate89a5b285-7e5a-4b72-9afb-09455165d71b is Pending, waiting for it to be Running (with Ready = true)
Jun 22 11:06:25.629: INFO: Pod "labelsupdate89a5b285-7e5a-4b72-9afb-09455165d71b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010820741s
Jun 22 11:06:25.629: INFO: The phase of Pod labelsupdate89a5b285-7e5a-4b72-9afb-09455165d71b is Running (Ready = true)
Jun 22 11:06:25.629: INFO: Pod "labelsupdate89a5b285-7e5a-4b72-9afb-09455165d71b" satisfied condition "running and ready"
Jun 22 11:06:26.168: INFO: Successfully updated pod "labelsupdate89a5b285-7e5a-4b72-9afb-09455165d71b"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun 22 11:06:30.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6393" for this suite. 06/22/23 11:06:30.207
------------------------------
• [SLOW TEST] [6.632 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:06:23.582
    Jun 22 11:06:23.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename downward-api 06/22/23 11:06:23.583
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:06:23.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:06:23.602
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 06/22/23 11:06:23.605
    W0622 11:06:23.618163      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:06:23.618: INFO: Waiting up to 5m0s for pod "labelsupdate89a5b285-7e5a-4b72-9afb-09455165d71b" in namespace "downward-api-6393" to be "running and ready"
    Jun 22 11:06:23.623: INFO: Pod "labelsupdate89a5b285-7e5a-4b72-9afb-09455165d71b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.081008ms
    Jun 22 11:06:23.623: INFO: The phase of Pod labelsupdate89a5b285-7e5a-4b72-9afb-09455165d71b is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 11:06:25.629: INFO: Pod "labelsupdate89a5b285-7e5a-4b72-9afb-09455165d71b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010820741s
    Jun 22 11:06:25.629: INFO: The phase of Pod labelsupdate89a5b285-7e5a-4b72-9afb-09455165d71b is Running (Ready = true)
    Jun 22 11:06:25.629: INFO: Pod "labelsupdate89a5b285-7e5a-4b72-9afb-09455165d71b" satisfied condition "running and ready"
    Jun 22 11:06:26.168: INFO: Successfully updated pod "labelsupdate89a5b285-7e5a-4b72-9afb-09455165d71b"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:06:30.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6393" for this suite. 06/22/23 11:06:30.207
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:06:30.214
Jun 22 11:06:30.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename container-runtime 06/22/23 11:06:30.216
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:06:30.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:06:30.237
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
W0622 11:06:30.255698      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "terminate-cmd-rpa" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "terminate-cmd-rpa" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "terminate-cmd-rpa" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "terminate-cmd-rpa" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 06/22/23 11:06:30.255
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 06/22/23 11:06:49.379
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 06/22/23 11:06:49.384
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 06/22/23 11:06:49.394
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 06/22/23 11:06:49.394
W0622 11:06:49.422238      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "terminate-cmd-rpof" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "terminate-cmd-rpof" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "terminate-cmd-rpof" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "terminate-cmd-rpof" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 06/22/23 11:06:49.422
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 06/22/23 11:06:52.444
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 06/22/23 11:06:54.461
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 06/22/23 11:06:54.47
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 06/22/23 11:06:54.47
W0622 11:06:54.503166      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "terminate-cmd-rpn" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "terminate-cmd-rpn" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "terminate-cmd-rpn" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "terminate-cmd-rpn" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 06/22/23 11:06:54.503
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 06/22/23 11:06:55.512
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 06/22/23 11:06:57.528
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 06/22/23 11:06:57.536
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 06/22/23 11:06:57.536
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jun 22 11:06:57.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8000" for this suite. 06/22/23 11:06:57.577
------------------------------
• [SLOW TEST] [27.370 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:06:30.214
    Jun 22 11:06:30.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename container-runtime 06/22/23 11:06:30.216
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:06:30.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:06:30.237
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    W0622 11:06:30.255698      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "terminate-cmd-rpa" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "terminate-cmd-rpa" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "terminate-cmd-rpa" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "terminate-cmd-rpa" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 06/22/23 11:06:30.255
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 06/22/23 11:06:49.379
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 06/22/23 11:06:49.384
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 06/22/23 11:06:49.394
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 06/22/23 11:06:49.394
    W0622 11:06:49.422238      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "terminate-cmd-rpof" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "terminate-cmd-rpof" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "terminate-cmd-rpof" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "terminate-cmd-rpof" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 06/22/23 11:06:49.422
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 06/22/23 11:06:52.444
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 06/22/23 11:06:54.461
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 06/22/23 11:06:54.47
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 06/22/23 11:06:54.47
    W0622 11:06:54.503166      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "terminate-cmd-rpn" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "terminate-cmd-rpn" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "terminate-cmd-rpn" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "terminate-cmd-rpn" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 06/22/23 11:06:54.503
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 06/22/23 11:06:55.512
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 06/22/23 11:06:57.528
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 06/22/23 11:06:57.536
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 06/22/23 11:06:57.536
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:06:57.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8000" for this suite. 06/22/23 11:06:57.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:06:57.587
Jun 22 11:06:57.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename disruption 06/22/23 11:06:57.588
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:06:57.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:06:57.608
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 06/22/23 11:06:57.611
STEP: Waiting for the pdb to be processed 06/22/23 11:06:57.618
STEP: updating the pdb 06/22/23 11:06:59.629
STEP: Waiting for the pdb to be processed 06/22/23 11:06:59.64
STEP: patching the pdb 06/22/23 11:07:01.65
STEP: Waiting for the pdb to be processed 06/22/23 11:07:01.665
STEP: Waiting for the pdb to be deleted 06/22/23 11:07:03.683
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jun 22 11:07:03.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4861" for this suite. 06/22/23 11:07:03.693
------------------------------
• [SLOW TEST] [6.115 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:06:57.587
    Jun 22 11:06:57.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename disruption 06/22/23 11:06:57.588
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:06:57.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:06:57.608
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 06/22/23 11:06:57.611
    STEP: Waiting for the pdb to be processed 06/22/23 11:06:57.618
    STEP: updating the pdb 06/22/23 11:06:59.629
    STEP: Waiting for the pdb to be processed 06/22/23 11:06:59.64
    STEP: patching the pdb 06/22/23 11:07:01.65
    STEP: Waiting for the pdb to be processed 06/22/23 11:07:01.665
    STEP: Waiting for the pdb to be deleted 06/22/23 11:07:03.683
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:07:03.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4861" for this suite. 06/22/23 11:07:03.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:07:03.704
Jun 22 11:07:03.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename lease-test 06/22/23 11:07:03.705
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:07:03.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:07:03.728
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Jun 22 11:07:03.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-9930" for this suite. 06/22/23 11:07:03.81
------------------------------
• [0.114 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:07:03.704
    Jun 22 11:07:03.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename lease-test 06/22/23 11:07:03.705
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:07:03.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:07:03.728
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:07:03.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-9930" for this suite. 06/22/23 11:07:03.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:07:03.819
Jun 22 11:07:03.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename statefulset 06/22/23 11:07:03.82
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:07:03.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:07:03.84
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4001 06/22/23 11:07:03.844
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
W0622 11:07:03.860201      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:07:03.865: INFO: Found 0 stateful pods, waiting for 1
Jun 22 11:07:13.872: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 06/22/23 11:07:13.88
W0622 11:07:13.889281      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
W0622 11:07:13.889334      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "test-ss", "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "test-ss", "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "test-ss", "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "test-ss", "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:07:13.899: INFO: Found 1 stateful pods, waiting for 2
Jun 22 11:07:23.910: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 11:07:23.910: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 06/22/23 11:07:23.919
STEP: Delete all of the StatefulSets 06/22/23 11:07:23.924
STEP: Verify that StatefulSets have been deleted 06/22/23 11:07:23.934
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun 22 11:07:23.946: INFO: Deleting all statefulset in ns statefulset-4001
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun 22 11:07:23.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4001" for this suite. 06/22/23 11:07:23.972
------------------------------
• [SLOW TEST] [20.164 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:07:03.819
    Jun 22 11:07:03.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename statefulset 06/22/23 11:07:03.82
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:07:03.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:07:03.84
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4001 06/22/23 11:07:03.844
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    W0622 11:07:03.860201      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:07:03.865: INFO: Found 0 stateful pods, waiting for 1
    Jun 22 11:07:13.872: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 06/22/23 11:07:13.88
    W0622 11:07:13.889281      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    W0622 11:07:13.889334      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "test-ss", "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "test-ss", "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "test-ss", "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "test-ss", "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:07:13.899: INFO: Found 1 stateful pods, waiting for 2
    Jun 22 11:07:23.910: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 22 11:07:23.910: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 06/22/23 11:07:23.919
    STEP: Delete all of the StatefulSets 06/22/23 11:07:23.924
    STEP: Verify that StatefulSets have been deleted 06/22/23 11:07:23.934
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun 22 11:07:23.946: INFO: Deleting all statefulset in ns statefulset-4001
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:07:23.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4001" for this suite. 06/22/23 11:07:23.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:07:23.985
Jun 22 11:07:23.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubelet-test 06/22/23 11:07:23.986
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:07:24.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:07:24.006
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
W0622 11:07:24.021281      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "bin-false46913390-4241-4f23-9791-fe4f36b2c29a" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "bin-false46913390-4241-4f23-9791-fe4f36b2c29a" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "bin-false46913390-4241-4f23-9791-fe4f36b2c29a" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "bin-false46913390-4241-4f23-9791-fe4f36b2c29a" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jun 22 11:07:24.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2729" for this suite. 06/22/23 11:07:24.048
------------------------------
• [0.072 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:07:23.985
    Jun 22 11:07:23.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubelet-test 06/22/23 11:07:23.986
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:07:24.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:07:24.006
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    W0622 11:07:24.021281      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "bin-false46913390-4241-4f23-9791-fe4f36b2c29a" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "bin-false46913390-4241-4f23-9791-fe4f36b2c29a" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "bin-false46913390-4241-4f23-9791-fe4f36b2c29a" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "bin-false46913390-4241-4f23-9791-fe4f36b2c29a" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:07:24.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2729" for this suite. 06/22/23 11:07:24.048
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:07:24.058
Jun 22 11:07:24.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubectl 06/22/23 11:07:24.059
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:07:24.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:07:24.085
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 06/22/23 11:07:24.089
Jun 22 11:07:24.090: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-8815 proxy --unix-socket=/tmp/kubectl-proxy-unix1150273302/test'
STEP: retrieving proxy /api/ output 06/22/23 11:07:24.178
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun 22 11:07:24.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8815" for this suite. 06/22/23 11:07:24.187
------------------------------
• [0.137 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:07:24.058
    Jun 22 11:07:24.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubectl 06/22/23 11:07:24.059
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:07:24.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:07:24.085
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 06/22/23 11:07:24.089
    Jun 22 11:07:24.090: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-8815 proxy --unix-socket=/tmp/kubectl-proxy-unix1150273302/test'
    STEP: retrieving proxy /api/ output 06/22/23 11:07:24.178
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:07:24.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8815" for this suite. 06/22/23 11:07:24.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:07:24.196
Jun 22 11:07:24.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename emptydir 06/22/23 11:07:24.197
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:07:24.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:07:24.219
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 06/22/23 11:07:24.222
W0622 11:07:24.236772      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:07:24.236: INFO: Waiting up to 5m0s for pod "pod-fe53467a-8b5d-4231-b1cd-f3d90fb5acf9" in namespace "emptydir-4672" to be "Succeeded or Failed"
Jun 22 11:07:24.243: INFO: Pod "pod-fe53467a-8b5d-4231-b1cd-f3d90fb5acf9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.38504ms
Jun 22 11:07:26.249: INFO: Pod "pod-fe53467a-8b5d-4231-b1cd-f3d90fb5acf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012661164s
Jun 22 11:07:28.250: INFO: Pod "pod-fe53467a-8b5d-4231-b1cd-f3d90fb5acf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013250697s
STEP: Saw pod success 06/22/23 11:07:28.25
Jun 22 11:07:28.250: INFO: Pod "pod-fe53467a-8b5d-4231-b1cd-f3d90fb5acf9" satisfied condition "Succeeded or Failed"
Jun 22 11:07:28.254: INFO: Trying to get logs from node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k pod pod-fe53467a-8b5d-4231-b1cd-f3d90fb5acf9 container test-container: <nil>
STEP: delete the pod 06/22/23 11:07:28.274
Jun 22 11:07:28.288: INFO: Waiting for pod pod-fe53467a-8b5d-4231-b1cd-f3d90fb5acf9 to disappear
Jun 22 11:07:28.292: INFO: Pod pod-fe53467a-8b5d-4231-b1cd-f3d90fb5acf9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun 22 11:07:28.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4672" for this suite. 06/22/23 11:07:28.395
------------------------------
• [4.286 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:07:24.196
    Jun 22 11:07:24.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename emptydir 06/22/23 11:07:24.197
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:07:24.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:07:24.219
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 06/22/23 11:07:24.222
    W0622 11:07:24.236772      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:07:24.236: INFO: Waiting up to 5m0s for pod "pod-fe53467a-8b5d-4231-b1cd-f3d90fb5acf9" in namespace "emptydir-4672" to be "Succeeded or Failed"
    Jun 22 11:07:24.243: INFO: Pod "pod-fe53467a-8b5d-4231-b1cd-f3d90fb5acf9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.38504ms
    Jun 22 11:07:26.249: INFO: Pod "pod-fe53467a-8b5d-4231-b1cd-f3d90fb5acf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012661164s
    Jun 22 11:07:28.250: INFO: Pod "pod-fe53467a-8b5d-4231-b1cd-f3d90fb5acf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013250697s
    STEP: Saw pod success 06/22/23 11:07:28.25
    Jun 22 11:07:28.250: INFO: Pod "pod-fe53467a-8b5d-4231-b1cd-f3d90fb5acf9" satisfied condition "Succeeded or Failed"
    Jun 22 11:07:28.254: INFO: Trying to get logs from node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k pod pod-fe53467a-8b5d-4231-b1cd-f3d90fb5acf9 container test-container: <nil>
    STEP: delete the pod 06/22/23 11:07:28.274
    Jun 22 11:07:28.288: INFO: Waiting for pod pod-fe53467a-8b5d-4231-b1cd-f3d90fb5acf9 to disappear
    Jun 22 11:07:28.292: INFO: Pod pod-fe53467a-8b5d-4231-b1cd-f3d90fb5acf9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:07:28.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4672" for this suite. 06/22/23 11:07:28.395
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:07:28.488
Jun 22 11:07:28.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename sched-preemption 06/22/23 11:07:28.505
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:07:28.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:07:28.565
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jun 22 11:07:28.598: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 22 11:08:28.723: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 06/22/23 11:08:28.728
Jun 22 11:08:28.767: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun 22 11:08:28.780: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun 22 11:08:28.808: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun 22 11:08:28.822: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jun 22 11:08:28.855: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jun 22 11:08:28.866: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 06/22/23 11:08:28.866
Jun 22 11:08:28.866: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3079" to be "running"
Jun 22 11:08:28.873: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.656898ms
Jun 22 11:08:30.879: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.012691501s
Jun 22 11:08:30.879: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jun 22 11:08:30.879: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3079" to be "running"
Jun 22 11:08:30.885: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.374603ms
Jun 22 11:08:30.885: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jun 22 11:08:30.885: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3079" to be "running"
Jun 22 11:08:30.889: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.603834ms
Jun 22 11:08:30.889: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jun 22 11:08:30.889: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3079" to be "running"
Jun 22 11:08:30.894: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.488431ms
Jun 22 11:08:30.894: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jun 22 11:08:30.894: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3079" to be "running"
Jun 22 11:08:30.898: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.111855ms
Jun 22 11:08:30.898: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jun 22 11:08:30.898: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3079" to be "running"
Jun 22 11:08:30.902: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.04969ms
Jun 22 11:08:30.902: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 06/22/23 11:08:30.902
Jun 22 11:08:30.917: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jun 22 11:08:30.925: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.630733ms
Jun 22 11:08:32.930: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012568276s
Jun 22 11:08:34.931: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013289298s
Jun 22 11:08:36.938: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.020374163s
Jun 22 11:08:36.938: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:08:36.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3079" for this suite. 06/22/23 11:08:37.066
------------------------------
• [SLOW TEST] [68.587 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:07:28.488
    Jun 22 11:07:28.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename sched-preemption 06/22/23 11:07:28.505
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:07:28.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:07:28.565
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jun 22 11:07:28.598: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 22 11:08:28.723: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 06/22/23 11:08:28.728
    Jun 22 11:08:28.767: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jun 22 11:08:28.780: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jun 22 11:08:28.808: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jun 22 11:08:28.822: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jun 22 11:08:28.855: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jun 22 11:08:28.866: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 06/22/23 11:08:28.866
    Jun 22 11:08:28.866: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3079" to be "running"
    Jun 22 11:08:28.873: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.656898ms
    Jun 22 11:08:30.879: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.012691501s
    Jun 22 11:08:30.879: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jun 22 11:08:30.879: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3079" to be "running"
    Jun 22 11:08:30.885: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.374603ms
    Jun 22 11:08:30.885: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun 22 11:08:30.885: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3079" to be "running"
    Jun 22 11:08:30.889: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.603834ms
    Jun 22 11:08:30.889: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun 22 11:08:30.889: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3079" to be "running"
    Jun 22 11:08:30.894: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.488431ms
    Jun 22 11:08:30.894: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun 22 11:08:30.894: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3079" to be "running"
    Jun 22 11:08:30.898: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.111855ms
    Jun 22 11:08:30.898: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun 22 11:08:30.898: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3079" to be "running"
    Jun 22 11:08:30.902: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.04969ms
    Jun 22 11:08:30.902: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 06/22/23 11:08:30.902
    Jun 22 11:08:30.917: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jun 22 11:08:30.925: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.630733ms
    Jun 22 11:08:32.930: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012568276s
    Jun 22 11:08:34.931: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013289298s
    Jun 22 11:08:36.938: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.020374163s
    Jun 22 11:08:36.938: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:08:36.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3079" for this suite. 06/22/23 11:08:37.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:08:37.079
Jun 22 11:08:37.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename configmap 06/22/23 11:08:37.081
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:08:37.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:08:37.101
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-255598a6-fdd7-4479-be75-ec5e50674339 06/22/23 11:08:37.104
STEP: Creating a pod to test consume configMaps 06/22/23 11:08:37.112
W0622 11:08:37.125999      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "configmap-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "configmap-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "configmap-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "configmap-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:08:37.126: INFO: Waiting up to 5m0s for pod "pod-configmaps-854027dd-208d-4dd3-81da-191e80b1a499" in namespace "configmap-7372" to be "Succeeded or Failed"
Jun 22 11:08:37.131: INFO: Pod "pod-configmaps-854027dd-208d-4dd3-81da-191e80b1a499": Phase="Pending", Reason="", readiness=false. Elapsed: 5.321254ms
Jun 22 11:08:39.137: INFO: Pod "pod-configmaps-854027dd-208d-4dd3-81da-191e80b1a499": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011300657s
Jun 22 11:08:41.139: INFO: Pod "pod-configmaps-854027dd-208d-4dd3-81da-191e80b1a499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013154838s
STEP: Saw pod success 06/22/23 11:08:41.139
Jun 22 11:08:41.139: INFO: Pod "pod-configmaps-854027dd-208d-4dd3-81da-191e80b1a499" satisfied condition "Succeeded or Failed"
Jun 22 11:08:41.144: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-configmaps-854027dd-208d-4dd3-81da-191e80b1a499 container configmap-volume-test: <nil>
STEP: delete the pod 06/22/23 11:08:41.165
Jun 22 11:08:41.184: INFO: Waiting for pod pod-configmaps-854027dd-208d-4dd3-81da-191e80b1a499 to disappear
Jun 22 11:08:41.188: INFO: Pod pod-configmaps-854027dd-208d-4dd3-81da-191e80b1a499 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun 22 11:08:41.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7372" for this suite. 06/22/23 11:08:41.195
------------------------------
• [4.124 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:08:37.079
    Jun 22 11:08:37.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename configmap 06/22/23 11:08:37.081
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:08:37.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:08:37.101
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-255598a6-fdd7-4479-be75-ec5e50674339 06/22/23 11:08:37.104
    STEP: Creating a pod to test consume configMaps 06/22/23 11:08:37.112
    W0622 11:08:37.125999      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "configmap-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "configmap-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "configmap-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "configmap-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:08:37.126: INFO: Waiting up to 5m0s for pod "pod-configmaps-854027dd-208d-4dd3-81da-191e80b1a499" in namespace "configmap-7372" to be "Succeeded or Failed"
    Jun 22 11:08:37.131: INFO: Pod "pod-configmaps-854027dd-208d-4dd3-81da-191e80b1a499": Phase="Pending", Reason="", readiness=false. Elapsed: 5.321254ms
    Jun 22 11:08:39.137: INFO: Pod "pod-configmaps-854027dd-208d-4dd3-81da-191e80b1a499": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011300657s
    Jun 22 11:08:41.139: INFO: Pod "pod-configmaps-854027dd-208d-4dd3-81da-191e80b1a499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013154838s
    STEP: Saw pod success 06/22/23 11:08:41.139
    Jun 22 11:08:41.139: INFO: Pod "pod-configmaps-854027dd-208d-4dd3-81da-191e80b1a499" satisfied condition "Succeeded or Failed"
    Jun 22 11:08:41.144: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-configmaps-854027dd-208d-4dd3-81da-191e80b1a499 container configmap-volume-test: <nil>
    STEP: delete the pod 06/22/23 11:08:41.165
    Jun 22 11:08:41.184: INFO: Waiting for pod pod-configmaps-854027dd-208d-4dd3-81da-191e80b1a499 to disappear
    Jun 22 11:08:41.188: INFO: Pod pod-configmaps-854027dd-208d-4dd3-81da-191e80b1a499 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:08:41.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7372" for this suite. 06/22/23 11:08:41.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:08:41.207
Jun 22 11:08:41.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename pods 06/22/23 11:08:41.208
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:08:41.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:08:41.229
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Jun 22 11:08:41.251: INFO: Waiting up to 5m0s for pod "server-envvars-f11e42fe-4fa2-412e-a358-9c3119119c2b" in namespace "pods-1531" to be "running and ready"
Jun 22 11:08:41.256: INFO: Pod "server-envvars-f11e42fe-4fa2-412e-a358-9c3119119c2b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.436269ms
Jun 22 11:08:41.256: INFO: The phase of Pod server-envvars-f11e42fe-4fa2-412e-a358-9c3119119c2b is Pending, waiting for it to be Running (with Ready = true)
Jun 22 11:08:43.262: INFO: Pod "server-envvars-f11e42fe-4fa2-412e-a358-9c3119119c2b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010533433s
Jun 22 11:08:43.262: INFO: The phase of Pod server-envvars-f11e42fe-4fa2-412e-a358-9c3119119c2b is Running (Ready = true)
Jun 22 11:08:43.262: INFO: Pod "server-envvars-f11e42fe-4fa2-412e-a358-9c3119119c2b" satisfied condition "running and ready"
Jun 22 11:08:43.300: INFO: Waiting up to 5m0s for pod "client-envvars-0c30900e-8322-47e0-9f6d-590d86de62ce" in namespace "pods-1531" to be "Succeeded or Failed"
Jun 22 11:08:43.305: INFO: Pod "client-envvars-0c30900e-8322-47e0-9f6d-590d86de62ce": Phase="Pending", Reason="", readiness=false. Elapsed: 5.491391ms
Jun 22 11:08:45.311: INFO: Pod "client-envvars-0c30900e-8322-47e0-9f6d-590d86de62ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011183138s
Jun 22 11:08:47.312: INFO: Pod "client-envvars-0c30900e-8322-47e0-9f6d-590d86de62ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012302046s
STEP: Saw pod success 06/22/23 11:08:47.312
Jun 22 11:08:47.312: INFO: Pod "client-envvars-0c30900e-8322-47e0-9f6d-590d86de62ce" satisfied condition "Succeeded or Failed"
Jun 22 11:08:47.317: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod client-envvars-0c30900e-8322-47e0-9f6d-590d86de62ce container env3cont: <nil>
STEP: delete the pod 06/22/23 11:08:47.33
Jun 22 11:08:47.351: INFO: Waiting for pod client-envvars-0c30900e-8322-47e0-9f6d-590d86de62ce to disappear
Jun 22 11:08:47.356: INFO: Pod client-envvars-0c30900e-8322-47e0-9f6d-590d86de62ce no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun 22 11:08:47.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1531" for this suite. 06/22/23 11:08:47.363
------------------------------
• [SLOW TEST] [6.166 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:08:41.207
    Jun 22 11:08:41.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename pods 06/22/23 11:08:41.208
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:08:41.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:08:41.229
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Jun 22 11:08:41.251: INFO: Waiting up to 5m0s for pod "server-envvars-f11e42fe-4fa2-412e-a358-9c3119119c2b" in namespace "pods-1531" to be "running and ready"
    Jun 22 11:08:41.256: INFO: Pod "server-envvars-f11e42fe-4fa2-412e-a358-9c3119119c2b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.436269ms
    Jun 22 11:08:41.256: INFO: The phase of Pod server-envvars-f11e42fe-4fa2-412e-a358-9c3119119c2b is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 11:08:43.262: INFO: Pod "server-envvars-f11e42fe-4fa2-412e-a358-9c3119119c2b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010533433s
    Jun 22 11:08:43.262: INFO: The phase of Pod server-envvars-f11e42fe-4fa2-412e-a358-9c3119119c2b is Running (Ready = true)
    Jun 22 11:08:43.262: INFO: Pod "server-envvars-f11e42fe-4fa2-412e-a358-9c3119119c2b" satisfied condition "running and ready"
    Jun 22 11:08:43.300: INFO: Waiting up to 5m0s for pod "client-envvars-0c30900e-8322-47e0-9f6d-590d86de62ce" in namespace "pods-1531" to be "Succeeded or Failed"
    Jun 22 11:08:43.305: INFO: Pod "client-envvars-0c30900e-8322-47e0-9f6d-590d86de62ce": Phase="Pending", Reason="", readiness=false. Elapsed: 5.491391ms
    Jun 22 11:08:45.311: INFO: Pod "client-envvars-0c30900e-8322-47e0-9f6d-590d86de62ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011183138s
    Jun 22 11:08:47.312: INFO: Pod "client-envvars-0c30900e-8322-47e0-9f6d-590d86de62ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012302046s
    STEP: Saw pod success 06/22/23 11:08:47.312
    Jun 22 11:08:47.312: INFO: Pod "client-envvars-0c30900e-8322-47e0-9f6d-590d86de62ce" satisfied condition "Succeeded or Failed"
    Jun 22 11:08:47.317: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod client-envvars-0c30900e-8322-47e0-9f6d-590d86de62ce container env3cont: <nil>
    STEP: delete the pod 06/22/23 11:08:47.33
    Jun 22 11:08:47.351: INFO: Waiting for pod client-envvars-0c30900e-8322-47e0-9f6d-590d86de62ce to disappear
    Jun 22 11:08:47.356: INFO: Pod client-envvars-0c30900e-8322-47e0-9f6d-590d86de62ce no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:08:47.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1531" for this suite. 06/22/23 11:08:47.363
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:08:47.376
Jun 22 11:08:47.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubectl 06/22/23 11:08:47.378
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:08:47.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:08:47.399
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 06/22/23 11:08:47.404
Jun 22 11:08:47.404: INFO: namespace kubectl-9653
Jun 22 11:08:47.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9653 create -f -'
Jun 22 11:08:49.489: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"agnhost-primary\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"agnhost-primary\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"agnhost-primary\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"agnhost-primary\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Jun 22 11:08:49.489: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 06/22/23 11:08:49.489
Jun 22 11:08:50.495: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 11:08:50.495: INFO: Found 0 / 1
Jun 22 11:08:51.496: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 11:08:51.496: INFO: Found 1 / 1
Jun 22 11:08:51.496: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 22 11:08:51.501: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 22 11:08:51.501: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 11:08:51.502: INFO: wait on agnhost-primary startup in kubectl-9653 
Jun 22 11:08:51.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9653 logs agnhost-primary-mbxxm agnhost-primary'
Jun 22 11:08:51.607: INFO: stderr: ""
Jun 22 11:08:51.607: INFO: stdout: "Paused\n"
STEP: exposing RC 06/22/23 11:08:51.607
Jun 22 11:08:51.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9653 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jun 22 11:08:51.710: INFO: stderr: ""
Jun 22 11:08:51.710: INFO: stdout: "service/rm2 exposed\n"
Jun 22 11:08:51.718: INFO: Service rm2 in namespace kubectl-9653 found.
STEP: exposing service 06/22/23 11:08:53.73
Jun 22 11:08:53.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9653 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jun 22 11:08:53.870: INFO: stderr: ""
Jun 22 11:08:53.870: INFO: stdout: "service/rm3 exposed\n"
Jun 22 11:08:53.877: INFO: Service rm3 in namespace kubectl-9653 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun 22 11:08:55.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9653" for this suite. 06/22/23 11:08:55.893
------------------------------
• [SLOW TEST] [8.527 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:08:47.376
    Jun 22 11:08:47.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubectl 06/22/23 11:08:47.378
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:08:47.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:08:47.399
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 06/22/23 11:08:47.404
    Jun 22 11:08:47.404: INFO: namespace kubectl-9653
    Jun 22 11:08:47.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9653 create -f -'
    Jun 22 11:08:49.489: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"agnhost-primary\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"agnhost-primary\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"agnhost-primary\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"agnhost-primary\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
    Jun 22 11:08:49.489: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 06/22/23 11:08:49.489
    Jun 22 11:08:50.495: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 22 11:08:50.495: INFO: Found 0 / 1
    Jun 22 11:08:51.496: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 22 11:08:51.496: INFO: Found 1 / 1
    Jun 22 11:08:51.496: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jun 22 11:08:51.501: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 22 11:08:51.501: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jun 22 11:08:51.502: INFO: wait on agnhost-primary startup in kubectl-9653 
    Jun 22 11:08:51.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9653 logs agnhost-primary-mbxxm agnhost-primary'
    Jun 22 11:08:51.607: INFO: stderr: ""
    Jun 22 11:08:51.607: INFO: stdout: "Paused\n"
    STEP: exposing RC 06/22/23 11:08:51.607
    Jun 22 11:08:51.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9653 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jun 22 11:08:51.710: INFO: stderr: ""
    Jun 22 11:08:51.710: INFO: stdout: "service/rm2 exposed\n"
    Jun 22 11:08:51.718: INFO: Service rm2 in namespace kubectl-9653 found.
    STEP: exposing service 06/22/23 11:08:53.73
    Jun 22 11:08:53.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9653 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jun 22 11:08:53.870: INFO: stderr: ""
    Jun 22 11:08:53.870: INFO: stdout: "service/rm3 exposed\n"
    Jun 22 11:08:53.877: INFO: Service rm3 in namespace kubectl-9653 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:08:55.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9653" for this suite. 06/22/23 11:08:55.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:08:55.904
Jun 22 11:08:55.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename prestop 06/22/23 11:08:55.905
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:08:55.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:08:55.926
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-2919 06/22/23 11:08:55.93
W0622 11:08:55.948820      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting for pods to come up. 06/22/23 11:08:55.948
Jun 22 11:08:55.949: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2919" to be "running"
Jun 22 11:08:55.953: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.529094ms
Jun 22 11:08:57.959: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.010198049s
Jun 22 11:08:57.959: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-2919 06/22/23 11:08:57.964
W0622 11:08:57.975198      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "tester" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "tester" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "tester" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "tester" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:08:57.975: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2919" to be "running"
Jun 22 11:08:57.981: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 5.653643ms
Jun 22 11:08:59.987: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.011934614s
Jun 22 11:08:59.987: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 06/22/23 11:08:59.987
Jun 22 11:09:05.008: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 06/22/23 11:09:05.009
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Jun 22 11:09:05.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-2919" for this suite. 06/22/23 11:09:05.032
------------------------------
• [SLOW TEST] [9.136 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:08:55.904
    Jun 22 11:08:55.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename prestop 06/22/23 11:08:55.905
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:08:55.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:08:55.926
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-2919 06/22/23 11:08:55.93
    W0622 11:08:55.948820      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Waiting for pods to come up. 06/22/23 11:08:55.948
    Jun 22 11:08:55.949: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2919" to be "running"
    Jun 22 11:08:55.953: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.529094ms
    Jun 22 11:08:57.959: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.010198049s
    Jun 22 11:08:57.959: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-2919 06/22/23 11:08:57.964
    W0622 11:08:57.975198      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "tester" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "tester" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "tester" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "tester" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:08:57.975: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2919" to be "running"
    Jun 22 11:08:57.981: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 5.653643ms
    Jun 22 11:08:59.987: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.011934614s
    Jun 22 11:08:59.987: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 06/22/23 11:08:59.987
    Jun 22 11:09:05.008: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 06/22/23 11:09:05.009
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:09:05.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-2919" for this suite. 06/22/23 11:09:05.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:09:05.042
Jun 22 11:09:05.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename security-context 06/22/23 11:09:05.043
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:09:05.059
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:09:05.062
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/22/23 11:09:05.065
W0622 11:09:05.077020      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:09:05.077: INFO: Waiting up to 5m0s for pod "security-context-6a694295-baef-40dd-ae42-0b37a41abf40" in namespace "security-context-8660" to be "Succeeded or Failed"
Jun 22 11:09:05.084: INFO: Pod "security-context-6a694295-baef-40dd-ae42-0b37a41abf40": Phase="Pending", Reason="", readiness=false. Elapsed: 7.74821ms
Jun 22 11:09:07.090: INFO: Pod "security-context-6a694295-baef-40dd-ae42-0b37a41abf40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013311698s
Jun 22 11:09:09.091: INFO: Pod "security-context-6a694295-baef-40dd-ae42-0b37a41abf40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013923171s
STEP: Saw pod success 06/22/23 11:09:09.091
Jun 22 11:09:09.091: INFO: Pod "security-context-6a694295-baef-40dd-ae42-0b37a41abf40" satisfied condition "Succeeded or Failed"
Jun 22 11:09:09.095: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod security-context-6a694295-baef-40dd-ae42-0b37a41abf40 container test-container: <nil>
STEP: delete the pod 06/22/23 11:09:09.105
Jun 22 11:09:09.119: INFO: Waiting for pod security-context-6a694295-baef-40dd-ae42-0b37a41abf40 to disappear
Jun 22 11:09:09.124: INFO: Pod security-context-6a694295-baef-40dd-ae42-0b37a41abf40 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jun 22 11:09:09.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-8660" for this suite. 06/22/23 11:09:09.131
------------------------------
• [4.097 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:09:05.042
    Jun 22 11:09:05.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename security-context 06/22/23 11:09:05.043
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:09:05.059
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:09:05.062
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/22/23 11:09:05.065
    W0622 11:09:05.077020      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:09:05.077: INFO: Waiting up to 5m0s for pod "security-context-6a694295-baef-40dd-ae42-0b37a41abf40" in namespace "security-context-8660" to be "Succeeded or Failed"
    Jun 22 11:09:05.084: INFO: Pod "security-context-6a694295-baef-40dd-ae42-0b37a41abf40": Phase="Pending", Reason="", readiness=false. Elapsed: 7.74821ms
    Jun 22 11:09:07.090: INFO: Pod "security-context-6a694295-baef-40dd-ae42-0b37a41abf40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013311698s
    Jun 22 11:09:09.091: INFO: Pod "security-context-6a694295-baef-40dd-ae42-0b37a41abf40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013923171s
    STEP: Saw pod success 06/22/23 11:09:09.091
    Jun 22 11:09:09.091: INFO: Pod "security-context-6a694295-baef-40dd-ae42-0b37a41abf40" satisfied condition "Succeeded or Failed"
    Jun 22 11:09:09.095: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod security-context-6a694295-baef-40dd-ae42-0b37a41abf40 container test-container: <nil>
    STEP: delete the pod 06/22/23 11:09:09.105
    Jun 22 11:09:09.119: INFO: Waiting for pod security-context-6a694295-baef-40dd-ae42-0b37a41abf40 to disappear
    Jun 22 11:09:09.124: INFO: Pod security-context-6a694295-baef-40dd-ae42-0b37a41abf40 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:09:09.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-8660" for this suite. 06/22/23 11:09:09.131
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:09:09.14
Jun 22 11:09:09.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename container-lifecycle-hook 06/22/23 11:09:09.141
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:09:09.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:09:09.162
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 06/22/23 11:09:09.181
W0622 11:09:09.193011      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:09:09.193: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5274" to be "running and ready"
Jun 22 11:09:09.197: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.911894ms
Jun 22 11:09:09.197: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 22 11:09:11.203: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010809428s
Jun 22 11:09:11.203: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun 22 11:09:11.204: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 06/22/23 11:09:11.209
W0622 11:09:11.218185      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-with-prestop-http-hook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-with-prestop-http-hook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-with-prestop-http-hook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-with-prestop-http-hook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:09:11.218: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-5274" to be "running and ready"
Jun 22 11:09:11.225: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.77863ms
Jun 22 11:09:11.225: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 22 11:09:13.230: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012542224s
Jun 22 11:09:13.230: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jun 22 11:09:13.230: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 06/22/23 11:09:13.235
Jun 22 11:09:13.246: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 11:09:13.251: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 11:09:15.251: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 11:09:15.256: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 06/22/23 11:09:15.256
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jun 22 11:09:15.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-5274" for this suite. 06/22/23 11:09:15.273
------------------------------
• [SLOW TEST] [6.142 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:09:09.14
    Jun 22 11:09:09.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/22/23 11:09:09.141
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:09:09.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:09:09.162
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 06/22/23 11:09:09.181
    W0622 11:09:09.193011      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:09:09.193: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5274" to be "running and ready"
    Jun 22 11:09:09.197: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.911894ms
    Jun 22 11:09:09.197: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 11:09:11.203: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010809428s
    Jun 22 11:09:11.203: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun 22 11:09:11.204: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 06/22/23 11:09:11.209
    W0622 11:09:11.218185      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-with-prestop-http-hook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-with-prestop-http-hook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-with-prestop-http-hook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-with-prestop-http-hook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:09:11.218: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-5274" to be "running and ready"
    Jun 22 11:09:11.225: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.77863ms
    Jun 22 11:09:11.225: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 11:09:13.230: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012542224s
    Jun 22 11:09:13.230: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jun 22 11:09:13.230: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 06/22/23 11:09:13.235
    Jun 22 11:09:13.246: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jun 22 11:09:13.251: INFO: Pod pod-with-prestop-http-hook still exists
    Jun 22 11:09:15.251: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jun 22 11:09:15.256: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 06/22/23 11:09:15.256
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:09:15.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-5274" for this suite. 06/22/23 11:09:15.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:09:15.283
Jun 22 11:09:15.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename container-runtime 06/22/23 11:09:15.284
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:09:15.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:09:15.318
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 06/22/23 11:09:15.32
W0622 11:09:15.333136      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: wait for the container to reach Succeeded 06/22/23 11:09:15.333
STEP: get the container status 06/22/23 11:09:19.365
STEP: the container should be terminated 06/22/23 11:09:19.37
STEP: the termination message should be set 06/22/23 11:09:19.37
Jun 22 11:09:19.370: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 06/22/23 11:09:19.37
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jun 22 11:09:19.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-909" for this suite. 06/22/23 11:09:19.402
------------------------------
• [4.127 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:09:15.283
    Jun 22 11:09:15.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename container-runtime 06/22/23 11:09:15.284
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:09:15.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:09:15.318
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 06/22/23 11:09:15.32
    W0622 11:09:15.333136      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: wait for the container to reach Succeeded 06/22/23 11:09:15.333
    STEP: get the container status 06/22/23 11:09:19.365
    STEP: the container should be terminated 06/22/23 11:09:19.37
    STEP: the termination message should be set 06/22/23 11:09:19.37
    Jun 22 11:09:19.370: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 06/22/23 11:09:19.37
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:09:19.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-909" for this suite. 06/22/23 11:09:19.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:09:19.411
Jun 22 11:09:19.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubelet-test 06/22/23 11:09:19.412
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:09:19.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:09:19.438
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
W0622 11:09:19.452705      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:09:19.452: INFO: Waiting up to 5m0s for pod "busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4" in namespace "kubelet-test-3201" to be "running and ready"
Jun 22 11:09:19.457: INFO: Pod "busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.382677ms
Jun 22 11:09:19.457: INFO: The phase of Pod busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 11:09:21.464: INFO: Pod "busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4": Phase="Running", Reason="", readiness=true. Elapsed: 2.011301569s
Jun 22 11:09:21.464: INFO: The phase of Pod busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4 is Running (Ready = true)
Jun 22 11:09:21.464: INFO: Pod "busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jun 22 11:09:21.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3201" for this suite. 06/22/23 11:09:21.545
------------------------------
• [2.144 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:09:19.411
    Jun 22 11:09:19.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubelet-test 06/22/23 11:09:19.412
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:09:19.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:09:19.438
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    W0622 11:09:19.452705      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:09:19.452: INFO: Waiting up to 5m0s for pod "busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4" in namespace "kubelet-test-3201" to be "running and ready"
    Jun 22 11:09:19.457: INFO: Pod "busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.382677ms
    Jun 22 11:09:19.457: INFO: The phase of Pod busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 11:09:21.464: INFO: Pod "busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4": Phase="Running", Reason="", readiness=true. Elapsed: 2.011301569s
    Jun 22 11:09:21.464: INFO: The phase of Pod busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4 is Running (Ready = true)
    Jun 22 11:09:21.464: INFO: Pod "busybox-scheduling-8e643b4a-b9a5-402e-b284-2f0fef8de1c4" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:09:21.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3201" for this suite. 06/22/23 11:09:21.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:09:21.556
Jun 22 11:09:21.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename aggregator 06/22/23 11:09:21.557
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:09:21.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:09:21.584
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jun 22 11:09:21.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 06/22/23 11:09:21.588
W0622 11:09:21.953373      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "sample-apiserver", "etcd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "sample-apiserver", "etcd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "sample-apiserver", "etcd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "sample-apiserver", "etcd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:09:21.967: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 22 11:09:24.034: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 11:09:26.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 11:09:28.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 11:09:30.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 11:09:32.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 11:09:34.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 11:09:36.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 11:09:38.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 11:09:40.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 11:09:42.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 11:09:44.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 11:09:46.176: INFO: Waited 125.895306ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 06/22/23 11:09:46.329
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 06/22/23 11:09:46.361
STEP: List APIServices 06/22/23 11:09:46.416
Jun 22 11:09:46.466: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Jun 22 11:09:47.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-4715" for this suite. 06/22/23 11:09:47.313
------------------------------
• [SLOW TEST] [25.811 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:09:21.556
    Jun 22 11:09:21.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename aggregator 06/22/23 11:09:21.557
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:09:21.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:09:21.584
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jun 22 11:09:21.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 06/22/23 11:09:21.588
    W0622 11:09:21.953373      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "sample-apiserver", "etcd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "sample-apiserver", "etcd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "sample-apiserver", "etcd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "sample-apiserver", "etcd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:09:21.967: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jun 22 11:09:24.034: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 22 11:09:26.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 22 11:09:28.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 22 11:09:30.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 22 11:09:32.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 22 11:09:34.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 22 11:09:36.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 22 11:09:38.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 22 11:09:40.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 22 11:09:42.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 22 11:09:44.039: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 9, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 22 11:09:46.176: INFO: Waited 125.895306ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 06/22/23 11:09:46.329
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 06/22/23 11:09:46.361
    STEP: List APIServices 06/22/23 11:09:46.416
    Jun 22 11:09:46.466: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:09:47.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-4715" for this suite. 06/22/23 11:09:47.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:09:47.369
Jun 22 11:09:47.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename downward-api 06/22/23 11:09:47.37
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:09:47.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:09:47.394
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 06/22/23 11:09:47.397
W0622 11:09:47.410950      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:09:47.411: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9cae2a6-a532-4313-981c-f3c00828607c" in namespace "downward-api-8984" to be "Succeeded or Failed"
Jun 22 11:09:47.415: INFO: Pod "downwardapi-volume-b9cae2a6-a532-4313-981c-f3c00828607c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.574015ms
Jun 22 11:09:49.422: INFO: Pod "downwardapi-volume-b9cae2a6-a532-4313-981c-f3c00828607c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010828543s
Jun 22 11:09:51.422: INFO: Pod "downwardapi-volume-b9cae2a6-a532-4313-981c-f3c00828607c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011448549s
STEP: Saw pod success 06/22/23 11:09:51.422
Jun 22 11:09:51.422: INFO: Pod "downwardapi-volume-b9cae2a6-a532-4313-981c-f3c00828607c" satisfied condition "Succeeded or Failed"
Jun 22 11:09:51.427: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-b9cae2a6-a532-4313-981c-f3c00828607c container client-container: <nil>
STEP: delete the pod 06/22/23 11:09:51.437
Jun 22 11:09:51.464: INFO: Waiting for pod downwardapi-volume-b9cae2a6-a532-4313-981c-f3c00828607c to disappear
Jun 22 11:09:51.468: INFO: Pod downwardapi-volume-b9cae2a6-a532-4313-981c-f3c00828607c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun 22 11:09:51.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8984" for this suite. 06/22/23 11:09:51.475
------------------------------
• [4.116 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:09:47.369
    Jun 22 11:09:47.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename downward-api 06/22/23 11:09:47.37
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:09:47.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:09:47.394
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 06/22/23 11:09:47.397
    W0622 11:09:47.410950      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:09:47.411: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9cae2a6-a532-4313-981c-f3c00828607c" in namespace "downward-api-8984" to be "Succeeded or Failed"
    Jun 22 11:09:47.415: INFO: Pod "downwardapi-volume-b9cae2a6-a532-4313-981c-f3c00828607c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.574015ms
    Jun 22 11:09:49.422: INFO: Pod "downwardapi-volume-b9cae2a6-a532-4313-981c-f3c00828607c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010828543s
    Jun 22 11:09:51.422: INFO: Pod "downwardapi-volume-b9cae2a6-a532-4313-981c-f3c00828607c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011448549s
    STEP: Saw pod success 06/22/23 11:09:51.422
    Jun 22 11:09:51.422: INFO: Pod "downwardapi-volume-b9cae2a6-a532-4313-981c-f3c00828607c" satisfied condition "Succeeded or Failed"
    Jun 22 11:09:51.427: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-b9cae2a6-a532-4313-981c-f3c00828607c container client-container: <nil>
    STEP: delete the pod 06/22/23 11:09:51.437
    Jun 22 11:09:51.464: INFO: Waiting for pod downwardapi-volume-b9cae2a6-a532-4313-981c-f3c00828607c to disappear
    Jun 22 11:09:51.468: INFO: Pod downwardapi-volume-b9cae2a6-a532-4313-981c-f3c00828607c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:09:51.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8984" for this suite. 06/22/23 11:09:51.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:09:51.486
Jun 22 11:09:51.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename configmap 06/22/23 11:09:51.487
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:09:51.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:09:51.514
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-9f9fa3cd-9008-4f7f-946e-b0e0c98c17bb 06/22/23 11:09:51.518
STEP: Creating a pod to test consume configMaps 06/22/23 11:09:51.525
W0622 11:09:51.537892      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:09:51.538: INFO: Waiting up to 5m0s for pod "pod-configmaps-967ba226-b560-45c7-97ca-c71639c6e0e1" in namespace "configmap-7553" to be "Succeeded or Failed"
Jun 22 11:09:51.546: INFO: Pod "pod-configmaps-967ba226-b560-45c7-97ca-c71639c6e0e1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039326ms
Jun 22 11:09:53.552: INFO: Pod "pod-configmaps-967ba226-b560-45c7-97ca-c71639c6e0e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013940433s
Jun 22 11:09:55.553: INFO: Pod "pod-configmaps-967ba226-b560-45c7-97ca-c71639c6e0e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015271559s
STEP: Saw pod success 06/22/23 11:09:55.553
Jun 22 11:09:55.553: INFO: Pod "pod-configmaps-967ba226-b560-45c7-97ca-c71639c6e0e1" satisfied condition "Succeeded or Failed"
Jun 22 11:09:55.558: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-configmaps-967ba226-b560-45c7-97ca-c71639c6e0e1 container agnhost-container: <nil>
STEP: delete the pod 06/22/23 11:09:55.568
Jun 22 11:09:55.585: INFO: Waiting for pod pod-configmaps-967ba226-b560-45c7-97ca-c71639c6e0e1 to disappear
Jun 22 11:09:55.589: INFO: Pod pod-configmaps-967ba226-b560-45c7-97ca-c71639c6e0e1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun 22 11:09:55.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7553" for this suite. 06/22/23 11:09:55.595
------------------------------
• [4.118 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:09:51.486
    Jun 22 11:09:51.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename configmap 06/22/23 11:09:51.487
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:09:51.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:09:51.514
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-9f9fa3cd-9008-4f7f-946e-b0e0c98c17bb 06/22/23 11:09:51.518
    STEP: Creating a pod to test consume configMaps 06/22/23 11:09:51.525
    W0622 11:09:51.537892      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:09:51.538: INFO: Waiting up to 5m0s for pod "pod-configmaps-967ba226-b560-45c7-97ca-c71639c6e0e1" in namespace "configmap-7553" to be "Succeeded or Failed"
    Jun 22 11:09:51.546: INFO: Pod "pod-configmaps-967ba226-b560-45c7-97ca-c71639c6e0e1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.039326ms
    Jun 22 11:09:53.552: INFO: Pod "pod-configmaps-967ba226-b560-45c7-97ca-c71639c6e0e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013940433s
    Jun 22 11:09:55.553: INFO: Pod "pod-configmaps-967ba226-b560-45c7-97ca-c71639c6e0e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015271559s
    STEP: Saw pod success 06/22/23 11:09:55.553
    Jun 22 11:09:55.553: INFO: Pod "pod-configmaps-967ba226-b560-45c7-97ca-c71639c6e0e1" satisfied condition "Succeeded or Failed"
    Jun 22 11:09:55.558: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-configmaps-967ba226-b560-45c7-97ca-c71639c6e0e1 container agnhost-container: <nil>
    STEP: delete the pod 06/22/23 11:09:55.568
    Jun 22 11:09:55.585: INFO: Waiting for pod pod-configmaps-967ba226-b560-45c7-97ca-c71639c6e0e1 to disappear
    Jun 22 11:09:55.589: INFO: Pod pod-configmaps-967ba226-b560-45c7-97ca-c71639c6e0e1 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:09:55.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7553" for this suite. 06/22/23 11:09:55.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:09:55.605
Jun 22 11:09:55.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename dns 06/22/23 11:09:55.607
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:09:55.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:09:55.627
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 06/22/23 11:09:55.63
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 06/22/23 11:09:55.63
STEP: creating a pod to probe DNS 06/22/23 11:09:55.63
STEP: submitting the pod to kubernetes 06/22/23 11:09:55.63
W0622 11:09:55.644873      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:09:55.645: INFO: Waiting up to 15m0s for pod "dns-test-90843d03-dcc5-42a0-ae46-2f3546ce3f94" in namespace "dns-385" to be "running"
Jun 22 11:09:55.652: INFO: Pod "dns-test-90843d03-dcc5-42a0-ae46-2f3546ce3f94": Phase="Pending", Reason="", readiness=false. Elapsed: 7.107728ms
Jun 22 11:09:57.657: INFO: Pod "dns-test-90843d03-dcc5-42a0-ae46-2f3546ce3f94": Phase="Running", Reason="", readiness=true. Elapsed: 2.012825448s
Jun 22 11:09:57.657: INFO: Pod "dns-test-90843d03-dcc5-42a0-ae46-2f3546ce3f94" satisfied condition "running"
STEP: retrieving the pod 06/22/23 11:09:57.657
STEP: looking for the results for each expected name from probers 06/22/23 11:09:57.662
Jun 22 11:09:57.688: INFO: DNS probes using dns-385/dns-test-90843d03-dcc5-42a0-ae46-2f3546ce3f94 succeeded

STEP: deleting the pod 06/22/23 11:09:57.688
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun 22 11:09:57.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-385" for this suite. 06/22/23 11:09:57.717
------------------------------
• [2.120 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:09:55.605
    Jun 22 11:09:55.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename dns 06/22/23 11:09:55.607
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:09:55.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:09:55.627
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     06/22/23 11:09:55.63
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     06/22/23 11:09:55.63
    STEP: creating a pod to probe DNS 06/22/23 11:09:55.63
    STEP: submitting the pod to kubernetes 06/22/23 11:09:55.63
    W0622 11:09:55.644873      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:09:55.645: INFO: Waiting up to 15m0s for pod "dns-test-90843d03-dcc5-42a0-ae46-2f3546ce3f94" in namespace "dns-385" to be "running"
    Jun 22 11:09:55.652: INFO: Pod "dns-test-90843d03-dcc5-42a0-ae46-2f3546ce3f94": Phase="Pending", Reason="", readiness=false. Elapsed: 7.107728ms
    Jun 22 11:09:57.657: INFO: Pod "dns-test-90843d03-dcc5-42a0-ae46-2f3546ce3f94": Phase="Running", Reason="", readiness=true. Elapsed: 2.012825448s
    Jun 22 11:09:57.657: INFO: Pod "dns-test-90843d03-dcc5-42a0-ae46-2f3546ce3f94" satisfied condition "running"
    STEP: retrieving the pod 06/22/23 11:09:57.657
    STEP: looking for the results for each expected name from probers 06/22/23 11:09:57.662
    Jun 22 11:09:57.688: INFO: DNS probes using dns-385/dns-test-90843d03-dcc5-42a0-ae46-2f3546ce3f94 succeeded

    STEP: deleting the pod 06/22/23 11:09:57.688
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:09:57.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-385" for this suite. 06/22/23 11:09:57.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:09:57.727
Jun 22 11:09:57.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename pod-network-test 06/22/23 11:09:57.728
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:09:57.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:09:57.749
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-7433 06/22/23 11:09:57.752
STEP: creating a selector 06/22/23 11:09:57.752
STEP: Creating the service pods in kubernetes 06/22/23 11:09:57.752
Jun 22 11:09:57.753: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
W0622 11:09:57.775481      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0622 11:09:57.784439      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0622 11:09:57.796977      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:09:57.797: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7433" to be "running and ready"
Jun 22 11:09:57.803: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.735954ms
Jun 22 11:09:57.803: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 11:09:59.810: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.012953286s
Jun 22 11:09:59.810: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:10:01.809: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.012177353s
Jun 22 11:10:01.809: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:10:03.809: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01259653s
Jun 22 11:10:03.809: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:10:05.809: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012109267s
Jun 22 11:10:05.809: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:10:07.808: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011431323s
Jun 22 11:10:07.808: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:10:09.808: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011678708s
Jun 22 11:10:09.809: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun 22 11:10:09.809: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun 22 11:10:09.814: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7433" to be "running and ready"
Jun 22 11:10:09.818: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.493975ms
Jun 22 11:10:09.818: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun 22 11:10:09.818: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun 22 11:10:09.822: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7433" to be "running and ready"
Jun 22 11:10:09.827: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.535991ms
Jun 22 11:10:09.827: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun 22 11:10:09.827: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/22/23 11:10:09.831
W0622 11:10:09.840216      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:10:09.840: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7433" to be "running"
Jun 22 11:10:09.847: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.066625ms
Jun 22 11:10:11.852: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012487038s
Jun 22 11:10:11.852: INFO: Pod "test-container-pod" satisfied condition "running"
Jun 22 11:10:11.857: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 22 11:10:11.857: INFO: Breadth first check of 100.96.4.223 on host 10.92.224.179...
Jun 22 11:10:11.862: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.118:9080/dial?request=hostname&protocol=http&host=100.96.4.223&port=8083&tries=1'] Namespace:pod-network-test-7433 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 11:10:11.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 11:10:11.862: INFO: ExecWithOptions: Clientset creation
Jun 22 11:10:11.863: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-7433/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.118%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.96.4.223%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 22 11:10:11.987: INFO: Waiting for responses: map[]
Jun 22 11:10:11.987: INFO: reached 100.96.4.223 after 0/1 tries
Jun 22 11:10:11.987: INFO: Breadth first check of 100.96.3.117 on host 10.92.226.162...
Jun 22 11:10:11.992: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.118:9080/dial?request=hostname&protocol=http&host=100.96.3.117&port=8083&tries=1'] Namespace:pod-network-test-7433 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 11:10:11.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 11:10:11.993: INFO: ExecWithOptions: Clientset creation
Jun 22 11:10:11.993: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-7433/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.118%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.96.3.117%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 22 11:10:12.098: INFO: Waiting for responses: map[]
Jun 22 11:10:12.098: INFO: reached 100.96.3.117 after 0/1 tries
Jun 22 11:10:12.098: INFO: Breadth first check of 100.96.2.213 on host 10.92.224.62...
Jun 22 11:10:12.102: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.118:9080/dial?request=hostname&protocol=http&host=100.96.2.213&port=8083&tries=1'] Namespace:pod-network-test-7433 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 11:10:12.102: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 11:10:12.103: INFO: ExecWithOptions: Clientset creation
Jun 22 11:10:12.103: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-7433/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.118%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.96.2.213%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 22 11:10:12.204: INFO: Waiting for responses: map[]
Jun 22 11:10:12.204: INFO: reached 100.96.2.213 after 0/1 tries
Jun 22 11:10:12.204: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jun 22 11:10:12.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7433" for this suite. 06/22/23 11:10:12.213
------------------------------
• [SLOW TEST] [14.494 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:09:57.727
    Jun 22 11:09:57.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename pod-network-test 06/22/23 11:09:57.728
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:09:57.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:09:57.749
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-7433 06/22/23 11:09:57.752
    STEP: creating a selector 06/22/23 11:09:57.752
    STEP: Creating the service pods in kubernetes 06/22/23 11:09:57.752
    Jun 22 11:09:57.753: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    W0622 11:09:57.775481      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0622 11:09:57.784439      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0622 11:09:57.796977      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:09:57.797: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7433" to be "running and ready"
    Jun 22 11:09:57.803: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.735954ms
    Jun 22 11:09:57.803: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 11:09:59.810: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.012953286s
    Jun 22 11:09:59.810: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:10:01.809: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.012177353s
    Jun 22 11:10:01.809: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:10:03.809: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01259653s
    Jun 22 11:10:03.809: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:10:05.809: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012109267s
    Jun 22 11:10:05.809: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:10:07.808: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011431323s
    Jun 22 11:10:07.808: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:10:09.808: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011678708s
    Jun 22 11:10:09.809: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun 22 11:10:09.809: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun 22 11:10:09.814: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7433" to be "running and ready"
    Jun 22 11:10:09.818: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.493975ms
    Jun 22 11:10:09.818: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun 22 11:10:09.818: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun 22 11:10:09.822: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7433" to be "running and ready"
    Jun 22 11:10:09.827: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.535991ms
    Jun 22 11:10:09.827: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun 22 11:10:09.827: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/22/23 11:10:09.831
    W0622 11:10:09.840216      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:10:09.840: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7433" to be "running"
    Jun 22 11:10:09.847: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.066625ms
    Jun 22 11:10:11.852: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012487038s
    Jun 22 11:10:11.852: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun 22 11:10:11.857: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun 22 11:10:11.857: INFO: Breadth first check of 100.96.4.223 on host 10.92.224.179...
    Jun 22 11:10:11.862: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.118:9080/dial?request=hostname&protocol=http&host=100.96.4.223&port=8083&tries=1'] Namespace:pod-network-test-7433 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 11:10:11.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 11:10:11.862: INFO: ExecWithOptions: Clientset creation
    Jun 22 11:10:11.863: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-7433/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.118%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.96.4.223%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 22 11:10:11.987: INFO: Waiting for responses: map[]
    Jun 22 11:10:11.987: INFO: reached 100.96.4.223 after 0/1 tries
    Jun 22 11:10:11.987: INFO: Breadth first check of 100.96.3.117 on host 10.92.226.162...
    Jun 22 11:10:11.992: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.118:9080/dial?request=hostname&protocol=http&host=100.96.3.117&port=8083&tries=1'] Namespace:pod-network-test-7433 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 11:10:11.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 11:10:11.993: INFO: ExecWithOptions: Clientset creation
    Jun 22 11:10:11.993: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-7433/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.118%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.96.3.117%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 22 11:10:12.098: INFO: Waiting for responses: map[]
    Jun 22 11:10:12.098: INFO: reached 100.96.3.117 after 0/1 tries
    Jun 22 11:10:12.098: INFO: Breadth first check of 100.96.2.213 on host 10.92.224.62...
    Jun 22 11:10:12.102: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.118:9080/dial?request=hostname&protocol=http&host=100.96.2.213&port=8083&tries=1'] Namespace:pod-network-test-7433 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 11:10:12.102: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 11:10:12.103: INFO: ExecWithOptions: Clientset creation
    Jun 22 11:10:12.103: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-7433/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.118%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D100.96.2.213%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 22 11:10:12.204: INFO: Waiting for responses: map[]
    Jun 22 11:10:12.204: INFO: reached 100.96.2.213 after 0/1 tries
    Jun 22 11:10:12.204: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:10:12.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7433" for this suite. 06/22/23 11:10:12.213
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:10:12.226
Jun 22 11:10:12.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename gc 06/22/23 11:10:12.227
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:12.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:12.252
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 06/22/23 11:10:12.255
W0622 11:10:12.262942      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the Deployment to create new ReplicaSet 06/22/23 11:10:12.263
STEP: delete the deployment 06/22/23 11:10:12.776
STEP: wait for all rs to be garbage collected 06/22/23 11:10:12.784
STEP: expected 0 rs, got 1 rs 06/22/23 11:10:12.794
STEP: expected 0 pods, got 2 pods 06/22/23 11:10:12.8
STEP: Gathering metrics 06/22/23 11:10:13.315
Jun 22 11:10:13.374: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" in namespace "kube-system" to be "running and ready"
Jun 22 11:10:13.386: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v": Phase="Running", Reason="", readiness=true. Elapsed: 12.408422ms
Jun 22 11:10:13.386: INFO: The phase of Pod kube-controller-manager-wl-antrea-prdzc-k4b4v is Running (Ready = true)
Jun 22 11:10:13.386: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" satisfied condition "running and ready"
Jun 22 11:10:13.525: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun 22 11:10:13.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2805" for this suite. 06/22/23 11:10:13.533
------------------------------
• [1.316 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:10:12.226
    Jun 22 11:10:12.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename gc 06/22/23 11:10:12.227
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:12.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:12.252
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 06/22/23 11:10:12.255
    W0622 11:10:12.262942      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the Deployment to create new ReplicaSet 06/22/23 11:10:12.263
    STEP: delete the deployment 06/22/23 11:10:12.776
    STEP: wait for all rs to be garbage collected 06/22/23 11:10:12.784
    STEP: expected 0 rs, got 1 rs 06/22/23 11:10:12.794
    STEP: expected 0 pods, got 2 pods 06/22/23 11:10:12.8
    STEP: Gathering metrics 06/22/23 11:10:13.315
    Jun 22 11:10:13.374: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" in namespace "kube-system" to be "running and ready"
    Jun 22 11:10:13.386: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v": Phase="Running", Reason="", readiness=true. Elapsed: 12.408422ms
    Jun 22 11:10:13.386: INFO: The phase of Pod kube-controller-manager-wl-antrea-prdzc-k4b4v is Running (Ready = true)
    Jun 22 11:10:13.386: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" satisfied condition "running and ready"
    Jun 22 11:10:13.525: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:10:13.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2805" for this suite. 06/22/23 11:10:13.533
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:10:13.542
Jun 22 11:10:13.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename watch 06/22/23 11:10:13.543
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:13.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:13.567
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 06/22/23 11:10:13.57
STEP: starting a background goroutine to produce watch events 06/22/23 11:10:13.574
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 06/22/23 11:10:13.574
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jun 22 11:10:16.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7168" for this suite. 06/22/23 11:10:16.402
------------------------------
• [2.910 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:10:13.542
    Jun 22 11:10:13.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename watch 06/22/23 11:10:13.543
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:13.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:13.567
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 06/22/23 11:10:13.57
    STEP: starting a background goroutine to produce watch events 06/22/23 11:10:13.574
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 06/22/23 11:10:13.574
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:10:16.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7168" for this suite. 06/22/23 11:10:16.402
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:10:16.454
Jun 22 11:10:16.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename emptydir 06/22/23 11:10:16.454
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:16.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:16.473
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 06/22/23 11:10:16.476
W0622 11:10:16.487250      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:10:16.487: INFO: Waiting up to 5m0s for pod "pod-d77a45a1-5c2e-48bd-b312-5b5870ec8731" in namespace "emptydir-2066" to be "Succeeded or Failed"
Jun 22 11:10:16.497: INFO: Pod "pod-d77a45a1-5c2e-48bd-b312-5b5870ec8731": Phase="Pending", Reason="", readiness=false. Elapsed: 10.206762ms
Jun 22 11:10:18.503: INFO: Pod "pod-d77a45a1-5c2e-48bd-b312-5b5870ec8731": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016377604s
Jun 22 11:10:20.506: INFO: Pod "pod-d77a45a1-5c2e-48bd-b312-5b5870ec8731": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018796587s
STEP: Saw pod success 06/22/23 11:10:20.506
Jun 22 11:10:20.506: INFO: Pod "pod-d77a45a1-5c2e-48bd-b312-5b5870ec8731" satisfied condition "Succeeded or Failed"
Jun 22 11:10:20.511: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-d77a45a1-5c2e-48bd-b312-5b5870ec8731 container test-container: <nil>
STEP: delete the pod 06/22/23 11:10:20.521
Jun 22 11:10:20.539: INFO: Waiting for pod pod-d77a45a1-5c2e-48bd-b312-5b5870ec8731 to disappear
Jun 22 11:10:20.543: INFO: Pod pod-d77a45a1-5c2e-48bd-b312-5b5870ec8731 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun 22 11:10:20.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2066" for this suite. 06/22/23 11:10:20.55
------------------------------
• [4.106 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:10:16.454
    Jun 22 11:10:16.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename emptydir 06/22/23 11:10:16.454
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:16.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:16.473
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 06/22/23 11:10:16.476
    W0622 11:10:16.487250      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:10:16.487: INFO: Waiting up to 5m0s for pod "pod-d77a45a1-5c2e-48bd-b312-5b5870ec8731" in namespace "emptydir-2066" to be "Succeeded or Failed"
    Jun 22 11:10:16.497: INFO: Pod "pod-d77a45a1-5c2e-48bd-b312-5b5870ec8731": Phase="Pending", Reason="", readiness=false. Elapsed: 10.206762ms
    Jun 22 11:10:18.503: INFO: Pod "pod-d77a45a1-5c2e-48bd-b312-5b5870ec8731": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016377604s
    Jun 22 11:10:20.506: INFO: Pod "pod-d77a45a1-5c2e-48bd-b312-5b5870ec8731": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018796587s
    STEP: Saw pod success 06/22/23 11:10:20.506
    Jun 22 11:10:20.506: INFO: Pod "pod-d77a45a1-5c2e-48bd-b312-5b5870ec8731" satisfied condition "Succeeded or Failed"
    Jun 22 11:10:20.511: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-d77a45a1-5c2e-48bd-b312-5b5870ec8731 container test-container: <nil>
    STEP: delete the pod 06/22/23 11:10:20.521
    Jun 22 11:10:20.539: INFO: Waiting for pod pod-d77a45a1-5c2e-48bd-b312-5b5870ec8731 to disappear
    Jun 22 11:10:20.543: INFO: Pod pod-d77a45a1-5c2e-48bd-b312-5b5870ec8731 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:10:20.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2066" for this suite. 06/22/23 11:10:20.55
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:10:20.559
Jun 22 11:10:20.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename services 06/22/23 11:10:20.56
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:20.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:20.582
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 06/22/23 11:10:20.586
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun 22 11:10:20.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8842" for this suite. 06/22/23 11:10:20.597
------------------------------
• [0.045 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:10:20.559
    Jun 22 11:10:20.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename services 06/22/23 11:10:20.56
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:20.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:20.582
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 06/22/23 11:10:20.586
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:10:20.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8842" for this suite. 06/22/23 11:10:20.597
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:10:20.605
Jun 22 11:10:20.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename emptydir 06/22/23 11:10:20.606
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:20.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:20.626
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 06/22/23 11:10:20.629
W0622 11:10:20.641310      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:10:20.641: INFO: Waiting up to 5m0s for pod "pod-a29bee15-9fbf-4561-9e6b-4810b4d6a181" in namespace "emptydir-9769" to be "Succeeded or Failed"
Jun 22 11:10:20.645: INFO: Pod "pod-a29bee15-9fbf-4561-9e6b-4810b4d6a181": Phase="Pending", Reason="", readiness=false. Elapsed: 3.917038ms
Jun 22 11:10:22.650: INFO: Pod "pod-a29bee15-9fbf-4561-9e6b-4810b4d6a181": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009560783s
Jun 22 11:10:24.651: INFO: Pod "pod-a29bee15-9fbf-4561-9e6b-4810b4d6a181": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010345548s
STEP: Saw pod success 06/22/23 11:10:24.651
Jun 22 11:10:24.651: INFO: Pod "pod-a29bee15-9fbf-4561-9e6b-4810b4d6a181" satisfied condition "Succeeded or Failed"
Jun 22 11:10:24.656: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-a29bee15-9fbf-4561-9e6b-4810b4d6a181 container test-container: <nil>
STEP: delete the pod 06/22/23 11:10:24.667
Jun 22 11:10:24.683: INFO: Waiting for pod pod-a29bee15-9fbf-4561-9e6b-4810b4d6a181 to disappear
Jun 22 11:10:24.687: INFO: Pod pod-a29bee15-9fbf-4561-9e6b-4810b4d6a181 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun 22 11:10:24.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9769" for this suite. 06/22/23 11:10:24.694
------------------------------
• [4.097 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:10:20.605
    Jun 22 11:10:20.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename emptydir 06/22/23 11:10:20.606
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:20.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:20.626
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 06/22/23 11:10:20.629
    W0622 11:10:20.641310      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:10:20.641: INFO: Waiting up to 5m0s for pod "pod-a29bee15-9fbf-4561-9e6b-4810b4d6a181" in namespace "emptydir-9769" to be "Succeeded or Failed"
    Jun 22 11:10:20.645: INFO: Pod "pod-a29bee15-9fbf-4561-9e6b-4810b4d6a181": Phase="Pending", Reason="", readiness=false. Elapsed: 3.917038ms
    Jun 22 11:10:22.650: INFO: Pod "pod-a29bee15-9fbf-4561-9e6b-4810b4d6a181": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009560783s
    Jun 22 11:10:24.651: INFO: Pod "pod-a29bee15-9fbf-4561-9e6b-4810b4d6a181": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010345548s
    STEP: Saw pod success 06/22/23 11:10:24.651
    Jun 22 11:10:24.651: INFO: Pod "pod-a29bee15-9fbf-4561-9e6b-4810b4d6a181" satisfied condition "Succeeded or Failed"
    Jun 22 11:10:24.656: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-a29bee15-9fbf-4561-9e6b-4810b4d6a181 container test-container: <nil>
    STEP: delete the pod 06/22/23 11:10:24.667
    Jun 22 11:10:24.683: INFO: Waiting for pod pod-a29bee15-9fbf-4561-9e6b-4810b4d6a181 to disappear
    Jun 22 11:10:24.687: INFO: Pod pod-a29bee15-9fbf-4561-9e6b-4810b4d6a181 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:10:24.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9769" for this suite. 06/22/23 11:10:24.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:10:24.703
Jun 22 11:10:24.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 11:10:24.704
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:24.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:24.725
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 06/22/23 11:10:24.728
W0622 11:10:24.738721      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:10:24.738: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e025d0f6-b240-4f31-ae10-4f29b8ecf5c1" in namespace "projected-8253" to be "Succeeded or Failed"
Jun 22 11:10:24.742: INFO: Pod "downwardapi-volume-e025d0f6-b240-4f31-ae10-4f29b8ecf5c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007674ms
Jun 22 11:10:26.749: INFO: Pod "downwardapi-volume-e025d0f6-b240-4f31-ae10-4f29b8ecf5c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010480738s
Jun 22 11:10:28.748: INFO: Pod "downwardapi-volume-e025d0f6-b240-4f31-ae10-4f29b8ecf5c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009924603s
STEP: Saw pod success 06/22/23 11:10:28.748
Jun 22 11:10:28.748: INFO: Pod "downwardapi-volume-e025d0f6-b240-4f31-ae10-4f29b8ecf5c1" satisfied condition "Succeeded or Failed"
Jun 22 11:10:28.752: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-e025d0f6-b240-4f31-ae10-4f29b8ecf5c1 container client-container: <nil>
STEP: delete the pod 06/22/23 11:10:28.761
Jun 22 11:10:28.773: INFO: Waiting for pod downwardapi-volume-e025d0f6-b240-4f31-ae10-4f29b8ecf5c1 to disappear
Jun 22 11:10:28.777: INFO: Pod downwardapi-volume-e025d0f6-b240-4f31-ae10-4f29b8ecf5c1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun 22 11:10:28.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8253" for this suite. 06/22/23 11:10:28.782
------------------------------
• [4.087 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:10:24.703
    Jun 22 11:10:24.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 11:10:24.704
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:24.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:24.725
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 06/22/23 11:10:24.728
    W0622 11:10:24.738721      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:10:24.738: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e025d0f6-b240-4f31-ae10-4f29b8ecf5c1" in namespace "projected-8253" to be "Succeeded or Failed"
    Jun 22 11:10:24.742: INFO: Pod "downwardapi-volume-e025d0f6-b240-4f31-ae10-4f29b8ecf5c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007674ms
    Jun 22 11:10:26.749: INFO: Pod "downwardapi-volume-e025d0f6-b240-4f31-ae10-4f29b8ecf5c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010480738s
    Jun 22 11:10:28.748: INFO: Pod "downwardapi-volume-e025d0f6-b240-4f31-ae10-4f29b8ecf5c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009924603s
    STEP: Saw pod success 06/22/23 11:10:28.748
    Jun 22 11:10:28.748: INFO: Pod "downwardapi-volume-e025d0f6-b240-4f31-ae10-4f29b8ecf5c1" satisfied condition "Succeeded or Failed"
    Jun 22 11:10:28.752: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-e025d0f6-b240-4f31-ae10-4f29b8ecf5c1 container client-container: <nil>
    STEP: delete the pod 06/22/23 11:10:28.761
    Jun 22 11:10:28.773: INFO: Waiting for pod downwardapi-volume-e025d0f6-b240-4f31-ae10-4f29b8ecf5c1 to disappear
    Jun 22 11:10:28.777: INFO: Pod downwardapi-volume-e025d0f6-b240-4f31-ae10-4f29b8ecf5c1 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:10:28.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8253" for this suite. 06/22/23 11:10:28.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:10:28.79
Jun 22 11:10:28.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename container-probe 06/22/23 11:10:28.791
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:28.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:28.81
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
W0622 11:10:28.824608      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:10:28.824: INFO: Waiting up to 5m0s for pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28" in namespace "container-probe-2854" to be "running and ready"
Jun 22 11:10:28.828: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Pending", Reason="", readiness=false. Elapsed: 3.789248ms
Jun 22 11:10:28.828: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 11:10:30.834: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 2.009755054s
Jun 22 11:10:30.834: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
Jun 22 11:10:32.835: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 4.010717514s
Jun 22 11:10:32.835: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
Jun 22 11:10:34.833: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 6.008819825s
Jun 22 11:10:34.833: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
Jun 22 11:10:36.834: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 8.010081343s
Jun 22 11:10:36.834: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
Jun 22 11:10:38.834: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 10.009547555s
Jun 22 11:10:38.834: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
Jun 22 11:10:40.835: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 12.011216395s
Jun 22 11:10:40.835: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
Jun 22 11:10:42.834: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 14.009510049s
Jun 22 11:10:42.834: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
Jun 22 11:10:44.834: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 16.009688212s
Jun 22 11:10:44.834: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
Jun 22 11:10:46.835: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 18.010859134s
Jun 22 11:10:46.835: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
Jun 22 11:10:48.836: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 20.011415138s
Jun 22 11:10:48.836: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
Jun 22 11:10:50.835: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=true. Elapsed: 22.010782609s
Jun 22 11:10:50.835: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = true)
Jun 22 11:10:50.835: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28" satisfied condition "running and ready"
Jun 22 11:10:50.840: INFO: Container started at 2023-06-22 11:10:29 +0000 UTC, pod became ready at 2023-06-22 11:10:49 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun 22 11:10:50.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2854" for this suite. 06/22/23 11:10:50.846
------------------------------
• [SLOW TEST] [22.069 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:10:28.79
    Jun 22 11:10:28.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename container-probe 06/22/23 11:10:28.791
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:28.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:28.81
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    W0622 11:10:28.824608      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:10:28.824: INFO: Waiting up to 5m0s for pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28" in namespace "container-probe-2854" to be "running and ready"
    Jun 22 11:10:28.828: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Pending", Reason="", readiness=false. Elapsed: 3.789248ms
    Jun 22 11:10:28.828: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 11:10:30.834: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 2.009755054s
    Jun 22 11:10:30.834: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
    Jun 22 11:10:32.835: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 4.010717514s
    Jun 22 11:10:32.835: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
    Jun 22 11:10:34.833: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 6.008819825s
    Jun 22 11:10:34.833: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
    Jun 22 11:10:36.834: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 8.010081343s
    Jun 22 11:10:36.834: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
    Jun 22 11:10:38.834: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 10.009547555s
    Jun 22 11:10:38.834: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
    Jun 22 11:10:40.835: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 12.011216395s
    Jun 22 11:10:40.835: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
    Jun 22 11:10:42.834: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 14.009510049s
    Jun 22 11:10:42.834: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
    Jun 22 11:10:44.834: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 16.009688212s
    Jun 22 11:10:44.834: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
    Jun 22 11:10:46.835: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 18.010859134s
    Jun 22 11:10:46.835: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
    Jun 22 11:10:48.836: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=false. Elapsed: 20.011415138s
    Jun 22 11:10:48.836: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = false)
    Jun 22 11:10:50.835: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28": Phase="Running", Reason="", readiness=true. Elapsed: 22.010782609s
    Jun 22 11:10:50.835: INFO: The phase of Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 is Running (Ready = true)
    Jun 22 11:10:50.835: INFO: Pod "test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28" satisfied condition "running and ready"
    Jun 22 11:10:50.840: INFO: Container started at 2023-06-22 11:10:29 +0000 UTC, pod became ready at 2023-06-22 11:10:49 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:10:50.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2854" for this suite. 06/22/23 11:10:50.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:10:50.861
Jun 22 11:10:50.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename sched-pred 06/22/23 11:10:50.862
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:50.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:50.883
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jun 22 11:10:50.886: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 11:10:50.897: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 11:10:50.902: INFO: 
Logging pods the apiserver thinks is on node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k before test
Jun 22 11:10:50.913: INFO: corgi-test-8c78878b-6r5tv from corgi-test-privileged started at 2023-06-22 06:12:10 +0000 UTC (1 container statuses recorded)
Jun 22 11:10:50.913: INFO: 	Container nginx ready: true, restart count 0
Jun 22 11:10:50.913: INFO: antrea-agent-dxs2v from kube-system started at 2023-06-22 05:45:55 +0000 UTC (2 container statuses recorded)
Jun 22 11:10:50.913: INFO: 	Container antrea-agent ready: true, restart count 0
Jun 22 11:10:50.913: INFO: 	Container antrea-ovs ready: true, restart count 0
Jun 22 11:10:50.913: INFO: kube-proxy-czx75 from kube-system started at 2023-06-22 05:45:55 +0000 UTC (1 container statuses recorded)
Jun 22 11:10:50.913: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 11:10:50.913: INFO: sonobuoy-e2e-job-38d4831e07f74ae5 from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
Jun 22 11:10:50.913: INFO: 	Container e2e ready: true, restart count 0
Jun 22 11:10:50.913: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 11:10:50.913: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-5d74n from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
Jun 22 11:10:50.913: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 11:10:50.913: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 11:10:50.913: INFO: register-placeholder-ktpl4 from vmware-system-antrea started at 2023-06-22 11:09:02 +0000 UTC (1 container statuses recorded)
Jun 22 11:10:50.913: INFO: 	Container register ready: false, restart count 0
Jun 22 11:10:50.913: INFO: vsphere-csi-node-mp5v2 from vmware-system-csi started at 2023-06-22 05:45:55 +0000 UTC (3 container statuses recorded)
Jun 22 11:10:50.913: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 22 11:10:50.913: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 22 11:10:50.913: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jun 22 11:10:50.913: INFO: 
Logging pods the apiserver thinks is on node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw before test
Jun 22 11:10:50.923: INFO: test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 from container-probe-2854 started at 2023-06-22 11:10:28 +0000 UTC (1 container statuses recorded)
Jun 22 11:10:50.923: INFO: 	Container test-webserver ready: true, restart count 0
Jun 22 11:10:50.923: INFO: antrea-agent-n9smp from kube-system started at 2023-06-22 05:45:26 +0000 UTC (2 container statuses recorded)
Jun 22 11:10:50.923: INFO: 	Container antrea-agent ready: true, restart count 0
Jun 22 11:10:50.923: INFO: 	Container antrea-ovs ready: true, restart count 0
Jun 22 11:10:50.923: INFO: kube-proxy-rlz8k from kube-system started at 2023-06-22 05:45:26 +0000 UTC (1 container statuses recorded)
Jun 22 11:10:50.923: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 11:10:50.923: INFO: sonobuoy from sonobuoy started at 2023-06-22 10:19:41 +0000 UTC (1 container statuses recorded)
Jun 22 11:10:50.923: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 11:10:50.923: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-f4lqb from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
Jun 22 11:10:50.923: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 11:10:50.923: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 11:10:50.923: INFO: vsphere-csi-node-v6pm5 from vmware-system-csi started at 2023-06-22 05:45:26 +0000 UTC (3 container statuses recorded)
Jun 22 11:10:50.923: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 22 11:10:50.923: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 22 11:10:50.923: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jun 22 11:10:50.923: INFO: 
Logging pods the apiserver thinks is on node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 before test
Jun 22 11:10:50.934: INFO: antrea-agent-94vtc from kube-system started at 2023-06-22 05:45:16 +0000 UTC (2 container statuses recorded)
Jun 22 11:10:50.934: INFO: 	Container antrea-agent ready: true, restart count 1
Jun 22 11:10:50.934: INFO: 	Container antrea-ovs ready: true, restart count 0
Jun 22 11:10:50.934: INFO: kube-proxy-8rvhg from kube-system started at 2023-06-22 05:45:16 +0000 UTC (1 container statuses recorded)
Jun 22 11:10:50.934: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 11:10:50.934: INFO: metrics-server-64f66d5b9-8j569 from kube-system started at 2023-06-22 05:45:49 +0000 UTC (1 container statuses recorded)
Jun 22 11:10:50.934: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 11:10:50.934: INFO: secretgen-controller-7bdd5787fd-h9nfx from secretgen-controller started at 2023-06-22 05:45:49 +0000 UTC (1 container statuses recorded)
Jun 22 11:10:50.935: INFO: 	Container secretgen-controller ready: true, restart count 0
Jun 22 11:10:50.935: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-7x88w from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
Jun 22 11:10:50.935: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 11:10:50.935: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 11:10:50.935: INFO: vsphere-csi-node-wsgg4 from vmware-system-csi started at 2023-06-22 05:45:17 +0000 UTC (3 container statuses recorded)
Jun 22 11:10:50.935: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 22 11:10:50.935: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 22 11:10:50.935: INFO: 	Container vsphere-csi-node ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k 06/22/23 11:10:50.965
STEP: verifying the node has the label node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw 06/22/23 11:10:50.984
STEP: verifying the node has the label node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 06/22/23 11:10:51.004
Jun 22 11:10:51.029: INFO: Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 requesting resource cpu=0m on Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw
Jun 22 11:10:51.029: INFO: Pod corgi-test-8c78878b-6r5tv requesting resource cpu=0m on Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k
Jun 22 11:10:51.029: INFO: Pod antrea-agent-94vtc requesting resource cpu=400m on Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9
Jun 22 11:10:51.029: INFO: Pod antrea-agent-dxs2v requesting resource cpu=400m on Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k
Jun 22 11:10:51.030: INFO: Pod antrea-agent-n9smp requesting resource cpu=400m on Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw
Jun 22 11:10:51.030: INFO: Pod kube-proxy-8rvhg requesting resource cpu=0m on Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9
Jun 22 11:10:51.030: INFO: Pod kube-proxy-czx75 requesting resource cpu=0m on Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k
Jun 22 11:10:51.030: INFO: Pod kube-proxy-rlz8k requesting resource cpu=0m on Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw
Jun 22 11:10:51.030: INFO: Pod metrics-server-64f66d5b9-8j569 requesting resource cpu=100m on Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9
Jun 22 11:10:51.030: INFO: Pod secretgen-controller-7bdd5787fd-h9nfx requesting resource cpu=120m on Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9
Jun 22 11:10:51.030: INFO: Pod sonobuoy requesting resource cpu=0m on Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw
Jun 22 11:10:51.030: INFO: Pod sonobuoy-e2e-job-38d4831e07f74ae5 requesting resource cpu=0m on Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k
Jun 22 11:10:51.030: INFO: Pod sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-5d74n requesting resource cpu=0m on Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k
Jun 22 11:10:51.030: INFO: Pod sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-7x88w requesting resource cpu=0m on Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9
Jun 22 11:10:51.030: INFO: Pod sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-f4lqb requesting resource cpu=0m on Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw
Jun 22 11:10:51.030: INFO: Pod vsphere-csi-node-mp5v2 requesting resource cpu=0m on Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k
Jun 22 11:10:51.030: INFO: Pod vsphere-csi-node-v6pm5 requesting resource cpu=0m on Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw
Jun 22 11:10:51.030: INFO: Pod vsphere-csi-node-wsgg4 requesting resource cpu=0m on Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9
STEP: Starting Pods to consume most of the cluster CPU. 06/22/23 11:10:51.03
Jun 22 11:10:51.030: INFO: Creating a pod which consumes cpu=2520m on Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k
Jun 22 11:10:51.041: INFO: Creating a pod which consumes cpu=2520m on Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw
Jun 22 11:10:51.051: INFO: Creating a pod which consumes cpu=2366m on Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9
Jun 22 11:10:51.060: INFO: Waiting up to 5m0s for pod "filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b" in namespace "sched-pred-465" to be "running"
Jun 22 11:10:51.065: INFO: Pod "filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.361793ms
Jun 22 11:10:53.071: INFO: Pod "filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b": Phase="Running", Reason="", readiness=true. Elapsed: 2.011759053s
Jun 22 11:10:53.071: INFO: Pod "filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b" satisfied condition "running"
Jun 22 11:10:53.071: INFO: Waiting up to 5m0s for pod "filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a" in namespace "sched-pred-465" to be "running"
Jun 22 11:10:53.076: INFO: Pod "filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a": Phase="Running", Reason="", readiness=true. Elapsed: 4.164353ms
Jun 22 11:10:53.076: INFO: Pod "filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a" satisfied condition "running"
Jun 22 11:10:53.076: INFO: Waiting up to 5m0s for pod "filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432" in namespace "sched-pred-465" to be "running"
Jun 22 11:10:53.080: INFO: Pod "filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432": Phase="Running", Reason="", readiness=true. Elapsed: 3.992366ms
Jun 22 11:10:53.080: INFO: Pod "filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 06/22/23 11:10:53.08
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a.176af6b122fd2b2d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-465/filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a to wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw] 06/22/23 11:10:53.085
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a.176af6b145c11765], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/22/23 11:10:53.085
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a.176af6b146c893c6], Reason = [Created], Message = [Created container filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a] 06/22/23 11:10:53.085
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a.176af6b14ed280a3], Reason = [Started], Message = [Started container filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a] 06/22/23 11:10:53.085
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b.176af6b12268afd8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-465/filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b to wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k] 06/22/23 11:10:53.086
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b.176af6b141d141be], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/22/23 11:10:53.086
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b.176af6b14311d45b], Reason = [Created], Message = [Created container filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b] 06/22/23 11:10:53.086
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b.176af6b14b25d251], Reason = [Started], Message = [Started container filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b] 06/22/23 11:10:53.086
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432.176af6b12364e546], Reason = [Scheduled], Message = [Successfully assigned sched-pred-465/filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432 to wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9] 06/22/23 11:10:53.086
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432.176af6b143c6124c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/22/23 11:10:53.086
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432.176af6b144dc51ed], Reason = [Created], Message = [Created container filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432] 06/22/23 11:10:53.086
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432.176af6b14d4f9ca3], Reason = [Started], Message = [Started container filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432] 06/22/23 11:10:53.086
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.176af6b19c387d14], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] 06/22/23 11:10:53.099
STEP: removing the label node off the node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw 06/22/23 11:10:54.102
STEP: verifying the node doesn't have the label node 06/22/23 11:10:54.121
STEP: removing the label node off the node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 06/22/23 11:10:54.126
STEP: verifying the node doesn't have the label node 06/22/23 11:10:54.143
STEP: removing the label node off the node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k 06/22/23 11:10:54.148
STEP: verifying the node doesn't have the label node 06/22/23 11:10:54.164
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:10:54.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-465" for this suite. 06/22/23 11:10:54.179
------------------------------
• [3.329 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:10:50.861
    Jun 22 11:10:50.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename sched-pred 06/22/23 11:10:50.862
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:50.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:50.883
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jun 22 11:10:50.886: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun 22 11:10:50.897: INFO: Waiting for terminating namespaces to be deleted...
    Jun 22 11:10:50.902: INFO: 
    Logging pods the apiserver thinks is on node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k before test
    Jun 22 11:10:50.913: INFO: corgi-test-8c78878b-6r5tv from corgi-test-privileged started at 2023-06-22 06:12:10 +0000 UTC (1 container statuses recorded)
    Jun 22 11:10:50.913: INFO: 	Container nginx ready: true, restart count 0
    Jun 22 11:10:50.913: INFO: antrea-agent-dxs2v from kube-system started at 2023-06-22 05:45:55 +0000 UTC (2 container statuses recorded)
    Jun 22 11:10:50.913: INFO: 	Container antrea-agent ready: true, restart count 0
    Jun 22 11:10:50.913: INFO: 	Container antrea-ovs ready: true, restart count 0
    Jun 22 11:10:50.913: INFO: kube-proxy-czx75 from kube-system started at 2023-06-22 05:45:55 +0000 UTC (1 container statuses recorded)
    Jun 22 11:10:50.913: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 22 11:10:50.913: INFO: sonobuoy-e2e-job-38d4831e07f74ae5 from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
    Jun 22 11:10:50.913: INFO: 	Container e2e ready: true, restart count 0
    Jun 22 11:10:50.913: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 22 11:10:50.913: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-5d74n from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
    Jun 22 11:10:50.913: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 22 11:10:50.913: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 22 11:10:50.913: INFO: register-placeholder-ktpl4 from vmware-system-antrea started at 2023-06-22 11:09:02 +0000 UTC (1 container statuses recorded)
    Jun 22 11:10:50.913: INFO: 	Container register ready: false, restart count 0
    Jun 22 11:10:50.913: INFO: vsphere-csi-node-mp5v2 from vmware-system-csi started at 2023-06-22 05:45:55 +0000 UTC (3 container statuses recorded)
    Jun 22 11:10:50.913: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 22 11:10:50.913: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 22 11:10:50.913: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Jun 22 11:10:50.913: INFO: 
    Logging pods the apiserver thinks is on node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw before test
    Jun 22 11:10:50.923: INFO: test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 from container-probe-2854 started at 2023-06-22 11:10:28 +0000 UTC (1 container statuses recorded)
    Jun 22 11:10:50.923: INFO: 	Container test-webserver ready: true, restart count 0
    Jun 22 11:10:50.923: INFO: antrea-agent-n9smp from kube-system started at 2023-06-22 05:45:26 +0000 UTC (2 container statuses recorded)
    Jun 22 11:10:50.923: INFO: 	Container antrea-agent ready: true, restart count 0
    Jun 22 11:10:50.923: INFO: 	Container antrea-ovs ready: true, restart count 0
    Jun 22 11:10:50.923: INFO: kube-proxy-rlz8k from kube-system started at 2023-06-22 05:45:26 +0000 UTC (1 container statuses recorded)
    Jun 22 11:10:50.923: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 22 11:10:50.923: INFO: sonobuoy from sonobuoy started at 2023-06-22 10:19:41 +0000 UTC (1 container statuses recorded)
    Jun 22 11:10:50.923: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun 22 11:10:50.923: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-f4lqb from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
    Jun 22 11:10:50.923: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 22 11:10:50.923: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 22 11:10:50.923: INFO: vsphere-csi-node-v6pm5 from vmware-system-csi started at 2023-06-22 05:45:26 +0000 UTC (3 container statuses recorded)
    Jun 22 11:10:50.923: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 22 11:10:50.923: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 22 11:10:50.923: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Jun 22 11:10:50.923: INFO: 
    Logging pods the apiserver thinks is on node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 before test
    Jun 22 11:10:50.934: INFO: antrea-agent-94vtc from kube-system started at 2023-06-22 05:45:16 +0000 UTC (2 container statuses recorded)
    Jun 22 11:10:50.934: INFO: 	Container antrea-agent ready: true, restart count 1
    Jun 22 11:10:50.934: INFO: 	Container antrea-ovs ready: true, restart count 0
    Jun 22 11:10:50.934: INFO: kube-proxy-8rvhg from kube-system started at 2023-06-22 05:45:16 +0000 UTC (1 container statuses recorded)
    Jun 22 11:10:50.934: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 22 11:10:50.934: INFO: metrics-server-64f66d5b9-8j569 from kube-system started at 2023-06-22 05:45:49 +0000 UTC (1 container statuses recorded)
    Jun 22 11:10:50.934: INFO: 	Container metrics-server ready: true, restart count 0
    Jun 22 11:10:50.934: INFO: secretgen-controller-7bdd5787fd-h9nfx from secretgen-controller started at 2023-06-22 05:45:49 +0000 UTC (1 container statuses recorded)
    Jun 22 11:10:50.935: INFO: 	Container secretgen-controller ready: true, restart count 0
    Jun 22 11:10:50.935: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-7x88w from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
    Jun 22 11:10:50.935: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 22 11:10:50.935: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 22 11:10:50.935: INFO: vsphere-csi-node-wsgg4 from vmware-system-csi started at 2023-06-22 05:45:17 +0000 UTC (3 container statuses recorded)
    Jun 22 11:10:50.935: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 22 11:10:50.935: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 22 11:10:50.935: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k 06/22/23 11:10:50.965
    STEP: verifying the node has the label node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw 06/22/23 11:10:50.984
    STEP: verifying the node has the label node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 06/22/23 11:10:51.004
    Jun 22 11:10:51.029: INFO: Pod test-webserver-eaf084b9-0198-4fc9-83b1-4ed7aace4f28 requesting resource cpu=0m on Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw
    Jun 22 11:10:51.029: INFO: Pod corgi-test-8c78878b-6r5tv requesting resource cpu=0m on Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k
    Jun 22 11:10:51.029: INFO: Pod antrea-agent-94vtc requesting resource cpu=400m on Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9
    Jun 22 11:10:51.029: INFO: Pod antrea-agent-dxs2v requesting resource cpu=400m on Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k
    Jun 22 11:10:51.030: INFO: Pod antrea-agent-n9smp requesting resource cpu=400m on Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw
    Jun 22 11:10:51.030: INFO: Pod kube-proxy-8rvhg requesting resource cpu=0m on Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9
    Jun 22 11:10:51.030: INFO: Pod kube-proxy-czx75 requesting resource cpu=0m on Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k
    Jun 22 11:10:51.030: INFO: Pod kube-proxy-rlz8k requesting resource cpu=0m on Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw
    Jun 22 11:10:51.030: INFO: Pod metrics-server-64f66d5b9-8j569 requesting resource cpu=100m on Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9
    Jun 22 11:10:51.030: INFO: Pod secretgen-controller-7bdd5787fd-h9nfx requesting resource cpu=120m on Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9
    Jun 22 11:10:51.030: INFO: Pod sonobuoy requesting resource cpu=0m on Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw
    Jun 22 11:10:51.030: INFO: Pod sonobuoy-e2e-job-38d4831e07f74ae5 requesting resource cpu=0m on Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k
    Jun 22 11:10:51.030: INFO: Pod sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-5d74n requesting resource cpu=0m on Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k
    Jun 22 11:10:51.030: INFO: Pod sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-7x88w requesting resource cpu=0m on Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9
    Jun 22 11:10:51.030: INFO: Pod sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-f4lqb requesting resource cpu=0m on Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw
    Jun 22 11:10:51.030: INFO: Pod vsphere-csi-node-mp5v2 requesting resource cpu=0m on Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k
    Jun 22 11:10:51.030: INFO: Pod vsphere-csi-node-v6pm5 requesting resource cpu=0m on Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw
    Jun 22 11:10:51.030: INFO: Pod vsphere-csi-node-wsgg4 requesting resource cpu=0m on Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9
    STEP: Starting Pods to consume most of the cluster CPU. 06/22/23 11:10:51.03
    Jun 22 11:10:51.030: INFO: Creating a pod which consumes cpu=2520m on Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k
    Jun 22 11:10:51.041: INFO: Creating a pod which consumes cpu=2520m on Node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw
    Jun 22 11:10:51.051: INFO: Creating a pod which consumes cpu=2366m on Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9
    Jun 22 11:10:51.060: INFO: Waiting up to 5m0s for pod "filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b" in namespace "sched-pred-465" to be "running"
    Jun 22 11:10:51.065: INFO: Pod "filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.361793ms
    Jun 22 11:10:53.071: INFO: Pod "filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b": Phase="Running", Reason="", readiness=true. Elapsed: 2.011759053s
    Jun 22 11:10:53.071: INFO: Pod "filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b" satisfied condition "running"
    Jun 22 11:10:53.071: INFO: Waiting up to 5m0s for pod "filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a" in namespace "sched-pred-465" to be "running"
    Jun 22 11:10:53.076: INFO: Pod "filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a": Phase="Running", Reason="", readiness=true. Elapsed: 4.164353ms
    Jun 22 11:10:53.076: INFO: Pod "filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a" satisfied condition "running"
    Jun 22 11:10:53.076: INFO: Waiting up to 5m0s for pod "filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432" in namespace "sched-pred-465" to be "running"
    Jun 22 11:10:53.080: INFO: Pod "filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432": Phase="Running", Reason="", readiness=true. Elapsed: 3.992366ms
    Jun 22 11:10:53.080: INFO: Pod "filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 06/22/23 11:10:53.08
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a.176af6b122fd2b2d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-465/filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a to wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw] 06/22/23 11:10:53.085
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a.176af6b145c11765], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/22/23 11:10:53.085
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a.176af6b146c893c6], Reason = [Created], Message = [Created container filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a] 06/22/23 11:10:53.085
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a.176af6b14ed280a3], Reason = [Started], Message = [Started container filler-pod-95086609-bc98-4321-ae08-adcfc0396d2a] 06/22/23 11:10:53.085
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b.176af6b12268afd8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-465/filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b to wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k] 06/22/23 11:10:53.086
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b.176af6b141d141be], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/22/23 11:10:53.086
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b.176af6b14311d45b], Reason = [Created], Message = [Created container filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b] 06/22/23 11:10:53.086
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b.176af6b14b25d251], Reason = [Started], Message = [Started container filler-pod-b10b88ef-c20b-448a-af0a-4e5f7caeb11b] 06/22/23 11:10:53.086
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432.176af6b12364e546], Reason = [Scheduled], Message = [Successfully assigned sched-pred-465/filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432 to wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9] 06/22/23 11:10:53.086
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432.176af6b143c6124c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 06/22/23 11:10:53.086
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432.176af6b144dc51ed], Reason = [Created], Message = [Created container filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432] 06/22/23 11:10:53.086
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432.176af6b14d4f9ca3], Reason = [Started], Message = [Started container filler-pod-bde17111-ccf0-44ce-abf8-f28821fde432] 06/22/23 11:10:53.086
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.176af6b19c387d14], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] 06/22/23 11:10:53.099
    STEP: removing the label node off the node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw 06/22/23 11:10:54.102
    STEP: verifying the node doesn't have the label node 06/22/23 11:10:54.121
    STEP: removing the label node off the node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 06/22/23 11:10:54.126
    STEP: verifying the node doesn't have the label node 06/22/23 11:10:54.143
    STEP: removing the label node off the node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k 06/22/23 11:10:54.148
    STEP: verifying the node doesn't have the label node 06/22/23 11:10:54.164
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:10:54.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-465" for this suite. 06/22/23 11:10:54.179
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:10:54.191
Jun 22 11:10:54.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 11:10:54.195
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:54.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:54.218
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-f541af92-8e36-4a01-9960-39b2944cdd1c 06/22/23 11:10:54.222
STEP: Creating a pod to test consume configMaps 06/22/23 11:10:54.229
W0622 11:10:54.238675      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:10:54.238: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-792758be-7bc7-47ec-b953-af1c8be832ee" in namespace "projected-8325" to be "Succeeded or Failed"
Jun 22 11:10:54.246: INFO: Pod "pod-projected-configmaps-792758be-7bc7-47ec-b953-af1c8be832ee": Phase="Pending", Reason="", readiness=false. Elapsed: 7.594868ms
Jun 22 11:10:56.252: INFO: Pod "pod-projected-configmaps-792758be-7bc7-47ec-b953-af1c8be832ee": Phase="Running", Reason="", readiness=false. Elapsed: 2.013519388s
Jun 22 11:10:58.252: INFO: Pod "pod-projected-configmaps-792758be-7bc7-47ec-b953-af1c8be832ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014059816s
STEP: Saw pod success 06/22/23 11:10:58.252
Jun 22 11:10:58.253: INFO: Pod "pod-projected-configmaps-792758be-7bc7-47ec-b953-af1c8be832ee" satisfied condition "Succeeded or Failed"
Jun 22 11:10:58.257: INFO: Trying to get logs from node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 pod pod-projected-configmaps-792758be-7bc7-47ec-b953-af1c8be832ee container agnhost-container: <nil>
STEP: delete the pod 06/22/23 11:10:58.282
Jun 22 11:10:58.299: INFO: Waiting for pod pod-projected-configmaps-792758be-7bc7-47ec-b953-af1c8be832ee to disappear
Jun 22 11:10:58.306: INFO: Pod pod-projected-configmaps-792758be-7bc7-47ec-b953-af1c8be832ee no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun 22 11:10:58.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8325" for this suite. 06/22/23 11:10:58.312
------------------------------
• [4.129 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:10:54.191
    Jun 22 11:10:54.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 11:10:54.195
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:54.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:54.218
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-f541af92-8e36-4a01-9960-39b2944cdd1c 06/22/23 11:10:54.222
    STEP: Creating a pod to test consume configMaps 06/22/23 11:10:54.229
    W0622 11:10:54.238675      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:10:54.238: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-792758be-7bc7-47ec-b953-af1c8be832ee" in namespace "projected-8325" to be "Succeeded or Failed"
    Jun 22 11:10:54.246: INFO: Pod "pod-projected-configmaps-792758be-7bc7-47ec-b953-af1c8be832ee": Phase="Pending", Reason="", readiness=false. Elapsed: 7.594868ms
    Jun 22 11:10:56.252: INFO: Pod "pod-projected-configmaps-792758be-7bc7-47ec-b953-af1c8be832ee": Phase="Running", Reason="", readiness=false. Elapsed: 2.013519388s
    Jun 22 11:10:58.252: INFO: Pod "pod-projected-configmaps-792758be-7bc7-47ec-b953-af1c8be832ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014059816s
    STEP: Saw pod success 06/22/23 11:10:58.252
    Jun 22 11:10:58.253: INFO: Pod "pod-projected-configmaps-792758be-7bc7-47ec-b953-af1c8be832ee" satisfied condition "Succeeded or Failed"
    Jun 22 11:10:58.257: INFO: Trying to get logs from node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 pod pod-projected-configmaps-792758be-7bc7-47ec-b953-af1c8be832ee container agnhost-container: <nil>
    STEP: delete the pod 06/22/23 11:10:58.282
    Jun 22 11:10:58.299: INFO: Waiting for pod pod-projected-configmaps-792758be-7bc7-47ec-b953-af1c8be832ee to disappear
    Jun 22 11:10:58.306: INFO: Pod pod-projected-configmaps-792758be-7bc7-47ec-b953-af1c8be832ee no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:10:58.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8325" for this suite. 06/22/23 11:10:58.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:10:58.323
Jun 22 11:10:58.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename emptydir-wrapper 06/22/23 11:10:58.323
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:58.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:58.343
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
W0622 11:10:58.372809      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:10:58.372: INFO: Waiting up to 5m0s for pod "pod-secrets-a14eebce-3ee3-488b-b429-02cf4b78b41e" in namespace "emptydir-wrapper-5144" to be "running and ready"
Jun 22 11:10:58.379: INFO: Pod "pod-secrets-a14eebce-3ee3-488b-b429-02cf4b78b41e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.801199ms
Jun 22 11:10:58.379: INFO: The phase of Pod pod-secrets-a14eebce-3ee3-488b-b429-02cf4b78b41e is Pending, waiting for it to be Running (with Ready = true)
Jun 22 11:11:00.386: INFO: Pod "pod-secrets-a14eebce-3ee3-488b-b429-02cf4b78b41e": Phase="Running", Reason="", readiness=true. Elapsed: 2.014004297s
Jun 22 11:11:00.386: INFO: The phase of Pod pod-secrets-a14eebce-3ee3-488b-b429-02cf4b78b41e is Running (Ready = true)
Jun 22 11:11:00.386: INFO: Pod "pod-secrets-a14eebce-3ee3-488b-b429-02cf4b78b41e" satisfied condition "running and ready"
STEP: Cleaning up the secret 06/22/23 11:11:00.391
STEP: Cleaning up the configmap 06/22/23 11:11:00.4
STEP: Cleaning up the pod 06/22/23 11:11:00.41
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jun 22 11:11:00.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-5144" for this suite. 06/22/23 11:11:00.429
------------------------------
• [2.117 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:10:58.323
    Jun 22 11:10:58.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename emptydir-wrapper 06/22/23 11:10:58.323
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:10:58.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:10:58.343
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    W0622 11:10:58.372809      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:10:58.372: INFO: Waiting up to 5m0s for pod "pod-secrets-a14eebce-3ee3-488b-b429-02cf4b78b41e" in namespace "emptydir-wrapper-5144" to be "running and ready"
    Jun 22 11:10:58.379: INFO: Pod "pod-secrets-a14eebce-3ee3-488b-b429-02cf4b78b41e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.801199ms
    Jun 22 11:10:58.379: INFO: The phase of Pod pod-secrets-a14eebce-3ee3-488b-b429-02cf4b78b41e is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 11:11:00.386: INFO: Pod "pod-secrets-a14eebce-3ee3-488b-b429-02cf4b78b41e": Phase="Running", Reason="", readiness=true. Elapsed: 2.014004297s
    Jun 22 11:11:00.386: INFO: The phase of Pod pod-secrets-a14eebce-3ee3-488b-b429-02cf4b78b41e is Running (Ready = true)
    Jun 22 11:11:00.386: INFO: Pod "pod-secrets-a14eebce-3ee3-488b-b429-02cf4b78b41e" satisfied condition "running and ready"
    STEP: Cleaning up the secret 06/22/23 11:11:00.391
    STEP: Cleaning up the configmap 06/22/23 11:11:00.4
    STEP: Cleaning up the pod 06/22/23 11:11:00.41
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:11:00.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-5144" for this suite. 06/22/23 11:11:00.429
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:11:00.441
Jun 22 11:11:00.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename replicaset 06/22/23 11:11:00.442
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:11:00.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:11:00.464
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 06/22/23 11:11:00.467
W0622 11:11:00.476108      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:11:00.481: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 22 11:11:05.516: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/22/23 11:11:05.517
STEP: getting scale subresource 06/22/23 11:11:05.52
STEP: updating a scale subresource 06/22/23 11:11:05.534
STEP: verifying the replicaset Spec.Replicas was modified 06/22/23 11:11:05.551
STEP: Patch a scale subresource 06/22/23 11:11:05.564
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jun 22 11:11:05.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7083" for this suite. 06/22/23 11:11:05.627
------------------------------
• [SLOW TEST] [5.211 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:11:00.441
    Jun 22 11:11:00.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename replicaset 06/22/23 11:11:00.442
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:11:00.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:11:00.464
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 06/22/23 11:11:00.467
    W0622 11:11:00.476108      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:11:00.481: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun 22 11:11:05.516: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/22/23 11:11:05.517
    STEP: getting scale subresource 06/22/23 11:11:05.52
    STEP: updating a scale subresource 06/22/23 11:11:05.534
    STEP: verifying the replicaset Spec.Replicas was modified 06/22/23 11:11:05.551
    STEP: Patch a scale subresource 06/22/23 11:11:05.564
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:11:05.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7083" for this suite. 06/22/23 11:11:05.627
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:11:05.655
Jun 22 11:11:05.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubectl 06/22/23 11:11:05.666
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:11:05.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:11:05.704
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 06/22/23 11:11:05.71
Jun 22 11:11:05.711: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-1076 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 06/22/23 11:11:05.845
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun 22 11:11:05.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1076" for this suite. 06/22/23 11:11:05.881
------------------------------
• [0.238 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:11:05.655
    Jun 22 11:11:05.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubectl 06/22/23 11:11:05.666
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:11:05.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:11:05.704
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 06/22/23 11:11:05.71
    Jun 22 11:11:05.711: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-1076 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 06/22/23 11:11:05.845
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:11:05.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1076" for this suite. 06/22/23 11:11:05.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:11:05.919
Jun 22 11:11:05.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename configmap 06/22/23 11:11:05.92
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:11:05.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:11:05.948
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-12f54e85-36a4-444a-bbbd-c4063bb08158 06/22/23 11:11:05.952
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun 22 11:11:05.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-352" for this suite. 06/22/23 11:11:05.966
------------------------------
• [0.063 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:11:05.919
    Jun 22 11:11:05.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename configmap 06/22/23 11:11:05.92
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:11:05.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:11:05.948
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-12f54e85-36a4-444a-bbbd-c4063bb08158 06/22/23 11:11:05.952
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:11:05.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-352" for this suite. 06/22/23 11:11:05.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:11:05.985
Jun 22 11:11:05.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename dns 06/22/23 11:11:05.987
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:11:06.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:11:06.017
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 06/22/23 11:11:06.027
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6375.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6375.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 06/22/23 11:11:06.039
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6375.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6375.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 06/22/23 11:11:06.039
STEP: creating a pod to probe DNS 06/22/23 11:11:06.039
STEP: submitting the pod to kubernetes 06/22/23 11:11:06.04
W0622 11:11:06.070180      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:11:06.070: INFO: Waiting up to 15m0s for pod "dns-test-23e04980-8183-44e2-92a9-6a70b9d8715b" in namespace "dns-6375" to be "running"
Jun 22 11:11:06.083: INFO: Pod "dns-test-23e04980-8183-44e2-92a9-6a70b9d8715b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.219414ms
Jun 22 11:11:08.091: INFO: Pod "dns-test-23e04980-8183-44e2-92a9-6a70b9d8715b": Phase="Running", Reason="", readiness=true. Elapsed: 2.020388781s
Jun 22 11:11:08.091: INFO: Pod "dns-test-23e04980-8183-44e2-92a9-6a70b9d8715b" satisfied condition "running"
STEP: retrieving the pod 06/22/23 11:11:08.091
STEP: looking for the results for each expected name from probers 06/22/23 11:11:08.096
Jun 22 11:11:08.126: INFO: DNS probes using dns-6375/dns-test-23e04980-8183-44e2-92a9-6a70b9d8715b succeeded

STEP: deleting the pod 06/22/23 11:11:08.126
STEP: deleting the test headless service 06/22/23 11:11:08.148
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun 22 11:11:08.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6375" for this suite. 06/22/23 11:11:08.189
------------------------------
• [2.221 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:11:05.985
    Jun 22 11:11:05.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename dns 06/22/23 11:11:05.987
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:11:06.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:11:06.017
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 06/22/23 11:11:06.027
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6375.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6375.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     06/22/23 11:11:06.039
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6375.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6375.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     06/22/23 11:11:06.039
    STEP: creating a pod to probe DNS 06/22/23 11:11:06.039
    STEP: submitting the pod to kubernetes 06/22/23 11:11:06.04
    W0622 11:11:06.070180      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:11:06.070: INFO: Waiting up to 15m0s for pod "dns-test-23e04980-8183-44e2-92a9-6a70b9d8715b" in namespace "dns-6375" to be "running"
    Jun 22 11:11:06.083: INFO: Pod "dns-test-23e04980-8183-44e2-92a9-6a70b9d8715b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.219414ms
    Jun 22 11:11:08.091: INFO: Pod "dns-test-23e04980-8183-44e2-92a9-6a70b9d8715b": Phase="Running", Reason="", readiness=true. Elapsed: 2.020388781s
    Jun 22 11:11:08.091: INFO: Pod "dns-test-23e04980-8183-44e2-92a9-6a70b9d8715b" satisfied condition "running"
    STEP: retrieving the pod 06/22/23 11:11:08.091
    STEP: looking for the results for each expected name from probers 06/22/23 11:11:08.096
    Jun 22 11:11:08.126: INFO: DNS probes using dns-6375/dns-test-23e04980-8183-44e2-92a9-6a70b9d8715b succeeded

    STEP: deleting the pod 06/22/23 11:11:08.126
    STEP: deleting the test headless service 06/22/23 11:11:08.148
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:11:08.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6375" for this suite. 06/22/23 11:11:08.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:11:08.208
Jun 22 11:11:08.208: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename webhook 06/22/23 11:11:08.211
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:11:08.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:11:08.235
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/22/23 11:11:08.263
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:11:08.785
STEP: Deploying the webhook pod 06/22/23 11:11:08.802
W0622 11:11:08.822759      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 11:11:08.823
Jun 22 11:11:08.833: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 06/22/23 11:11:10.85
STEP: Verifying the service has paired with the endpoint 06/22/23 11:11:10.877
Jun 22 11:11:11.877: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 06/22/23 11:11:11.883
STEP: create a pod that should be updated by the webhook 06/22/23 11:11:11.91
W0622 11:11:11.932262      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webhook-added-init-container", "example" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webhook-added-init-container", "example" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webhook-added-init-container", "example" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webhook-added-init-container", "example" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:11:11.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2149" for this suite. 06/22/23 11:11:12.034
STEP: Destroying namespace "webhook-2149-markers" for this suite. 06/22/23 11:11:12.046
------------------------------
• [3.850 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:11:08.208
    Jun 22 11:11:08.208: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename webhook 06/22/23 11:11:08.211
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:11:08.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:11:08.235
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/22/23 11:11:08.263
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:11:08.785
    STEP: Deploying the webhook pod 06/22/23 11:11:08.802
    W0622 11:11:08.822759      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 11:11:08.823
    Jun 22 11:11:08.833: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 06/22/23 11:11:10.85
    STEP: Verifying the service has paired with the endpoint 06/22/23 11:11:10.877
    Jun 22 11:11:11.877: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 06/22/23 11:11:11.883
    STEP: create a pod that should be updated by the webhook 06/22/23 11:11:11.91
    W0622 11:11:11.932262      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webhook-added-init-container", "example" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webhook-added-init-container", "example" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webhook-added-init-container", "example" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webhook-added-init-container", "example" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:11:11.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2149" for this suite. 06/22/23 11:11:12.034
    STEP: Destroying namespace "webhook-2149-markers" for this suite. 06/22/23 11:11:12.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:11:12.066
Jun 22 11:11:12.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename container-probe 06/22/23 11:11:12.068
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:11:12.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:11:12.089
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
W0622 11:11:12.105121      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun 22 11:12:12.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7010" for this suite. 06/22/23 11:12:12.12
------------------------------
• [SLOW TEST] [60.063 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:11:12.066
    Jun 22 11:11:12.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename container-probe 06/22/23 11:11:12.068
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:11:12.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:11:12.089
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    W0622 11:11:12.105121      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:12:12.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7010" for this suite. 06/22/23 11:12:12.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:12:12.132
Jun 22 11:12:12.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename podtemplate 06/22/23 11:12:12.134
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:12:12.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:12:12.156
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 06/22/23 11:12:12.16
W0622 11:12:12.166776      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:12:12.166: INFO: created test-podtemplate-1
W0622 11:12:12.173234      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:12:12.173: INFO: created test-podtemplate-2
W0622 11:12:12.179910      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:12:12.180: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 06/22/23 11:12:12.18
STEP: delete collection of pod templates 06/22/23 11:12:12.185
Jun 22 11:12:12.185: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 06/22/23 11:12:12.207
Jun 22 11:12:12.207: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jun 22 11:12:12.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-295" for this suite. 06/22/23 11:12:12.219
------------------------------
• [0.095 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:12:12.132
    Jun 22 11:12:12.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename podtemplate 06/22/23 11:12:12.134
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:12:12.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:12:12.156
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 06/22/23 11:12:12.16
    W0622 11:12:12.166776      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:12:12.166: INFO: created test-podtemplate-1
    W0622 11:12:12.173234      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:12:12.173: INFO: created test-podtemplate-2
    W0622 11:12:12.179910      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:12:12.180: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 06/22/23 11:12:12.18
    STEP: delete collection of pod templates 06/22/23 11:12:12.185
    Jun 22 11:12:12.185: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 06/22/23 11:12:12.207
    Jun 22 11:12:12.207: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:12:12.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-295" for this suite. 06/22/23 11:12:12.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:12:12.228
Jun 22 11:12:12.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename downward-api 06/22/23 11:12:12.229
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:12:12.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:12:12.25
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 06/22/23 11:12:12.254
W0622 11:12:12.267645      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:12:12.267: INFO: Waiting up to 5m0s for pod "downwardapi-volume-081df192-1d0f-47a6-a6b4-efd3e2a95acb" in namespace "downward-api-2515" to be "Succeeded or Failed"
Jun 22 11:12:12.272: INFO: Pod "downwardapi-volume-081df192-1d0f-47a6-a6b4-efd3e2a95acb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.206745ms
Jun 22 11:12:14.278: INFO: Pod "downwardapi-volume-081df192-1d0f-47a6-a6b4-efd3e2a95acb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0108386s
Jun 22 11:12:16.279: INFO: Pod "downwardapi-volume-081df192-1d0f-47a6-a6b4-efd3e2a95acb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011650115s
STEP: Saw pod success 06/22/23 11:12:16.279
Jun 22 11:12:16.279: INFO: Pod "downwardapi-volume-081df192-1d0f-47a6-a6b4-efd3e2a95acb" satisfied condition "Succeeded or Failed"
Jun 22 11:12:16.284: INFO: Trying to get logs from node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 pod downwardapi-volume-081df192-1d0f-47a6-a6b4-efd3e2a95acb container client-container: <nil>
STEP: delete the pod 06/22/23 11:12:16.293
Jun 22 11:12:16.308: INFO: Waiting for pod downwardapi-volume-081df192-1d0f-47a6-a6b4-efd3e2a95acb to disappear
Jun 22 11:12:16.313: INFO: Pod downwardapi-volume-081df192-1d0f-47a6-a6b4-efd3e2a95acb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun 22 11:12:16.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2515" for this suite. 06/22/23 11:12:16.333
------------------------------
• [4.115 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:12:12.228
    Jun 22 11:12:12.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename downward-api 06/22/23 11:12:12.229
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:12:12.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:12:12.25
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 06/22/23 11:12:12.254
    W0622 11:12:12.267645      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:12:12.267: INFO: Waiting up to 5m0s for pod "downwardapi-volume-081df192-1d0f-47a6-a6b4-efd3e2a95acb" in namespace "downward-api-2515" to be "Succeeded or Failed"
    Jun 22 11:12:12.272: INFO: Pod "downwardapi-volume-081df192-1d0f-47a6-a6b4-efd3e2a95acb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.206745ms
    Jun 22 11:12:14.278: INFO: Pod "downwardapi-volume-081df192-1d0f-47a6-a6b4-efd3e2a95acb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0108386s
    Jun 22 11:12:16.279: INFO: Pod "downwardapi-volume-081df192-1d0f-47a6-a6b4-efd3e2a95acb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011650115s
    STEP: Saw pod success 06/22/23 11:12:16.279
    Jun 22 11:12:16.279: INFO: Pod "downwardapi-volume-081df192-1d0f-47a6-a6b4-efd3e2a95acb" satisfied condition "Succeeded or Failed"
    Jun 22 11:12:16.284: INFO: Trying to get logs from node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 pod downwardapi-volume-081df192-1d0f-47a6-a6b4-efd3e2a95acb container client-container: <nil>
    STEP: delete the pod 06/22/23 11:12:16.293
    Jun 22 11:12:16.308: INFO: Waiting for pod downwardapi-volume-081df192-1d0f-47a6-a6b4-efd3e2a95acb to disappear
    Jun 22 11:12:16.313: INFO: Pod downwardapi-volume-081df192-1d0f-47a6-a6b4-efd3e2a95acb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:12:16.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2515" for this suite. 06/22/23 11:12:16.333
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:12:16.343
Jun 22 11:12:16.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename pod-network-test 06/22/23 11:12:16.345
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:12:16.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:12:16.366
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-4469 06/22/23 11:12:16.369
STEP: creating a selector 06/22/23 11:12:16.37
STEP: Creating the service pods in kubernetes 06/22/23 11:12:16.37
Jun 22 11:12:16.370: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
W0622 11:12:16.395215      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0622 11:12:16.403757      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0622 11:12:16.413805      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:12:16.414: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4469" to be "running and ready"
Jun 22 11:12:16.426: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.70939ms
Jun 22 11:12:16.426: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 11:12:18.433: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.019252248s
Jun 22 11:12:18.433: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:12:20.433: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.019461631s
Jun 22 11:12:20.433: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:12:22.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.02349581s
Jun 22 11:12:22.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:12:24.432: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.017805019s
Jun 22 11:12:24.432: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:12:26.431: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016977219s
Jun 22 11:12:26.431: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:12:28.433: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.019019713s
Jun 22 11:12:28.433: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:12:30.434: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.019845376s
Jun 22 11:12:30.434: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:12:32.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.021695877s
Jun 22 11:12:32.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:12:34.431: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.017346632s
Jun 22 11:12:34.431: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:12:36.432: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.018115927s
Jun 22 11:12:36.432: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:12:38.434: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.019605155s
Jun 22 11:12:38.434: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun 22 11:12:38.434: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun 22 11:12:38.438: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4469" to be "running and ready"
Jun 22 11:12:38.443: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.37397ms
Jun 22 11:12:38.443: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun 22 11:12:38.443: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun 22 11:12:38.446: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4469" to be "running and ready"
Jun 22 11:12:38.451: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.121566ms
Jun 22 11:12:38.451: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun 22 11:12:38.451: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/22/23 11:12:38.455
W0622 11:12:38.461847      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0622 11:12:38.469926      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": host namespaces (hostNetwork=true), allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:12:38.470: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4469" to be "running"
Jun 22 11:12:38.477: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.159076ms
Jun 22 11:12:40.485: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015006132s
Jun 22 11:12:40.485: INFO: Pod "test-container-pod" satisfied condition "running"
Jun 22 11:12:40.489: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4469" to be "running"
Jun 22 11:12:40.493: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.108043ms
Jun 22 11:12:40.493: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jun 22 11:12:40.497: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 22 11:12:40.497: INFO: Going to poll 100.96.4.225 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun 22 11:12:40.501: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.4.225 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4469 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 11:12:40.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 11:12:40.502: INFO: ExecWithOptions: Clientset creation
Jun 22 11:12:40.502: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-4469/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.96.4.225+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 22 11:12:41.629: INFO: Found all 1 expected endpoints: [netserver-0]
Jun 22 11:12:41.629: INFO: Going to poll 100.96.3.130 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun 22 11:12:41.634: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.3.130 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4469 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 11:12:41.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 11:12:41.636: INFO: ExecWithOptions: Clientset creation
Jun 22 11:12:41.636: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-4469/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.96.3.130+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 22 11:12:42.734: INFO: Found all 1 expected endpoints: [netserver-1]
Jun 22 11:12:42.734: INFO: Going to poll 100.96.2.218 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun 22 11:12:42.739: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.2.218 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4469 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 11:12:42.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 11:12:42.740: INFO: ExecWithOptions: Clientset creation
Jun 22 11:12:42.740: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-4469/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.96.2.218+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 22 11:12:43.882: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jun 22 11:12:43.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4469" for this suite. 06/22/23 11:12:43.89
------------------------------
• [SLOW TEST] [27.555 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:12:16.343
    Jun 22 11:12:16.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename pod-network-test 06/22/23 11:12:16.345
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:12:16.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:12:16.366
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-4469 06/22/23 11:12:16.369
    STEP: creating a selector 06/22/23 11:12:16.37
    STEP: Creating the service pods in kubernetes 06/22/23 11:12:16.37
    Jun 22 11:12:16.370: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    W0622 11:12:16.395215      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0622 11:12:16.403757      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0622 11:12:16.413805      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:12:16.414: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4469" to be "running and ready"
    Jun 22 11:12:16.426: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.70939ms
    Jun 22 11:12:16.426: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 11:12:18.433: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.019252248s
    Jun 22 11:12:18.433: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:12:20.433: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.019461631s
    Jun 22 11:12:20.433: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:12:22.437: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.02349581s
    Jun 22 11:12:22.437: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:12:24.432: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.017805019s
    Jun 22 11:12:24.432: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:12:26.431: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016977219s
    Jun 22 11:12:26.431: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:12:28.433: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.019019713s
    Jun 22 11:12:28.433: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:12:30.434: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.019845376s
    Jun 22 11:12:30.434: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:12:32.436: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.021695877s
    Jun 22 11:12:32.436: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:12:34.431: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.017346632s
    Jun 22 11:12:34.431: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:12:36.432: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.018115927s
    Jun 22 11:12:36.432: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:12:38.434: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.019605155s
    Jun 22 11:12:38.434: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun 22 11:12:38.434: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun 22 11:12:38.438: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4469" to be "running and ready"
    Jun 22 11:12:38.443: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.37397ms
    Jun 22 11:12:38.443: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun 22 11:12:38.443: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun 22 11:12:38.446: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4469" to be "running and ready"
    Jun 22 11:12:38.451: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.121566ms
    Jun 22 11:12:38.451: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun 22 11:12:38.451: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/22/23 11:12:38.455
    W0622 11:12:38.461847      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0622 11:12:38.469926      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": host namespaces (hostNetwork=true), allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:12:38.470: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4469" to be "running"
    Jun 22 11:12:38.477: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.159076ms
    Jun 22 11:12:40.485: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015006132s
    Jun 22 11:12:40.485: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun 22 11:12:40.489: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4469" to be "running"
    Jun 22 11:12:40.493: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.108043ms
    Jun 22 11:12:40.493: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jun 22 11:12:40.497: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun 22 11:12:40.497: INFO: Going to poll 100.96.4.225 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jun 22 11:12:40.501: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.4.225 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4469 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 11:12:40.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 11:12:40.502: INFO: ExecWithOptions: Clientset creation
    Jun 22 11:12:40.502: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-4469/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.96.4.225+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 22 11:12:41.629: INFO: Found all 1 expected endpoints: [netserver-0]
    Jun 22 11:12:41.629: INFO: Going to poll 100.96.3.130 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jun 22 11:12:41.634: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.3.130 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4469 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 11:12:41.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 11:12:41.636: INFO: ExecWithOptions: Clientset creation
    Jun 22 11:12:41.636: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-4469/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.96.3.130+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 22 11:12:42.734: INFO: Found all 1 expected endpoints: [netserver-1]
    Jun 22 11:12:42.734: INFO: Going to poll 100.96.2.218 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jun 22 11:12:42.739: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.2.218 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4469 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 11:12:42.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 11:12:42.740: INFO: ExecWithOptions: Clientset creation
    Jun 22 11:12:42.740: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-4469/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+100.96.2.218+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 22 11:12:43.882: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:12:43.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4469" for this suite. 06/22/23 11:12:43.89
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:12:43.898
Jun 22 11:12:43.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename downward-api 06/22/23 11:12:43.9
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:12:43.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:12:43.919
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 06/22/23 11:12:43.923
W0622 11:12:43.934474      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:12:43.934: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3d1ef2a-cfc0-46f7-bb05-0ffc256719f5" in namespace "downward-api-3299" to be "Succeeded or Failed"
Jun 22 11:12:43.941: INFO: Pod "downwardapi-volume-d3d1ef2a-cfc0-46f7-bb05-0ffc256719f5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.98194ms
Jun 22 11:12:45.949: INFO: Pod "downwardapi-volume-d3d1ef2a-cfc0-46f7-bb05-0ffc256719f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014974796s
Jun 22 11:12:47.950: INFO: Pod "downwardapi-volume-d3d1ef2a-cfc0-46f7-bb05-0ffc256719f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01570158s
STEP: Saw pod success 06/22/23 11:12:47.95
Jun 22 11:12:47.951: INFO: Pod "downwardapi-volume-d3d1ef2a-cfc0-46f7-bb05-0ffc256719f5" satisfied condition "Succeeded or Failed"
Jun 22 11:12:47.956: INFO: Trying to get logs from node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k pod downwardapi-volume-d3d1ef2a-cfc0-46f7-bb05-0ffc256719f5 container client-container: <nil>
STEP: delete the pod 06/22/23 11:12:47.977
Jun 22 11:12:47.992: INFO: Waiting for pod downwardapi-volume-d3d1ef2a-cfc0-46f7-bb05-0ffc256719f5 to disappear
Jun 22 11:12:47.998: INFO: Pod downwardapi-volume-d3d1ef2a-cfc0-46f7-bb05-0ffc256719f5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun 22 11:12:47.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3299" for this suite. 06/22/23 11:12:48.007
------------------------------
• [4.117 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:12:43.898
    Jun 22 11:12:43.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename downward-api 06/22/23 11:12:43.9
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:12:43.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:12:43.919
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 06/22/23 11:12:43.923
    W0622 11:12:43.934474      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:12:43.934: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3d1ef2a-cfc0-46f7-bb05-0ffc256719f5" in namespace "downward-api-3299" to be "Succeeded or Failed"
    Jun 22 11:12:43.941: INFO: Pod "downwardapi-volume-d3d1ef2a-cfc0-46f7-bb05-0ffc256719f5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.98194ms
    Jun 22 11:12:45.949: INFO: Pod "downwardapi-volume-d3d1ef2a-cfc0-46f7-bb05-0ffc256719f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014974796s
    Jun 22 11:12:47.950: INFO: Pod "downwardapi-volume-d3d1ef2a-cfc0-46f7-bb05-0ffc256719f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01570158s
    STEP: Saw pod success 06/22/23 11:12:47.95
    Jun 22 11:12:47.951: INFO: Pod "downwardapi-volume-d3d1ef2a-cfc0-46f7-bb05-0ffc256719f5" satisfied condition "Succeeded or Failed"
    Jun 22 11:12:47.956: INFO: Trying to get logs from node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k pod downwardapi-volume-d3d1ef2a-cfc0-46f7-bb05-0ffc256719f5 container client-container: <nil>
    STEP: delete the pod 06/22/23 11:12:47.977
    Jun 22 11:12:47.992: INFO: Waiting for pod downwardapi-volume-d3d1ef2a-cfc0-46f7-bb05-0ffc256719f5 to disappear
    Jun 22 11:12:47.998: INFO: Pod downwardapi-volume-d3d1ef2a-cfc0-46f7-bb05-0ffc256719f5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:12:47.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3299" for this suite. 06/22/23 11:12:48.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:12:48.018
Jun 22 11:12:48.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename webhook 06/22/23 11:12:48.019
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:12:48.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:12:48.039
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/22/23 11:12:48.059
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:12:48.472
STEP: Deploying the webhook pod 06/22/23 11:12:48.485
W0622 11:12:48.500237      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 11:12:48.5
Jun 22 11:12:48.513: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/22/23 11:12:50.528
STEP: Verifying the service has paired with the endpoint 06/22/23 11:12:50.544
Jun 22 11:12:51.544: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 06/22/23 11:12:51.549
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 06/22/23 11:12:51.551
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 06/22/23 11:12:51.552
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 06/22/23 11:12:51.552
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 06/22/23 11:12:51.553
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 06/22/23 11:12:51.553
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 06/22/23 11:12:51.555
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:12:51.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2905" for this suite. 06/22/23 11:12:51.626
STEP: Destroying namespace "webhook-2905-markers" for this suite. 06/22/23 11:12:51.637
------------------------------
• [3.627 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:12:48.018
    Jun 22 11:12:48.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename webhook 06/22/23 11:12:48.019
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:12:48.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:12:48.039
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/22/23 11:12:48.059
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:12:48.472
    STEP: Deploying the webhook pod 06/22/23 11:12:48.485
    W0622 11:12:48.500237      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 11:12:48.5
    Jun 22 11:12:48.513: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/22/23 11:12:50.528
    STEP: Verifying the service has paired with the endpoint 06/22/23 11:12:50.544
    Jun 22 11:12:51.544: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 06/22/23 11:12:51.549
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 06/22/23 11:12:51.551
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 06/22/23 11:12:51.552
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 06/22/23 11:12:51.552
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 06/22/23 11:12:51.553
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 06/22/23 11:12:51.553
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 06/22/23 11:12:51.555
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:12:51.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2905" for this suite. 06/22/23 11:12:51.626
    STEP: Destroying namespace "webhook-2905-markers" for this suite. 06/22/23 11:12:51.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:12:51.652
Jun 22 11:12:51.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename watch 06/22/23 11:12:51.653
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:12:51.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:12:51.676
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 06/22/23 11:12:51.68
STEP: creating a watch on configmaps with label B 06/22/23 11:12:51.682
STEP: creating a watch on configmaps with label A or B 06/22/23 11:12:51.683
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 06/22/23 11:12:51.685
Jun 22 11:12:51.691: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6245  9a84ab12-cc53-4b45-a6e8-9f108bf44c17 148490 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 11:12:51.692: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6245  9a84ab12-cc53-4b45-a6e8-9f108bf44c17 148490 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 06/22/23 11:12:51.692
Jun 22 11:12:51.701: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6245  9a84ab12-cc53-4b45-a6e8-9f108bf44c17 148491 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 11:12:51.701: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6245  9a84ab12-cc53-4b45-a6e8-9f108bf44c17 148491 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 06/22/23 11:12:51.701
Jun 22 11:12:51.712: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6245  9a84ab12-cc53-4b45-a6e8-9f108bf44c17 148492 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 11:12:51.712: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6245  9a84ab12-cc53-4b45-a6e8-9f108bf44c17 148492 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 06/22/23 11:12:51.712
Jun 22 11:12:51.720: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6245  9a84ab12-cc53-4b45-a6e8-9f108bf44c17 148493 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 11:12:51.720: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6245  9a84ab12-cc53-4b45-a6e8-9f108bf44c17 148493 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 06/22/23 11:12:51.72
Jun 22 11:12:51.726: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6245  db2748e4-6674-4ab6-9361-fc2a23324bf8 148495 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 11:12:51.726: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6245  db2748e4-6674-4ab6-9361-fc2a23324bf8 148495 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 06/22/23 11:13:01.727
Jun 22 11:13:01.736: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6245  db2748e4-6674-4ab6-9361-fc2a23324bf8 148555 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 11:13:01.736: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6245  db2748e4-6674-4ab6-9361-fc2a23324bf8 148555 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jun 22 11:13:11.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6245" for this suite. 06/22/23 11:13:11.746
------------------------------
• [SLOW TEST] [20.103 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:12:51.652
    Jun 22 11:12:51.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename watch 06/22/23 11:12:51.653
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:12:51.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:12:51.676
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 06/22/23 11:12:51.68
    STEP: creating a watch on configmaps with label B 06/22/23 11:12:51.682
    STEP: creating a watch on configmaps with label A or B 06/22/23 11:12:51.683
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 06/22/23 11:12:51.685
    Jun 22 11:12:51.691: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6245  9a84ab12-cc53-4b45-a6e8-9f108bf44c17 148490 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 22 11:12:51.692: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6245  9a84ab12-cc53-4b45-a6e8-9f108bf44c17 148490 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 06/22/23 11:12:51.692
    Jun 22 11:12:51.701: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6245  9a84ab12-cc53-4b45-a6e8-9f108bf44c17 148491 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 22 11:12:51.701: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6245  9a84ab12-cc53-4b45-a6e8-9f108bf44c17 148491 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 06/22/23 11:12:51.701
    Jun 22 11:12:51.712: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6245  9a84ab12-cc53-4b45-a6e8-9f108bf44c17 148492 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 22 11:12:51.712: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6245  9a84ab12-cc53-4b45-a6e8-9f108bf44c17 148492 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 06/22/23 11:12:51.712
    Jun 22 11:12:51.720: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6245  9a84ab12-cc53-4b45-a6e8-9f108bf44c17 148493 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 22 11:12:51.720: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6245  9a84ab12-cc53-4b45-a6e8-9f108bf44c17 148493 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 06/22/23 11:12:51.72
    Jun 22 11:12:51.726: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6245  db2748e4-6674-4ab6-9361-fc2a23324bf8 148495 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 22 11:12:51.726: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6245  db2748e4-6674-4ab6-9361-fc2a23324bf8 148495 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 06/22/23 11:13:01.727
    Jun 22 11:13:01.736: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6245  db2748e4-6674-4ab6-9361-fc2a23324bf8 148555 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 22 11:13:01.736: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6245  db2748e4-6674-4ab6-9361-fc2a23324bf8 148555 0 2023-06-22 11:12:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-22 11:12:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:13:11.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6245" for this suite. 06/22/23 11:13:11.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:13:11.757
Jun 22 11:13:11.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename daemonsets 06/22/23 11:13:11.758
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:13:11.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:13:11.777
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Jun 22 11:13:11.807: INFO: Creating simple daemon set daemon-set
W0622 11:13:11.815654      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Check that daemon pods launch on every node of the cluster. 06/22/23 11:13:11.815
Jun 22 11:13:11.824: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:11.824: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:11.824: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:11.828: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 11:13:11.828: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
Jun 22 11:13:12.836: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:12.836: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:12.836: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:12.841: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 22 11:13:12.841: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
Jun 22 11:13:13.837: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:13.837: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:13.837: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:13.843: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 22 11:13:13.843: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 06/22/23 11:13:13.864
W0622 11:13:13.872246      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Check that daemon pods images are updated. 06/22/23 11:13:13.88
Jun 22 11:13:13.891: INFO: Wrong image for pod: daemon-set-hthzl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun 22 11:13:13.891: INFO: Wrong image for pod: daemon-set-mjv4q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun 22 11:13:13.891: INFO: Wrong image for pod: daemon-set-rllbn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun 22 11:13:13.900: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:13.900: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:13.900: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:14.906: INFO: Wrong image for pod: daemon-set-hthzl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun 22 11:13:14.906: INFO: Wrong image for pod: daemon-set-rllbn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun 22 11:13:14.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:14.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:14.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:15.906: INFO: Pod daemon-set-22m54 is not available
Jun 22 11:13:15.906: INFO: Wrong image for pod: daemon-set-hthzl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun 22 11:13:15.906: INFO: Wrong image for pod: daemon-set-rllbn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun 22 11:13:15.913: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:15.913: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:15.913: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:16.905: INFO: Wrong image for pod: daemon-set-rllbn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun 22 11:13:16.911: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:16.911: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:16.911: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:17.906: INFO: Wrong image for pod: daemon-set-rllbn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun 22 11:13:17.913: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:17.913: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:17.913: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:18.906: INFO: Pod daemon-set-lhhs8 is not available
Jun 22 11:13:18.906: INFO: Wrong image for pod: daemon-set-rllbn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jun 22 11:13:18.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:18.913: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:18.913: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:19.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:19.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:19.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:20.906: INFO: Pod daemon-set-5kngt is not available
Jun 22 11:13:20.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:20.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:20.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 06/22/23 11:13:20.913
Jun 22 11:13:20.919: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:20.919: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:20.919: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:20.924: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 22 11:13:20.924: INFO: Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 is running 0 daemon pod, expected 1
Jun 22 11:13:21.932: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:21.932: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:21.932: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:13:21.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 22 11:13:21.937: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 06/22/23 11:13:21.958
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6687, will wait for the garbage collector to delete the pods 06/22/23 11:13:21.958
Jun 22 11:13:22.023: INFO: Deleting DaemonSet.extensions daemon-set took: 8.817432ms
Jun 22 11:13:22.124: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.264603ms
Jun 22 11:13:24.629: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 11:13:24.629: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 22 11:13:24.634: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"148767"},"items":null}

Jun 22 11:13:24.638: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"148767"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:13:24.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6687" for this suite. 06/22/23 11:13:24.663
------------------------------
• [SLOW TEST] [12.915 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:13:11.757
    Jun 22 11:13:11.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename daemonsets 06/22/23 11:13:11.758
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:13:11.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:13:11.777
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Jun 22 11:13:11.807: INFO: Creating simple daemon set daemon-set
    W0622 11:13:11.815654      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Check that daemon pods launch on every node of the cluster. 06/22/23 11:13:11.815
    Jun 22 11:13:11.824: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:11.824: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:11.824: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:11.828: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 11:13:11.828: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
    Jun 22 11:13:12.836: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:12.836: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:12.836: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:12.841: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 22 11:13:12.841: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
    Jun 22 11:13:13.837: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:13.837: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:13.837: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:13.843: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 22 11:13:13.843: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 06/22/23 11:13:13.864
    W0622 11:13:13.872246      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Check that daemon pods images are updated. 06/22/23 11:13:13.88
    Jun 22 11:13:13.891: INFO: Wrong image for pod: daemon-set-hthzl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun 22 11:13:13.891: INFO: Wrong image for pod: daemon-set-mjv4q. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun 22 11:13:13.891: INFO: Wrong image for pod: daemon-set-rllbn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun 22 11:13:13.900: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:13.900: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:13.900: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:14.906: INFO: Wrong image for pod: daemon-set-hthzl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun 22 11:13:14.906: INFO: Wrong image for pod: daemon-set-rllbn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun 22 11:13:14.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:14.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:14.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:15.906: INFO: Pod daemon-set-22m54 is not available
    Jun 22 11:13:15.906: INFO: Wrong image for pod: daemon-set-hthzl. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun 22 11:13:15.906: INFO: Wrong image for pod: daemon-set-rllbn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun 22 11:13:15.913: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:15.913: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:15.913: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:16.905: INFO: Wrong image for pod: daemon-set-rllbn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun 22 11:13:16.911: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:16.911: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:16.911: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:17.906: INFO: Wrong image for pod: daemon-set-rllbn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun 22 11:13:17.913: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:17.913: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:17.913: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:18.906: INFO: Pod daemon-set-lhhs8 is not available
    Jun 22 11:13:18.906: INFO: Wrong image for pod: daemon-set-rllbn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jun 22 11:13:18.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:18.913: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:18.913: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:19.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:19.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:19.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:20.906: INFO: Pod daemon-set-5kngt is not available
    Jun 22 11:13:20.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:20.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:20.912: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 06/22/23 11:13:20.913
    Jun 22 11:13:20.919: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:20.919: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:20.919: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:20.924: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 22 11:13:20.924: INFO: Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 is running 0 daemon pod, expected 1
    Jun 22 11:13:21.932: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:21.932: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:21.932: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:13:21.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 22 11:13:21.937: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 06/22/23 11:13:21.958
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6687, will wait for the garbage collector to delete the pods 06/22/23 11:13:21.958
    Jun 22 11:13:22.023: INFO: Deleting DaemonSet.extensions daemon-set took: 8.817432ms
    Jun 22 11:13:22.124: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.264603ms
    Jun 22 11:13:24.629: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 11:13:24.629: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 22 11:13:24.634: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"148767"},"items":null}

    Jun 22 11:13:24.638: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"148767"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:13:24.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6687" for this suite. 06/22/23 11:13:24.663
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:13:24.672
Jun 22 11:13:24.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename var-expansion 06/22/23 11:13:24.673
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:13:24.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:13:24.692
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 06/22/23 11:13:24.696
W0622 11:13:24.706732      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:13:24.706: INFO: Waiting up to 5m0s for pod "var-expansion-28836347-df12-40e5-abba-8ee4c3c2f1a8" in namespace "var-expansion-7261" to be "Succeeded or Failed"
Jun 22 11:13:24.710: INFO: Pod "var-expansion-28836347-df12-40e5-abba-8ee4c3c2f1a8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.890068ms
Jun 22 11:13:26.716: INFO: Pod "var-expansion-28836347-df12-40e5-abba-8ee4c3c2f1a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009422433s
Jun 22 11:13:28.717: INFO: Pod "var-expansion-28836347-df12-40e5-abba-8ee4c3c2f1a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010122679s
STEP: Saw pod success 06/22/23 11:13:28.717
Jun 22 11:13:28.717: INFO: Pod "var-expansion-28836347-df12-40e5-abba-8ee4c3c2f1a8" satisfied condition "Succeeded or Failed"
Jun 22 11:13:28.721: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod var-expansion-28836347-df12-40e5-abba-8ee4c3c2f1a8 container dapi-container: <nil>
STEP: delete the pod 06/22/23 11:13:28.741
Jun 22 11:13:28.757: INFO: Waiting for pod var-expansion-28836347-df12-40e5-abba-8ee4c3c2f1a8 to disappear
Jun 22 11:13:28.761: INFO: Pod var-expansion-28836347-df12-40e5-abba-8ee4c3c2f1a8 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun 22 11:13:28.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7261" for this suite. 06/22/23 11:13:28.768
------------------------------
• [4.103 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:13:24.672
    Jun 22 11:13:24.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename var-expansion 06/22/23 11:13:24.673
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:13:24.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:13:24.692
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 06/22/23 11:13:24.696
    W0622 11:13:24.706732      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:13:24.706: INFO: Waiting up to 5m0s for pod "var-expansion-28836347-df12-40e5-abba-8ee4c3c2f1a8" in namespace "var-expansion-7261" to be "Succeeded or Failed"
    Jun 22 11:13:24.710: INFO: Pod "var-expansion-28836347-df12-40e5-abba-8ee4c3c2f1a8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.890068ms
    Jun 22 11:13:26.716: INFO: Pod "var-expansion-28836347-df12-40e5-abba-8ee4c3c2f1a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009422433s
    Jun 22 11:13:28.717: INFO: Pod "var-expansion-28836347-df12-40e5-abba-8ee4c3c2f1a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010122679s
    STEP: Saw pod success 06/22/23 11:13:28.717
    Jun 22 11:13:28.717: INFO: Pod "var-expansion-28836347-df12-40e5-abba-8ee4c3c2f1a8" satisfied condition "Succeeded or Failed"
    Jun 22 11:13:28.721: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod var-expansion-28836347-df12-40e5-abba-8ee4c3c2f1a8 container dapi-container: <nil>
    STEP: delete the pod 06/22/23 11:13:28.741
    Jun 22 11:13:28.757: INFO: Waiting for pod var-expansion-28836347-df12-40e5-abba-8ee4c3c2f1a8 to disappear
    Jun 22 11:13:28.761: INFO: Pod var-expansion-28836347-df12-40e5-abba-8ee4c3c2f1a8 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:13:28.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7261" for this suite. 06/22/23 11:13:28.768
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:13:28.776
Jun 22 11:13:28.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename crd-publish-openapi 06/22/23 11:13:28.777
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:13:28.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:13:28.796
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Jun 22 11:13:28.799: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/22/23 11:13:32.501
Jun 22 11:13:32.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-8302 --namespace=crd-publish-openapi-8302 create -f -'
Jun 22 11:13:34.024: INFO: stderr: ""
Jun 22 11:13:34.024: INFO: stdout: "e2e-test-crd-publish-openapi-9661-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 22 11:13:34.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-8302 --namespace=crd-publish-openapi-8302 delete e2e-test-crd-publish-openapi-9661-crds test-cr'
Jun 22 11:13:34.112: INFO: stderr: ""
Jun 22 11:13:34.113: INFO: stdout: "e2e-test-crd-publish-openapi-9661-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun 22 11:13:34.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-8302 --namespace=crd-publish-openapi-8302 apply -f -'
Jun 22 11:13:35.123: INFO: stderr: ""
Jun 22 11:13:35.123: INFO: stdout: "e2e-test-crd-publish-openapi-9661-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 22 11:13:35.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-8302 --namespace=crd-publish-openapi-8302 delete e2e-test-crd-publish-openapi-9661-crds test-cr'
Jun 22 11:13:35.211: INFO: stderr: ""
Jun 22 11:13:35.211: INFO: stdout: "e2e-test-crd-publish-openapi-9661-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 06/22/23 11:13:35.211
Jun 22 11:13:35.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-8302 explain e2e-test-crd-publish-openapi-9661-crds'
Jun 22 11:13:35.576: INFO: stderr: ""
Jun 22 11:13:35.576: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9661-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:13:38.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8302" for this suite. 06/22/23 11:13:38.229
------------------------------
• [SLOW TEST] [9.473 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:13:28.776
    Jun 22 11:13:28.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename crd-publish-openapi 06/22/23 11:13:28.777
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:13:28.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:13:28.796
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Jun 22 11:13:28.799: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/22/23 11:13:32.501
    Jun 22 11:13:32.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-8302 --namespace=crd-publish-openapi-8302 create -f -'
    Jun 22 11:13:34.024: INFO: stderr: ""
    Jun 22 11:13:34.024: INFO: stdout: "e2e-test-crd-publish-openapi-9661-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jun 22 11:13:34.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-8302 --namespace=crd-publish-openapi-8302 delete e2e-test-crd-publish-openapi-9661-crds test-cr'
    Jun 22 11:13:34.112: INFO: stderr: ""
    Jun 22 11:13:34.113: INFO: stdout: "e2e-test-crd-publish-openapi-9661-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jun 22 11:13:34.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-8302 --namespace=crd-publish-openapi-8302 apply -f -'
    Jun 22 11:13:35.123: INFO: stderr: ""
    Jun 22 11:13:35.123: INFO: stdout: "e2e-test-crd-publish-openapi-9661-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jun 22 11:13:35.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-8302 --namespace=crd-publish-openapi-8302 delete e2e-test-crd-publish-openapi-9661-crds test-cr'
    Jun 22 11:13:35.211: INFO: stderr: ""
    Jun 22 11:13:35.211: INFO: stdout: "e2e-test-crd-publish-openapi-9661-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 06/22/23 11:13:35.211
    Jun 22 11:13:35.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=crd-publish-openapi-8302 explain e2e-test-crd-publish-openapi-9661-crds'
    Jun 22 11:13:35.576: INFO: stderr: ""
    Jun 22 11:13:35.576: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9661-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:13:38.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8302" for this suite. 06/22/23 11:13:38.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:13:38.249
Jun 22 11:13:38.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename pods 06/22/23 11:13:38.25
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:13:38.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:13:38.279
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Jun 22 11:13:38.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: creating the pod 06/22/23 11:13:38.283
STEP: submitting the pod to kubernetes 06/22/23 11:13:38.283
Jun 22 11:13:38.296: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-57d52515-bd04-45f0-8858-ae7045974e97" in namespace "pods-5225" to be "running and ready"
Jun 22 11:13:38.301: INFO: Pod "pod-logs-websocket-57d52515-bd04-45f0-8858-ae7045974e97": Phase="Pending", Reason="", readiness=false. Elapsed: 5.000655ms
Jun 22 11:13:38.302: INFO: The phase of Pod pod-logs-websocket-57d52515-bd04-45f0-8858-ae7045974e97 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 11:13:40.308: INFO: Pod "pod-logs-websocket-57d52515-bd04-45f0-8858-ae7045974e97": Phase="Running", Reason="", readiness=true. Elapsed: 2.011473952s
Jun 22 11:13:40.308: INFO: The phase of Pod pod-logs-websocket-57d52515-bd04-45f0-8858-ae7045974e97 is Running (Ready = true)
Jun 22 11:13:40.308: INFO: Pod "pod-logs-websocket-57d52515-bd04-45f0-8858-ae7045974e97" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun 22 11:13:40.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5225" for this suite. 06/22/23 11:13:40.344
------------------------------
• [2.105 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:13:38.249
    Jun 22 11:13:38.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename pods 06/22/23 11:13:38.25
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:13:38.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:13:38.279
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Jun 22 11:13:38.282: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: creating the pod 06/22/23 11:13:38.283
    STEP: submitting the pod to kubernetes 06/22/23 11:13:38.283
    Jun 22 11:13:38.296: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-57d52515-bd04-45f0-8858-ae7045974e97" in namespace "pods-5225" to be "running and ready"
    Jun 22 11:13:38.301: INFO: Pod "pod-logs-websocket-57d52515-bd04-45f0-8858-ae7045974e97": Phase="Pending", Reason="", readiness=false. Elapsed: 5.000655ms
    Jun 22 11:13:38.302: INFO: The phase of Pod pod-logs-websocket-57d52515-bd04-45f0-8858-ae7045974e97 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 11:13:40.308: INFO: Pod "pod-logs-websocket-57d52515-bd04-45f0-8858-ae7045974e97": Phase="Running", Reason="", readiness=true. Elapsed: 2.011473952s
    Jun 22 11:13:40.308: INFO: The phase of Pod pod-logs-websocket-57d52515-bd04-45f0-8858-ae7045974e97 is Running (Ready = true)
    Jun 22 11:13:40.308: INFO: Pod "pod-logs-websocket-57d52515-bd04-45f0-8858-ae7045974e97" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:13:40.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5225" for this suite. 06/22/23 11:13:40.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:13:40.356
Jun 22 11:13:40.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename job 06/22/23 11:13:40.357
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:13:40.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:13:40.379
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 06/22/23 11:13:40.382
W0622 11:13:40.392092      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism 06/22/23 11:13:40.392
STEP: delete a job 06/22/23 11:13:42.399
STEP: deleting Job.batch foo in namespace job-2996, will wait for the garbage collector to delete the pods 06/22/23 11:13:42.4
Jun 22 11:13:42.466: INFO: Deleting Job.batch foo took: 9.476591ms
Jun 22 11:13:42.567: INFO: Terminating Job.batch foo pods took: 101.149846ms
STEP: Ensuring job was deleted 06/22/23 11:14:15.068
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jun 22 11:14:15.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2996" for this suite. 06/22/23 11:14:15.094
------------------------------
• [SLOW TEST] [34.751 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:13:40.356
    Jun 22 11:13:40.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename job 06/22/23 11:13:40.357
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:13:40.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:13:40.379
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 06/22/23 11:13:40.382
    W0622 11:13:40.392092      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring active pods == parallelism 06/22/23 11:13:40.392
    STEP: delete a job 06/22/23 11:13:42.399
    STEP: deleting Job.batch foo in namespace job-2996, will wait for the garbage collector to delete the pods 06/22/23 11:13:42.4
    Jun 22 11:13:42.466: INFO: Deleting Job.batch foo took: 9.476591ms
    Jun 22 11:13:42.567: INFO: Terminating Job.batch foo pods took: 101.149846ms
    STEP: Ensuring job was deleted 06/22/23 11:14:15.068
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:14:15.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2996" for this suite. 06/22/23 11:14:15.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:14:15.11
Jun 22 11:14:15.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename secrets 06/22/23 11:14:15.111
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:14:15.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:14:15.151
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-fc342672-44df-45d9-88db-44f6d0dba498 06/22/23 11:14:15.191
STEP: Creating a pod to test consume secrets 06/22/23 11:14:15.203
W0622 11:14:15.222734      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:14:15.222: INFO: Waiting up to 5m0s for pod "pod-secrets-c1a1b946-c6f4-4bf0-a45a-8d8e871d9ce3" in namespace "secrets-4426" to be "Succeeded or Failed"
Jun 22 11:14:15.229: INFO: Pod "pod-secrets-c1a1b946-c6f4-4bf0-a45a-8d8e871d9ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.381585ms
Jun 22 11:14:17.236: INFO: Pod "pod-secrets-c1a1b946-c6f4-4bf0-a45a-8d8e871d9ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013997591s
Jun 22 11:14:19.237: INFO: Pod "pod-secrets-c1a1b946-c6f4-4bf0-a45a-8d8e871d9ce3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014636282s
STEP: Saw pod success 06/22/23 11:14:19.237
Jun 22 11:14:19.237: INFO: Pod "pod-secrets-c1a1b946-c6f4-4bf0-a45a-8d8e871d9ce3" satisfied condition "Succeeded or Failed"
Jun 22 11:14:19.243: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-secrets-c1a1b946-c6f4-4bf0-a45a-8d8e871d9ce3 container secret-volume-test: <nil>
STEP: delete the pod 06/22/23 11:14:19.262
Jun 22 11:14:19.281: INFO: Waiting for pod pod-secrets-c1a1b946-c6f4-4bf0-a45a-8d8e871d9ce3 to disappear
Jun 22 11:14:19.286: INFO: Pod pod-secrets-c1a1b946-c6f4-4bf0-a45a-8d8e871d9ce3 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun 22 11:14:19.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4426" for this suite. 06/22/23 11:14:19.294
STEP: Destroying namespace "secret-namespace-8673" for this suite. 06/22/23 11:14:19.304
------------------------------
• [4.203 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:14:15.11
    Jun 22 11:14:15.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename secrets 06/22/23 11:14:15.111
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:14:15.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:14:15.151
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-fc342672-44df-45d9-88db-44f6d0dba498 06/22/23 11:14:15.191
    STEP: Creating a pod to test consume secrets 06/22/23 11:14:15.203
    W0622 11:14:15.222734      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:14:15.222: INFO: Waiting up to 5m0s for pod "pod-secrets-c1a1b946-c6f4-4bf0-a45a-8d8e871d9ce3" in namespace "secrets-4426" to be "Succeeded or Failed"
    Jun 22 11:14:15.229: INFO: Pod "pod-secrets-c1a1b946-c6f4-4bf0-a45a-8d8e871d9ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.381585ms
    Jun 22 11:14:17.236: INFO: Pod "pod-secrets-c1a1b946-c6f4-4bf0-a45a-8d8e871d9ce3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013997591s
    Jun 22 11:14:19.237: INFO: Pod "pod-secrets-c1a1b946-c6f4-4bf0-a45a-8d8e871d9ce3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014636282s
    STEP: Saw pod success 06/22/23 11:14:19.237
    Jun 22 11:14:19.237: INFO: Pod "pod-secrets-c1a1b946-c6f4-4bf0-a45a-8d8e871d9ce3" satisfied condition "Succeeded or Failed"
    Jun 22 11:14:19.243: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-secrets-c1a1b946-c6f4-4bf0-a45a-8d8e871d9ce3 container secret-volume-test: <nil>
    STEP: delete the pod 06/22/23 11:14:19.262
    Jun 22 11:14:19.281: INFO: Waiting for pod pod-secrets-c1a1b946-c6f4-4bf0-a45a-8d8e871d9ce3 to disappear
    Jun 22 11:14:19.286: INFO: Pod pod-secrets-c1a1b946-c6f4-4bf0-a45a-8d8e871d9ce3 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:14:19.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4426" for this suite. 06/22/23 11:14:19.294
    STEP: Destroying namespace "secret-namespace-8673" for this suite. 06/22/23 11:14:19.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:14:19.314
Jun 22 11:14:19.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubectl 06/22/23 11:14:19.315
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:14:19.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:14:19.339
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 06/22/23 11:14:19.343
Jun 22 11:14:19.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-6635 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun 22 11:14:19.438: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"logs-generator\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"logs-generator\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"logs-generator\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"logs-generator\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Jun 22 11:14:19.438: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 06/22/23 11:14:19.438
Jun 22 11:14:19.438: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun 22 11:14:19.438: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6635" to be "running and ready, or succeeded"
Jun 22 11:14:19.444: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.243522ms
Jun 22 11:14:19.445: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw' to be 'Running' but was 'Pending'
Jun 22 11:14:21.452: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.01356666s
Jun 22 11:14:21.452: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun 22 11:14:21.452: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 06/22/23 11:14:21.452
Jun 22 11:14:21.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-6635 logs logs-generator logs-generator'
Jun 22 11:14:21.544: INFO: stderr: ""
Jun 22 11:14:21.544: INFO: stdout: "I0622 11:14:20.136314       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/48l 286\nI0622 11:14:20.336842       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/t8nk 425\nI0622 11:14:20.537371       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/ltc 365\nI0622 11:14:20.736934       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/kql 251\nI0622 11:14:20.936367       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/5qq 582\nI0622 11:14:21.136929       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/wtg 272\nI0622 11:14:21.336383       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/ch5 480\nI0622 11:14:21.536911       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/tr6 576\n"
STEP: limiting log lines 06/22/23 11:14:21.544
Jun 22 11:14:21.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-6635 logs logs-generator logs-generator --tail=1'
Jun 22 11:14:21.631: INFO: stderr: ""
Jun 22 11:14:21.631: INFO: stdout: "I0622 11:14:21.536911       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/tr6 576\n"
Jun 22 11:14:21.631: INFO: got output "I0622 11:14:21.536911       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/tr6 576\n"
STEP: limiting log bytes 06/22/23 11:14:21.631
Jun 22 11:14:21.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-6635 logs logs-generator logs-generator --limit-bytes=1'
Jun 22 11:14:21.723: INFO: stderr: ""
Jun 22 11:14:21.723: INFO: stdout: "I"
Jun 22 11:14:21.723: INFO: got output "I"
STEP: exposing timestamps 06/22/23 11:14:21.723
Jun 22 11:14:21.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-6635 logs logs-generator logs-generator --tail=1 --timestamps'
Jun 22 11:14:21.812: INFO: stderr: ""
Jun 22 11:14:21.812: INFO: stdout: "2023-06-22T11:14:21.736589561Z I0622 11:14:21.736296       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/8dtr 293\n"
Jun 22 11:14:21.812: INFO: got output "2023-06-22T11:14:21.736589561Z I0622 11:14:21.736296       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/8dtr 293\n"
STEP: restricting to a time range 06/22/23 11:14:21.812
Jun 22 11:14:24.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-6635 logs logs-generator logs-generator --since=1s'
Jun 22 11:14:24.412: INFO: stderr: ""
Jun 22 11:14:24.412: INFO: stdout: "I0622 11:14:23.536451       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/k6h 332\nI0622 11:14:23.736959       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/drg 568\nI0622 11:14:23.936361       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/4mk 599\nI0622 11:14:24.136812       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/c876 280\nI0622 11:14:24.337386       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/v7g 200\n"
Jun 22 11:14:24.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-6635 logs logs-generator logs-generator --since=24h'
Jun 22 11:14:24.505: INFO: stderr: ""
Jun 22 11:14:24.505: INFO: stdout: "I0622 11:14:20.136314       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/48l 286\nI0622 11:14:20.336842       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/t8nk 425\nI0622 11:14:20.537371       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/ltc 365\nI0622 11:14:20.736934       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/kql 251\nI0622 11:14:20.936367       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/5qq 582\nI0622 11:14:21.136929       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/wtg 272\nI0622 11:14:21.336383       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/ch5 480\nI0622 11:14:21.536911       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/tr6 576\nI0622 11:14:21.736296       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/8dtr 293\nI0622 11:14:21.936654       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/p2w 402\nI0622 11:14:22.137171       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/bjl 350\nI0622 11:14:22.336392       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/t8mk 326\nI0622 11:14:22.536899       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/7wj 225\nI0622 11:14:22.736304       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/t2g 455\nI0622 11:14:22.936899       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/dlvq 329\nI0622 11:14:23.136355       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/7vj 596\nI0622 11:14:23.336994       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/pfq 334\nI0622 11:14:23.536451       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/k6h 332\nI0622 11:14:23.736959       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/drg 568\nI0622 11:14:23.936361       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/4mk 599\nI0622 11:14:24.136812       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/c876 280\nI0622 11:14:24.337386       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/v7g 200\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Jun 22 11:14:24.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-6635 delete pod logs-generator'
Jun 22 11:14:26.059: INFO: stderr: ""
Jun 22 11:14:26.059: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun 22 11:14:26.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6635" for this suite. 06/22/23 11:14:26.069
------------------------------
• [SLOW TEST] [6.764 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:14:19.314
    Jun 22 11:14:19.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubectl 06/22/23 11:14:19.315
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:14:19.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:14:19.339
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 06/22/23 11:14:19.343
    Jun 22 11:14:19.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-6635 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jun 22 11:14:19.438: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"logs-generator\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"logs-generator\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"logs-generator\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"logs-generator\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
    Jun 22 11:14:19.438: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 06/22/23 11:14:19.438
    Jun 22 11:14:19.438: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jun 22 11:14:19.438: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-6635" to be "running and ready, or succeeded"
    Jun 22 11:14:19.444: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.243522ms
    Jun 22 11:14:19.445: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw' to be 'Running' but was 'Pending'
    Jun 22 11:14:21.452: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.01356666s
    Jun 22 11:14:21.452: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jun 22 11:14:21.452: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 06/22/23 11:14:21.452
    Jun 22 11:14:21.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-6635 logs logs-generator logs-generator'
    Jun 22 11:14:21.544: INFO: stderr: ""
    Jun 22 11:14:21.544: INFO: stdout: "I0622 11:14:20.136314       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/48l 286\nI0622 11:14:20.336842       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/t8nk 425\nI0622 11:14:20.537371       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/ltc 365\nI0622 11:14:20.736934       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/kql 251\nI0622 11:14:20.936367       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/5qq 582\nI0622 11:14:21.136929       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/wtg 272\nI0622 11:14:21.336383       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/ch5 480\nI0622 11:14:21.536911       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/tr6 576\n"
    STEP: limiting log lines 06/22/23 11:14:21.544
    Jun 22 11:14:21.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-6635 logs logs-generator logs-generator --tail=1'
    Jun 22 11:14:21.631: INFO: stderr: ""
    Jun 22 11:14:21.631: INFO: stdout: "I0622 11:14:21.536911       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/tr6 576\n"
    Jun 22 11:14:21.631: INFO: got output "I0622 11:14:21.536911       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/tr6 576\n"
    STEP: limiting log bytes 06/22/23 11:14:21.631
    Jun 22 11:14:21.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-6635 logs logs-generator logs-generator --limit-bytes=1'
    Jun 22 11:14:21.723: INFO: stderr: ""
    Jun 22 11:14:21.723: INFO: stdout: "I"
    Jun 22 11:14:21.723: INFO: got output "I"
    STEP: exposing timestamps 06/22/23 11:14:21.723
    Jun 22 11:14:21.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-6635 logs logs-generator logs-generator --tail=1 --timestamps'
    Jun 22 11:14:21.812: INFO: stderr: ""
    Jun 22 11:14:21.812: INFO: stdout: "2023-06-22T11:14:21.736589561Z I0622 11:14:21.736296       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/8dtr 293\n"
    Jun 22 11:14:21.812: INFO: got output "2023-06-22T11:14:21.736589561Z I0622 11:14:21.736296       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/8dtr 293\n"
    STEP: restricting to a time range 06/22/23 11:14:21.812
    Jun 22 11:14:24.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-6635 logs logs-generator logs-generator --since=1s'
    Jun 22 11:14:24.412: INFO: stderr: ""
    Jun 22 11:14:24.412: INFO: stdout: "I0622 11:14:23.536451       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/k6h 332\nI0622 11:14:23.736959       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/drg 568\nI0622 11:14:23.936361       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/4mk 599\nI0622 11:14:24.136812       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/c876 280\nI0622 11:14:24.337386       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/v7g 200\n"
    Jun 22 11:14:24.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-6635 logs logs-generator logs-generator --since=24h'
    Jun 22 11:14:24.505: INFO: stderr: ""
    Jun 22 11:14:24.505: INFO: stdout: "I0622 11:14:20.136314       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/48l 286\nI0622 11:14:20.336842       1 logs_generator.go:76] 1 POST /api/v1/namespaces/ns/pods/t8nk 425\nI0622 11:14:20.537371       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/ltc 365\nI0622 11:14:20.736934       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/kql 251\nI0622 11:14:20.936367       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/5qq 582\nI0622 11:14:21.136929       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/wtg 272\nI0622 11:14:21.336383       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/ch5 480\nI0622 11:14:21.536911       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/tr6 576\nI0622 11:14:21.736296       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/8dtr 293\nI0622 11:14:21.936654       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/p2w 402\nI0622 11:14:22.137171       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/bjl 350\nI0622 11:14:22.336392       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/t8mk 326\nI0622 11:14:22.536899       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/7wj 225\nI0622 11:14:22.736304       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/t2g 455\nI0622 11:14:22.936899       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/dlvq 329\nI0622 11:14:23.136355       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/7vj 596\nI0622 11:14:23.336994       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/pfq 334\nI0622 11:14:23.536451       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/k6h 332\nI0622 11:14:23.736959       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/drg 568\nI0622 11:14:23.936361       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/4mk 599\nI0622 11:14:24.136812       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/c876 280\nI0622 11:14:24.337386       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/v7g 200\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Jun 22 11:14:24.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-6635 delete pod logs-generator'
    Jun 22 11:14:26.059: INFO: stderr: ""
    Jun 22 11:14:26.059: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:14:26.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6635" for this suite. 06/22/23 11:14:26.069
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:14:26.078
Jun 22 11:14:26.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename var-expansion 06/22/23 11:14:26.079
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:14:26.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:14:26.106
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 06/22/23 11:14:26.11
W0622 11:14:26.127616      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:14:26.127: INFO: Waiting up to 5m0s for pod "var-expansion-7924fd4b-a258-4678-b323-070b6d7dd789" in namespace "var-expansion-1631" to be "Succeeded or Failed"
Jun 22 11:14:26.134: INFO: Pod "var-expansion-7924fd4b-a258-4678-b323-070b6d7dd789": Phase="Pending", Reason="", readiness=false. Elapsed: 6.282222ms
Jun 22 11:14:28.140: INFO: Pod "var-expansion-7924fd4b-a258-4678-b323-070b6d7dd789": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012916158s
Jun 22 11:14:30.141: INFO: Pod "var-expansion-7924fd4b-a258-4678-b323-070b6d7dd789": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01367261s
STEP: Saw pod success 06/22/23 11:14:30.141
Jun 22 11:14:30.141: INFO: Pod "var-expansion-7924fd4b-a258-4678-b323-070b6d7dd789" satisfied condition "Succeeded or Failed"
Jun 22 11:14:30.147: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod var-expansion-7924fd4b-a258-4678-b323-070b6d7dd789 container dapi-container: <nil>
STEP: delete the pod 06/22/23 11:14:30.157
Jun 22 11:14:30.175: INFO: Waiting for pod var-expansion-7924fd4b-a258-4678-b323-070b6d7dd789 to disappear
Jun 22 11:14:30.179: INFO: Pod var-expansion-7924fd4b-a258-4678-b323-070b6d7dd789 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun 22 11:14:30.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1631" for this suite. 06/22/23 11:14:30.187
------------------------------
• [4.117 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:14:26.078
    Jun 22 11:14:26.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename var-expansion 06/22/23 11:14:26.079
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:14:26.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:14:26.106
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 06/22/23 11:14:26.11
    W0622 11:14:26.127616      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:14:26.127: INFO: Waiting up to 5m0s for pod "var-expansion-7924fd4b-a258-4678-b323-070b6d7dd789" in namespace "var-expansion-1631" to be "Succeeded or Failed"
    Jun 22 11:14:26.134: INFO: Pod "var-expansion-7924fd4b-a258-4678-b323-070b6d7dd789": Phase="Pending", Reason="", readiness=false. Elapsed: 6.282222ms
    Jun 22 11:14:28.140: INFO: Pod "var-expansion-7924fd4b-a258-4678-b323-070b6d7dd789": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012916158s
    Jun 22 11:14:30.141: INFO: Pod "var-expansion-7924fd4b-a258-4678-b323-070b6d7dd789": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01367261s
    STEP: Saw pod success 06/22/23 11:14:30.141
    Jun 22 11:14:30.141: INFO: Pod "var-expansion-7924fd4b-a258-4678-b323-070b6d7dd789" satisfied condition "Succeeded or Failed"
    Jun 22 11:14:30.147: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod var-expansion-7924fd4b-a258-4678-b323-070b6d7dd789 container dapi-container: <nil>
    STEP: delete the pod 06/22/23 11:14:30.157
    Jun 22 11:14:30.175: INFO: Waiting for pod var-expansion-7924fd4b-a258-4678-b323-070b6d7dd789 to disappear
    Jun 22 11:14:30.179: INFO: Pod var-expansion-7924fd4b-a258-4678-b323-070b6d7dd789 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:14:30.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1631" for this suite. 06/22/23 11:14:30.187
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:14:30.196
Jun 22 11:14:30.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename webhook 06/22/23 11:14:30.197
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:14:30.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:14:30.224
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/22/23 11:14:30.244
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:14:30.813
STEP: Deploying the webhook pod 06/22/23 11:14:30.826
W0622 11:14:30.847513      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 11:14:30.847
Jun 22 11:14:30.860: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/22/23 11:14:32.879
STEP: Verifying the service has paired with the endpoint 06/22/23 11:14:32.908
Jun 22 11:14:33.909: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 06/22/23 11:14:34.007
STEP: Creating a configMap that does not comply to the validation webhook rules 06/22/23 11:14:34.054
STEP: Deleting the collection of validation webhooks 06/22/23 11:14:34.086
STEP: Creating a configMap that does not comply to the validation webhook rules 06/22/23 11:14:34.161
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:14:34.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3206" for this suite. 06/22/23 11:14:34.246
STEP: Destroying namespace "webhook-3206-markers" for this suite. 06/22/23 11:14:34.262
------------------------------
• [4.075 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:14:30.196
    Jun 22 11:14:30.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename webhook 06/22/23 11:14:30.197
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:14:30.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:14:30.224
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/22/23 11:14:30.244
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:14:30.813
    STEP: Deploying the webhook pod 06/22/23 11:14:30.826
    W0622 11:14:30.847513      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 11:14:30.847
    Jun 22 11:14:30.860: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/22/23 11:14:32.879
    STEP: Verifying the service has paired with the endpoint 06/22/23 11:14:32.908
    Jun 22 11:14:33.909: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 06/22/23 11:14:34.007
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/22/23 11:14:34.054
    STEP: Deleting the collection of validation webhooks 06/22/23 11:14:34.086
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/22/23 11:14:34.161
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:14:34.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3206" for this suite. 06/22/23 11:14:34.246
    STEP: Destroying namespace "webhook-3206-markers" for this suite. 06/22/23 11:14:34.262
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:14:34.273
Jun 22 11:14:34.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename job 06/22/23 11:14:34.275
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:14:34.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:14:34.301
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 06/22/23 11:14:34.305
W0622 11:14:34.314481      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism 06/22/23 11:14:34.314
STEP: Orphaning one of the Job's Pods 06/22/23 11:14:36.323
Jun 22 11:14:36.850: INFO: Successfully updated pod "adopt-release-b4cdz"
STEP: Checking that the Job readopts the Pod 06/22/23 11:14:36.85
Jun 22 11:14:36.850: INFO: Waiting up to 15m0s for pod "adopt-release-b4cdz" in namespace "job-5732" to be "adopted"
Jun 22 11:14:36.855: INFO: Pod "adopt-release-b4cdz": Phase="Running", Reason="", readiness=true. Elapsed: 4.923993ms
Jun 22 11:14:38.861: INFO: Pod "adopt-release-b4cdz": Phase="Running", Reason="", readiness=true. Elapsed: 2.011062981s
Jun 22 11:14:38.861: INFO: Pod "adopt-release-b4cdz" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 06/22/23 11:14:38.861
Jun 22 11:14:39.378: INFO: Successfully updated pod "adopt-release-b4cdz"
STEP: Checking that the Job releases the Pod 06/22/23 11:14:39.378
Jun 22 11:14:39.378: INFO: Waiting up to 15m0s for pod "adopt-release-b4cdz" in namespace "job-5732" to be "released"
Jun 22 11:14:39.383: INFO: Pod "adopt-release-b4cdz": Phase="Running", Reason="", readiness=true. Elapsed: 4.505606ms
Jun 22 11:14:41.390: INFO: Pod "adopt-release-b4cdz": Phase="Running", Reason="", readiness=true. Elapsed: 2.011628206s
Jun 22 11:14:41.390: INFO: Pod "adopt-release-b4cdz" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jun 22 11:14:41.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5732" for this suite. 06/22/23 11:14:41.399
------------------------------
• [SLOW TEST] [7.134 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:14:34.273
    Jun 22 11:14:34.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename job 06/22/23 11:14:34.275
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:14:34.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:14:34.301
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 06/22/23 11:14:34.305
    W0622 11:14:34.314481      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring active pods == parallelism 06/22/23 11:14:34.314
    STEP: Orphaning one of the Job's Pods 06/22/23 11:14:36.323
    Jun 22 11:14:36.850: INFO: Successfully updated pod "adopt-release-b4cdz"
    STEP: Checking that the Job readopts the Pod 06/22/23 11:14:36.85
    Jun 22 11:14:36.850: INFO: Waiting up to 15m0s for pod "adopt-release-b4cdz" in namespace "job-5732" to be "adopted"
    Jun 22 11:14:36.855: INFO: Pod "adopt-release-b4cdz": Phase="Running", Reason="", readiness=true. Elapsed: 4.923993ms
    Jun 22 11:14:38.861: INFO: Pod "adopt-release-b4cdz": Phase="Running", Reason="", readiness=true. Elapsed: 2.011062981s
    Jun 22 11:14:38.861: INFO: Pod "adopt-release-b4cdz" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 06/22/23 11:14:38.861
    Jun 22 11:14:39.378: INFO: Successfully updated pod "adopt-release-b4cdz"
    STEP: Checking that the Job releases the Pod 06/22/23 11:14:39.378
    Jun 22 11:14:39.378: INFO: Waiting up to 15m0s for pod "adopt-release-b4cdz" in namespace "job-5732" to be "released"
    Jun 22 11:14:39.383: INFO: Pod "adopt-release-b4cdz": Phase="Running", Reason="", readiness=true. Elapsed: 4.505606ms
    Jun 22 11:14:41.390: INFO: Pod "adopt-release-b4cdz": Phase="Running", Reason="", readiness=true. Elapsed: 2.011628206s
    Jun 22 11:14:41.390: INFO: Pod "adopt-release-b4cdz" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:14:41.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5732" for this suite. 06/22/23 11:14:41.399
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:14:41.408
Jun 22 11:14:41.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename emptydir 06/22/23 11:14:41.409
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:14:41.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:14:41.437
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 06/22/23 11:14:41.441
W0622 11:14:41.453539      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:14:41.453: INFO: Waiting up to 5m0s for pod "pod-4b7874c4-2dce-4895-8418-68fe6bba375a" in namespace "emptydir-1724" to be "Succeeded or Failed"
Jun 22 11:14:41.461: INFO: Pod "pod-4b7874c4-2dce-4895-8418-68fe6bba375a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009329ms
Jun 22 11:14:43.468: INFO: Pod "pod-4b7874c4-2dce-4895-8418-68fe6bba375a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014961357s
Jun 22 11:14:45.468: INFO: Pod "pod-4b7874c4-2dce-4895-8418-68fe6bba375a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014995646s
STEP: Saw pod success 06/22/23 11:14:45.468
Jun 22 11:14:45.468: INFO: Pod "pod-4b7874c4-2dce-4895-8418-68fe6bba375a" satisfied condition "Succeeded or Failed"
Jun 22 11:14:45.474: INFO: Trying to get logs from node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 pod pod-4b7874c4-2dce-4895-8418-68fe6bba375a container test-container: <nil>
STEP: delete the pod 06/22/23 11:14:45.497
Jun 22 11:14:45.518: INFO: Waiting for pod pod-4b7874c4-2dce-4895-8418-68fe6bba375a to disappear
Jun 22 11:14:45.524: INFO: Pod pod-4b7874c4-2dce-4895-8418-68fe6bba375a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun 22 11:14:45.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1724" for this suite. 06/22/23 11:14:45.532
------------------------------
• [4.134 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:14:41.408
    Jun 22 11:14:41.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename emptydir 06/22/23 11:14:41.409
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:14:41.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:14:41.437
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 06/22/23 11:14:41.441
    W0622 11:14:41.453539      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:14:41.453: INFO: Waiting up to 5m0s for pod "pod-4b7874c4-2dce-4895-8418-68fe6bba375a" in namespace "emptydir-1724" to be "Succeeded or Failed"
    Jun 22 11:14:41.461: INFO: Pod "pod-4b7874c4-2dce-4895-8418-68fe6bba375a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009329ms
    Jun 22 11:14:43.468: INFO: Pod "pod-4b7874c4-2dce-4895-8418-68fe6bba375a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014961357s
    Jun 22 11:14:45.468: INFO: Pod "pod-4b7874c4-2dce-4895-8418-68fe6bba375a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014995646s
    STEP: Saw pod success 06/22/23 11:14:45.468
    Jun 22 11:14:45.468: INFO: Pod "pod-4b7874c4-2dce-4895-8418-68fe6bba375a" satisfied condition "Succeeded or Failed"
    Jun 22 11:14:45.474: INFO: Trying to get logs from node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 pod pod-4b7874c4-2dce-4895-8418-68fe6bba375a container test-container: <nil>
    STEP: delete the pod 06/22/23 11:14:45.497
    Jun 22 11:14:45.518: INFO: Waiting for pod pod-4b7874c4-2dce-4895-8418-68fe6bba375a to disappear
    Jun 22 11:14:45.524: INFO: Pod pod-4b7874c4-2dce-4895-8418-68fe6bba375a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:14:45.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1724" for this suite. 06/22/23 11:14:45.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:14:45.543
Jun 22 11:14:45.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename configmap 06/22/23 11:14:45.544
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:14:45.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:14:45.571
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 06/22/23 11:14:45.575
STEP: fetching the ConfigMap 06/22/23 11:14:45.583
STEP: patching the ConfigMap 06/22/23 11:14:45.588
STEP: listing all ConfigMaps in all namespaces with a label selector 06/22/23 11:14:45.595
STEP: deleting the ConfigMap by collection with a label selector 06/22/23 11:14:45.604
STEP: listing all ConfigMaps in test namespace 06/22/23 11:14:45.616
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun 22 11:14:45.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6569" for this suite. 06/22/23 11:14:45.631
------------------------------
• [0.099 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:14:45.543
    Jun 22 11:14:45.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename configmap 06/22/23 11:14:45.544
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:14:45.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:14:45.571
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 06/22/23 11:14:45.575
    STEP: fetching the ConfigMap 06/22/23 11:14:45.583
    STEP: patching the ConfigMap 06/22/23 11:14:45.588
    STEP: listing all ConfigMaps in all namespaces with a label selector 06/22/23 11:14:45.595
    STEP: deleting the ConfigMap by collection with a label selector 06/22/23 11:14:45.604
    STEP: listing all ConfigMaps in test namespace 06/22/23 11:14:45.616
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:14:45.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6569" for this suite. 06/22/23 11:14:45.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:14:45.643
Jun 22 11:14:45.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename server-version 06/22/23 11:14:45.644
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:14:45.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:14:45.669
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 06/22/23 11:14:45.673
STEP: Confirm major version 06/22/23 11:14:45.674
Jun 22 11:14:45.675: INFO: Major version: 1
STEP: Confirm minor version 06/22/23 11:14:45.675
Jun 22 11:14:45.675: INFO: cleanMinorVersion: 26
Jun 22 11:14:45.675: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Jun 22 11:14:45.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-7007" for this suite. 06/22/23 11:14:45.683
------------------------------
• [0.052 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:14:45.643
    Jun 22 11:14:45.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename server-version 06/22/23 11:14:45.644
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:14:45.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:14:45.669
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 06/22/23 11:14:45.673
    STEP: Confirm major version 06/22/23 11:14:45.674
    Jun 22 11:14:45.675: INFO: Major version: 1
    STEP: Confirm minor version 06/22/23 11:14:45.675
    Jun 22 11:14:45.675: INFO: cleanMinorVersion: 26
    Jun 22 11:14:45.675: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:14:45.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-7007" for this suite. 06/22/23 11:14:45.683
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:14:45.696
Jun 22 11:14:45.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename gc 06/22/23 11:14:45.698
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:14:45.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:14:45.724
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 06/22/23 11:14:45.739
W0622 11:14:45.752305      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the rc 06/22/23 11:14:50.761
STEP: wait for the rc to be deleted 06/22/23 11:14:50.77
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 06/22/23 11:14:55.777
STEP: Gathering metrics 06/22/23 11:15:25.799
Jun 22 11:15:25.839: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" in namespace "kube-system" to be "running and ready"
Jun 22 11:15:25.845: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v": Phase="Running", Reason="", readiness=true. Elapsed: 5.434083ms
Jun 22 11:15:25.845: INFO: The phase of Pod kube-controller-manager-wl-antrea-prdzc-k4b4v is Running (Ready = true)
Jun 22 11:15:25.845: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" satisfied condition "running and ready"
Jun 22 11:15:25.951: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun 22 11:15:25.951: INFO: Deleting pod "simpletest.rc-247d8" in namespace "gc-3406"
Jun 22 11:15:25.967: INFO: Deleting pod "simpletest.rc-2g4sx" in namespace "gc-3406"
Jun 22 11:15:25.982: INFO: Deleting pod "simpletest.rc-2wpmf" in namespace "gc-3406"
Jun 22 11:15:25.997: INFO: Deleting pod "simpletest.rc-2zxtl" in namespace "gc-3406"
Jun 22 11:15:26.015: INFO: Deleting pod "simpletest.rc-455j2" in namespace "gc-3406"
Jun 22 11:15:26.046: INFO: Deleting pod "simpletest.rc-45jwx" in namespace "gc-3406"
Jun 22 11:15:26.087: INFO: Deleting pod "simpletest.rc-4prrp" in namespace "gc-3406"
Jun 22 11:15:26.105: INFO: Deleting pod "simpletest.rc-52vtv" in namespace "gc-3406"
Jun 22 11:15:26.144: INFO: Deleting pod "simpletest.rc-5bld5" in namespace "gc-3406"
Jun 22 11:15:26.164: INFO: Deleting pod "simpletest.rc-5fbsq" in namespace "gc-3406"
Jun 22 11:15:26.190: INFO: Deleting pod "simpletest.rc-5j9cf" in namespace "gc-3406"
Jun 22 11:15:26.209: INFO: Deleting pod "simpletest.rc-5lqjs" in namespace "gc-3406"
Jun 22 11:15:26.224: INFO: Deleting pod "simpletest.rc-5rvh9" in namespace "gc-3406"
Jun 22 11:15:26.247: INFO: Deleting pod "simpletest.rc-6267r" in namespace "gc-3406"
Jun 22 11:15:26.268: INFO: Deleting pod "simpletest.rc-64nk5" in namespace "gc-3406"
Jun 22 11:15:26.297: INFO: Deleting pod "simpletest.rc-6lq8f" in namespace "gc-3406"
Jun 22 11:15:26.323: INFO: Deleting pod "simpletest.rc-78ccg" in namespace "gc-3406"
Jun 22 11:15:26.341: INFO: Deleting pod "simpletest.rc-7bx92" in namespace "gc-3406"
Jun 22 11:15:26.366: INFO: Deleting pod "simpletest.rc-7tcdp" in namespace "gc-3406"
Jun 22 11:15:26.408: INFO: Deleting pod "simpletest.rc-82zpv" in namespace "gc-3406"
Jun 22 11:15:26.422: INFO: Deleting pod "simpletest.rc-86g8p" in namespace "gc-3406"
Jun 22 11:15:26.552: INFO: Deleting pod "simpletest.rc-8hl8b" in namespace "gc-3406"
Jun 22 11:15:26.582: INFO: Deleting pod "simpletest.rc-99kdp" in namespace "gc-3406"
Jun 22 11:15:26.634: INFO: Deleting pod "simpletest.rc-9bg4f" in namespace "gc-3406"
Jun 22 11:15:26.666: INFO: Deleting pod "simpletest.rc-9nszl" in namespace "gc-3406"
Jun 22 11:15:26.690: INFO: Deleting pod "simpletest.rc-9zr28" in namespace "gc-3406"
Jun 22 11:15:26.747: INFO: Deleting pod "simpletest.rc-b4qw6" in namespace "gc-3406"
Jun 22 11:15:26.767: INFO: Deleting pod "simpletest.rc-b4xtb" in namespace "gc-3406"
Jun 22 11:15:26.782: INFO: Deleting pod "simpletest.rc-bfl6g" in namespace "gc-3406"
Jun 22 11:15:26.799: INFO: Deleting pod "simpletest.rc-bw5pb" in namespace "gc-3406"
Jun 22 11:15:26.819: INFO: Deleting pod "simpletest.rc-cnmnx" in namespace "gc-3406"
Jun 22 11:15:26.836: INFO: Deleting pod "simpletest.rc-cq6pj" in namespace "gc-3406"
Jun 22 11:15:26.855: INFO: Deleting pod "simpletest.rc-crs79" in namespace "gc-3406"
Jun 22 11:15:26.876: INFO: Deleting pod "simpletest.rc-crsbs" in namespace "gc-3406"
Jun 22 11:15:26.891: INFO: Deleting pod "simpletest.rc-cwzw7" in namespace "gc-3406"
Jun 22 11:15:26.912: INFO: Deleting pod "simpletest.rc-d4s52" in namespace "gc-3406"
Jun 22 11:15:26.932: INFO: Deleting pod "simpletest.rc-d9dsg" in namespace "gc-3406"
Jun 22 11:15:26.947: INFO: Deleting pod "simpletest.rc-dvhcj" in namespace "gc-3406"
Jun 22 11:15:26.968: INFO: Deleting pod "simpletest.rc-dzqp2" in namespace "gc-3406"
Jun 22 11:15:26.990: INFO: Deleting pod "simpletest.rc-f2qw8" in namespace "gc-3406"
Jun 22 11:15:27.010: INFO: Deleting pod "simpletest.rc-f957f" in namespace "gc-3406"
Jun 22 11:15:27.032: INFO: Deleting pod "simpletest.rc-fccqt" in namespace "gc-3406"
Jun 22 11:15:27.053: INFO: Deleting pod "simpletest.rc-fj5s6" in namespace "gc-3406"
Jun 22 11:15:27.075: INFO: Deleting pod "simpletest.rc-fvhxl" in namespace "gc-3406"
Jun 22 11:15:27.096: INFO: Deleting pod "simpletest.rc-grbmt" in namespace "gc-3406"
Jun 22 11:15:27.118: INFO: Deleting pod "simpletest.rc-h4g8z" in namespace "gc-3406"
Jun 22 11:15:27.137: INFO: Deleting pod "simpletest.rc-h6cfj" in namespace "gc-3406"
Jun 22 11:15:27.155: INFO: Deleting pod "simpletest.rc-h6wwh" in namespace "gc-3406"
Jun 22 11:15:27.174: INFO: Deleting pod "simpletest.rc-hc8ml" in namespace "gc-3406"
Jun 22 11:15:27.194: INFO: Deleting pod "simpletest.rc-hp9fm" in namespace "gc-3406"
Jun 22 11:15:27.209: INFO: Deleting pod "simpletest.rc-j6ptq" in namespace "gc-3406"
Jun 22 11:15:27.225: INFO: Deleting pod "simpletest.rc-jhlrq" in namespace "gc-3406"
Jun 22 11:15:27.240: INFO: Deleting pod "simpletest.rc-l7qt5" in namespace "gc-3406"
Jun 22 11:15:27.256: INFO: Deleting pod "simpletest.rc-l8jtg" in namespace "gc-3406"
Jun 22 11:15:27.271: INFO: Deleting pod "simpletest.rc-lc2dc" in namespace "gc-3406"
Jun 22 11:15:27.294: INFO: Deleting pod "simpletest.rc-lcwzm" in namespace "gc-3406"
Jun 22 11:15:27.316: INFO: Deleting pod "simpletest.rc-lmxgt" in namespace "gc-3406"
Jun 22 11:15:27.336: INFO: Deleting pod "simpletest.rc-lrkjt" in namespace "gc-3406"
Jun 22 11:15:27.357: INFO: Deleting pod "simpletest.rc-lsfsj" in namespace "gc-3406"
Jun 22 11:15:27.382: INFO: Deleting pod "simpletest.rc-lzjz4" in namespace "gc-3406"
Jun 22 11:15:27.399: INFO: Deleting pod "simpletest.rc-mfmcj" in namespace "gc-3406"
Jun 22 11:15:27.423: INFO: Deleting pod "simpletest.rc-mqcs5" in namespace "gc-3406"
Jun 22 11:15:27.440: INFO: Deleting pod "simpletest.rc-n6gvw" in namespace "gc-3406"
Jun 22 11:15:27.460: INFO: Deleting pod "simpletest.rc-n867l" in namespace "gc-3406"
Jun 22 11:15:27.480: INFO: Deleting pod "simpletest.rc-nz28q" in namespace "gc-3406"
Jun 22 11:15:27.505: INFO: Deleting pod "simpletest.rc-pdgvf" in namespace "gc-3406"
Jun 22 11:15:27.525: INFO: Deleting pod "simpletest.rc-pf7cb" in namespace "gc-3406"
Jun 22 11:15:27.549: INFO: Deleting pod "simpletest.rc-pj45l" in namespace "gc-3406"
Jun 22 11:15:27.569: INFO: Deleting pod "simpletest.rc-ppx9f" in namespace "gc-3406"
Jun 22 11:15:27.590: INFO: Deleting pod "simpletest.rc-pwprg" in namespace "gc-3406"
Jun 22 11:15:27.613: INFO: Deleting pod "simpletest.rc-q5ndq" in namespace "gc-3406"
Jun 22 11:15:27.640: INFO: Deleting pod "simpletest.rc-q7hcq" in namespace "gc-3406"
Jun 22 11:15:27.658: INFO: Deleting pod "simpletest.rc-qrgdd" in namespace "gc-3406"
Jun 22 11:15:27.680: INFO: Deleting pod "simpletest.rc-qscwm" in namespace "gc-3406"
Jun 22 11:15:27.697: INFO: Deleting pod "simpletest.rc-qwpg4" in namespace "gc-3406"
Jun 22 11:15:27.718: INFO: Deleting pod "simpletest.rc-r28gf" in namespace "gc-3406"
Jun 22 11:15:27.735: INFO: Deleting pod "simpletest.rc-r2l5s" in namespace "gc-3406"
Jun 22 11:15:27.752: INFO: Deleting pod "simpletest.rc-r2rb8" in namespace "gc-3406"
Jun 22 11:15:27.768: INFO: Deleting pod "simpletest.rc-s2279" in namespace "gc-3406"
Jun 22 11:15:27.794: INFO: Deleting pod "simpletest.rc-s22xk" in namespace "gc-3406"
Jun 22 11:15:27.819: INFO: Deleting pod "simpletest.rc-s7mcr" in namespace "gc-3406"
Jun 22 11:15:27.838: INFO: Deleting pod "simpletest.rc-s9fp8" in namespace "gc-3406"
Jun 22 11:15:27.860: INFO: Deleting pod "simpletest.rc-sgptb" in namespace "gc-3406"
Jun 22 11:15:27.886: INFO: Deleting pod "simpletest.rc-swlb7" in namespace "gc-3406"
Jun 22 11:15:27.903: INFO: Deleting pod "simpletest.rc-t4xkf" in namespace "gc-3406"
Jun 22 11:15:27.917: INFO: Deleting pod "simpletest.rc-t79rd" in namespace "gc-3406"
Jun 22 11:15:27.941: INFO: Deleting pod "simpletest.rc-ttv22" in namespace "gc-3406"
Jun 22 11:15:27.964: INFO: Deleting pod "simpletest.rc-vddhc" in namespace "gc-3406"
Jun 22 11:15:27.982: INFO: Deleting pod "simpletest.rc-vg2df" in namespace "gc-3406"
Jun 22 11:15:28.001: INFO: Deleting pod "simpletest.rc-vrpf4" in namespace "gc-3406"
Jun 22 11:15:28.021: INFO: Deleting pod "simpletest.rc-wqm8w" in namespace "gc-3406"
Jun 22 11:15:28.049: INFO: Deleting pod "simpletest.rc-wvcpw" in namespace "gc-3406"
Jun 22 11:15:28.103: INFO: Deleting pod "simpletest.rc-wvskf" in namespace "gc-3406"
Jun 22 11:15:28.149: INFO: Deleting pod "simpletest.rc-x5lj8" in namespace "gc-3406"
Jun 22 11:15:28.201: INFO: Deleting pod "simpletest.rc-x96l6" in namespace "gc-3406"
Jun 22 11:15:28.251: INFO: Deleting pod "simpletest.rc-x98hk" in namespace "gc-3406"
Jun 22 11:15:28.301: INFO: Deleting pod "simpletest.rc-z7rvv" in namespace "gc-3406"
Jun 22 11:15:28.352: INFO: Deleting pod "simpletest.rc-zh8hj" in namespace "gc-3406"
Jun 22 11:15:28.405: INFO: Deleting pod "simpletest.rc-zn46s" in namespace "gc-3406"
Jun 22 11:15:28.451: INFO: Deleting pod "simpletest.rc-zxjw6" in namespace "gc-3406"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun 22 11:15:28.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3406" for this suite. 06/22/23 11:15:28.538
------------------------------
• [SLOW TEST] [42.894 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:14:45.696
    Jun 22 11:14:45.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename gc 06/22/23 11:14:45.698
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:14:45.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:14:45.724
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 06/22/23 11:14:45.739
    W0622 11:14:45.752305      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: delete the rc 06/22/23 11:14:50.761
    STEP: wait for the rc to be deleted 06/22/23 11:14:50.77
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 06/22/23 11:14:55.777
    STEP: Gathering metrics 06/22/23 11:15:25.799
    Jun 22 11:15:25.839: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" in namespace "kube-system" to be "running and ready"
    Jun 22 11:15:25.845: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v": Phase="Running", Reason="", readiness=true. Elapsed: 5.434083ms
    Jun 22 11:15:25.845: INFO: The phase of Pod kube-controller-manager-wl-antrea-prdzc-k4b4v is Running (Ready = true)
    Jun 22 11:15:25.845: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" satisfied condition "running and ready"
    Jun 22 11:15:25.951: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jun 22 11:15:25.951: INFO: Deleting pod "simpletest.rc-247d8" in namespace "gc-3406"
    Jun 22 11:15:25.967: INFO: Deleting pod "simpletest.rc-2g4sx" in namespace "gc-3406"
    Jun 22 11:15:25.982: INFO: Deleting pod "simpletest.rc-2wpmf" in namespace "gc-3406"
    Jun 22 11:15:25.997: INFO: Deleting pod "simpletest.rc-2zxtl" in namespace "gc-3406"
    Jun 22 11:15:26.015: INFO: Deleting pod "simpletest.rc-455j2" in namespace "gc-3406"
    Jun 22 11:15:26.046: INFO: Deleting pod "simpletest.rc-45jwx" in namespace "gc-3406"
    Jun 22 11:15:26.087: INFO: Deleting pod "simpletest.rc-4prrp" in namespace "gc-3406"
    Jun 22 11:15:26.105: INFO: Deleting pod "simpletest.rc-52vtv" in namespace "gc-3406"
    Jun 22 11:15:26.144: INFO: Deleting pod "simpletest.rc-5bld5" in namespace "gc-3406"
    Jun 22 11:15:26.164: INFO: Deleting pod "simpletest.rc-5fbsq" in namespace "gc-3406"
    Jun 22 11:15:26.190: INFO: Deleting pod "simpletest.rc-5j9cf" in namespace "gc-3406"
    Jun 22 11:15:26.209: INFO: Deleting pod "simpletest.rc-5lqjs" in namespace "gc-3406"
    Jun 22 11:15:26.224: INFO: Deleting pod "simpletest.rc-5rvh9" in namespace "gc-3406"
    Jun 22 11:15:26.247: INFO: Deleting pod "simpletest.rc-6267r" in namespace "gc-3406"
    Jun 22 11:15:26.268: INFO: Deleting pod "simpletest.rc-64nk5" in namespace "gc-3406"
    Jun 22 11:15:26.297: INFO: Deleting pod "simpletest.rc-6lq8f" in namespace "gc-3406"
    Jun 22 11:15:26.323: INFO: Deleting pod "simpletest.rc-78ccg" in namespace "gc-3406"
    Jun 22 11:15:26.341: INFO: Deleting pod "simpletest.rc-7bx92" in namespace "gc-3406"
    Jun 22 11:15:26.366: INFO: Deleting pod "simpletest.rc-7tcdp" in namespace "gc-3406"
    Jun 22 11:15:26.408: INFO: Deleting pod "simpletest.rc-82zpv" in namespace "gc-3406"
    Jun 22 11:15:26.422: INFO: Deleting pod "simpletest.rc-86g8p" in namespace "gc-3406"
    Jun 22 11:15:26.552: INFO: Deleting pod "simpletest.rc-8hl8b" in namespace "gc-3406"
    Jun 22 11:15:26.582: INFO: Deleting pod "simpletest.rc-99kdp" in namespace "gc-3406"
    Jun 22 11:15:26.634: INFO: Deleting pod "simpletest.rc-9bg4f" in namespace "gc-3406"
    Jun 22 11:15:26.666: INFO: Deleting pod "simpletest.rc-9nszl" in namespace "gc-3406"
    Jun 22 11:15:26.690: INFO: Deleting pod "simpletest.rc-9zr28" in namespace "gc-3406"
    Jun 22 11:15:26.747: INFO: Deleting pod "simpletest.rc-b4qw6" in namespace "gc-3406"
    Jun 22 11:15:26.767: INFO: Deleting pod "simpletest.rc-b4xtb" in namespace "gc-3406"
    Jun 22 11:15:26.782: INFO: Deleting pod "simpletest.rc-bfl6g" in namespace "gc-3406"
    Jun 22 11:15:26.799: INFO: Deleting pod "simpletest.rc-bw5pb" in namespace "gc-3406"
    Jun 22 11:15:26.819: INFO: Deleting pod "simpletest.rc-cnmnx" in namespace "gc-3406"
    Jun 22 11:15:26.836: INFO: Deleting pod "simpletest.rc-cq6pj" in namespace "gc-3406"
    Jun 22 11:15:26.855: INFO: Deleting pod "simpletest.rc-crs79" in namespace "gc-3406"
    Jun 22 11:15:26.876: INFO: Deleting pod "simpletest.rc-crsbs" in namespace "gc-3406"
    Jun 22 11:15:26.891: INFO: Deleting pod "simpletest.rc-cwzw7" in namespace "gc-3406"
    Jun 22 11:15:26.912: INFO: Deleting pod "simpletest.rc-d4s52" in namespace "gc-3406"
    Jun 22 11:15:26.932: INFO: Deleting pod "simpletest.rc-d9dsg" in namespace "gc-3406"
    Jun 22 11:15:26.947: INFO: Deleting pod "simpletest.rc-dvhcj" in namespace "gc-3406"
    Jun 22 11:15:26.968: INFO: Deleting pod "simpletest.rc-dzqp2" in namespace "gc-3406"
    Jun 22 11:15:26.990: INFO: Deleting pod "simpletest.rc-f2qw8" in namespace "gc-3406"
    Jun 22 11:15:27.010: INFO: Deleting pod "simpletest.rc-f957f" in namespace "gc-3406"
    Jun 22 11:15:27.032: INFO: Deleting pod "simpletest.rc-fccqt" in namespace "gc-3406"
    Jun 22 11:15:27.053: INFO: Deleting pod "simpletest.rc-fj5s6" in namespace "gc-3406"
    Jun 22 11:15:27.075: INFO: Deleting pod "simpletest.rc-fvhxl" in namespace "gc-3406"
    Jun 22 11:15:27.096: INFO: Deleting pod "simpletest.rc-grbmt" in namespace "gc-3406"
    Jun 22 11:15:27.118: INFO: Deleting pod "simpletest.rc-h4g8z" in namespace "gc-3406"
    Jun 22 11:15:27.137: INFO: Deleting pod "simpletest.rc-h6cfj" in namespace "gc-3406"
    Jun 22 11:15:27.155: INFO: Deleting pod "simpletest.rc-h6wwh" in namespace "gc-3406"
    Jun 22 11:15:27.174: INFO: Deleting pod "simpletest.rc-hc8ml" in namespace "gc-3406"
    Jun 22 11:15:27.194: INFO: Deleting pod "simpletest.rc-hp9fm" in namespace "gc-3406"
    Jun 22 11:15:27.209: INFO: Deleting pod "simpletest.rc-j6ptq" in namespace "gc-3406"
    Jun 22 11:15:27.225: INFO: Deleting pod "simpletest.rc-jhlrq" in namespace "gc-3406"
    Jun 22 11:15:27.240: INFO: Deleting pod "simpletest.rc-l7qt5" in namespace "gc-3406"
    Jun 22 11:15:27.256: INFO: Deleting pod "simpletest.rc-l8jtg" in namespace "gc-3406"
    Jun 22 11:15:27.271: INFO: Deleting pod "simpletest.rc-lc2dc" in namespace "gc-3406"
    Jun 22 11:15:27.294: INFO: Deleting pod "simpletest.rc-lcwzm" in namespace "gc-3406"
    Jun 22 11:15:27.316: INFO: Deleting pod "simpletest.rc-lmxgt" in namespace "gc-3406"
    Jun 22 11:15:27.336: INFO: Deleting pod "simpletest.rc-lrkjt" in namespace "gc-3406"
    Jun 22 11:15:27.357: INFO: Deleting pod "simpletest.rc-lsfsj" in namespace "gc-3406"
    Jun 22 11:15:27.382: INFO: Deleting pod "simpletest.rc-lzjz4" in namespace "gc-3406"
    Jun 22 11:15:27.399: INFO: Deleting pod "simpletest.rc-mfmcj" in namespace "gc-3406"
    Jun 22 11:15:27.423: INFO: Deleting pod "simpletest.rc-mqcs5" in namespace "gc-3406"
    Jun 22 11:15:27.440: INFO: Deleting pod "simpletest.rc-n6gvw" in namespace "gc-3406"
    Jun 22 11:15:27.460: INFO: Deleting pod "simpletest.rc-n867l" in namespace "gc-3406"
    Jun 22 11:15:27.480: INFO: Deleting pod "simpletest.rc-nz28q" in namespace "gc-3406"
    Jun 22 11:15:27.505: INFO: Deleting pod "simpletest.rc-pdgvf" in namespace "gc-3406"
    Jun 22 11:15:27.525: INFO: Deleting pod "simpletest.rc-pf7cb" in namespace "gc-3406"
    Jun 22 11:15:27.549: INFO: Deleting pod "simpletest.rc-pj45l" in namespace "gc-3406"
    Jun 22 11:15:27.569: INFO: Deleting pod "simpletest.rc-ppx9f" in namespace "gc-3406"
    Jun 22 11:15:27.590: INFO: Deleting pod "simpletest.rc-pwprg" in namespace "gc-3406"
    Jun 22 11:15:27.613: INFO: Deleting pod "simpletest.rc-q5ndq" in namespace "gc-3406"
    Jun 22 11:15:27.640: INFO: Deleting pod "simpletest.rc-q7hcq" in namespace "gc-3406"
    Jun 22 11:15:27.658: INFO: Deleting pod "simpletest.rc-qrgdd" in namespace "gc-3406"
    Jun 22 11:15:27.680: INFO: Deleting pod "simpletest.rc-qscwm" in namespace "gc-3406"
    Jun 22 11:15:27.697: INFO: Deleting pod "simpletest.rc-qwpg4" in namespace "gc-3406"
    Jun 22 11:15:27.718: INFO: Deleting pod "simpletest.rc-r28gf" in namespace "gc-3406"
    Jun 22 11:15:27.735: INFO: Deleting pod "simpletest.rc-r2l5s" in namespace "gc-3406"
    Jun 22 11:15:27.752: INFO: Deleting pod "simpletest.rc-r2rb8" in namespace "gc-3406"
    Jun 22 11:15:27.768: INFO: Deleting pod "simpletest.rc-s2279" in namespace "gc-3406"
    Jun 22 11:15:27.794: INFO: Deleting pod "simpletest.rc-s22xk" in namespace "gc-3406"
    Jun 22 11:15:27.819: INFO: Deleting pod "simpletest.rc-s7mcr" in namespace "gc-3406"
    Jun 22 11:15:27.838: INFO: Deleting pod "simpletest.rc-s9fp8" in namespace "gc-3406"
    Jun 22 11:15:27.860: INFO: Deleting pod "simpletest.rc-sgptb" in namespace "gc-3406"
    Jun 22 11:15:27.886: INFO: Deleting pod "simpletest.rc-swlb7" in namespace "gc-3406"
    Jun 22 11:15:27.903: INFO: Deleting pod "simpletest.rc-t4xkf" in namespace "gc-3406"
    Jun 22 11:15:27.917: INFO: Deleting pod "simpletest.rc-t79rd" in namespace "gc-3406"
    Jun 22 11:15:27.941: INFO: Deleting pod "simpletest.rc-ttv22" in namespace "gc-3406"
    Jun 22 11:15:27.964: INFO: Deleting pod "simpletest.rc-vddhc" in namespace "gc-3406"
    Jun 22 11:15:27.982: INFO: Deleting pod "simpletest.rc-vg2df" in namespace "gc-3406"
    Jun 22 11:15:28.001: INFO: Deleting pod "simpletest.rc-vrpf4" in namespace "gc-3406"
    Jun 22 11:15:28.021: INFO: Deleting pod "simpletest.rc-wqm8w" in namespace "gc-3406"
    Jun 22 11:15:28.049: INFO: Deleting pod "simpletest.rc-wvcpw" in namespace "gc-3406"
    Jun 22 11:15:28.103: INFO: Deleting pod "simpletest.rc-wvskf" in namespace "gc-3406"
    Jun 22 11:15:28.149: INFO: Deleting pod "simpletest.rc-x5lj8" in namespace "gc-3406"
    Jun 22 11:15:28.201: INFO: Deleting pod "simpletest.rc-x96l6" in namespace "gc-3406"
    Jun 22 11:15:28.251: INFO: Deleting pod "simpletest.rc-x98hk" in namespace "gc-3406"
    Jun 22 11:15:28.301: INFO: Deleting pod "simpletest.rc-z7rvv" in namespace "gc-3406"
    Jun 22 11:15:28.352: INFO: Deleting pod "simpletest.rc-zh8hj" in namespace "gc-3406"
    Jun 22 11:15:28.405: INFO: Deleting pod "simpletest.rc-zn46s" in namespace "gc-3406"
    Jun 22 11:15:28.451: INFO: Deleting pod "simpletest.rc-zxjw6" in namespace "gc-3406"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:15:28.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3406" for this suite. 06/22/23 11:15:28.538
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:15:28.595
Jun 22 11:15:28.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename subpath 06/22/23 11:15:28.596
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:15:28.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:15:28.627
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/22/23 11:15:28.632
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-5j9q 06/22/23 11:15:28.647
STEP: Creating a pod to test atomic-volume-subpath 06/22/23 11:15:28.647
W0622 11:15:28.661503      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container-subpath-secret-5j9q" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-subpath-secret-5j9q" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-subpath-secret-5j9q" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-subpath-secret-5j9q" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:15:28.661: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-5j9q" in namespace "subpath-938" to be "Succeeded or Failed"
Jun 22 11:15:28.668: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Pending", Reason="", readiness=false. Elapsed: 6.679654ms
Jun 22 11:15:30.675: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013788827s
Jun 22 11:15:32.675: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013474679s
Jun 22 11:15:34.674: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01258915s
Jun 22 11:15:36.674: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012904531s
Jun 22 11:15:38.674: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013260065s
Jun 22 11:15:40.674: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Running", Reason="", readiness=true. Elapsed: 12.0128011s
Jun 22 11:15:42.674: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Running", Reason="", readiness=true. Elapsed: 14.01235768s
Jun 22 11:15:44.674: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Running", Reason="", readiness=true. Elapsed: 16.012436049s
Jun 22 11:15:46.675: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Running", Reason="", readiness=true. Elapsed: 18.013622477s
Jun 22 11:15:48.676: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Running", Reason="", readiness=true. Elapsed: 20.014778861s
Jun 22 11:15:50.674: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Running", Reason="", readiness=true. Elapsed: 22.013123697s
Jun 22 11:15:52.675: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Running", Reason="", readiness=false. Elapsed: 24.013764866s
Jun 22 11:15:54.674: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.012968908s
STEP: Saw pod success 06/22/23 11:15:54.674
Jun 22 11:15:54.674: INFO: Pod "pod-subpath-test-secret-5j9q" satisfied condition "Succeeded or Failed"
Jun 22 11:15:54.680: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-subpath-test-secret-5j9q container test-container-subpath-secret-5j9q: <nil>
STEP: delete the pod 06/22/23 11:15:54.692
Jun 22 11:15:54.708: INFO: Waiting for pod pod-subpath-test-secret-5j9q to disappear
Jun 22 11:15:54.713: INFO: Pod pod-subpath-test-secret-5j9q no longer exists
STEP: Deleting pod pod-subpath-test-secret-5j9q 06/22/23 11:15:54.713
Jun 22 11:15:54.713: INFO: Deleting pod "pod-subpath-test-secret-5j9q" in namespace "subpath-938"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jun 22 11:15:54.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-938" for this suite. 06/22/23 11:15:54.725
------------------------------
• [SLOW TEST] [26.141 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:15:28.595
    Jun 22 11:15:28.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename subpath 06/22/23 11:15:28.596
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:15:28.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:15:28.627
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/22/23 11:15:28.632
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-5j9q 06/22/23 11:15:28.647
    STEP: Creating a pod to test atomic-volume-subpath 06/22/23 11:15:28.647
    W0622 11:15:28.661503      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container-subpath-secret-5j9q" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-subpath-secret-5j9q" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-subpath-secret-5j9q" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-subpath-secret-5j9q" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:15:28.661: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-5j9q" in namespace "subpath-938" to be "Succeeded or Failed"
    Jun 22 11:15:28.668: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Pending", Reason="", readiness=false. Elapsed: 6.679654ms
    Jun 22 11:15:30.675: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013788827s
    Jun 22 11:15:32.675: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013474679s
    Jun 22 11:15:34.674: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01258915s
    Jun 22 11:15:36.674: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012904531s
    Jun 22 11:15:38.674: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013260065s
    Jun 22 11:15:40.674: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Running", Reason="", readiness=true. Elapsed: 12.0128011s
    Jun 22 11:15:42.674: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Running", Reason="", readiness=true. Elapsed: 14.01235768s
    Jun 22 11:15:44.674: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Running", Reason="", readiness=true. Elapsed: 16.012436049s
    Jun 22 11:15:46.675: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Running", Reason="", readiness=true. Elapsed: 18.013622477s
    Jun 22 11:15:48.676: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Running", Reason="", readiness=true. Elapsed: 20.014778861s
    Jun 22 11:15:50.674: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Running", Reason="", readiness=true. Elapsed: 22.013123697s
    Jun 22 11:15:52.675: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Running", Reason="", readiness=false. Elapsed: 24.013764866s
    Jun 22 11:15:54.674: INFO: Pod "pod-subpath-test-secret-5j9q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.012968908s
    STEP: Saw pod success 06/22/23 11:15:54.674
    Jun 22 11:15:54.674: INFO: Pod "pod-subpath-test-secret-5j9q" satisfied condition "Succeeded or Failed"
    Jun 22 11:15:54.680: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-subpath-test-secret-5j9q container test-container-subpath-secret-5j9q: <nil>
    STEP: delete the pod 06/22/23 11:15:54.692
    Jun 22 11:15:54.708: INFO: Waiting for pod pod-subpath-test-secret-5j9q to disappear
    Jun 22 11:15:54.713: INFO: Pod pod-subpath-test-secret-5j9q no longer exists
    STEP: Deleting pod pod-subpath-test-secret-5j9q 06/22/23 11:15:54.713
    Jun 22 11:15:54.713: INFO: Deleting pod "pod-subpath-test-secret-5j9q" in namespace "subpath-938"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:15:54.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-938" for this suite. 06/22/23 11:15:54.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:15:54.737
Jun 22 11:15:54.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename webhook 06/22/23 11:15:54.738
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:15:54.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:15:54.764
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/22/23 11:15:54.784
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:15:55.355
STEP: Deploying the webhook pod 06/22/23 11:15:55.372
W0622 11:15:55.391193      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 11:15:55.391
Jun 22 11:15:55.402: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/22/23 11:15:57.421
STEP: Verifying the service has paired with the endpoint 06/22/23 11:15:57.445
Jun 22 11:15:58.445: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 06/22/23 11:15:58.451
STEP: create a pod 06/22/23 11:15:58.474
W0622 11:15:58.487258      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "container1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "container1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "container1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "container1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:15:58.487: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1946" to be "running"
Jun 22 11:15:58.492: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.444842ms
Jun 22 11:16:00.499: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012352303s
Jun 22 11:16:00.499: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 06/22/23 11:16:00.499
Jun 22 11:16:00.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=webhook-1946 attach --namespace=webhook-1946 to-be-attached-pod -i -c=container1'
Jun 22 11:16:00.614: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:16:00.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1946" for this suite. 06/22/23 11:16:00.691
STEP: Destroying namespace "webhook-1946-markers" for this suite. 06/22/23 11:16:00.7
------------------------------
• [SLOW TEST] [5.972 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:15:54.737
    Jun 22 11:15:54.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename webhook 06/22/23 11:15:54.738
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:15:54.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:15:54.764
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/22/23 11:15:54.784
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:15:55.355
    STEP: Deploying the webhook pod 06/22/23 11:15:55.372
    W0622 11:15:55.391193      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 11:15:55.391
    Jun 22 11:15:55.402: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/22/23 11:15:57.421
    STEP: Verifying the service has paired with the endpoint 06/22/23 11:15:57.445
    Jun 22 11:15:58.445: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 06/22/23 11:15:58.451
    STEP: create a pod 06/22/23 11:15:58.474
    W0622 11:15:58.487258      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "container1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "container1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "container1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "container1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:15:58.487: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-1946" to be "running"
    Jun 22 11:15:58.492: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.444842ms
    Jun 22 11:16:00.499: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012352303s
    Jun 22 11:16:00.499: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 06/22/23 11:16:00.499
    Jun 22 11:16:00.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=webhook-1946 attach --namespace=webhook-1946 to-be-attached-pod -i -c=container1'
    Jun 22 11:16:00.614: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:16:00.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1946" for this suite. 06/22/23 11:16:00.691
    STEP: Destroying namespace "webhook-1946-markers" for this suite. 06/22/23 11:16:00.7
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:16:00.709
Jun 22 11:16:00.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename container-probe 06/22/23 11:16:00.711
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:16:00.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:16:00.74
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-e9bdb8f5-e6f0-4787-9be3-32b1530fff67 in namespace container-probe-9585 06/22/23 11:16:00.744
W0622 11:16:00.762363      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:16:00.762: INFO: Waiting up to 5m0s for pod "test-webserver-e9bdb8f5-e6f0-4787-9be3-32b1530fff67" in namespace "container-probe-9585" to be "not pending"
Jun 22 11:16:00.767: INFO: Pod "test-webserver-e9bdb8f5-e6f0-4787-9be3-32b1530fff67": Phase="Pending", Reason="", readiness=false. Elapsed: 5.269946ms
Jun 22 11:16:02.774: INFO: Pod "test-webserver-e9bdb8f5-e6f0-4787-9be3-32b1530fff67": Phase="Running", Reason="", readiness=true. Elapsed: 2.011708368s
Jun 22 11:16:02.774: INFO: Pod "test-webserver-e9bdb8f5-e6f0-4787-9be3-32b1530fff67" satisfied condition "not pending"
Jun 22 11:16:02.774: INFO: Started pod test-webserver-e9bdb8f5-e6f0-4787-9be3-32b1530fff67 in namespace container-probe-9585
STEP: checking the pod's current state and verifying that restartCount is present 06/22/23 11:16:02.774
Jun 22 11:16:02.778: INFO: Initial restart count of pod test-webserver-e9bdb8f5-e6f0-4787-9be3-32b1530fff67 is 0
STEP: deleting the pod 06/22/23 11:20:03.751
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun 22 11:20:03.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9585" for this suite. 06/22/23 11:20:03.8
------------------------------
• [SLOW TEST] [243.103 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:16:00.709
    Jun 22 11:16:00.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename container-probe 06/22/23 11:16:00.711
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:16:00.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:16:00.74
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-e9bdb8f5-e6f0-4787-9be3-32b1530fff67 in namespace container-probe-9585 06/22/23 11:16:00.744
    W0622 11:16:00.762363      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:16:00.762: INFO: Waiting up to 5m0s for pod "test-webserver-e9bdb8f5-e6f0-4787-9be3-32b1530fff67" in namespace "container-probe-9585" to be "not pending"
    Jun 22 11:16:00.767: INFO: Pod "test-webserver-e9bdb8f5-e6f0-4787-9be3-32b1530fff67": Phase="Pending", Reason="", readiness=false. Elapsed: 5.269946ms
    Jun 22 11:16:02.774: INFO: Pod "test-webserver-e9bdb8f5-e6f0-4787-9be3-32b1530fff67": Phase="Running", Reason="", readiness=true. Elapsed: 2.011708368s
    Jun 22 11:16:02.774: INFO: Pod "test-webserver-e9bdb8f5-e6f0-4787-9be3-32b1530fff67" satisfied condition "not pending"
    Jun 22 11:16:02.774: INFO: Started pod test-webserver-e9bdb8f5-e6f0-4787-9be3-32b1530fff67 in namespace container-probe-9585
    STEP: checking the pod's current state and verifying that restartCount is present 06/22/23 11:16:02.774
    Jun 22 11:16:02.778: INFO: Initial restart count of pod test-webserver-e9bdb8f5-e6f0-4787-9be3-32b1530fff67 is 0
    STEP: deleting the pod 06/22/23 11:20:03.751
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:20:03.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9585" for this suite. 06/22/23 11:20:03.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:20:03.818
Jun 22 11:20:03.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename limitrange 06/22/23 11:20:03.825
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:20:03.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:20:03.859
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-tkx76" in namespace "limitrange-1477" 06/22/23 11:20:03.864
STEP: Creating another limitRange in another namespace 06/22/23 11:20:03.875
Jun 22 11:20:03.913: INFO: Namespace "e2e-limitrange-tkx76-8561" created
Jun 22 11:20:03.913: INFO: Creating LimitRange "e2e-limitrange-tkx76" in namespace "e2e-limitrange-tkx76-8561"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-tkx76" 06/22/23 11:20:03.92
Jun 22 11:20:03.927: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-tkx76" in "limitrange-1477" namespace 06/22/23 11:20:03.928
Jun 22 11:20:03.947: INFO: LimitRange "e2e-limitrange-tkx76" has been patched
STEP: Delete LimitRange "e2e-limitrange-tkx76" by Collection with labelSelector: "e2e-limitrange-tkx76=patched" 06/22/23 11:20:03.947
STEP: Confirm that the limitRange "e2e-limitrange-tkx76" has been deleted 06/22/23 11:20:03.961
Jun 22 11:20:03.962: INFO: Requesting list of LimitRange to confirm quantity
Jun 22 11:20:03.968: INFO: Found 0 LimitRange with label "e2e-limitrange-tkx76=patched"
Jun 22 11:20:03.968: INFO: LimitRange "e2e-limitrange-tkx76" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-tkx76" 06/22/23 11:20:03.968
Jun 22 11:20:03.974: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jun 22 11:20:03.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-1477" for this suite. 06/22/23 11:20:03.987
STEP: Destroying namespace "e2e-limitrange-tkx76-8561" for this suite. 06/22/23 11:20:03.999
------------------------------
• [0.192 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:20:03.818
    Jun 22 11:20:03.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename limitrange 06/22/23 11:20:03.825
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:20:03.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:20:03.859
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-tkx76" in namespace "limitrange-1477" 06/22/23 11:20:03.864
    STEP: Creating another limitRange in another namespace 06/22/23 11:20:03.875
    Jun 22 11:20:03.913: INFO: Namespace "e2e-limitrange-tkx76-8561" created
    Jun 22 11:20:03.913: INFO: Creating LimitRange "e2e-limitrange-tkx76" in namespace "e2e-limitrange-tkx76-8561"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-tkx76" 06/22/23 11:20:03.92
    Jun 22 11:20:03.927: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-tkx76" in "limitrange-1477" namespace 06/22/23 11:20:03.928
    Jun 22 11:20:03.947: INFO: LimitRange "e2e-limitrange-tkx76" has been patched
    STEP: Delete LimitRange "e2e-limitrange-tkx76" by Collection with labelSelector: "e2e-limitrange-tkx76=patched" 06/22/23 11:20:03.947
    STEP: Confirm that the limitRange "e2e-limitrange-tkx76" has been deleted 06/22/23 11:20:03.961
    Jun 22 11:20:03.962: INFO: Requesting list of LimitRange to confirm quantity
    Jun 22 11:20:03.968: INFO: Found 0 LimitRange with label "e2e-limitrange-tkx76=patched"
    Jun 22 11:20:03.968: INFO: LimitRange "e2e-limitrange-tkx76" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-tkx76" 06/22/23 11:20:03.968
    Jun 22 11:20:03.974: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:20:03.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-1477" for this suite. 06/22/23 11:20:03.987
    STEP: Destroying namespace "e2e-limitrange-tkx76-8561" for this suite. 06/22/23 11:20:03.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:20:04.011
Jun 22 11:20:04.011: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename services 06/22/23 11:20:04.012
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:20:04.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:20:04.039
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-2254 06/22/23 11:20:04.044
STEP: creating service affinity-clusterip-transition in namespace services-2254 06/22/23 11:20:04.044
STEP: creating replication controller affinity-clusterip-transition in namespace services-2254 06/22/23 11:20:04.075
W0622 11:20:04.096749      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "affinity-clusterip-transition" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-clusterip-transition" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-clusterip-transition" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-clusterip-transition" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0622 11:20:04.097146      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2254, replica count: 3
I0622 11:20:07.148875      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 11:20:07.161: INFO: Creating new exec pod
W0622 11:20:07.175248      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:20:07.175: INFO: Waiting up to 5m0s for pod "execpod-affinitygcpsn" in namespace "services-2254" to be "running"
Jun 22 11:20:07.185: INFO: Pod "execpod-affinitygcpsn": Phase="Pending", Reason="", readiness=false. Elapsed: 9.561342ms
Jun 22 11:20:09.192: INFO: Pod "execpod-affinitygcpsn": Phase="Running", Reason="", readiness=true. Elapsed: 2.016810818s
Jun 22 11:20:09.192: INFO: Pod "execpod-affinitygcpsn" satisfied condition "running"
Jun 22 11:20:10.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-2254 exec execpod-affinitygcpsn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jun 22 11:20:10.472: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jun 22 11:20:10.472: INFO: stdout: ""
Jun 22 11:20:10.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-2254 exec execpod-affinitygcpsn -- /bin/sh -x -c nc -v -z -w 2 100.65.233.171 80'
Jun 22 11:20:10.672: INFO: stderr: "+ nc -v -z -w 2 100.65.233.171 80\nConnection to 100.65.233.171 80 port [tcp/http] succeeded!\n"
Jun 22 11:20:10.672: INFO: stdout: ""
Jun 22 11:20:10.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-2254 exec execpod-affinitygcpsn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.65.233.171:80/ ; done'
Jun 22 11:20:11.004: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n"
Jun 22 11:20:11.005: INFO: stdout: "\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-hnkqw\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-xrw64\naffinity-clusterip-transition-xrw64\naffinity-clusterip-transition-xrw64\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-xrw64\naffinity-clusterip-transition-xrw64\naffinity-clusterip-transition-hnkqw\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-xrw64\naffinity-clusterip-transition-hnkqw\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2"
Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-hnkqw
Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-xrw64
Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-xrw64
Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-xrw64
Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-xrw64
Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-xrw64
Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-hnkqw
Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-xrw64
Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-hnkqw
Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-2254 exec execpod-affinitygcpsn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.65.233.171:80/ ; done'
Jun 22 11:20:11.342: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n"
Jun 22 11:20:11.342: INFO: stdout: "\naffinity-clusterip-transition-xrw64\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2"
Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-xrw64
Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
Jun 22 11:20:11.342: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2254, will wait for the garbage collector to delete the pods 06/22/23 11:20:11.367
Jun 22 11:20:11.436: INFO: Deleting ReplicationController affinity-clusterip-transition took: 11.558693ms
Jun 22 11:20:11.537: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.970345ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun 22 11:20:14.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2254" for this suite. 06/22/23 11:20:14.079
------------------------------
• [SLOW TEST] [10.076 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:20:04.011
    Jun 22 11:20:04.011: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename services 06/22/23 11:20:04.012
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:20:04.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:20:04.039
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-2254 06/22/23 11:20:04.044
    STEP: creating service affinity-clusterip-transition in namespace services-2254 06/22/23 11:20:04.044
    STEP: creating replication controller affinity-clusterip-transition in namespace services-2254 06/22/23 11:20:04.075
    W0622 11:20:04.096749      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "affinity-clusterip-transition" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-clusterip-transition" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-clusterip-transition" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-clusterip-transition" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0622 11:20:04.097146      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-2254, replica count: 3
    I0622 11:20:07.148875      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 22 11:20:07.161: INFO: Creating new exec pod
    W0622 11:20:07.175248      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:20:07.175: INFO: Waiting up to 5m0s for pod "execpod-affinitygcpsn" in namespace "services-2254" to be "running"
    Jun 22 11:20:07.185: INFO: Pod "execpod-affinitygcpsn": Phase="Pending", Reason="", readiness=false. Elapsed: 9.561342ms
    Jun 22 11:20:09.192: INFO: Pod "execpod-affinitygcpsn": Phase="Running", Reason="", readiness=true. Elapsed: 2.016810818s
    Jun 22 11:20:09.192: INFO: Pod "execpod-affinitygcpsn" satisfied condition "running"
    Jun 22 11:20:10.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-2254 exec execpod-affinitygcpsn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jun 22 11:20:10.472: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jun 22 11:20:10.472: INFO: stdout: ""
    Jun 22 11:20:10.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-2254 exec execpod-affinitygcpsn -- /bin/sh -x -c nc -v -z -w 2 100.65.233.171 80'
    Jun 22 11:20:10.672: INFO: stderr: "+ nc -v -z -w 2 100.65.233.171 80\nConnection to 100.65.233.171 80 port [tcp/http] succeeded!\n"
    Jun 22 11:20:10.672: INFO: stdout: ""
    Jun 22 11:20:10.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-2254 exec execpod-affinitygcpsn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.65.233.171:80/ ; done'
    Jun 22 11:20:11.004: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n"
    Jun 22 11:20:11.005: INFO: stdout: "\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-hnkqw\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-xrw64\naffinity-clusterip-transition-xrw64\naffinity-clusterip-transition-xrw64\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-xrw64\naffinity-clusterip-transition-xrw64\naffinity-clusterip-transition-hnkqw\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-xrw64\naffinity-clusterip-transition-hnkqw\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2"
    Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-hnkqw
    Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-xrw64
    Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-xrw64
    Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-xrw64
    Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-xrw64
    Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-xrw64
    Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-hnkqw
    Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-xrw64
    Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-hnkqw
    Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.005: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-2254 exec execpod-affinitygcpsn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.65.233.171:80/ ; done'
    Jun 22 11:20:11.342: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.65.233.171:80/\n"
    Jun 22 11:20:11.342: INFO: stdout: "\naffinity-clusterip-transition-xrw64\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2\naffinity-clusterip-transition-jdnr2"
    Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-xrw64
    Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.342: INFO: Received response from host: affinity-clusterip-transition-jdnr2
    Jun 22 11:20:11.342: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2254, will wait for the garbage collector to delete the pods 06/22/23 11:20:11.367
    Jun 22 11:20:11.436: INFO: Deleting ReplicationController affinity-clusterip-transition took: 11.558693ms
    Jun 22 11:20:11.537: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.970345ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:20:14.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2254" for this suite. 06/22/23 11:20:14.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:20:14.094
Jun 22 11:20:14.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename svcaccounts 06/22/23 11:20:14.095
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:20:14.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:20:14.128
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
W0622 11:20:14.153264      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:20:14.153: INFO: Waiting up to 5m0s for pod "pod-service-account-451a3a6f-7af2-4aae-bc3e-26a9fdc2e65e" in namespace "svcaccounts-1863" to be "running"
Jun 22 11:20:14.159: INFO: Pod "pod-service-account-451a3a6f-7af2-4aae-bc3e-26a9fdc2e65e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.859435ms
Jun 22 11:20:16.166: INFO: Pod "pod-service-account-451a3a6f-7af2-4aae-bc3e-26a9fdc2e65e": Phase="Running", Reason="", readiness=true. Elapsed: 2.01299712s
Jun 22 11:20:16.166: INFO: Pod "pod-service-account-451a3a6f-7af2-4aae-bc3e-26a9fdc2e65e" satisfied condition "running"
STEP: reading a file in the container 06/22/23 11:20:16.166
Jun 22 11:20:16.167: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1863 pod-service-account-451a3a6f-7af2-4aae-bc3e-26a9fdc2e65e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 06/22/23 11:20:16.352
Jun 22 11:20:16.352: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1863 pod-service-account-451a3a6f-7af2-4aae-bc3e-26a9fdc2e65e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 06/22/23 11:20:16.523
Jun 22 11:20:16.524: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1863 pod-service-account-451a3a6f-7af2-4aae-bc3e-26a9fdc2e65e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jun 22 11:20:16.706: INFO: Got root ca configmap in namespace "svcaccounts-1863"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun 22 11:20:16.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1863" for this suite. 06/22/23 11:20:16.721
------------------------------
• [2.637 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:20:14.094
    Jun 22 11:20:14.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename svcaccounts 06/22/23 11:20:14.095
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:20:14.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:20:14.128
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    W0622 11:20:14.153264      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:20:14.153: INFO: Waiting up to 5m0s for pod "pod-service-account-451a3a6f-7af2-4aae-bc3e-26a9fdc2e65e" in namespace "svcaccounts-1863" to be "running"
    Jun 22 11:20:14.159: INFO: Pod "pod-service-account-451a3a6f-7af2-4aae-bc3e-26a9fdc2e65e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.859435ms
    Jun 22 11:20:16.166: INFO: Pod "pod-service-account-451a3a6f-7af2-4aae-bc3e-26a9fdc2e65e": Phase="Running", Reason="", readiness=true. Elapsed: 2.01299712s
    Jun 22 11:20:16.166: INFO: Pod "pod-service-account-451a3a6f-7af2-4aae-bc3e-26a9fdc2e65e" satisfied condition "running"
    STEP: reading a file in the container 06/22/23 11:20:16.166
    Jun 22 11:20:16.167: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1863 pod-service-account-451a3a6f-7af2-4aae-bc3e-26a9fdc2e65e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 06/22/23 11:20:16.352
    Jun 22 11:20:16.352: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1863 pod-service-account-451a3a6f-7af2-4aae-bc3e-26a9fdc2e65e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 06/22/23 11:20:16.523
    Jun 22 11:20:16.524: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1863 pod-service-account-451a3a6f-7af2-4aae-bc3e-26a9fdc2e65e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jun 22 11:20:16.706: INFO: Got root ca configmap in namespace "svcaccounts-1863"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:20:16.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1863" for this suite. 06/22/23 11:20:16.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:20:16.737
Jun 22 11:20:16.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename downward-api 06/22/23 11:20:16.738
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:20:16.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:20:16.765
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 06/22/23 11:20:16.769
W0622 11:20:16.786177      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:20:16.786: INFO: Waiting up to 5m0s for pod "annotationupdate57e59d1f-7ee3-4a34-b1f4-87cfbfde40a1" in namespace "downward-api-7194" to be "running and ready"
Jun 22 11:20:16.792: INFO: Pod "annotationupdate57e59d1f-7ee3-4a34-b1f4-87cfbfde40a1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.506619ms
Jun 22 11:20:16.792: INFO: The phase of Pod annotationupdate57e59d1f-7ee3-4a34-b1f4-87cfbfde40a1 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 11:20:18.799: INFO: Pod "annotationupdate57e59d1f-7ee3-4a34-b1f4-87cfbfde40a1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013293177s
Jun 22 11:20:18.799: INFO: The phase of Pod annotationupdate57e59d1f-7ee3-4a34-b1f4-87cfbfde40a1 is Running (Ready = true)
Jun 22 11:20:18.799: INFO: Pod "annotationupdate57e59d1f-7ee3-4a34-b1f4-87cfbfde40a1" satisfied condition "running and ready"
Jun 22 11:20:19.368: INFO: Successfully updated pod "annotationupdate57e59d1f-7ee3-4a34-b1f4-87cfbfde40a1"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun 22 11:20:23.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7194" for this suite. 06/22/23 11:20:23.418
------------------------------
• [SLOW TEST] [6.692 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:20:16.737
    Jun 22 11:20:16.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename downward-api 06/22/23 11:20:16.738
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:20:16.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:20:16.765
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 06/22/23 11:20:16.769
    W0622 11:20:16.786177      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:20:16.786: INFO: Waiting up to 5m0s for pod "annotationupdate57e59d1f-7ee3-4a34-b1f4-87cfbfde40a1" in namespace "downward-api-7194" to be "running and ready"
    Jun 22 11:20:16.792: INFO: Pod "annotationupdate57e59d1f-7ee3-4a34-b1f4-87cfbfde40a1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.506619ms
    Jun 22 11:20:16.792: INFO: The phase of Pod annotationupdate57e59d1f-7ee3-4a34-b1f4-87cfbfde40a1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 11:20:18.799: INFO: Pod "annotationupdate57e59d1f-7ee3-4a34-b1f4-87cfbfde40a1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013293177s
    Jun 22 11:20:18.799: INFO: The phase of Pod annotationupdate57e59d1f-7ee3-4a34-b1f4-87cfbfde40a1 is Running (Ready = true)
    Jun 22 11:20:18.799: INFO: Pod "annotationupdate57e59d1f-7ee3-4a34-b1f4-87cfbfde40a1" satisfied condition "running and ready"
    Jun 22 11:20:19.368: INFO: Successfully updated pod "annotationupdate57e59d1f-7ee3-4a34-b1f4-87cfbfde40a1"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:20:23.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7194" for this suite. 06/22/23 11:20:23.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:20:23.43
Jun 22 11:20:23.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename dns 06/22/23 11:20:23.431
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:20:23.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:20:23.458
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 06/22/23 11:20:23.462
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 230.202.71.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.71.202.230_udp@PTR;check="$$(dig +tcp +noall +answer +search 230.202.71.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.71.202.230_tcp@PTR;sleep 1; done
 06/22/23 11:20:23.497
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 230.202.71.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.71.202.230_udp@PTR;check="$$(dig +tcp +noall +answer +search 230.202.71.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.71.202.230_tcp@PTR;sleep 1; done
 06/22/23 11:20:23.498
STEP: creating a pod to probe DNS 06/22/23 11:20:23.498
STEP: submitting the pod to kubernetes 06/22/23 11:20:23.498
W0622 11:20:23.518314      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:20:23.518: INFO: Waiting up to 15m0s for pod "dns-test-19119ce7-9268-4647-bb68-546ac383f14a" in namespace "dns-4479" to be "running"
Jun 22 11:20:23.527: INFO: Pod "dns-test-19119ce7-9268-4647-bb68-546ac383f14a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.111363ms
Jun 22 11:20:25.535: INFO: Pod "dns-test-19119ce7-9268-4647-bb68-546ac383f14a": Phase="Running", Reason="", readiness=true. Elapsed: 2.016808308s
Jun 22 11:20:25.535: INFO: Pod "dns-test-19119ce7-9268-4647-bb68-546ac383f14a" satisfied condition "running"
STEP: retrieving the pod 06/22/23 11:20:25.535
STEP: looking for the results for each expected name from probers 06/22/23 11:20:25.542
Jun 22 11:20:25.552: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:25.560: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:25.567: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:25.574: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:25.606: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:25.612: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:25.619: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:25.625: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:25.650: INFO: Lookups using dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

Jun 22 11:20:30.660: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:30.678: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:30.685: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:30.691: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:30.723: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:30.730: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:30.736: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:30.742: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:30.765: INFO: Lookups using dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

Jun 22 11:20:35.659: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:35.666: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:35.672: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:35.678: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:35.710: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:35.715: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:35.721: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:35.728: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:35.753: INFO: Lookups using dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

Jun 22 11:20:40.660: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:40.667: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:40.674: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:40.680: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:40.712: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:40.719: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:40.725: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:40.732: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:40.757: INFO: Lookups using dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

Jun 22 11:20:45.662: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:45.670: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:45.676: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:45.683: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:45.717: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:45.724: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:45.731: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:45.738: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:45.764: INFO: Lookups using dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

Jun 22 11:20:50.661: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:50.667: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:50.674: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:50.680: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:50.712: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:50.718: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:50.726: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:50.732: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
Jun 22 11:20:50.758: INFO: Lookups using dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

Jun 22 11:20:55.763: INFO: DNS probes using dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a succeeded

STEP: deleting the pod 06/22/23 11:20:55.763
STEP: deleting the test service 06/22/23 11:20:55.792
STEP: deleting the test headless service 06/22/23 11:20:55.827
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun 22 11:20:55.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4479" for this suite. 06/22/23 11:20:55.858
------------------------------
• [SLOW TEST] [32.441 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:20:23.43
    Jun 22 11:20:23.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename dns 06/22/23 11:20:23.431
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:20:23.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:20:23.458
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 06/22/23 11:20:23.462
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 230.202.71.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.71.202.230_udp@PTR;check="$$(dig +tcp +noall +answer +search 230.202.71.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.71.202.230_tcp@PTR;sleep 1; done
     06/22/23 11:20:23.497
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4479.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4479.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4479.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4479.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 230.202.71.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.71.202.230_udp@PTR;check="$$(dig +tcp +noall +answer +search 230.202.71.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.71.202.230_tcp@PTR;sleep 1; done
     06/22/23 11:20:23.498
    STEP: creating a pod to probe DNS 06/22/23 11:20:23.498
    STEP: submitting the pod to kubernetes 06/22/23 11:20:23.498
    W0622 11:20:23.518314      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:20:23.518: INFO: Waiting up to 15m0s for pod "dns-test-19119ce7-9268-4647-bb68-546ac383f14a" in namespace "dns-4479" to be "running"
    Jun 22 11:20:23.527: INFO: Pod "dns-test-19119ce7-9268-4647-bb68-546ac383f14a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.111363ms
    Jun 22 11:20:25.535: INFO: Pod "dns-test-19119ce7-9268-4647-bb68-546ac383f14a": Phase="Running", Reason="", readiness=true. Elapsed: 2.016808308s
    Jun 22 11:20:25.535: INFO: Pod "dns-test-19119ce7-9268-4647-bb68-546ac383f14a" satisfied condition "running"
    STEP: retrieving the pod 06/22/23 11:20:25.535
    STEP: looking for the results for each expected name from probers 06/22/23 11:20:25.542
    Jun 22 11:20:25.552: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:25.560: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:25.567: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:25.574: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:25.606: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:25.612: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:25.619: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:25.625: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:25.650: INFO: Lookups using dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

    Jun 22 11:20:30.660: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:30.678: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:30.685: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:30.691: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:30.723: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:30.730: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:30.736: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:30.742: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:30.765: INFO: Lookups using dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

    Jun 22 11:20:35.659: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:35.666: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:35.672: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:35.678: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:35.710: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:35.715: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:35.721: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:35.728: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:35.753: INFO: Lookups using dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

    Jun 22 11:20:40.660: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:40.667: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:40.674: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:40.680: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:40.712: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:40.719: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:40.725: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:40.732: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:40.757: INFO: Lookups using dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

    Jun 22 11:20:45.662: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:45.670: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:45.676: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:45.683: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:45.717: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:45.724: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:45.731: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:45.738: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:45.764: INFO: Lookups using dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

    Jun 22 11:20:50.661: INFO: Unable to read wheezy_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:50.667: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:50.674: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:50.680: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:50.712: INFO: Unable to read jessie_udp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:50.718: INFO: Unable to read jessie_tcp@dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:50.726: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:50.732: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local from pod dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a: the server could not find the requested resource (get pods dns-test-19119ce7-9268-4647-bb68-546ac383f14a)
    Jun 22 11:20:50.758: INFO: Lookups using dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a failed for: [wheezy_udp@dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@dns-test-service.dns-4479.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_udp@dns-test-service.dns-4479.svc.cluster.local jessie_tcp@dns-test-service.dns-4479.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4479.svc.cluster.local]

    Jun 22 11:20:55.763: INFO: DNS probes using dns-4479/dns-test-19119ce7-9268-4647-bb68-546ac383f14a succeeded

    STEP: deleting the pod 06/22/23 11:20:55.763
    STEP: deleting the test service 06/22/23 11:20:55.792
    STEP: deleting the test headless service 06/22/23 11:20:55.827
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:20:55.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4479" for this suite. 06/22/23 11:20:55.858
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:20:55.876
Jun 22 11:20:55.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename sched-pred 06/22/23 11:20:55.877
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:20:55.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:20:55.905
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jun 22 11:20:55.910: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 11:20:55.933: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 11:20:55.939: INFO: 
Logging pods the apiserver thinks is on node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k before test
Jun 22 11:20:55.960: INFO: corgi-test-8c78878b-6r5tv from corgi-test-privileged started at 2023-06-22 06:12:10 +0000 UTC (1 container statuses recorded)
Jun 22 11:20:55.960: INFO: 	Container nginx ready: true, restart count 0
Jun 22 11:20:55.960: INFO: antrea-agent-dxs2v from kube-system started at 2023-06-22 05:45:55 +0000 UTC (2 container statuses recorded)
Jun 22 11:20:55.960: INFO: 	Container antrea-agent ready: true, restart count 0
Jun 22 11:20:55.960: INFO: 	Container antrea-ovs ready: true, restart count 0
Jun 22 11:20:55.960: INFO: kube-proxy-czx75 from kube-system started at 2023-06-22 05:45:55 +0000 UTC (1 container statuses recorded)
Jun 22 11:20:55.960: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 11:20:55.960: INFO: sonobuoy-e2e-job-38d4831e07f74ae5 from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
Jun 22 11:20:55.960: INFO: 	Container e2e ready: true, restart count 0
Jun 22 11:20:55.960: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 11:20:55.960: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-5d74n from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
Jun 22 11:20:55.960: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 11:20:55.960: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 11:20:55.960: INFO: register-placeholder-99v8d from vmware-system-antrea started at 2023-06-22 11:19:24 +0000 UTC (1 container statuses recorded)
Jun 22 11:20:55.960: INFO: 	Container register ready: false, restart count 0
Jun 22 11:20:55.960: INFO: vsphere-csi-node-mp5v2 from vmware-system-csi started at 2023-06-22 05:45:55 +0000 UTC (3 container statuses recorded)
Jun 22 11:20:55.960: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 22 11:20:55.960: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 22 11:20:55.960: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jun 22 11:20:55.960: INFO: 
Logging pods the apiserver thinks is on node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw before test
Jun 22 11:20:55.982: INFO: antrea-agent-n9smp from kube-system started at 2023-06-22 05:45:26 +0000 UTC (2 container statuses recorded)
Jun 22 11:20:55.982: INFO: 	Container antrea-agent ready: true, restart count 0
Jun 22 11:20:55.982: INFO: 	Container antrea-ovs ready: true, restart count 0
Jun 22 11:20:55.982: INFO: kube-proxy-rlz8k from kube-system started at 2023-06-22 05:45:26 +0000 UTC (1 container statuses recorded)
Jun 22 11:20:55.982: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 11:20:55.982: INFO: sonobuoy from sonobuoy started at 2023-06-22 10:19:41 +0000 UTC (1 container statuses recorded)
Jun 22 11:20:55.982: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 11:20:55.982: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-f4lqb from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
Jun 22 11:20:55.982: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 11:20:55.982: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 11:20:55.982: INFO: vsphere-csi-node-v6pm5 from vmware-system-csi started at 2023-06-22 05:45:26 +0000 UTC (3 container statuses recorded)
Jun 22 11:20:55.982: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 22 11:20:55.982: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 22 11:20:55.982: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jun 22 11:20:55.982: INFO: 
Logging pods the apiserver thinks is on node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 before test
Jun 22 11:20:56.002: INFO: antrea-agent-94vtc from kube-system started at 2023-06-22 05:45:16 +0000 UTC (2 container statuses recorded)
Jun 22 11:20:56.002: INFO: 	Container antrea-agent ready: true, restart count 1
Jun 22 11:20:56.002: INFO: 	Container antrea-ovs ready: true, restart count 0
Jun 22 11:20:56.002: INFO: kube-proxy-8rvhg from kube-system started at 2023-06-22 05:45:16 +0000 UTC (1 container statuses recorded)
Jun 22 11:20:56.002: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 11:20:56.002: INFO: metrics-server-64f66d5b9-8j569 from kube-system started at 2023-06-22 05:45:49 +0000 UTC (1 container statuses recorded)
Jun 22 11:20:56.002: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 11:20:56.002: INFO: secretgen-controller-7bdd5787fd-h9nfx from secretgen-controller started at 2023-06-22 05:45:49 +0000 UTC (1 container statuses recorded)
Jun 22 11:20:56.002: INFO: 	Container secretgen-controller ready: true, restart count 0
Jun 22 11:20:56.002: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-7x88w from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
Jun 22 11:20:56.002: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 11:20:56.002: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 11:20:56.002: INFO: vsphere-csi-node-wsgg4 from vmware-system-csi started at 2023-06-22 05:45:17 +0000 UTC (3 container statuses recorded)
Jun 22 11:20:56.002: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 22 11:20:56.002: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 22 11:20:56.002: INFO: 	Container vsphere-csi-node ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 06/22/23 11:20:56.002
Jun 22 11:20:56.018: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-367" to be "running"
Jun 22 11:20:56.026: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.146202ms
Jun 22 11:20:58.032: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.013165894s
Jun 22 11:20:58.032: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 06/22/23 11:20:58.036
STEP: Trying to apply a random label on the found node. 06/22/23 11:20:58.059
STEP: verifying the node has the label kubernetes.io/e2e-f9b177a6-5ee0-43d3-bb64-f3c87f61bf5a 95 06/22/23 11:20:58.076
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 06/22/23 11:20:58.082
W0622 11:20:58.090945      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": hostPort (container "agnhost" uses hostPort 54322), allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:20:58.091: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-367" to be "not pending"
Jun 22 11:20:58.098: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.489782ms
Jun 22 11:21:00.105: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.014033772s
Jun 22 11:21:00.105: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.92.226.162 on the node which pod4 resides and expect not scheduled 06/22/23 11:21:00.105
W0622 11:21:00.116296      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": hostPort (container "agnhost" uses hostPort 54322), allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:21:00.116: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-367" to be "not pending"
Jun 22 11:21:00.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.093087ms
Jun 22 11:21:02.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013160172s
Jun 22 11:21:04.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011643038s
Jun 22 11:21:06.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012624402s
Jun 22 11:21:08.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01096404s
Jun 22 11:21:10.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011814134s
Jun 22 11:21:12.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011047909s
Jun 22 11:21:14.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012026875s
Jun 22 11:21:16.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013403331s
Jun 22 11:21:18.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01277682s
Jun 22 11:21:20.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.015422635s
Jun 22 11:21:22.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.012650751s
Jun 22 11:21:24.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.024889048s
Jun 22 11:21:26.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.014353292s
Jun 22 11:21:28.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.013110374s
Jun 22 11:21:30.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.015122195s
Jun 22 11:21:32.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.013113743s
Jun 22 11:21:34.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.014908734s
Jun 22 11:21:36.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.013938569s
Jun 22 11:21:38.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.012194433s
Jun 22 11:21:40.135: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.01939354s
Jun 22 11:21:42.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.011746944s
Jun 22 11:21:44.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.01119415s
Jun 22 11:21:46.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.013856535s
Jun 22 11:21:48.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.012777277s
Jun 22 11:21:50.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.01263568s
Jun 22 11:21:52.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.013813673s
Jun 22 11:21:54.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.012007977s
Jun 22 11:21:56.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.012428527s
Jun 22 11:21:58.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.01232225s
Jun 22 11:22:00.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.01189894s
Jun 22 11:22:02.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011545793s
Jun 22 11:22:04.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.01223486s
Jun 22 11:22:06.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011621626s
Jun 22 11:22:08.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.011682612s
Jun 22 11:22:10.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.011922795s
Jun 22 11:22:12.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012185422s
Jun 22 11:22:14.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.011953561s
Jun 22 11:22:16.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.012956529s
Jun 22 11:22:18.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.014047795s
Jun 22 11:22:20.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.013300598s
Jun 22 11:22:22.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.013599645s
Jun 22 11:22:24.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.012171348s
Jun 22 11:22:26.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.011192695s
Jun 22 11:22:28.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.012294034s
Jun 22 11:22:30.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.011944825s
Jun 22 11:22:32.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011467599s
Jun 22 11:22:34.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.013415896s
Jun 22 11:22:36.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.01092274s
Jun 22 11:22:38.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.013470015s
Jun 22 11:22:40.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.012323228s
Jun 22 11:22:42.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011915146s
Jun 22 11:22:44.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010955657s
Jun 22 11:22:46.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.01168421s
Jun 22 11:22:48.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.012372739s
Jun 22 11:22:50.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.014836623s
Jun 22 11:22:52.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.012170736s
Jun 22 11:22:54.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.012205109s
Jun 22 11:22:56.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.012263066s
Jun 22 11:22:58.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.012198649s
Jun 22 11:23:00.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.012381639s
Jun 22 11:23:02.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.011906198s
Jun 22 11:23:04.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.011922671s
Jun 22 11:23:06.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.01229197s
Jun 22 11:23:08.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.012203007s
Jun 22 11:23:10.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.012024414s
Jun 22 11:23:12.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.012923267s
Jun 22 11:23:14.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.012073957s
Jun 22 11:23:16.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.011257208s
Jun 22 11:23:18.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.01142897s
Jun 22 11:23:20.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.012638508s
Jun 22 11:23:22.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.012776653s
Jun 22 11:23:24.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.012526187s
Jun 22 11:23:26.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.011623962s
Jun 22 11:23:28.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.012111551s
Jun 22 11:23:30.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.011313501s
Jun 22 11:23:32.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.013074448s
Jun 22 11:23:34.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.011838045s
Jun 22 11:23:36.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.013107712s
Jun 22 11:23:38.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.012651468s
Jun 22 11:23:40.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.011873889s
Jun 22 11:23:42.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.011186624s
Jun 22 11:23:44.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.012227821s
Jun 22 11:23:46.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.011493778s
Jun 22 11:23:48.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.012243159s
Jun 22 11:23:50.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.012636664s
Jun 22 11:23:52.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.013771238s
Jun 22 11:23:54.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.011068967s
Jun 22 11:23:56.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.012790065s
Jun 22 11:23:58.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.01239873s
Jun 22 11:24:00.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.012010875s
Jun 22 11:24:02.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.01309454s
Jun 22 11:24:04.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.011195257s
Jun 22 11:24:06.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.012992361s
Jun 22 11:24:08.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.013501255s
Jun 22 11:24:10.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.011731009s
Jun 22 11:24:12.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.011926517s
Jun 22 11:24:14.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.012026549s
Jun 22 11:24:16.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.013139352s
Jun 22 11:24:18.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.013728779s
Jun 22 11:24:20.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.013077094s
Jun 22 11:24:22.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.013703772s
Jun 22 11:24:24.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.011815238s
Jun 22 11:24:26.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.013437261s
Jun 22 11:24:28.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.01114904s
Jun 22 11:24:30.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.011889981s
Jun 22 11:24:32.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.012512563s
Jun 22 11:24:34.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.012588535s
Jun 22 11:24:36.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.011226044s
Jun 22 11:24:38.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.012147725s
Jun 22 11:24:40.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.012618704s
Jun 22 11:24:42.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.011773747s
Jun 22 11:24:44.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.012210646s
Jun 22 11:24:46.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.01297291s
Jun 22 11:24:48.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.015135324s
Jun 22 11:24:50.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.012510783s
Jun 22 11:24:52.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.013468649s
Jun 22 11:24:54.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.012049718s
Jun 22 11:24:56.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.015146741s
Jun 22 11:24:58.126: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.010275203s
Jun 22 11:25:00.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.011577036s
Jun 22 11:25:02.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.012265667s
Jun 22 11:25:04.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.011899552s
Jun 22 11:25:06.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.012763743s
Jun 22 11:25:08.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.012793799s
Jun 22 11:25:10.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.011648743s
Jun 22 11:25:12.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.011780039s
Jun 22 11:25:14.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.011845642s
Jun 22 11:25:16.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.011733398s
Jun 22 11:25:18.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.012552528s
Jun 22 11:25:20.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.011669893s
Jun 22 11:25:22.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.012527954s
Jun 22 11:25:24.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.01226272s
Jun 22 11:25:26.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.014799901s
Jun 22 11:25:28.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.011160177s
Jun 22 11:25:30.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.013444088s
Jun 22 11:25:32.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.013565828s
Jun 22 11:25:34.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.012848959s
Jun 22 11:25:36.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.010981522s
Jun 22 11:25:38.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.012404899s
Jun 22 11:25:40.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.011467188s
Jun 22 11:25:42.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.013488077s
Jun 22 11:25:44.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.011846775s
Jun 22 11:25:46.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.012782873s
Jun 22 11:25:48.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.011333626s
Jun 22 11:25:50.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.012146977s
Jun 22 11:25:52.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.014350834s
Jun 22 11:25:54.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.011448614s
Jun 22 11:25:56.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.013538324s
Jun 22 11:25:58.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.01081645s
Jun 22 11:26:00.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.01227727s
Jun 22 11:26:00.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.022820281s
STEP: removing the label kubernetes.io/e2e-f9b177a6-5ee0-43d3-bb64-f3c87f61bf5a off the node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw 06/22/23 11:26:00.139
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f9b177a6-5ee0-43d3-bb64-f3c87f61bf5a 06/22/23 11:26:00.164
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:26:00.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-367" for this suite. 06/22/23 11:26:00.182
------------------------------
• [SLOW TEST] [304.316 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:20:55.876
    Jun 22 11:20:55.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename sched-pred 06/22/23 11:20:55.877
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:20:55.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:20:55.905
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jun 22 11:20:55.910: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun 22 11:20:55.933: INFO: Waiting for terminating namespaces to be deleted...
    Jun 22 11:20:55.939: INFO: 
    Logging pods the apiserver thinks is on node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k before test
    Jun 22 11:20:55.960: INFO: corgi-test-8c78878b-6r5tv from corgi-test-privileged started at 2023-06-22 06:12:10 +0000 UTC (1 container statuses recorded)
    Jun 22 11:20:55.960: INFO: 	Container nginx ready: true, restart count 0
    Jun 22 11:20:55.960: INFO: antrea-agent-dxs2v from kube-system started at 2023-06-22 05:45:55 +0000 UTC (2 container statuses recorded)
    Jun 22 11:20:55.960: INFO: 	Container antrea-agent ready: true, restart count 0
    Jun 22 11:20:55.960: INFO: 	Container antrea-ovs ready: true, restart count 0
    Jun 22 11:20:55.960: INFO: kube-proxy-czx75 from kube-system started at 2023-06-22 05:45:55 +0000 UTC (1 container statuses recorded)
    Jun 22 11:20:55.960: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 22 11:20:55.960: INFO: sonobuoy-e2e-job-38d4831e07f74ae5 from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
    Jun 22 11:20:55.960: INFO: 	Container e2e ready: true, restart count 0
    Jun 22 11:20:55.960: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 22 11:20:55.960: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-5d74n from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
    Jun 22 11:20:55.960: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 22 11:20:55.960: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 22 11:20:55.960: INFO: register-placeholder-99v8d from vmware-system-antrea started at 2023-06-22 11:19:24 +0000 UTC (1 container statuses recorded)
    Jun 22 11:20:55.960: INFO: 	Container register ready: false, restart count 0
    Jun 22 11:20:55.960: INFO: vsphere-csi-node-mp5v2 from vmware-system-csi started at 2023-06-22 05:45:55 +0000 UTC (3 container statuses recorded)
    Jun 22 11:20:55.960: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 22 11:20:55.960: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 22 11:20:55.960: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Jun 22 11:20:55.960: INFO: 
    Logging pods the apiserver thinks is on node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw before test
    Jun 22 11:20:55.982: INFO: antrea-agent-n9smp from kube-system started at 2023-06-22 05:45:26 +0000 UTC (2 container statuses recorded)
    Jun 22 11:20:55.982: INFO: 	Container antrea-agent ready: true, restart count 0
    Jun 22 11:20:55.982: INFO: 	Container antrea-ovs ready: true, restart count 0
    Jun 22 11:20:55.982: INFO: kube-proxy-rlz8k from kube-system started at 2023-06-22 05:45:26 +0000 UTC (1 container statuses recorded)
    Jun 22 11:20:55.982: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 22 11:20:55.982: INFO: sonobuoy from sonobuoy started at 2023-06-22 10:19:41 +0000 UTC (1 container statuses recorded)
    Jun 22 11:20:55.982: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun 22 11:20:55.982: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-f4lqb from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
    Jun 22 11:20:55.982: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 22 11:20:55.982: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 22 11:20:55.982: INFO: vsphere-csi-node-v6pm5 from vmware-system-csi started at 2023-06-22 05:45:26 +0000 UTC (3 container statuses recorded)
    Jun 22 11:20:55.982: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 22 11:20:55.982: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 22 11:20:55.982: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Jun 22 11:20:55.982: INFO: 
    Logging pods the apiserver thinks is on node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 before test
    Jun 22 11:20:56.002: INFO: antrea-agent-94vtc from kube-system started at 2023-06-22 05:45:16 +0000 UTC (2 container statuses recorded)
    Jun 22 11:20:56.002: INFO: 	Container antrea-agent ready: true, restart count 1
    Jun 22 11:20:56.002: INFO: 	Container antrea-ovs ready: true, restart count 0
    Jun 22 11:20:56.002: INFO: kube-proxy-8rvhg from kube-system started at 2023-06-22 05:45:16 +0000 UTC (1 container statuses recorded)
    Jun 22 11:20:56.002: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 22 11:20:56.002: INFO: metrics-server-64f66d5b9-8j569 from kube-system started at 2023-06-22 05:45:49 +0000 UTC (1 container statuses recorded)
    Jun 22 11:20:56.002: INFO: 	Container metrics-server ready: true, restart count 0
    Jun 22 11:20:56.002: INFO: secretgen-controller-7bdd5787fd-h9nfx from secretgen-controller started at 2023-06-22 05:45:49 +0000 UTC (1 container statuses recorded)
    Jun 22 11:20:56.002: INFO: 	Container secretgen-controller ready: true, restart count 0
    Jun 22 11:20:56.002: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-7x88w from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
    Jun 22 11:20:56.002: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 22 11:20:56.002: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 22 11:20:56.002: INFO: vsphere-csi-node-wsgg4 from vmware-system-csi started at 2023-06-22 05:45:17 +0000 UTC (3 container statuses recorded)
    Jun 22 11:20:56.002: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 22 11:20:56.002: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 22 11:20:56.002: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 06/22/23 11:20:56.002
    Jun 22 11:20:56.018: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-367" to be "running"
    Jun 22 11:20:56.026: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.146202ms
    Jun 22 11:20:58.032: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.013165894s
    Jun 22 11:20:58.032: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 06/22/23 11:20:58.036
    STEP: Trying to apply a random label on the found node. 06/22/23 11:20:58.059
    STEP: verifying the node has the label kubernetes.io/e2e-f9b177a6-5ee0-43d3-bb64-f3c87f61bf5a 95 06/22/23 11:20:58.076
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 06/22/23 11:20:58.082
    W0622 11:20:58.090945      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": hostPort (container "agnhost" uses hostPort 54322), allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:20:58.091: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-367" to be "not pending"
    Jun 22 11:20:58.098: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.489782ms
    Jun 22 11:21:00.105: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.014033772s
    Jun 22 11:21:00.105: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.92.226.162 on the node which pod4 resides and expect not scheduled 06/22/23 11:21:00.105
    W0622 11:21:00.116296      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": hostPort (container "agnhost" uses hostPort 54322), allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:21:00.116: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-367" to be "not pending"
    Jun 22 11:21:00.121: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.093087ms
    Jun 22 11:21:02.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013160172s
    Jun 22 11:21:04.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011643038s
    Jun 22 11:21:06.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012624402s
    Jun 22 11:21:08.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01096404s
    Jun 22 11:21:10.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011814134s
    Jun 22 11:21:12.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011047909s
    Jun 22 11:21:14.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012026875s
    Jun 22 11:21:16.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013403331s
    Jun 22 11:21:18.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.01277682s
    Jun 22 11:21:20.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.015422635s
    Jun 22 11:21:22.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.012650751s
    Jun 22 11:21:24.141: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.024889048s
    Jun 22 11:21:26.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.014353292s
    Jun 22 11:21:28.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.013110374s
    Jun 22 11:21:30.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.015122195s
    Jun 22 11:21:32.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.013113743s
    Jun 22 11:21:34.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.014908734s
    Jun 22 11:21:36.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.013938569s
    Jun 22 11:21:38.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.012194433s
    Jun 22 11:21:40.135: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.01939354s
    Jun 22 11:21:42.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.011746944s
    Jun 22 11:21:44.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.01119415s
    Jun 22 11:21:46.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.013856535s
    Jun 22 11:21:48.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.012777277s
    Jun 22 11:21:50.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.01263568s
    Jun 22 11:21:52.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.013813673s
    Jun 22 11:21:54.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.012007977s
    Jun 22 11:21:56.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.012428527s
    Jun 22 11:21:58.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.01232225s
    Jun 22 11:22:00.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.01189894s
    Jun 22 11:22:02.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.011545793s
    Jun 22 11:22:04.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.01223486s
    Jun 22 11:22:06.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011621626s
    Jun 22 11:22:08.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.011682612s
    Jun 22 11:22:10.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.011922795s
    Jun 22 11:22:12.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012185422s
    Jun 22 11:22:14.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.011953561s
    Jun 22 11:22:16.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.012956529s
    Jun 22 11:22:18.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.014047795s
    Jun 22 11:22:20.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.013300598s
    Jun 22 11:22:22.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.013599645s
    Jun 22 11:22:24.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.012171348s
    Jun 22 11:22:26.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.011192695s
    Jun 22 11:22:28.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.012294034s
    Jun 22 11:22:30.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.011944825s
    Jun 22 11:22:32.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.011467599s
    Jun 22 11:22:34.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.013415896s
    Jun 22 11:22:36.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.01092274s
    Jun 22 11:22:38.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.013470015s
    Jun 22 11:22:40.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.012323228s
    Jun 22 11:22:42.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.011915146s
    Jun 22 11:22:44.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010955657s
    Jun 22 11:22:46.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.01168421s
    Jun 22 11:22:48.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.012372739s
    Jun 22 11:22:50.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.014836623s
    Jun 22 11:22:52.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.012170736s
    Jun 22 11:22:54.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.012205109s
    Jun 22 11:22:56.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.012263066s
    Jun 22 11:22:58.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.012198649s
    Jun 22 11:23:00.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.012381639s
    Jun 22 11:23:02.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.011906198s
    Jun 22 11:23:04.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.011922671s
    Jun 22 11:23:06.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.01229197s
    Jun 22 11:23:08.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.012203007s
    Jun 22 11:23:10.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.012024414s
    Jun 22 11:23:12.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.012923267s
    Jun 22 11:23:14.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.012073957s
    Jun 22 11:23:16.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.011257208s
    Jun 22 11:23:18.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.01142897s
    Jun 22 11:23:20.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.012638508s
    Jun 22 11:23:22.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.012776653s
    Jun 22 11:23:24.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.012526187s
    Jun 22 11:23:26.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.011623962s
    Jun 22 11:23:28.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.012111551s
    Jun 22 11:23:30.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.011313501s
    Jun 22 11:23:32.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.013074448s
    Jun 22 11:23:34.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.011838045s
    Jun 22 11:23:36.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.013107712s
    Jun 22 11:23:38.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.012651468s
    Jun 22 11:23:40.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.011873889s
    Jun 22 11:23:42.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.011186624s
    Jun 22 11:23:44.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.012227821s
    Jun 22 11:23:46.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.011493778s
    Jun 22 11:23:48.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.012243159s
    Jun 22 11:23:50.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.012636664s
    Jun 22 11:23:52.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.013771238s
    Jun 22 11:23:54.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.011068967s
    Jun 22 11:23:56.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.012790065s
    Jun 22 11:23:58.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.01239873s
    Jun 22 11:24:00.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.012010875s
    Jun 22 11:24:02.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.01309454s
    Jun 22 11:24:04.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.011195257s
    Jun 22 11:24:06.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.012992361s
    Jun 22 11:24:08.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.013501255s
    Jun 22 11:24:10.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.011731009s
    Jun 22 11:24:12.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.011926517s
    Jun 22 11:24:14.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.012026549s
    Jun 22 11:24:16.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.013139352s
    Jun 22 11:24:18.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.013728779s
    Jun 22 11:24:20.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.013077094s
    Jun 22 11:24:22.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.013703772s
    Jun 22 11:24:24.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.011815238s
    Jun 22 11:24:26.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.013437261s
    Jun 22 11:24:28.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.01114904s
    Jun 22 11:24:30.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.011889981s
    Jun 22 11:24:32.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.012512563s
    Jun 22 11:24:34.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.012588535s
    Jun 22 11:24:36.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.011226044s
    Jun 22 11:24:38.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.012147725s
    Jun 22 11:24:40.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.012618704s
    Jun 22 11:24:42.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.011773747s
    Jun 22 11:24:44.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.012210646s
    Jun 22 11:24:46.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.01297291s
    Jun 22 11:24:48.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.015135324s
    Jun 22 11:24:50.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.012510783s
    Jun 22 11:24:52.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.013468649s
    Jun 22 11:24:54.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.012049718s
    Jun 22 11:24:56.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.015146741s
    Jun 22 11:24:58.126: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.010275203s
    Jun 22 11:25:00.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.011577036s
    Jun 22 11:25:02.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.012265667s
    Jun 22 11:25:04.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.011899552s
    Jun 22 11:25:06.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.012763743s
    Jun 22 11:25:08.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.012793799s
    Jun 22 11:25:10.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.011648743s
    Jun 22 11:25:12.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.011780039s
    Jun 22 11:25:14.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.011845642s
    Jun 22 11:25:16.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.011733398s
    Jun 22 11:25:18.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.012552528s
    Jun 22 11:25:20.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.011669893s
    Jun 22 11:25:22.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.012527954s
    Jun 22 11:25:24.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.01226272s
    Jun 22 11:25:26.131: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.014799901s
    Jun 22 11:25:28.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.011160177s
    Jun 22 11:25:30.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.013444088s
    Jun 22 11:25:32.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.013565828s
    Jun 22 11:25:34.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.012848959s
    Jun 22 11:25:36.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.010981522s
    Jun 22 11:25:38.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.012404899s
    Jun 22 11:25:40.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.011467188s
    Jun 22 11:25:42.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.013488077s
    Jun 22 11:25:44.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.011846775s
    Jun 22 11:25:46.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.012782873s
    Jun 22 11:25:48.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.011333626s
    Jun 22 11:25:50.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.012146977s
    Jun 22 11:25:52.130: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.014350834s
    Jun 22 11:25:54.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.011448614s
    Jun 22 11:25:56.129: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.013538324s
    Jun 22 11:25:58.127: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.01081645s
    Jun 22 11:26:00.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.01227727s
    Jun 22 11:26:00.139: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.022820281s
    STEP: removing the label kubernetes.io/e2e-f9b177a6-5ee0-43d3-bb64-f3c87f61bf5a off the node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw 06/22/23 11:26:00.139
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-f9b177a6-5ee0-43d3-bb64-f3c87f61bf5a 06/22/23 11:26:00.164
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:26:00.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-367" for this suite. 06/22/23 11:26:00.182
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:26:00.195
Jun 22 11:26:00.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename downward-api 06/22/23 11:26:00.202
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:00.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:00.229
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 06/22/23 11:26:00.234
W0622 11:26:00.250663      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:26:00.251: INFO: Waiting up to 5m0s for pod "downwardapi-volume-55b5f4d2-461d-45e6-8c3a-f7e955c4c20b" in namespace "downward-api-7470" to be "Succeeded or Failed"
Jun 22 11:26:00.255: INFO: Pod "downwardapi-volume-55b5f4d2-461d-45e6-8c3a-f7e955c4c20b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.238287ms
Jun 22 11:26:02.263: INFO: Pod "downwardapi-volume-55b5f4d2-461d-45e6-8c3a-f7e955c4c20b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012331724s
Jun 22 11:26:04.263: INFO: Pod "downwardapi-volume-55b5f4d2-461d-45e6-8c3a-f7e955c4c20b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012856451s
STEP: Saw pod success 06/22/23 11:26:04.264
Jun 22 11:26:04.264: INFO: Pod "downwardapi-volume-55b5f4d2-461d-45e6-8c3a-f7e955c4c20b" satisfied condition "Succeeded or Failed"
Jun 22 11:26:04.271: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-55b5f4d2-461d-45e6-8c3a-f7e955c4c20b container client-container: <nil>
STEP: delete the pod 06/22/23 11:26:04.305
Jun 22 11:26:04.327: INFO: Waiting for pod downwardapi-volume-55b5f4d2-461d-45e6-8c3a-f7e955c4c20b to disappear
Jun 22 11:26:04.333: INFO: Pod downwardapi-volume-55b5f4d2-461d-45e6-8c3a-f7e955c4c20b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun 22 11:26:04.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7470" for this suite. 06/22/23 11:26:04.346
------------------------------
• [4.170 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:26:00.195
    Jun 22 11:26:00.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename downward-api 06/22/23 11:26:00.202
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:00.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:00.229
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 06/22/23 11:26:00.234
    W0622 11:26:00.250663      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:26:00.251: INFO: Waiting up to 5m0s for pod "downwardapi-volume-55b5f4d2-461d-45e6-8c3a-f7e955c4c20b" in namespace "downward-api-7470" to be "Succeeded or Failed"
    Jun 22 11:26:00.255: INFO: Pod "downwardapi-volume-55b5f4d2-461d-45e6-8c3a-f7e955c4c20b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.238287ms
    Jun 22 11:26:02.263: INFO: Pod "downwardapi-volume-55b5f4d2-461d-45e6-8c3a-f7e955c4c20b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012331724s
    Jun 22 11:26:04.263: INFO: Pod "downwardapi-volume-55b5f4d2-461d-45e6-8c3a-f7e955c4c20b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012856451s
    STEP: Saw pod success 06/22/23 11:26:04.264
    Jun 22 11:26:04.264: INFO: Pod "downwardapi-volume-55b5f4d2-461d-45e6-8c3a-f7e955c4c20b" satisfied condition "Succeeded or Failed"
    Jun 22 11:26:04.271: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-55b5f4d2-461d-45e6-8c3a-f7e955c4c20b container client-container: <nil>
    STEP: delete the pod 06/22/23 11:26:04.305
    Jun 22 11:26:04.327: INFO: Waiting for pod downwardapi-volume-55b5f4d2-461d-45e6-8c3a-f7e955c4c20b to disappear
    Jun 22 11:26:04.333: INFO: Pod downwardapi-volume-55b5f4d2-461d-45e6-8c3a-f7e955c4c20b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:26:04.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7470" for this suite. 06/22/23 11:26:04.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:26:04.367
Jun 22 11:26:04.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename var-expansion 06/22/23 11:26:04.369
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:04.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:04.406
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
W0622 11:26:04.431512      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:26:04.431: INFO: Waiting up to 2m0s for pod "var-expansion-c528b85c-1ee1-439f-900b-76ee01e4ab9e" in namespace "var-expansion-2696" to be "container 0 failed with reason CreateContainerConfigError"
Jun 22 11:26:04.437: INFO: Pod "var-expansion-c528b85c-1ee1-439f-900b-76ee01e4ab9e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.002236ms
Jun 22 11:26:06.446: INFO: Pod "var-expansion-c528b85c-1ee1-439f-900b-76ee01e4ab9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014143524s
Jun 22 11:26:06.446: INFO: Pod "var-expansion-c528b85c-1ee1-439f-900b-76ee01e4ab9e" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jun 22 11:26:06.446: INFO: Deleting pod "var-expansion-c528b85c-1ee1-439f-900b-76ee01e4ab9e" in namespace "var-expansion-2696"
Jun 22 11:26:06.461: INFO: Wait up to 5m0s for pod "var-expansion-c528b85c-1ee1-439f-900b-76ee01e4ab9e" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun 22 11:26:08.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2696" for this suite. 06/22/23 11:26:08.486
------------------------------
• [4.129 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:26:04.367
    Jun 22 11:26:04.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename var-expansion 06/22/23 11:26:04.369
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:04.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:04.406
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    W0622 11:26:04.431512      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:26:04.431: INFO: Waiting up to 2m0s for pod "var-expansion-c528b85c-1ee1-439f-900b-76ee01e4ab9e" in namespace "var-expansion-2696" to be "container 0 failed with reason CreateContainerConfigError"
    Jun 22 11:26:04.437: INFO: Pod "var-expansion-c528b85c-1ee1-439f-900b-76ee01e4ab9e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.002236ms
    Jun 22 11:26:06.446: INFO: Pod "var-expansion-c528b85c-1ee1-439f-900b-76ee01e4ab9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014143524s
    Jun 22 11:26:06.446: INFO: Pod "var-expansion-c528b85c-1ee1-439f-900b-76ee01e4ab9e" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jun 22 11:26:06.446: INFO: Deleting pod "var-expansion-c528b85c-1ee1-439f-900b-76ee01e4ab9e" in namespace "var-expansion-2696"
    Jun 22 11:26:06.461: INFO: Wait up to 5m0s for pod "var-expansion-c528b85c-1ee1-439f-900b-76ee01e4ab9e" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:26:08.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2696" for this suite. 06/22/23 11:26:08.486
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:26:08.497
Jun 22 11:26:08.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename ingress 06/22/23 11:26:08.498
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:08.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:08.528
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 06/22/23 11:26:08.533
STEP: getting /apis/networking.k8s.io 06/22/23 11:26:08.539
STEP: getting /apis/networking.k8s.iov1 06/22/23 11:26:08.541
STEP: creating 06/22/23 11:26:08.544
STEP: getting 06/22/23 11:26:08.573
STEP: listing 06/22/23 11:26:08.578
STEP: watching 06/22/23 11:26:08.584
Jun 22 11:26:08.584: INFO: starting watch
STEP: cluster-wide listing 06/22/23 11:26:08.587
STEP: cluster-wide watching 06/22/23 11:26:08.592
Jun 22 11:26:08.593: INFO: starting watch
STEP: patching 06/22/23 11:26:08.595
STEP: updating 06/22/23 11:26:08.605
Jun 22 11:26:08.621: INFO: waiting for watch events with expected annotations
Jun 22 11:26:08.621: INFO: saw patched and updated annotations
STEP: patching /status 06/22/23 11:26:08.621
STEP: updating /status 06/22/23 11:26:08.631
STEP: get /status 06/22/23 11:26:08.648
STEP: deleting 06/22/23 11:26:08.654
STEP: deleting a collection 06/22/23 11:26:08.673
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Jun 22 11:26:08.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-6023" for this suite. 06/22/23 11:26:08.707
------------------------------
• [0.221 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:26:08.497
    Jun 22 11:26:08.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename ingress 06/22/23 11:26:08.498
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:08.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:08.528
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 06/22/23 11:26:08.533
    STEP: getting /apis/networking.k8s.io 06/22/23 11:26:08.539
    STEP: getting /apis/networking.k8s.iov1 06/22/23 11:26:08.541
    STEP: creating 06/22/23 11:26:08.544
    STEP: getting 06/22/23 11:26:08.573
    STEP: listing 06/22/23 11:26:08.578
    STEP: watching 06/22/23 11:26:08.584
    Jun 22 11:26:08.584: INFO: starting watch
    STEP: cluster-wide listing 06/22/23 11:26:08.587
    STEP: cluster-wide watching 06/22/23 11:26:08.592
    Jun 22 11:26:08.593: INFO: starting watch
    STEP: patching 06/22/23 11:26:08.595
    STEP: updating 06/22/23 11:26:08.605
    Jun 22 11:26:08.621: INFO: waiting for watch events with expected annotations
    Jun 22 11:26:08.621: INFO: saw patched and updated annotations
    STEP: patching /status 06/22/23 11:26:08.621
    STEP: updating /status 06/22/23 11:26:08.631
    STEP: get /status 06/22/23 11:26:08.648
    STEP: deleting 06/22/23 11:26:08.654
    STEP: deleting a collection 06/22/23 11:26:08.673
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:26:08.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-6023" for this suite. 06/22/23 11:26:08.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:26:08.721
Jun 22 11:26:08.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename pods 06/22/23 11:26:08.722
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:08.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:08.75
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 06/22/23 11:26:08.785
STEP: watching for Pod to be ready 06/22/23 11:26:08.806
Jun 22 11:26:08.809: INFO: observed Pod pod-test in namespace pods-5364 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jun 22 11:26:08.813: INFO: observed Pod pod-test in namespace pods-5364 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:26:08 +0000 UTC  }]
Jun 22 11:26:08.833: INFO: observed Pod pod-test in namespace pods-5364 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:26:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:26:08 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:26:08 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:26:08 +0000 UTC  }]
Jun 22 11:26:10.537: INFO: Found Pod pod-test in namespace pods-5364 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:26:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:26:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:26:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:26:08 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 06/22/23 11:26:10.543
STEP: getting the Pod and ensuring that it's patched 06/22/23 11:26:10.56
STEP: replacing the Pod's status Ready condition to False 06/22/23 11:26:10.564
STEP: check the Pod again to ensure its Ready conditions are False 06/22/23 11:26:10.585
STEP: deleting the Pod via a Collection with a LabelSelector 06/22/23 11:26:10.585
STEP: watching for the Pod to be deleted 06/22/23 11:26:10.599
Jun 22 11:26:10.602: INFO: observed event type MODIFIED
Jun 22 11:26:12.543: INFO: observed event type MODIFIED
Jun 22 11:26:13.550: INFO: observed event type MODIFIED
Jun 22 11:26:13.564: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun 22 11:26:13.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5364" for this suite. 06/22/23 11:26:13.592
------------------------------
• [4.882 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:26:08.721
    Jun 22 11:26:08.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename pods 06/22/23 11:26:08.722
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:08.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:08.75
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 06/22/23 11:26:08.785
    STEP: watching for Pod to be ready 06/22/23 11:26:08.806
    Jun 22 11:26:08.809: INFO: observed Pod pod-test in namespace pods-5364 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jun 22 11:26:08.813: INFO: observed Pod pod-test in namespace pods-5364 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:26:08 +0000 UTC  }]
    Jun 22 11:26:08.833: INFO: observed Pod pod-test in namespace pods-5364 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:26:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:26:08 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:26:08 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:26:08 +0000 UTC  }]
    Jun 22 11:26:10.537: INFO: Found Pod pod-test in namespace pods-5364 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:26:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:26:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:26:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:26:08 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 06/22/23 11:26:10.543
    STEP: getting the Pod and ensuring that it's patched 06/22/23 11:26:10.56
    STEP: replacing the Pod's status Ready condition to False 06/22/23 11:26:10.564
    STEP: check the Pod again to ensure its Ready conditions are False 06/22/23 11:26:10.585
    STEP: deleting the Pod via a Collection with a LabelSelector 06/22/23 11:26:10.585
    STEP: watching for the Pod to be deleted 06/22/23 11:26:10.599
    Jun 22 11:26:10.602: INFO: observed event type MODIFIED
    Jun 22 11:26:12.543: INFO: observed event type MODIFIED
    Jun 22 11:26:13.550: INFO: observed event type MODIFIED
    Jun 22 11:26:13.564: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:26:13.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5364" for this suite. 06/22/23 11:26:13.592
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:26:13.604
Jun 22 11:26:13.604: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename security-context-test 06/22/23 11:26:13.606
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:13.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:13.634
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
W0622 11:26:13.652801      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": unrestricted capabilities (container "alpine-nnp-false-e9d4e0b9-bd97-4810-bc44-f739f19393fe" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "alpine-nnp-false-e9d4e0b9-bd97-4810-bc44-f739f19393fe" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "alpine-nnp-false-e9d4e0b9-bd97-4810-bc44-f739f19393fe" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:26:13.652: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-e9d4e0b9-bd97-4810-bc44-f739f19393fe" in namespace "security-context-test-9111" to be "Succeeded or Failed"
Jun 22 11:26:13.659: INFO: Pod "alpine-nnp-false-e9d4e0b9-bd97-4810-bc44-f739f19393fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.328422ms
Jun 22 11:26:15.667: INFO: Pod "alpine-nnp-false-e9d4e0b9-bd97-4810-bc44-f739f19393fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01422788s
Jun 22 11:26:17.667: INFO: Pod "alpine-nnp-false-e9d4e0b9-bd97-4810-bc44-f739f19393fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014108119s
Jun 22 11:26:17.667: INFO: Pod "alpine-nnp-false-e9d4e0b9-bd97-4810-bc44-f739f19393fe" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jun 22 11:26:17.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-9111" for this suite. 06/22/23 11:26:17.689
------------------------------
• [4.097 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:26:13.604
    Jun 22 11:26:13.604: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename security-context-test 06/22/23 11:26:13.606
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:13.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:13.634
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    W0622 11:26:13.652801      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": unrestricted capabilities (container "alpine-nnp-false-e9d4e0b9-bd97-4810-bc44-f739f19393fe" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "alpine-nnp-false-e9d4e0b9-bd97-4810-bc44-f739f19393fe" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "alpine-nnp-false-e9d4e0b9-bd97-4810-bc44-f739f19393fe" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:26:13.652: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-e9d4e0b9-bd97-4810-bc44-f739f19393fe" in namespace "security-context-test-9111" to be "Succeeded or Failed"
    Jun 22 11:26:13.659: INFO: Pod "alpine-nnp-false-e9d4e0b9-bd97-4810-bc44-f739f19393fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.328422ms
    Jun 22 11:26:15.667: INFO: Pod "alpine-nnp-false-e9d4e0b9-bd97-4810-bc44-f739f19393fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01422788s
    Jun 22 11:26:17.667: INFO: Pod "alpine-nnp-false-e9d4e0b9-bd97-4810-bc44-f739f19393fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014108119s
    Jun 22 11:26:17.667: INFO: Pod "alpine-nnp-false-e9d4e0b9-bd97-4810-bc44-f739f19393fe" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:26:17.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-9111" for this suite. 06/22/23 11:26:17.689
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:26:17.704
Jun 22 11:26:17.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename subpath 06/22/23 11:26:17.706
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:17.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:17.735
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/22/23 11:26:17.74
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-pq5t 06/22/23 11:26:17.757
STEP: Creating a pod to test atomic-volume-subpath 06/22/23 11:26:17.757
W0622 11:26:17.771603      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container-subpath-configmap-pq5t" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-subpath-configmap-pq5t" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-subpath-configmap-pq5t" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-subpath-configmap-pq5t" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:26:17.771: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pq5t" in namespace "subpath-7149" to be "Succeeded or Failed"
Jun 22 11:26:17.778: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Pending", Reason="", readiness=false. Elapsed: 6.364852ms
Jun 22 11:26:19.789: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 2.017256832s
Jun 22 11:26:21.785: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 4.0139324s
Jun 22 11:26:23.785: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 6.014087595s
Jun 22 11:26:25.786: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 8.014350267s
Jun 22 11:26:27.785: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 10.013756921s
Jun 22 11:26:29.785: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 12.013994819s
Jun 22 11:26:31.786: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 14.014481209s
Jun 22 11:26:33.787: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 16.01570934s
Jun 22 11:26:35.785: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 18.013785852s
Jun 22 11:26:37.786: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 20.014771926s
Jun 22 11:26:39.785: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=false. Elapsed: 22.013618243s
Jun 22 11:26:41.786: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014631077s
STEP: Saw pod success 06/22/23 11:26:41.786
Jun 22 11:26:41.786: INFO: Pod "pod-subpath-test-configmap-pq5t" satisfied condition "Succeeded or Failed"
Jun 22 11:26:41.791: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-subpath-test-configmap-pq5t container test-container-subpath-configmap-pq5t: <nil>
STEP: delete the pod 06/22/23 11:26:41.802
Jun 22 11:26:41.817: INFO: Waiting for pod pod-subpath-test-configmap-pq5t to disappear
Jun 22 11:26:41.821: INFO: Pod pod-subpath-test-configmap-pq5t no longer exists
STEP: Deleting pod pod-subpath-test-configmap-pq5t 06/22/23 11:26:41.821
Jun 22 11:26:41.821: INFO: Deleting pod "pod-subpath-test-configmap-pq5t" in namespace "subpath-7149"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jun 22 11:26:41.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7149" for this suite. 06/22/23 11:26:41.835
------------------------------
• [SLOW TEST] [24.140 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:26:17.704
    Jun 22 11:26:17.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename subpath 06/22/23 11:26:17.706
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:17.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:17.735
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/22/23 11:26:17.74
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-pq5t 06/22/23 11:26:17.757
    STEP: Creating a pod to test atomic-volume-subpath 06/22/23 11:26:17.757
    W0622 11:26:17.771603      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container-subpath-configmap-pq5t" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-subpath-configmap-pq5t" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-subpath-configmap-pq5t" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-subpath-configmap-pq5t" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:26:17.771: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-pq5t" in namespace "subpath-7149" to be "Succeeded or Failed"
    Jun 22 11:26:17.778: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Pending", Reason="", readiness=false. Elapsed: 6.364852ms
    Jun 22 11:26:19.789: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 2.017256832s
    Jun 22 11:26:21.785: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 4.0139324s
    Jun 22 11:26:23.785: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 6.014087595s
    Jun 22 11:26:25.786: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 8.014350267s
    Jun 22 11:26:27.785: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 10.013756921s
    Jun 22 11:26:29.785: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 12.013994819s
    Jun 22 11:26:31.786: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 14.014481209s
    Jun 22 11:26:33.787: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 16.01570934s
    Jun 22 11:26:35.785: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 18.013785852s
    Jun 22 11:26:37.786: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=true. Elapsed: 20.014771926s
    Jun 22 11:26:39.785: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Running", Reason="", readiness=false. Elapsed: 22.013618243s
    Jun 22 11:26:41.786: INFO: Pod "pod-subpath-test-configmap-pq5t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014631077s
    STEP: Saw pod success 06/22/23 11:26:41.786
    Jun 22 11:26:41.786: INFO: Pod "pod-subpath-test-configmap-pq5t" satisfied condition "Succeeded or Failed"
    Jun 22 11:26:41.791: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-subpath-test-configmap-pq5t container test-container-subpath-configmap-pq5t: <nil>
    STEP: delete the pod 06/22/23 11:26:41.802
    Jun 22 11:26:41.817: INFO: Waiting for pod pod-subpath-test-configmap-pq5t to disappear
    Jun 22 11:26:41.821: INFO: Pod pod-subpath-test-configmap-pq5t no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-pq5t 06/22/23 11:26:41.821
    Jun 22 11:26:41.821: INFO: Deleting pod "pod-subpath-test-configmap-pq5t" in namespace "subpath-7149"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:26:41.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7149" for this suite. 06/22/23 11:26:41.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:26:41.846
Jun 22 11:26:41.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 11:26:41.847
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:41.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:41.874
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-aa5fd9ee-88db-45d4-93d6-44a499fa04e8 06/22/23 11:26:41.877
STEP: Creating secret with name secret-projected-all-test-volume-0168e046-0ebd-46c6-8939-52eaea1d0f9f 06/22/23 11:26:41.884
STEP: Creating a pod to test Check all projections for projected volume plugin 06/22/23 11:26:41.89
W0622 11:26:41.903264      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "projected-all-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-all-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-all-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-all-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:26:41.903: INFO: Waiting up to 5m0s for pod "projected-volume-7eecbb8b-e621-4c32-b7fe-235e17b46944" in namespace "projected-3282" to be "Succeeded or Failed"
Jun 22 11:26:41.910: INFO: Pod "projected-volume-7eecbb8b-e621-4c32-b7fe-235e17b46944": Phase="Pending", Reason="", readiness=false. Elapsed: 7.045456ms
Jun 22 11:26:43.918: INFO: Pod "projected-volume-7eecbb8b-e621-4c32-b7fe-235e17b46944": Phase="Running", Reason="", readiness=false. Elapsed: 2.015102641s
Jun 22 11:26:45.919: INFO: Pod "projected-volume-7eecbb8b-e621-4c32-b7fe-235e17b46944": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015963625s
STEP: Saw pod success 06/22/23 11:26:45.919
Jun 22 11:26:45.919: INFO: Pod "projected-volume-7eecbb8b-e621-4c32-b7fe-235e17b46944" satisfied condition "Succeeded or Failed"
Jun 22 11:26:45.925: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod projected-volume-7eecbb8b-e621-4c32-b7fe-235e17b46944 container projected-all-volume-test: <nil>
STEP: delete the pod 06/22/23 11:26:45.937
Jun 22 11:26:45.956: INFO: Waiting for pod projected-volume-7eecbb8b-e621-4c32-b7fe-235e17b46944 to disappear
Jun 22 11:26:45.960: INFO: Pod projected-volume-7eecbb8b-e621-4c32-b7fe-235e17b46944 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Jun 22 11:26:45.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3282" for this suite. 06/22/23 11:26:45.97
------------------------------
• [4.133 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:26:41.846
    Jun 22 11:26:41.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 11:26:41.847
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:41.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:41.874
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-aa5fd9ee-88db-45d4-93d6-44a499fa04e8 06/22/23 11:26:41.877
    STEP: Creating secret with name secret-projected-all-test-volume-0168e046-0ebd-46c6-8939-52eaea1d0f9f 06/22/23 11:26:41.884
    STEP: Creating a pod to test Check all projections for projected volume plugin 06/22/23 11:26:41.89
    W0622 11:26:41.903264      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "projected-all-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-all-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-all-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-all-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:26:41.903: INFO: Waiting up to 5m0s for pod "projected-volume-7eecbb8b-e621-4c32-b7fe-235e17b46944" in namespace "projected-3282" to be "Succeeded or Failed"
    Jun 22 11:26:41.910: INFO: Pod "projected-volume-7eecbb8b-e621-4c32-b7fe-235e17b46944": Phase="Pending", Reason="", readiness=false. Elapsed: 7.045456ms
    Jun 22 11:26:43.918: INFO: Pod "projected-volume-7eecbb8b-e621-4c32-b7fe-235e17b46944": Phase="Running", Reason="", readiness=false. Elapsed: 2.015102641s
    Jun 22 11:26:45.919: INFO: Pod "projected-volume-7eecbb8b-e621-4c32-b7fe-235e17b46944": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015963625s
    STEP: Saw pod success 06/22/23 11:26:45.919
    Jun 22 11:26:45.919: INFO: Pod "projected-volume-7eecbb8b-e621-4c32-b7fe-235e17b46944" satisfied condition "Succeeded or Failed"
    Jun 22 11:26:45.925: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod projected-volume-7eecbb8b-e621-4c32-b7fe-235e17b46944 container projected-all-volume-test: <nil>
    STEP: delete the pod 06/22/23 11:26:45.937
    Jun 22 11:26:45.956: INFO: Waiting for pod projected-volume-7eecbb8b-e621-4c32-b7fe-235e17b46944 to disappear
    Jun 22 11:26:45.960: INFO: Pod projected-volume-7eecbb8b-e621-4c32-b7fe-235e17b46944 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:26:45.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3282" for this suite. 06/22/23 11:26:45.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:26:45.981
Jun 22 11:26:45.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename container-runtime 06/22/23 11:26:45.982
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:46.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:46.008
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 06/22/23 11:26:46.012
W0622 11:26:46.025168      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: wait for the container to reach Failed 06/22/23 11:26:46.025
STEP: get the container status 06/22/23 11:26:50.059
STEP: the container should be terminated 06/22/23 11:26:50.064
STEP: the termination message should be set 06/22/23 11:26:50.065
Jun 22 11:26:50.065: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 06/22/23 11:26:50.065
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jun 22 11:26:50.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3770" for this suite. 06/22/23 11:26:50.095
------------------------------
• [4.125 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:26:45.981
    Jun 22 11:26:45.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename container-runtime 06/22/23 11:26:45.982
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:46.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:46.008
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 06/22/23 11:26:46.012
    W0622 11:26:46.025168      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: wait for the container to reach Failed 06/22/23 11:26:46.025
    STEP: get the container status 06/22/23 11:26:50.059
    STEP: the container should be terminated 06/22/23 11:26:50.064
    STEP: the termination message should be set 06/22/23 11:26:50.065
    Jun 22 11:26:50.065: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 06/22/23 11:26:50.065
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:26:50.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3770" for this suite. 06/22/23 11:26:50.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:26:50.112
Jun 22 11:26:50.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename controllerrevisions 06/22/23 11:26:50.113
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:50.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:50.139
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-kg4fb-daemon-set" 06/22/23 11:26:50.175
W0622 11:26:50.185236      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Check that daemon pods launch on every node of the cluster. 06/22/23 11:26:50.185
Jun 22 11:26:50.194: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:26:50.194: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:26:50.194: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:26:50.199: INFO: Number of nodes with available pods controlled by daemonset e2e-kg4fb-daemon-set: 0
Jun 22 11:26:50.199: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
Jun 22 11:26:51.209: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:26:51.209: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:26:51.209: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:26:51.214: INFO: Number of nodes with available pods controlled by daemonset e2e-kg4fb-daemon-set: 0
Jun 22 11:26:51.214: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
Jun 22 11:26:52.210: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:26:52.210: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:26:52.210: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:26:52.217: INFO: Number of nodes with available pods controlled by daemonset e2e-kg4fb-daemon-set: 3
Jun 22 11:26:52.217: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-kg4fb-daemon-set
STEP: Confirm DaemonSet "e2e-kg4fb-daemon-set" successfully created with "daemonset-name=e2e-kg4fb-daemon-set" label 06/22/23 11:26:52.222
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-kg4fb-daemon-set" 06/22/23 11:26:52.233
Jun 22 11:26:52.240: INFO: Located ControllerRevision: "e2e-kg4fb-daemon-set-656677459"
STEP: Patching ControllerRevision "e2e-kg4fb-daemon-set-656677459" 06/22/23 11:26:52.245
Jun 22 11:26:52.256: INFO: e2e-kg4fb-daemon-set-656677459 has been patched
STEP: Create a new ControllerRevision 06/22/23 11:26:52.256
Jun 22 11:26:52.264: INFO: Created ControllerRevision: e2e-kg4fb-daemon-set-68d9ccdff5
STEP: Confirm that there are two ControllerRevisions 06/22/23 11:26:52.264
Jun 22 11:26:52.265: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 22 11:26:52.269: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-kg4fb-daemon-set-656677459" 06/22/23 11:26:52.269
STEP: Confirm that there is only one ControllerRevision 06/22/23 11:26:52.278
Jun 22 11:26:52.278: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 22 11:26:52.282: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-kg4fb-daemon-set-68d9ccdff5" 06/22/23 11:26:52.286
Jun 22 11:26:52.298: INFO: e2e-kg4fb-daemon-set-68d9ccdff5 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 06/22/23 11:26:52.298
W0622 11:26:52.308277      23 warnings.go:70] unknown field "updateStrategy"
W0622 11:26:52.308312      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Confirm that there are two ControllerRevisions 06/22/23 11:26:52.308
Jun 22 11:26:52.308: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 22 11:26:53.315: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 22 11:26:53.322: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-kg4fb-daemon-set-68d9ccdff5=updated" 06/22/23 11:26:53.322
STEP: Confirm that there is only one ControllerRevision 06/22/23 11:26:53.334
Jun 22 11:26:53.334: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 22 11:26:53.338: INFO: Found 1 ControllerRevisions
Jun 22 11:26:53.342: INFO: ControllerRevision "e2e-kg4fb-daemon-set-cd75cb5c8" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-kg4fb-daemon-set" 06/22/23 11:26:53.348
STEP: deleting DaemonSet.extensions e2e-kg4fb-daemon-set in namespace controllerrevisions-9037, will wait for the garbage collector to delete the pods 06/22/23 11:26:53.348
Jun 22 11:26:53.413: INFO: Deleting DaemonSet.extensions e2e-kg4fb-daemon-set took: 9.875901ms
Jun 22 11:26:53.514: INFO: Terminating DaemonSet.extensions e2e-kg4fb-daemon-set pods took: 100.260122ms
Jun 22 11:26:55.021: INFO: Number of nodes with available pods controlled by daemonset e2e-kg4fb-daemon-set: 0
Jun 22 11:26:55.021: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-kg4fb-daemon-set
Jun 22 11:26:55.087: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"154824"},"items":null}

Jun 22 11:26:55.092: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"154824"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:26:55.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-9037" for this suite. 06/22/23 11:26:55.272
------------------------------
• [SLOW TEST] [5.171 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:26:50.112
    Jun 22 11:26:50.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename controllerrevisions 06/22/23 11:26:50.113
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:50.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:50.139
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-kg4fb-daemon-set" 06/22/23 11:26:50.175
    W0622 11:26:50.185236      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Check that daemon pods launch on every node of the cluster. 06/22/23 11:26:50.185
    Jun 22 11:26:50.194: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:26:50.194: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:26:50.194: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:26:50.199: INFO: Number of nodes with available pods controlled by daemonset e2e-kg4fb-daemon-set: 0
    Jun 22 11:26:50.199: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
    Jun 22 11:26:51.209: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:26:51.209: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:26:51.209: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:26:51.214: INFO: Number of nodes with available pods controlled by daemonset e2e-kg4fb-daemon-set: 0
    Jun 22 11:26:51.214: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
    Jun 22 11:26:52.210: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:26:52.210: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:26:52.210: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:26:52.217: INFO: Number of nodes with available pods controlled by daemonset e2e-kg4fb-daemon-set: 3
    Jun 22 11:26:52.217: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-kg4fb-daemon-set
    STEP: Confirm DaemonSet "e2e-kg4fb-daemon-set" successfully created with "daemonset-name=e2e-kg4fb-daemon-set" label 06/22/23 11:26:52.222
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-kg4fb-daemon-set" 06/22/23 11:26:52.233
    Jun 22 11:26:52.240: INFO: Located ControllerRevision: "e2e-kg4fb-daemon-set-656677459"
    STEP: Patching ControllerRevision "e2e-kg4fb-daemon-set-656677459" 06/22/23 11:26:52.245
    Jun 22 11:26:52.256: INFO: e2e-kg4fb-daemon-set-656677459 has been patched
    STEP: Create a new ControllerRevision 06/22/23 11:26:52.256
    Jun 22 11:26:52.264: INFO: Created ControllerRevision: e2e-kg4fb-daemon-set-68d9ccdff5
    STEP: Confirm that there are two ControllerRevisions 06/22/23 11:26:52.264
    Jun 22 11:26:52.265: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 22 11:26:52.269: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-kg4fb-daemon-set-656677459" 06/22/23 11:26:52.269
    STEP: Confirm that there is only one ControllerRevision 06/22/23 11:26:52.278
    Jun 22 11:26:52.278: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 22 11:26:52.282: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-kg4fb-daemon-set-68d9ccdff5" 06/22/23 11:26:52.286
    Jun 22 11:26:52.298: INFO: e2e-kg4fb-daemon-set-68d9ccdff5 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 06/22/23 11:26:52.298
    W0622 11:26:52.308277      23 warnings.go:70] unknown field "updateStrategy"
    W0622 11:26:52.308312      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Confirm that there are two ControllerRevisions 06/22/23 11:26:52.308
    Jun 22 11:26:52.308: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 22 11:26:53.315: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 22 11:26:53.322: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-kg4fb-daemon-set-68d9ccdff5=updated" 06/22/23 11:26:53.322
    STEP: Confirm that there is only one ControllerRevision 06/22/23 11:26:53.334
    Jun 22 11:26:53.334: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 22 11:26:53.338: INFO: Found 1 ControllerRevisions
    Jun 22 11:26:53.342: INFO: ControllerRevision "e2e-kg4fb-daemon-set-cd75cb5c8" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-kg4fb-daemon-set" 06/22/23 11:26:53.348
    STEP: deleting DaemonSet.extensions e2e-kg4fb-daemon-set in namespace controllerrevisions-9037, will wait for the garbage collector to delete the pods 06/22/23 11:26:53.348
    Jun 22 11:26:53.413: INFO: Deleting DaemonSet.extensions e2e-kg4fb-daemon-set took: 9.875901ms
    Jun 22 11:26:53.514: INFO: Terminating DaemonSet.extensions e2e-kg4fb-daemon-set pods took: 100.260122ms
    Jun 22 11:26:55.021: INFO: Number of nodes with available pods controlled by daemonset e2e-kg4fb-daemon-set: 0
    Jun 22 11:26:55.021: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-kg4fb-daemon-set
    Jun 22 11:26:55.087: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"154824"},"items":null}

    Jun 22 11:26:55.092: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"154824"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:26:55.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-9037" for this suite. 06/22/23 11:26:55.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:26:55.283
Jun 22 11:26:55.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename container-probe 06/22/23 11:26:55.285
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:55.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:55.31
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-9cf1f6c6-37f0-4cc1-abca-a2991135f66d in namespace container-probe-6050 06/22/23 11:26:55.313
W0622 11:26:55.326359      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "busybox" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:26:55.326: INFO: Waiting up to 5m0s for pod "busybox-9cf1f6c6-37f0-4cc1-abca-a2991135f66d" in namespace "container-probe-6050" to be "not pending"
Jun 22 11:26:55.334: INFO: Pod "busybox-9cf1f6c6-37f0-4cc1-abca-a2991135f66d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.097036ms
Jun 22 11:26:57.340: INFO: Pod "busybox-9cf1f6c6-37f0-4cc1-abca-a2991135f66d": Phase="Running", Reason="", readiness=true. Elapsed: 2.014449897s
Jun 22 11:26:57.341: INFO: Pod "busybox-9cf1f6c6-37f0-4cc1-abca-a2991135f66d" satisfied condition "not pending"
Jun 22 11:26:57.341: INFO: Started pod busybox-9cf1f6c6-37f0-4cc1-abca-a2991135f66d in namespace container-probe-6050
STEP: checking the pod's current state and verifying that restartCount is present 06/22/23 11:26:57.341
Jun 22 11:26:57.346: INFO: Initial restart count of pod busybox-9cf1f6c6-37f0-4cc1-abca-a2991135f66d is 0
STEP: deleting the pod 06/22/23 11:30:58.336
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun 22 11:30:58.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6050" for this suite. 06/22/23 11:30:58.367
------------------------------
• [SLOW TEST] [243.094 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:26:55.283
    Jun 22 11:26:55.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename container-probe 06/22/23 11:26:55.285
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:26:55.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:26:55.31
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-9cf1f6c6-37f0-4cc1-abca-a2991135f66d in namespace container-probe-6050 06/22/23 11:26:55.313
    W0622 11:26:55.326359      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "busybox" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:26:55.326: INFO: Waiting up to 5m0s for pod "busybox-9cf1f6c6-37f0-4cc1-abca-a2991135f66d" in namespace "container-probe-6050" to be "not pending"
    Jun 22 11:26:55.334: INFO: Pod "busybox-9cf1f6c6-37f0-4cc1-abca-a2991135f66d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.097036ms
    Jun 22 11:26:57.340: INFO: Pod "busybox-9cf1f6c6-37f0-4cc1-abca-a2991135f66d": Phase="Running", Reason="", readiness=true. Elapsed: 2.014449897s
    Jun 22 11:26:57.341: INFO: Pod "busybox-9cf1f6c6-37f0-4cc1-abca-a2991135f66d" satisfied condition "not pending"
    Jun 22 11:26:57.341: INFO: Started pod busybox-9cf1f6c6-37f0-4cc1-abca-a2991135f66d in namespace container-probe-6050
    STEP: checking the pod's current state and verifying that restartCount is present 06/22/23 11:26:57.341
    Jun 22 11:26:57.346: INFO: Initial restart count of pod busybox-9cf1f6c6-37f0-4cc1-abca-a2991135f66d is 0
    STEP: deleting the pod 06/22/23 11:30:58.336
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:30:58.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6050" for this suite. 06/22/23 11:30:58.367
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:30:58.38
Jun 22 11:30:58.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename subpath 06/22/23 11:30:58.387
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:30:58.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:30:58.417
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/22/23 11:30:58.422
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-kpkr 06/22/23 11:30:58.437
STEP: Creating a pod to test atomic-volume-subpath 06/22/23 11:30:58.437
W0622 11:30:58.453185      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container-subpath-downwardapi-kpkr" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-subpath-downwardapi-kpkr" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-subpath-downwardapi-kpkr" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-subpath-downwardapi-kpkr" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:30:58.453: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-kpkr" in namespace "subpath-5315" to be "Succeeded or Failed"
Jun 22 11:30:58.458: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.754679ms
Jun 22 11:31:00.465: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 2.012118562s
Jun 22 11:31:02.465: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 4.011720786s
Jun 22 11:31:04.466: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 6.012546171s
Jun 22 11:31:06.466: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 8.013079195s
Jun 22 11:31:08.466: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 10.012387823s
Jun 22 11:31:10.466: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 12.012394258s
Jun 22 11:31:12.466: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 14.012364852s
Jun 22 11:31:14.465: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 16.01155788s
Jun 22 11:31:16.466: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 18.01277203s
Jun 22 11:31:18.468: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 20.014269805s
Jun 22 11:31:20.467: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=false. Elapsed: 22.014144826s
Jun 22 11:31:22.466: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012605269s
STEP: Saw pod success 06/22/23 11:31:22.466
Jun 22 11:31:22.466: INFO: Pod "pod-subpath-test-downwardapi-kpkr" satisfied condition "Succeeded or Failed"
Jun 22 11:31:22.472: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-subpath-test-downwardapi-kpkr container test-container-subpath-downwardapi-kpkr: <nil>
STEP: delete the pod 06/22/23 11:31:22.499
Jun 22 11:31:22.516: INFO: Waiting for pod pod-subpath-test-downwardapi-kpkr to disappear
Jun 22 11:31:22.523: INFO: Pod pod-subpath-test-downwardapi-kpkr no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-kpkr 06/22/23 11:31:22.523
Jun 22 11:31:22.523: INFO: Deleting pod "pod-subpath-test-downwardapi-kpkr" in namespace "subpath-5315"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jun 22 11:31:22.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5315" for this suite. 06/22/23 11:31:22.536
------------------------------
• [SLOW TEST] [24.168 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:30:58.38
    Jun 22 11:30:58.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename subpath 06/22/23 11:30:58.387
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:30:58.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:30:58.417
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/22/23 11:30:58.422
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-kpkr 06/22/23 11:30:58.437
    STEP: Creating a pod to test atomic-volume-subpath 06/22/23 11:30:58.437
    W0622 11:30:58.453185      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container-subpath-downwardapi-kpkr" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-subpath-downwardapi-kpkr" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-subpath-downwardapi-kpkr" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-subpath-downwardapi-kpkr" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:30:58.453: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-kpkr" in namespace "subpath-5315" to be "Succeeded or Failed"
    Jun 22 11:30:58.458: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.754679ms
    Jun 22 11:31:00.465: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 2.012118562s
    Jun 22 11:31:02.465: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 4.011720786s
    Jun 22 11:31:04.466: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 6.012546171s
    Jun 22 11:31:06.466: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 8.013079195s
    Jun 22 11:31:08.466: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 10.012387823s
    Jun 22 11:31:10.466: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 12.012394258s
    Jun 22 11:31:12.466: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 14.012364852s
    Jun 22 11:31:14.465: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 16.01155788s
    Jun 22 11:31:16.466: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 18.01277203s
    Jun 22 11:31:18.468: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=true. Elapsed: 20.014269805s
    Jun 22 11:31:20.467: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Running", Reason="", readiness=false. Elapsed: 22.014144826s
    Jun 22 11:31:22.466: INFO: Pod "pod-subpath-test-downwardapi-kpkr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012605269s
    STEP: Saw pod success 06/22/23 11:31:22.466
    Jun 22 11:31:22.466: INFO: Pod "pod-subpath-test-downwardapi-kpkr" satisfied condition "Succeeded or Failed"
    Jun 22 11:31:22.472: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-subpath-test-downwardapi-kpkr container test-container-subpath-downwardapi-kpkr: <nil>
    STEP: delete the pod 06/22/23 11:31:22.499
    Jun 22 11:31:22.516: INFO: Waiting for pod pod-subpath-test-downwardapi-kpkr to disappear
    Jun 22 11:31:22.523: INFO: Pod pod-subpath-test-downwardapi-kpkr no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-kpkr 06/22/23 11:31:22.523
    Jun 22 11:31:22.523: INFO: Deleting pod "pod-subpath-test-downwardapi-kpkr" in namespace "subpath-5315"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:31:22.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5315" for this suite. 06/22/23 11:31:22.536
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:31:22.551
Jun 22 11:31:22.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 11:31:22.553
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:31:22.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:31:22.579
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-d120fa3f-3447-4c90-a2b8-255b428d7f8e 06/22/23 11:31:22.583
STEP: Creating a pod to test consume secrets 06/22/23 11:31:22.59
W0622 11:31:22.603119      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:31:22.603: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-06014cc9-f13b-4eac-9ff7-59cc4fc1f95c" in namespace "projected-2530" to be "Succeeded or Failed"
Jun 22 11:31:22.608: INFO: Pod "pod-projected-secrets-06014cc9-f13b-4eac-9ff7-59cc4fc1f95c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.229666ms
Jun 22 11:31:24.616: INFO: Pod "pod-projected-secrets-06014cc9-f13b-4eac-9ff7-59cc4fc1f95c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012515696s
Jun 22 11:31:26.616: INFO: Pod "pod-projected-secrets-06014cc9-f13b-4eac-9ff7-59cc4fc1f95c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013418989s
STEP: Saw pod success 06/22/23 11:31:26.616
Jun 22 11:31:26.617: INFO: Pod "pod-projected-secrets-06014cc9-f13b-4eac-9ff7-59cc4fc1f95c" satisfied condition "Succeeded or Failed"
Jun 22 11:31:26.627: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-secrets-06014cc9-f13b-4eac-9ff7-59cc4fc1f95c container secret-volume-test: <nil>
STEP: delete the pod 06/22/23 11:31:26.639
Jun 22 11:31:26.655: INFO: Waiting for pod pod-projected-secrets-06014cc9-f13b-4eac-9ff7-59cc4fc1f95c to disappear
Jun 22 11:31:26.659: INFO: Pod pod-projected-secrets-06014cc9-f13b-4eac-9ff7-59cc4fc1f95c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun 22 11:31:26.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2530" for this suite. 06/22/23 11:31:26.669
------------------------------
• [4.131 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:31:22.551
    Jun 22 11:31:22.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 11:31:22.553
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:31:22.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:31:22.579
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-d120fa3f-3447-4c90-a2b8-255b428d7f8e 06/22/23 11:31:22.583
    STEP: Creating a pod to test consume secrets 06/22/23 11:31:22.59
    W0622 11:31:22.603119      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:31:22.603: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-06014cc9-f13b-4eac-9ff7-59cc4fc1f95c" in namespace "projected-2530" to be "Succeeded or Failed"
    Jun 22 11:31:22.608: INFO: Pod "pod-projected-secrets-06014cc9-f13b-4eac-9ff7-59cc4fc1f95c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.229666ms
    Jun 22 11:31:24.616: INFO: Pod "pod-projected-secrets-06014cc9-f13b-4eac-9ff7-59cc4fc1f95c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012515696s
    Jun 22 11:31:26.616: INFO: Pod "pod-projected-secrets-06014cc9-f13b-4eac-9ff7-59cc4fc1f95c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013418989s
    STEP: Saw pod success 06/22/23 11:31:26.616
    Jun 22 11:31:26.617: INFO: Pod "pod-projected-secrets-06014cc9-f13b-4eac-9ff7-59cc4fc1f95c" satisfied condition "Succeeded or Failed"
    Jun 22 11:31:26.627: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-secrets-06014cc9-f13b-4eac-9ff7-59cc4fc1f95c container secret-volume-test: <nil>
    STEP: delete the pod 06/22/23 11:31:26.639
    Jun 22 11:31:26.655: INFO: Waiting for pod pod-projected-secrets-06014cc9-f13b-4eac-9ff7-59cc4fc1f95c to disappear
    Jun 22 11:31:26.659: INFO: Pod pod-projected-secrets-06014cc9-f13b-4eac-9ff7-59cc4fc1f95c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:31:26.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2530" for this suite. 06/22/23 11:31:26.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:31:26.685
Jun 22 11:31:26.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 11:31:26.687
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:31:26.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:31:26.713
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 06/22/23 11:31:26.717
W0622 11:31:26.730740      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:31:26.730: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1bcd7db4-d35c-47c4-8216-01a4a77e0bbe" in namespace "projected-3971" to be "Succeeded or Failed"
Jun 22 11:31:26.738: INFO: Pod "downwardapi-volume-1bcd7db4-d35c-47c4-8216-01a4a77e0bbe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.543235ms
Jun 22 11:31:28.746: INFO: Pod "downwardapi-volume-1bcd7db4-d35c-47c4-8216-01a4a77e0bbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015800982s
Jun 22 11:31:30.746: INFO: Pod "downwardapi-volume-1bcd7db4-d35c-47c4-8216-01a4a77e0bbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01516407s
STEP: Saw pod success 06/22/23 11:31:30.746
Jun 22 11:31:30.746: INFO: Pod "downwardapi-volume-1bcd7db4-d35c-47c4-8216-01a4a77e0bbe" satisfied condition "Succeeded or Failed"
Jun 22 11:31:30.752: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-1bcd7db4-d35c-47c4-8216-01a4a77e0bbe container client-container: <nil>
STEP: delete the pod 06/22/23 11:31:30.761
Jun 22 11:31:30.778: INFO: Waiting for pod downwardapi-volume-1bcd7db4-d35c-47c4-8216-01a4a77e0bbe to disappear
Jun 22 11:31:30.783: INFO: Pod downwardapi-volume-1bcd7db4-d35c-47c4-8216-01a4a77e0bbe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun 22 11:31:30.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3971" for this suite. 06/22/23 11:31:30.792
------------------------------
• [4.116 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:31:26.685
    Jun 22 11:31:26.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 11:31:26.687
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:31:26.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:31:26.713
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 06/22/23 11:31:26.717
    W0622 11:31:26.730740      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:31:26.730: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1bcd7db4-d35c-47c4-8216-01a4a77e0bbe" in namespace "projected-3971" to be "Succeeded or Failed"
    Jun 22 11:31:26.738: INFO: Pod "downwardapi-volume-1bcd7db4-d35c-47c4-8216-01a4a77e0bbe": Phase="Pending", Reason="", readiness=false. Elapsed: 7.543235ms
    Jun 22 11:31:28.746: INFO: Pod "downwardapi-volume-1bcd7db4-d35c-47c4-8216-01a4a77e0bbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015800982s
    Jun 22 11:31:30.746: INFO: Pod "downwardapi-volume-1bcd7db4-d35c-47c4-8216-01a4a77e0bbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01516407s
    STEP: Saw pod success 06/22/23 11:31:30.746
    Jun 22 11:31:30.746: INFO: Pod "downwardapi-volume-1bcd7db4-d35c-47c4-8216-01a4a77e0bbe" satisfied condition "Succeeded or Failed"
    Jun 22 11:31:30.752: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-1bcd7db4-d35c-47c4-8216-01a4a77e0bbe container client-container: <nil>
    STEP: delete the pod 06/22/23 11:31:30.761
    Jun 22 11:31:30.778: INFO: Waiting for pod downwardapi-volume-1bcd7db4-d35c-47c4-8216-01a4a77e0bbe to disappear
    Jun 22 11:31:30.783: INFO: Pod downwardapi-volume-1bcd7db4-d35c-47c4-8216-01a4a77e0bbe no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:31:30.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3971" for this suite. 06/22/23 11:31:30.792
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:31:30.801
Jun 22 11:31:30.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename daemonsets 06/22/23 11:31:30.803
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:31:30.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:31:30.826
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Jun 22 11:31:30.873: INFO: Create a RollingUpdate DaemonSet
W0622 11:31:30.881982      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:31:30.882: INFO: Check that daemon pods launch on every node of the cluster
Jun 22 11:31:30.891: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:30.891: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:30.891: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:30.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 11:31:30.896: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
Jun 22 11:31:31.909: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:31.910: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:31.910: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:31.915: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 11:31:31.915: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
Jun 22 11:31:32.905: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:32.905: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:32.906: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:32.911: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 22 11:31:32.911: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Jun 22 11:31:32.911: INFO: Update the DaemonSet to trigger a rollout
W0622 11:31:32.926837      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:31:32.926: INFO: Updating DaemonSet daemon-set
Jun 22 11:31:35.952: INFO: Roll back the DaemonSet before rollout is complete
W0622 11:31:35.964189      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:31:35.964: INFO: Updating DaemonSet daemon-set
Jun 22 11:31:35.964: INFO: Make sure DaemonSet rollback is complete
Jun 22 11:31:35.969: INFO: Wrong image for pod: daemon-set-wxjzt. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Jun 22 11:31:35.969: INFO: Pod daemon-set-wxjzt is not available
Jun 22 11:31:35.976: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:35.976: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:35.976: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:36.992: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:36.992: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:36.992: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:37.990: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:37.991: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:37.991: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:38.984: INFO: Pod daemon-set-27lfv is not available
Jun 22 11:31:39.000: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:39.000: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:31:39.000: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 06/22/23 11:31:39.011
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2628, will wait for the garbage collector to delete the pods 06/22/23 11:31:39.012
Jun 22 11:31:39.080: INFO: Deleting DaemonSet.extensions daemon-set took: 12.168622ms
Jun 22 11:31:39.182: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.240866ms
Jun 22 11:31:40.688: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 11:31:40.688: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 22 11:31:40.693: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"156207"},"items":null}

Jun 22 11:31:40.698: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"156207"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:31:40.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2628" for this suite. 06/22/23 11:31:40.728
------------------------------
• [SLOW TEST] [9.935 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:31:30.801
    Jun 22 11:31:30.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename daemonsets 06/22/23 11:31:30.803
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:31:30.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:31:30.826
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Jun 22 11:31:30.873: INFO: Create a RollingUpdate DaemonSet
    W0622 11:31:30.881982      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:31:30.882: INFO: Check that daemon pods launch on every node of the cluster
    Jun 22 11:31:30.891: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:30.891: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:30.891: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:30.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 11:31:30.896: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
    Jun 22 11:31:31.909: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:31.910: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:31.910: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:31.915: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 11:31:31.915: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
    Jun 22 11:31:32.905: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:32.905: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:32.906: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:32.911: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 22 11:31:32.911: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Jun 22 11:31:32.911: INFO: Update the DaemonSet to trigger a rollout
    W0622 11:31:32.926837      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:31:32.926: INFO: Updating DaemonSet daemon-set
    Jun 22 11:31:35.952: INFO: Roll back the DaemonSet before rollout is complete
    W0622 11:31:35.964189      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:31:35.964: INFO: Updating DaemonSet daemon-set
    Jun 22 11:31:35.964: INFO: Make sure DaemonSet rollback is complete
    Jun 22 11:31:35.969: INFO: Wrong image for pod: daemon-set-wxjzt. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Jun 22 11:31:35.969: INFO: Pod daemon-set-wxjzt is not available
    Jun 22 11:31:35.976: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:35.976: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:35.976: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:36.992: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:36.992: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:36.992: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:37.990: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:37.991: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:37.991: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:38.984: INFO: Pod daemon-set-27lfv is not available
    Jun 22 11:31:39.000: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:39.000: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:31:39.000: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 06/22/23 11:31:39.011
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2628, will wait for the garbage collector to delete the pods 06/22/23 11:31:39.012
    Jun 22 11:31:39.080: INFO: Deleting DaemonSet.extensions daemon-set took: 12.168622ms
    Jun 22 11:31:39.182: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.240866ms
    Jun 22 11:31:40.688: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 11:31:40.688: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 22 11:31:40.693: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"156207"},"items":null}

    Jun 22 11:31:40.698: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"156207"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:31:40.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2628" for this suite. 06/22/23 11:31:40.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:31:40.738
Jun 22 11:31:40.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename resourcequota 06/22/23 11:31:40.739
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:31:40.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:31:40.769
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 06/22/23 11:31:40.773
STEP: Getting a ResourceQuota 06/22/23 11:31:40.781
STEP: Updating a ResourceQuota 06/22/23 11:31:40.786
STEP: Verifying a ResourceQuota was modified 06/22/23 11:31:40.796
STEP: Deleting a ResourceQuota 06/22/23 11:31:40.803
STEP: Verifying the deleted ResourceQuota 06/22/23 11:31:40.811
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun 22 11:31:40.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1772" for this suite. 06/22/23 11:31:40.823
------------------------------
• [0.096 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:31:40.738
    Jun 22 11:31:40.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename resourcequota 06/22/23 11:31:40.739
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:31:40.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:31:40.769
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 06/22/23 11:31:40.773
    STEP: Getting a ResourceQuota 06/22/23 11:31:40.781
    STEP: Updating a ResourceQuota 06/22/23 11:31:40.786
    STEP: Verifying a ResourceQuota was modified 06/22/23 11:31:40.796
    STEP: Deleting a ResourceQuota 06/22/23 11:31:40.803
    STEP: Verifying the deleted ResourceQuota 06/22/23 11:31:40.811
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:31:40.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1772" for this suite. 06/22/23 11:31:40.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:31:40.837
Jun 22 11:31:40.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 11:31:40.838
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:31:40.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:31:40.865
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-13ef0361-00a5-406c-888d-38fc16d72e68 06/22/23 11:31:40.869
STEP: Creating a pod to test consume configMaps 06/22/23 11:31:40.876
W0622 11:31:40.891992      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:31:40.892: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e91647b2-66f7-4e39-bfb4-2e49ab7a068f" in namespace "projected-2661" to be "Succeeded or Failed"
Jun 22 11:31:40.899: INFO: Pod "pod-projected-configmaps-e91647b2-66f7-4e39-bfb4-2e49ab7a068f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.580244ms
Jun 22 11:31:42.905: INFO: Pod "pod-projected-configmaps-e91647b2-66f7-4e39-bfb4-2e49ab7a068f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013293732s
Jun 22 11:31:44.908: INFO: Pod "pod-projected-configmaps-e91647b2-66f7-4e39-bfb4-2e49ab7a068f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016257963s
STEP: Saw pod success 06/22/23 11:31:44.908
Jun 22 11:31:44.908: INFO: Pod "pod-projected-configmaps-e91647b2-66f7-4e39-bfb4-2e49ab7a068f" satisfied condition "Succeeded or Failed"
Jun 22 11:31:44.914: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-configmaps-e91647b2-66f7-4e39-bfb4-2e49ab7a068f container agnhost-container: <nil>
STEP: delete the pod 06/22/23 11:31:44.923
Jun 22 11:31:44.941: INFO: Waiting for pod pod-projected-configmaps-e91647b2-66f7-4e39-bfb4-2e49ab7a068f to disappear
Jun 22 11:31:44.945: INFO: Pod pod-projected-configmaps-e91647b2-66f7-4e39-bfb4-2e49ab7a068f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun 22 11:31:44.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2661" for this suite. 06/22/23 11:31:44.954
------------------------------
• [4.126 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:31:40.837
    Jun 22 11:31:40.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 11:31:40.838
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:31:40.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:31:40.865
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-13ef0361-00a5-406c-888d-38fc16d72e68 06/22/23 11:31:40.869
    STEP: Creating a pod to test consume configMaps 06/22/23 11:31:40.876
    W0622 11:31:40.891992      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:31:40.892: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e91647b2-66f7-4e39-bfb4-2e49ab7a068f" in namespace "projected-2661" to be "Succeeded or Failed"
    Jun 22 11:31:40.899: INFO: Pod "pod-projected-configmaps-e91647b2-66f7-4e39-bfb4-2e49ab7a068f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.580244ms
    Jun 22 11:31:42.905: INFO: Pod "pod-projected-configmaps-e91647b2-66f7-4e39-bfb4-2e49ab7a068f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013293732s
    Jun 22 11:31:44.908: INFO: Pod "pod-projected-configmaps-e91647b2-66f7-4e39-bfb4-2e49ab7a068f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016257963s
    STEP: Saw pod success 06/22/23 11:31:44.908
    Jun 22 11:31:44.908: INFO: Pod "pod-projected-configmaps-e91647b2-66f7-4e39-bfb4-2e49ab7a068f" satisfied condition "Succeeded or Failed"
    Jun 22 11:31:44.914: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-configmaps-e91647b2-66f7-4e39-bfb4-2e49ab7a068f container agnhost-container: <nil>
    STEP: delete the pod 06/22/23 11:31:44.923
    Jun 22 11:31:44.941: INFO: Waiting for pod pod-projected-configmaps-e91647b2-66f7-4e39-bfb4-2e49ab7a068f to disappear
    Jun 22 11:31:44.945: INFO: Pod pod-projected-configmaps-e91647b2-66f7-4e39-bfb4-2e49ab7a068f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:31:44.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2661" for this suite. 06/22/23 11:31:44.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:31:44.965
Jun 22 11:31:44.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename statefulset 06/22/23 11:31:44.966
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:31:44.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:31:44.989
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2003 06/22/23 11:31:44.993
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-2003 06/22/23 11:31:45.009
W0622 11:31:45.019143      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:31:45.024: INFO: Found 0 stateful pods, waiting for 1
Jun 22 11:31:55.034: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 06/22/23 11:31:55.049
W0622 11:31:55.061348      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Getting /status 06/22/23 11:31:55.061
Jun 22 11:31:55.072: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 06/22/23 11:31:55.072
Jun 22 11:31:55.091: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 06/22/23 11:31:55.091
Jun 22 11:31:55.096: INFO: Observed &StatefulSet event: ADDED
Jun 22 11:31:55.096: INFO: Found Statefulset ss in namespace statefulset-2003 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 22 11:31:55.096: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 06/22/23 11:31:55.096
Jun 22 11:31:55.096: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun 22 11:31:55.110: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 06/22/23 11:31:55.11
Jun 22 11:31:55.113: INFO: Observed &StatefulSet event: ADDED
Jun 22 11:31:55.114: INFO: Observed Statefulset ss in namespace statefulset-2003 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 22 11:31:55.114: INFO: Observed &StatefulSet event: MODIFIED
Jun 22 11:31:55.114: INFO: Found Statefulset ss in namespace statefulset-2003 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun 22 11:31:55.114: INFO: Deleting all statefulset in ns statefulset-2003
Jun 22 11:31:55.120: INFO: Scaling statefulset ss to 0
W0622 11:31:55.136834      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:32:05.159: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 11:32:05.168: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun 22 11:32:05.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2003" for this suite. 06/22/23 11:32:05.206
------------------------------
• [SLOW TEST] [20.253 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:31:44.965
    Jun 22 11:31:44.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename statefulset 06/22/23 11:31:44.966
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:31:44.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:31:44.989
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2003 06/22/23 11:31:44.993
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-2003 06/22/23 11:31:45.009
    W0622 11:31:45.019143      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:31:45.024: INFO: Found 0 stateful pods, waiting for 1
    Jun 22 11:31:55.034: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 06/22/23 11:31:55.049
    W0622 11:31:55.061348      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Getting /status 06/22/23 11:31:55.061
    Jun 22 11:31:55.072: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 06/22/23 11:31:55.072
    Jun 22 11:31:55.091: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 06/22/23 11:31:55.091
    Jun 22 11:31:55.096: INFO: Observed &StatefulSet event: ADDED
    Jun 22 11:31:55.096: INFO: Found Statefulset ss in namespace statefulset-2003 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun 22 11:31:55.096: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 06/22/23 11:31:55.096
    Jun 22 11:31:55.096: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jun 22 11:31:55.110: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 06/22/23 11:31:55.11
    Jun 22 11:31:55.113: INFO: Observed &StatefulSet event: ADDED
    Jun 22 11:31:55.114: INFO: Observed Statefulset ss in namespace statefulset-2003 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun 22 11:31:55.114: INFO: Observed &StatefulSet event: MODIFIED
    Jun 22 11:31:55.114: INFO: Found Statefulset ss in namespace statefulset-2003 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun 22 11:31:55.114: INFO: Deleting all statefulset in ns statefulset-2003
    Jun 22 11:31:55.120: INFO: Scaling statefulset ss to 0
    W0622 11:31:55.136834      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:32:05.159: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 22 11:32:05.168: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:32:05.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2003" for this suite. 06/22/23 11:32:05.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:32:05.23
Jun 22 11:32:05.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename var-expansion 06/22/23 11:32:05.24
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:32:05.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:32:05.283
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 06/22/23 11:32:05.289
W0622 11:32:05.304520      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:32:05.305: INFO: Waiting up to 2m0s for pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2" in namespace "var-expansion-5528" to be "running"
Jun 22 11:32:05.310: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.997173ms
Jun 22 11:32:07.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012521511s
Jun 22 11:32:09.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012031267s
Jun 22 11:32:11.320: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015278604s
Jun 22 11:32:13.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012718504s
Jun 22 11:32:15.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012284891s
Jun 22 11:32:17.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012955213s
Jun 22 11:32:19.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012284542s
Jun 22 11:32:21.319: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013950471s
Jun 22 11:32:23.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.012577383s
Jun 22 11:32:25.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.013328169s
Jun 22 11:32:27.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.013515696s
Jun 22 11:32:29.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.012491177s
Jun 22 11:32:31.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012719498s
Jun 22 11:32:33.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 28.012728915s
Jun 22 11:32:35.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012966648s
Jun 22 11:32:37.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 32.013440161s
Jun 22 11:32:39.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 34.012202389s
Jun 22 11:32:41.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 36.011920781s
Jun 22 11:32:43.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 38.012761664s
Jun 22 11:32:45.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 40.01264082s
Jun 22 11:32:47.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012389639s
Jun 22 11:32:49.316: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 44.011494639s
Jun 22 11:32:51.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 46.013270186s
Jun 22 11:32:53.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 48.013296993s
Jun 22 11:32:55.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 50.013287155s
Jun 22 11:32:57.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 52.013570582s
Jun 22 11:32:59.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 54.011944394s
Jun 22 11:33:01.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 56.012567313s
Jun 22 11:33:03.319: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 58.014089525s
Jun 22 11:33:05.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.012871334s
Jun 22 11:33:07.320: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.014750671s
Jun 22 11:33:09.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.012739753s
Jun 22 11:33:11.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011780922s
Jun 22 11:33:13.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.0132749s
Jun 22 11:33:15.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.011720239s
Jun 22 11:33:17.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012836829s
Jun 22 11:33:19.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.011954491s
Jun 22 11:33:21.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011992098s
Jun 22 11:33:23.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.011920242s
Jun 22 11:33:25.326: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.020836914s
Jun 22 11:33:27.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.012664377s
Jun 22 11:33:29.319: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.014142753s
Jun 22 11:33:31.319: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.014176809s
Jun 22 11:33:33.319: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.01409717s
Jun 22 11:33:35.319: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.014131243s
Jun 22 11:33:37.320: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.015481552s
Jun 22 11:33:39.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.013624997s
Jun 22 11:33:41.319: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.0143168s
Jun 22 11:33:43.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.013336345s
Jun 22 11:33:45.319: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.013796442s
Jun 22 11:33:47.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.013098174s
Jun 22 11:33:49.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.013468666s
Jun 22 11:33:51.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.013080927s
Jun 22 11:33:53.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.013536968s
Jun 22 11:33:55.319: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.014170669s
Jun 22 11:33:57.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.013638842s
Jun 22 11:33:59.316: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011445972s
Jun 22 11:34:01.334: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.029352603s
Jun 22 11:34:03.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.013533553s
Jun 22 11:34:05.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.012905663s
Jun 22 11:34:05.323: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.018484691s
STEP: updating the pod 06/22/23 11:34:05.324
Jun 22 11:34:05.852: INFO: Successfully updated pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2"
STEP: waiting for pod running 06/22/23 11:34:05.853
Jun 22 11:34:05.853: INFO: Waiting up to 2m0s for pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2" in namespace "var-expansion-5528" to be "running"
Jun 22 11:34:05.860: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.272055ms
Jun 22 11:34:07.867: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013821503s
Jun 22 11:34:07.867: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2" satisfied condition "running"
STEP: deleting the pod gracefully 06/22/23 11:34:07.867
Jun 22 11:34:07.867: INFO: Deleting pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2" in namespace "var-expansion-5528"
Jun 22 11:34:07.879: INFO: Wait up to 5m0s for pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun 22 11:34:39.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5528" for this suite. 06/22/23 11:34:39.903
------------------------------
• [SLOW TEST] [154.684 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:32:05.23
    Jun 22 11:32:05.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename var-expansion 06/22/23 11:32:05.24
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:32:05.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:32:05.283
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 06/22/23 11:32:05.289
    W0622 11:32:05.304520      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:32:05.305: INFO: Waiting up to 2m0s for pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2" in namespace "var-expansion-5528" to be "running"
    Jun 22 11:32:05.310: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.997173ms
    Jun 22 11:32:07.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012521511s
    Jun 22 11:32:09.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012031267s
    Jun 22 11:32:11.320: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015278604s
    Jun 22 11:32:13.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.012718504s
    Jun 22 11:32:15.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012284891s
    Jun 22 11:32:17.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.012955213s
    Jun 22 11:32:19.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.012284542s
    Jun 22 11:32:21.319: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013950471s
    Jun 22 11:32:23.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.012577383s
    Jun 22 11:32:25.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.013328169s
    Jun 22 11:32:27.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.013515696s
    Jun 22 11:32:29.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.012491177s
    Jun 22 11:32:31.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 26.012719498s
    Jun 22 11:32:33.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 28.012728915s
    Jun 22 11:32:35.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012966648s
    Jun 22 11:32:37.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 32.013440161s
    Jun 22 11:32:39.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 34.012202389s
    Jun 22 11:32:41.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 36.011920781s
    Jun 22 11:32:43.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 38.012761664s
    Jun 22 11:32:45.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 40.01264082s
    Jun 22 11:32:47.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012389639s
    Jun 22 11:32:49.316: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 44.011494639s
    Jun 22 11:32:51.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 46.013270186s
    Jun 22 11:32:53.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 48.013296993s
    Jun 22 11:32:55.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 50.013287155s
    Jun 22 11:32:57.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 52.013570582s
    Jun 22 11:32:59.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 54.011944394s
    Jun 22 11:33:01.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 56.012567313s
    Jun 22 11:33:03.319: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 58.014089525s
    Jun 22 11:33:05.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.012871334s
    Jun 22 11:33:07.320: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.014750671s
    Jun 22 11:33:09.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.012739753s
    Jun 22 11:33:11.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.011780922s
    Jun 22 11:33:13.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.0132749s
    Jun 22 11:33:15.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.011720239s
    Jun 22 11:33:17.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.012836829s
    Jun 22 11:33:19.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.011954491s
    Jun 22 11:33:21.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011992098s
    Jun 22 11:33:23.317: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.011920242s
    Jun 22 11:33:25.326: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.020836914s
    Jun 22 11:33:27.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.012664377s
    Jun 22 11:33:29.319: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.014142753s
    Jun 22 11:33:31.319: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.014176809s
    Jun 22 11:33:33.319: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.01409717s
    Jun 22 11:33:35.319: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.014131243s
    Jun 22 11:33:37.320: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.015481552s
    Jun 22 11:33:39.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.013624997s
    Jun 22 11:33:41.319: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.0143168s
    Jun 22 11:33:43.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.013336345s
    Jun 22 11:33:45.319: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.013796442s
    Jun 22 11:33:47.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.013098174s
    Jun 22 11:33:49.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.013468666s
    Jun 22 11:33:51.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.013080927s
    Jun 22 11:33:53.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.013536968s
    Jun 22 11:33:55.319: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.014170669s
    Jun 22 11:33:57.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.013638842s
    Jun 22 11:33:59.316: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.011445972s
    Jun 22 11:34:01.334: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.029352603s
    Jun 22 11:34:03.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.013533553s
    Jun 22 11:34:05.318: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.012905663s
    Jun 22 11:34:05.323: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.018484691s
    STEP: updating the pod 06/22/23 11:34:05.324
    Jun 22 11:34:05.852: INFO: Successfully updated pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2"
    STEP: waiting for pod running 06/22/23 11:34:05.853
    Jun 22 11:34:05.853: INFO: Waiting up to 2m0s for pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2" in namespace "var-expansion-5528" to be "running"
    Jun 22 11:34:05.860: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.272055ms
    Jun 22 11:34:07.867: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2": Phase="Running", Reason="", readiness=true. Elapsed: 2.013821503s
    Jun 22 11:34:07.867: INFO: Pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2" satisfied condition "running"
    STEP: deleting the pod gracefully 06/22/23 11:34:07.867
    Jun 22 11:34:07.867: INFO: Deleting pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2" in namespace "var-expansion-5528"
    Jun 22 11:34:07.879: INFO: Wait up to 5m0s for pod "var-expansion-b77d610a-95b5-47d4-a843-9dead1fd33e2" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:34:39.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5528" for this suite. 06/22/23 11:34:39.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:34:39.916
Jun 22 11:34:39.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename statefulset 06/22/23 11:34:39.918
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:34:39.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:34:39.947
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4873 06/22/23 11:34:39.952
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 06/22/23 11:34:39.963
STEP: Creating stateful set ss in namespace statefulset-4873 06/22/23 11:34:39.972
W0622 11:34:39.984780      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4873 06/22/23 11:34:39.984
Jun 22 11:34:39.990: INFO: Found 0 stateful pods, waiting for 1
Jun 22 11:34:49.998: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 06/22/23 11:34:49.998
Jun 22 11:34:50.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4873 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 11:34:50.272: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 11:34:50.272: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 11:34:50.272: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 11:34:50.278: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 22 11:35:00.290: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 11:35:00.290: INFO: Waiting for statefulset status.replicas updated to 0
W0622 11:35:00.312942      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:35:00.318: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999847s
Jun 22 11:35:01.325: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994413643s
Jun 22 11:35:02.332: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.986240171s
Jun 22 11:35:03.339: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.979814907s
Jun 22 11:35:04.347: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.972291874s
Jun 22 11:35:05.355: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.965620813s
Jun 22 11:35:06.362: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.957259363s
Jun 22 11:35:07.369: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.950067155s
Jun 22 11:35:08.376: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.943356162s
Jun 22 11:35:09.384: INFO: Verifying statefulset ss doesn't scale past 1 for another 935.081251ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4873 06/22/23 11:35:10.385
Jun 22 11:35:10.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4873 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 11:35:10.616: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 11:35:10.616: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 11:35:10.616: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 11:35:10.622: INFO: Found 1 stateful pods, waiting for 3
Jun 22 11:35:20.630: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 11:35:20.630: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 11:35:20.630: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 06/22/23 11:35:20.631
STEP: Scale down will halt with unhealthy stateful pod 06/22/23 11:35:20.631
Jun 22 11:35:20.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4873 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 11:35:20.823: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 11:35:20.823: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 11:35:20.823: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 11:35:20.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4873 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 11:35:21.035: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 11:35:21.035: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 11:35:21.035: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 11:35:21.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4873 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 11:35:21.222: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 11:35:21.222: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 11:35:21.222: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 22 11:35:21.222: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 11:35:21.227: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun 22 11:35:31.261: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 11:35:31.261: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 11:35:31.261: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
W0622 11:35:31.276901      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:35:31.284: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999722s
Jun 22 11:35:32.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992659407s
Jun 22 11:35:33.300: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984457587s
Jun 22 11:35:34.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975625557s
Jun 22 11:35:35.319: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.966194278s
Jun 22 11:35:36.328: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.957645729s
Jun 22 11:35:37.335: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.948833338s
Jun 22 11:35:38.343: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.941181798s
Jun 22 11:35:39.351: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.932886189s
Jun 22 11:35:40.358: INFO: Verifying statefulset ss doesn't scale past 3 for another 925.464821ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4873 06/22/23 11:35:41.359
Jun 22 11:35:41.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4873 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 11:35:41.626: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 11:35:41.626: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 11:35:41.626: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 11:35:41.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4873 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 11:35:41.837: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 11:35:41.837: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 11:35:41.837: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 11:35:41.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4873 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 11:35:42.023: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 11:35:42.024: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 11:35:42.024: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 22 11:35:42.024: INFO: Scaling statefulset ss to 0
W0622 11:35:42.037308      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Verifying that stateful set ss was scaled down in reverse order 06/22/23 11:35:52.053
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun 22 11:35:52.054: INFO: Deleting all statefulset in ns statefulset-4873
Jun 22 11:35:52.060: INFO: Scaling statefulset ss to 0
W0622 11:35:52.071043      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:35:52.076: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 11:35:52.081: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun 22 11:35:52.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4873" for this suite. 06/22/23 11:35:52.11
------------------------------
• [SLOW TEST] [72.205 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:34:39.916
    Jun 22 11:34:39.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename statefulset 06/22/23 11:34:39.918
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:34:39.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:34:39.947
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4873 06/22/23 11:34:39.952
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 06/22/23 11:34:39.963
    STEP: Creating stateful set ss in namespace statefulset-4873 06/22/23 11:34:39.972
    W0622 11:34:39.984780      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4873 06/22/23 11:34:39.984
    Jun 22 11:34:39.990: INFO: Found 0 stateful pods, waiting for 1
    Jun 22 11:34:49.998: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 06/22/23 11:34:49.998
    Jun 22 11:34:50.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4873 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 22 11:34:50.272: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 22 11:34:50.272: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 22 11:34:50.272: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 22 11:34:50.278: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jun 22 11:35:00.290: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun 22 11:35:00.290: INFO: Waiting for statefulset status.replicas updated to 0
    W0622 11:35:00.312942      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:35:00.318: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999847s
    Jun 22 11:35:01.325: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994413643s
    Jun 22 11:35:02.332: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.986240171s
    Jun 22 11:35:03.339: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.979814907s
    Jun 22 11:35:04.347: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.972291874s
    Jun 22 11:35:05.355: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.965620813s
    Jun 22 11:35:06.362: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.957259363s
    Jun 22 11:35:07.369: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.950067155s
    Jun 22 11:35:08.376: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.943356162s
    Jun 22 11:35:09.384: INFO: Verifying statefulset ss doesn't scale past 1 for another 935.081251ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4873 06/22/23 11:35:10.385
    Jun 22 11:35:10.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4873 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 22 11:35:10.616: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 22 11:35:10.616: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 22 11:35:10.616: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 22 11:35:10.622: INFO: Found 1 stateful pods, waiting for 3
    Jun 22 11:35:20.630: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 22 11:35:20.630: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 22 11:35:20.630: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 06/22/23 11:35:20.631
    STEP: Scale down will halt with unhealthy stateful pod 06/22/23 11:35:20.631
    Jun 22 11:35:20.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4873 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 22 11:35:20.823: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 22 11:35:20.823: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 22 11:35:20.823: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 22 11:35:20.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4873 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 22 11:35:21.035: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 22 11:35:21.035: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 22 11:35:21.035: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 22 11:35:21.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4873 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 22 11:35:21.222: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 22 11:35:21.222: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 22 11:35:21.222: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 22 11:35:21.222: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 22 11:35:21.227: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jun 22 11:35:31.261: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun 22 11:35:31.261: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jun 22 11:35:31.261: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    W0622 11:35:31.276901      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:35:31.284: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999722s
    Jun 22 11:35:32.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992659407s
    Jun 22 11:35:33.300: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984457587s
    Jun 22 11:35:34.310: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.975625557s
    Jun 22 11:35:35.319: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.966194278s
    Jun 22 11:35:36.328: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.957645729s
    Jun 22 11:35:37.335: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.948833338s
    Jun 22 11:35:38.343: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.941181798s
    Jun 22 11:35:39.351: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.932886189s
    Jun 22 11:35:40.358: INFO: Verifying statefulset ss doesn't scale past 3 for another 925.464821ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4873 06/22/23 11:35:41.359
    Jun 22 11:35:41.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4873 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 22 11:35:41.626: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 22 11:35:41.626: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 22 11:35:41.626: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 22 11:35:41.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4873 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 22 11:35:41.837: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 22 11:35:41.837: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 22 11:35:41.837: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 22 11:35:41.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-4873 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 22 11:35:42.023: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 22 11:35:42.024: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 22 11:35:42.024: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 22 11:35:42.024: INFO: Scaling statefulset ss to 0
    W0622 11:35:42.037308      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Verifying that stateful set ss was scaled down in reverse order 06/22/23 11:35:52.053
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun 22 11:35:52.054: INFO: Deleting all statefulset in ns statefulset-4873
    Jun 22 11:35:52.060: INFO: Scaling statefulset ss to 0
    W0622 11:35:52.071043      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:35:52.076: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 22 11:35:52.081: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:35:52.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4873" for this suite. 06/22/23 11:35:52.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:35:52.127
Jun 22 11:35:52.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename endpointslice 06/22/23 11:35:52.134
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:35:52.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:35:52.168
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 06/22/23 11:35:52.172
STEP: getting /apis/discovery.k8s.io 06/22/23 11:35:52.177
STEP: getting /apis/discovery.k8s.iov1 06/22/23 11:35:52.179
STEP: creating 06/22/23 11:35:52.181
STEP: getting 06/22/23 11:35:52.202
STEP: listing 06/22/23 11:35:52.207
STEP: watching 06/22/23 11:35:52.212
Jun 22 11:35:52.213: INFO: starting watch
STEP: cluster-wide listing 06/22/23 11:35:52.214
STEP: cluster-wide watching 06/22/23 11:35:52.22
Jun 22 11:35:52.220: INFO: starting watch
STEP: patching 06/22/23 11:35:52.222
STEP: updating 06/22/23 11:35:52.233
Jun 22 11:35:52.246: INFO: waiting for watch events with expected annotations
Jun 22 11:35:52.246: INFO: saw patched and updated annotations
STEP: deleting 06/22/23 11:35:52.246
STEP: deleting a collection 06/22/23 11:35:52.264
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jun 22 11:35:52.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2815" for this suite. 06/22/23 11:35:52.293
------------------------------
• [0.175 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:35:52.127
    Jun 22 11:35:52.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename endpointslice 06/22/23 11:35:52.134
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:35:52.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:35:52.168
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 06/22/23 11:35:52.172
    STEP: getting /apis/discovery.k8s.io 06/22/23 11:35:52.177
    STEP: getting /apis/discovery.k8s.iov1 06/22/23 11:35:52.179
    STEP: creating 06/22/23 11:35:52.181
    STEP: getting 06/22/23 11:35:52.202
    STEP: listing 06/22/23 11:35:52.207
    STEP: watching 06/22/23 11:35:52.212
    Jun 22 11:35:52.213: INFO: starting watch
    STEP: cluster-wide listing 06/22/23 11:35:52.214
    STEP: cluster-wide watching 06/22/23 11:35:52.22
    Jun 22 11:35:52.220: INFO: starting watch
    STEP: patching 06/22/23 11:35:52.222
    STEP: updating 06/22/23 11:35:52.233
    Jun 22 11:35:52.246: INFO: waiting for watch events with expected annotations
    Jun 22 11:35:52.246: INFO: saw patched and updated annotations
    STEP: deleting 06/22/23 11:35:52.246
    STEP: deleting a collection 06/22/23 11:35:52.264
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:35:52.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2815" for this suite. 06/22/23 11:35:52.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:35:52.304
Jun 22 11:35:52.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename services 06/22/23 11:35:52.306
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:35:52.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:35:52.332
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8617 06/22/23 11:35:52.336
STEP: changing the ExternalName service to type=NodePort 06/22/23 11:35:52.343
STEP: creating replication controller externalname-service in namespace services-8617 06/22/23 11:35:52.379
W0622 11:35:52.389713      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "externalname-service" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "externalname-service" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "externalname-service" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "externalname-service" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0622 11:35:52.390107      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8617, replica count: 2
I0622 11:35:55.441500      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 11:35:55.441: INFO: Creating new exec pod
W0622 11:35:55.457220      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:35:55.457: INFO: Waiting up to 5m0s for pod "execpod8p5vr" in namespace "services-8617" to be "running"
Jun 22 11:35:55.463: INFO: Pod "execpod8p5vr": Phase="Pending", Reason="", readiness=false. Elapsed: 5.57177ms
Jun 22 11:35:57.470: INFO: Pod "execpod8p5vr": Phase="Running", Reason="", readiness=true. Elapsed: 2.012728383s
Jun 22 11:35:57.470: INFO: Pod "execpod8p5vr" satisfied condition "running"
Jun 22 11:35:58.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-8617 exec execpod8p5vr -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jun 22 11:35:58.717: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 22 11:35:58.717: INFO: stdout: ""
Jun 22 11:35:58.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-8617 exec execpod8p5vr -- /bin/sh -x -c nc -v -z -w 2 100.69.89.45 80'
Jun 22 11:35:58.894: INFO: stderr: "+ nc -v -z -w 2 100.69.89.45 80\nConnection to 100.69.89.45 80 port [tcp/http] succeeded!\n"
Jun 22 11:35:58.894: INFO: stdout: ""
Jun 22 11:35:58.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-8617 exec execpod8p5vr -- /bin/sh -x -c nc -v -z -w 2 10.92.226.162 31057'
Jun 22 11:35:59.084: INFO: stderr: "+ nc -v -z -w 2 10.92.226.162 31057\nConnection to 10.92.226.162 31057 port [tcp/*] succeeded!\n"
Jun 22 11:35:59.084: INFO: stdout: ""
Jun 22 11:35:59.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-8617 exec execpod8p5vr -- /bin/sh -x -c nc -v -z -w 2 10.92.224.179 31057'
Jun 22 11:35:59.280: INFO: stderr: "+ nc -v -z -w 2 10.92.224.179 31057\nConnection to 10.92.224.179 31057 port [tcp/*] succeeded!\n"
Jun 22 11:35:59.280: INFO: stdout: ""
Jun 22 11:35:59.280: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun 22 11:35:59.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8617" for this suite. 06/22/23 11:35:59.342
------------------------------
• [SLOW TEST] [7.055 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:35:52.304
    Jun 22 11:35:52.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename services 06/22/23 11:35:52.306
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:35:52.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:35:52.332
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-8617 06/22/23 11:35:52.336
    STEP: changing the ExternalName service to type=NodePort 06/22/23 11:35:52.343
    STEP: creating replication controller externalname-service in namespace services-8617 06/22/23 11:35:52.379
    W0622 11:35:52.389713      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "externalname-service" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "externalname-service" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "externalname-service" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "externalname-service" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0622 11:35:52.390107      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8617, replica count: 2
    I0622 11:35:55.441500      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 22 11:35:55.441: INFO: Creating new exec pod
    W0622 11:35:55.457220      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:35:55.457: INFO: Waiting up to 5m0s for pod "execpod8p5vr" in namespace "services-8617" to be "running"
    Jun 22 11:35:55.463: INFO: Pod "execpod8p5vr": Phase="Pending", Reason="", readiness=false. Elapsed: 5.57177ms
    Jun 22 11:35:57.470: INFO: Pod "execpod8p5vr": Phase="Running", Reason="", readiness=true. Elapsed: 2.012728383s
    Jun 22 11:35:57.470: INFO: Pod "execpod8p5vr" satisfied condition "running"
    Jun 22 11:35:58.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-8617 exec execpod8p5vr -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jun 22 11:35:58.717: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 22 11:35:58.717: INFO: stdout: ""
    Jun 22 11:35:58.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-8617 exec execpod8p5vr -- /bin/sh -x -c nc -v -z -w 2 100.69.89.45 80'
    Jun 22 11:35:58.894: INFO: stderr: "+ nc -v -z -w 2 100.69.89.45 80\nConnection to 100.69.89.45 80 port [tcp/http] succeeded!\n"
    Jun 22 11:35:58.894: INFO: stdout: ""
    Jun 22 11:35:58.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-8617 exec execpod8p5vr -- /bin/sh -x -c nc -v -z -w 2 10.92.226.162 31057'
    Jun 22 11:35:59.084: INFO: stderr: "+ nc -v -z -w 2 10.92.226.162 31057\nConnection to 10.92.226.162 31057 port [tcp/*] succeeded!\n"
    Jun 22 11:35:59.084: INFO: stdout: ""
    Jun 22 11:35:59.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-8617 exec execpod8p5vr -- /bin/sh -x -c nc -v -z -w 2 10.92.224.179 31057'
    Jun 22 11:35:59.280: INFO: stderr: "+ nc -v -z -w 2 10.92.224.179 31057\nConnection to 10.92.224.179 31057 port [tcp/*] succeeded!\n"
    Jun 22 11:35:59.280: INFO: stdout: ""
    Jun 22 11:35:59.280: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:35:59.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8617" for this suite. 06/22/23 11:35:59.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:35:59.362
Jun 22 11:35:59.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename downward-api 06/22/23 11:35:59.363
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:35:59.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:35:59.4
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 06/22/23 11:35:59.404
W0622 11:35:59.423134      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:35:59.423: INFO: Waiting up to 5m0s for pod "downward-api-55775d2c-1d8e-4bcc-85ce-d38d7734abc5" in namespace "downward-api-5625" to be "Succeeded or Failed"
Jun 22 11:35:59.428: INFO: Pod "downward-api-55775d2c-1d8e-4bcc-85ce-d38d7734abc5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.543126ms
Jun 22 11:36:01.436: INFO: Pod "downward-api-55775d2c-1d8e-4bcc-85ce-d38d7734abc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013021701s
Jun 22 11:36:03.436: INFO: Pod "downward-api-55775d2c-1d8e-4bcc-85ce-d38d7734abc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01324368s
STEP: Saw pod success 06/22/23 11:36:03.436
Jun 22 11:36:03.437: INFO: Pod "downward-api-55775d2c-1d8e-4bcc-85ce-d38d7734abc5" satisfied condition "Succeeded or Failed"
Jun 22 11:36:03.441: INFO: Trying to get logs from node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 pod downward-api-55775d2c-1d8e-4bcc-85ce-d38d7734abc5 container dapi-container: <nil>
STEP: delete the pod 06/22/23 11:36:03.462
Jun 22 11:36:03.478: INFO: Waiting for pod downward-api-55775d2c-1d8e-4bcc-85ce-d38d7734abc5 to disappear
Jun 22 11:36:03.482: INFO: Pod downward-api-55775d2c-1d8e-4bcc-85ce-d38d7734abc5 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jun 22 11:36:03.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5625" for this suite. 06/22/23 11:36:03.491
------------------------------
• [4.139 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:35:59.362
    Jun 22 11:35:59.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename downward-api 06/22/23 11:35:59.363
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:35:59.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:35:59.4
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 06/22/23 11:35:59.404
    W0622 11:35:59.423134      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:35:59.423: INFO: Waiting up to 5m0s for pod "downward-api-55775d2c-1d8e-4bcc-85ce-d38d7734abc5" in namespace "downward-api-5625" to be "Succeeded or Failed"
    Jun 22 11:35:59.428: INFO: Pod "downward-api-55775d2c-1d8e-4bcc-85ce-d38d7734abc5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.543126ms
    Jun 22 11:36:01.436: INFO: Pod "downward-api-55775d2c-1d8e-4bcc-85ce-d38d7734abc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013021701s
    Jun 22 11:36:03.436: INFO: Pod "downward-api-55775d2c-1d8e-4bcc-85ce-d38d7734abc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01324368s
    STEP: Saw pod success 06/22/23 11:36:03.436
    Jun 22 11:36:03.437: INFO: Pod "downward-api-55775d2c-1d8e-4bcc-85ce-d38d7734abc5" satisfied condition "Succeeded or Failed"
    Jun 22 11:36:03.441: INFO: Trying to get logs from node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 pod downward-api-55775d2c-1d8e-4bcc-85ce-d38d7734abc5 container dapi-container: <nil>
    STEP: delete the pod 06/22/23 11:36:03.462
    Jun 22 11:36:03.478: INFO: Waiting for pod downward-api-55775d2c-1d8e-4bcc-85ce-d38d7734abc5 to disappear
    Jun 22 11:36:03.482: INFO: Pod downward-api-55775d2c-1d8e-4bcc-85ce-d38d7734abc5 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:36:03.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5625" for this suite. 06/22/23 11:36:03.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:36:03.502
Jun 22 11:36:03.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename webhook 06/22/23 11:36:03.504
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:03.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:03.528
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/22/23 11:36:03.549
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:36:03.965
STEP: Deploying the webhook pod 06/22/23 11:36:03.979
W0622 11:36:03.998592      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 11:36:03.998
Jun 22 11:36:04.016: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/22/23 11:36:06.034
STEP: Verifying the service has paired with the endpoint 06/22/23 11:36:06.06
Jun 22 11:36:07.061: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 06/22/23 11:36:07.067
STEP: Updating a mutating webhook configuration's rules to not include the create operation 06/22/23 11:36:07.1
STEP: Creating a configMap that should not be mutated 06/22/23 11:36:07.11
STEP: Patching a mutating webhook configuration's rules to include the create operation 06/22/23 11:36:07.125
STEP: Creating a configMap that should be mutated 06/22/23 11:36:07.136
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:36:07.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7744" for this suite. 06/22/23 11:36:07.245
STEP: Destroying namespace "webhook-7744-markers" for this suite. 06/22/23 11:36:07.258
------------------------------
• [3.765 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:36:03.502
    Jun 22 11:36:03.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename webhook 06/22/23 11:36:03.504
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:03.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:03.528
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/22/23 11:36:03.549
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:36:03.965
    STEP: Deploying the webhook pod 06/22/23 11:36:03.979
    W0622 11:36:03.998592      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 11:36:03.998
    Jun 22 11:36:04.016: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/22/23 11:36:06.034
    STEP: Verifying the service has paired with the endpoint 06/22/23 11:36:06.06
    Jun 22 11:36:07.061: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 06/22/23 11:36:07.067
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 06/22/23 11:36:07.1
    STEP: Creating a configMap that should not be mutated 06/22/23 11:36:07.11
    STEP: Patching a mutating webhook configuration's rules to include the create operation 06/22/23 11:36:07.125
    STEP: Creating a configMap that should be mutated 06/22/23 11:36:07.136
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:36:07.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7744" for this suite. 06/22/23 11:36:07.245
    STEP: Destroying namespace "webhook-7744-markers" for this suite. 06/22/23 11:36:07.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:36:07.269
Jun 22 11:36:07.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename namespaces 06/22/23 11:36:07.27
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:07.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:07.302
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 06/22/23 11:36:07.306
Jun 22 11:36:07.313: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 06/22/23 11:36:07.313
Jun 22 11:36:07.320: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 06/22/23 11:36:07.321
Jun 22 11:36:07.333: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:36:07.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1058" for this suite. 06/22/23 11:36:07.342
------------------------------
• [0.084 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:36:07.269
    Jun 22 11:36:07.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename namespaces 06/22/23 11:36:07.27
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:07.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:07.302
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 06/22/23 11:36:07.306
    Jun 22 11:36:07.313: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 06/22/23 11:36:07.313
    Jun 22 11:36:07.320: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 06/22/23 11:36:07.321
    Jun 22 11:36:07.333: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:36:07.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1058" for this suite. 06/22/23 11:36:07.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:36:07.354
Jun 22 11:36:07.354: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 11:36:07.356
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:07.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:07.38
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 06/22/23 11:36:07.384
W0622 11:36:07.397704      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:36:07.398: INFO: Waiting up to 5m0s for pod "downwardapi-volume-09380760-09a9-4539-abaa-5fe896bb5dba" in namespace "projected-2115" to be "Succeeded or Failed"
Jun 22 11:36:07.402: INFO: Pod "downwardapi-volume-09380760-09a9-4539-abaa-5fe896bb5dba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.591591ms
Jun 22 11:36:09.409: INFO: Pod "downwardapi-volume-09380760-09a9-4539-abaa-5fe896bb5dba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011069166s
Jun 22 11:36:11.410: INFO: Pod "downwardapi-volume-09380760-09a9-4539-abaa-5fe896bb5dba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012222434s
STEP: Saw pod success 06/22/23 11:36:11.41
Jun 22 11:36:11.410: INFO: Pod "downwardapi-volume-09380760-09a9-4539-abaa-5fe896bb5dba" satisfied condition "Succeeded or Failed"
Jun 22 11:36:11.415: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-09380760-09a9-4539-abaa-5fe896bb5dba container client-container: <nil>
STEP: delete the pod 06/22/23 11:36:11.432
Jun 22 11:36:11.450: INFO: Waiting for pod downwardapi-volume-09380760-09a9-4539-abaa-5fe896bb5dba to disappear
Jun 22 11:36:11.454: INFO: Pod downwardapi-volume-09380760-09a9-4539-abaa-5fe896bb5dba no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun 22 11:36:11.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2115" for this suite. 06/22/23 11:36:11.463
------------------------------
• [4.117 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:36:07.354
    Jun 22 11:36:07.354: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 11:36:07.356
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:07.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:07.38
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 06/22/23 11:36:07.384
    W0622 11:36:07.397704      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:36:07.398: INFO: Waiting up to 5m0s for pod "downwardapi-volume-09380760-09a9-4539-abaa-5fe896bb5dba" in namespace "projected-2115" to be "Succeeded or Failed"
    Jun 22 11:36:07.402: INFO: Pod "downwardapi-volume-09380760-09a9-4539-abaa-5fe896bb5dba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.591591ms
    Jun 22 11:36:09.409: INFO: Pod "downwardapi-volume-09380760-09a9-4539-abaa-5fe896bb5dba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011069166s
    Jun 22 11:36:11.410: INFO: Pod "downwardapi-volume-09380760-09a9-4539-abaa-5fe896bb5dba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012222434s
    STEP: Saw pod success 06/22/23 11:36:11.41
    Jun 22 11:36:11.410: INFO: Pod "downwardapi-volume-09380760-09a9-4539-abaa-5fe896bb5dba" satisfied condition "Succeeded or Failed"
    Jun 22 11:36:11.415: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-09380760-09a9-4539-abaa-5fe896bb5dba container client-container: <nil>
    STEP: delete the pod 06/22/23 11:36:11.432
    Jun 22 11:36:11.450: INFO: Waiting for pod downwardapi-volume-09380760-09a9-4539-abaa-5fe896bb5dba to disappear
    Jun 22 11:36:11.454: INFO: Pod downwardapi-volume-09380760-09a9-4539-abaa-5fe896bb5dba no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:36:11.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2115" for this suite. 06/22/23 11:36:11.463
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:36:11.472
Jun 22 11:36:11.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 11:36:11.473
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:11.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:11.498
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-d2ec13ea-93fd-4586-bf29-4d956927d9d8 06/22/23 11:36:11.51
STEP: Creating secret with name s-test-opt-upd-ba3c40e7-9c1b-4f6d-8d50-228a4db30144 06/22/23 11:36:11.516
STEP: Creating the pod 06/22/23 11:36:11.521
W0622 11:36:11.535351      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:36:11.535: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ea4b2fd6-3ab3-4e1e-9a0b-e23f06420628" in namespace "projected-5879" to be "running and ready"
Jun 22 11:36:11.540: INFO: Pod "pod-projected-secrets-ea4b2fd6-3ab3-4e1e-9a0b-e23f06420628": Phase="Pending", Reason="", readiness=false. Elapsed: 4.680664ms
Jun 22 11:36:11.540: INFO: The phase of Pod pod-projected-secrets-ea4b2fd6-3ab3-4e1e-9a0b-e23f06420628 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 11:36:13.547: INFO: Pod "pod-projected-secrets-ea4b2fd6-3ab3-4e1e-9a0b-e23f06420628": Phase="Running", Reason="", readiness=true. Elapsed: 2.012202612s
Jun 22 11:36:13.547: INFO: The phase of Pod pod-projected-secrets-ea4b2fd6-3ab3-4e1e-9a0b-e23f06420628 is Running (Ready = true)
Jun 22 11:36:13.547: INFO: Pod "pod-projected-secrets-ea4b2fd6-3ab3-4e1e-9a0b-e23f06420628" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-d2ec13ea-93fd-4586-bf29-4d956927d9d8 06/22/23 11:36:13.583
STEP: Updating secret s-test-opt-upd-ba3c40e7-9c1b-4f6d-8d50-228a4db30144 06/22/23 11:36:13.593
STEP: Creating secret with name s-test-opt-create-9bfdf806-4231-4af4-b337-a5e60b28f4f0 06/22/23 11:36:13.603
STEP: waiting to observe update in volume 06/22/23 11:36:13.61
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun 22 11:36:17.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5879" for this suite. 06/22/23 11:36:17.671
------------------------------
• [SLOW TEST] [6.209 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:36:11.472
    Jun 22 11:36:11.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 11:36:11.473
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:11.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:11.498
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-d2ec13ea-93fd-4586-bf29-4d956927d9d8 06/22/23 11:36:11.51
    STEP: Creating secret with name s-test-opt-upd-ba3c40e7-9c1b-4f6d-8d50-228a4db30144 06/22/23 11:36:11.516
    STEP: Creating the pod 06/22/23 11:36:11.521
    W0622 11:36:11.535351      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:36:11.535: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ea4b2fd6-3ab3-4e1e-9a0b-e23f06420628" in namespace "projected-5879" to be "running and ready"
    Jun 22 11:36:11.540: INFO: Pod "pod-projected-secrets-ea4b2fd6-3ab3-4e1e-9a0b-e23f06420628": Phase="Pending", Reason="", readiness=false. Elapsed: 4.680664ms
    Jun 22 11:36:11.540: INFO: The phase of Pod pod-projected-secrets-ea4b2fd6-3ab3-4e1e-9a0b-e23f06420628 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 11:36:13.547: INFO: Pod "pod-projected-secrets-ea4b2fd6-3ab3-4e1e-9a0b-e23f06420628": Phase="Running", Reason="", readiness=true. Elapsed: 2.012202612s
    Jun 22 11:36:13.547: INFO: The phase of Pod pod-projected-secrets-ea4b2fd6-3ab3-4e1e-9a0b-e23f06420628 is Running (Ready = true)
    Jun 22 11:36:13.547: INFO: Pod "pod-projected-secrets-ea4b2fd6-3ab3-4e1e-9a0b-e23f06420628" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-d2ec13ea-93fd-4586-bf29-4d956927d9d8 06/22/23 11:36:13.583
    STEP: Updating secret s-test-opt-upd-ba3c40e7-9c1b-4f6d-8d50-228a4db30144 06/22/23 11:36:13.593
    STEP: Creating secret with name s-test-opt-create-9bfdf806-4231-4af4-b337-a5e60b28f4f0 06/22/23 11:36:13.603
    STEP: waiting to observe update in volume 06/22/23 11:36:13.61
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:36:17.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5879" for this suite. 06/22/23 11:36:17.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:36:17.681
Jun 22 11:36:17.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename downward-api 06/22/23 11:36:17.682
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:17.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:17.708
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 06/22/23 11:36:17.712
W0622 11:36:17.725994      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:36:17.726: INFO: Waiting up to 5m0s for pod "downward-api-941a9678-29c3-4338-8f42-a4f118d08fb4" in namespace "downward-api-5502" to be "Succeeded or Failed"
Jun 22 11:36:17.734: INFO: Pod "downward-api-941a9678-29c3-4338-8f42-a4f118d08fb4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.948895ms
Jun 22 11:36:19.741: INFO: Pod "downward-api-941a9678-29c3-4338-8f42-a4f118d08fb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015338104s
Jun 22 11:36:21.740: INFO: Pod "downward-api-941a9678-29c3-4338-8f42-a4f118d08fb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01477714s
STEP: Saw pod success 06/22/23 11:36:21.74
Jun 22 11:36:21.741: INFO: Pod "downward-api-941a9678-29c3-4338-8f42-a4f118d08fb4" satisfied condition "Succeeded or Failed"
Jun 22 11:36:21.746: INFO: Trying to get logs from node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k pod downward-api-941a9678-29c3-4338-8f42-a4f118d08fb4 container dapi-container: <nil>
STEP: delete the pod 06/22/23 11:36:21.769
Jun 22 11:36:21.784: INFO: Waiting for pod downward-api-941a9678-29c3-4338-8f42-a4f118d08fb4 to disappear
Jun 22 11:36:21.789: INFO: Pod downward-api-941a9678-29c3-4338-8f42-a4f118d08fb4 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jun 22 11:36:21.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5502" for this suite. 06/22/23 11:36:21.798
------------------------------
• [4.136 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:36:17.681
    Jun 22 11:36:17.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename downward-api 06/22/23 11:36:17.682
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:17.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:17.708
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 06/22/23 11:36:17.712
    W0622 11:36:17.725994      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:36:17.726: INFO: Waiting up to 5m0s for pod "downward-api-941a9678-29c3-4338-8f42-a4f118d08fb4" in namespace "downward-api-5502" to be "Succeeded or Failed"
    Jun 22 11:36:17.734: INFO: Pod "downward-api-941a9678-29c3-4338-8f42-a4f118d08fb4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.948895ms
    Jun 22 11:36:19.741: INFO: Pod "downward-api-941a9678-29c3-4338-8f42-a4f118d08fb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015338104s
    Jun 22 11:36:21.740: INFO: Pod "downward-api-941a9678-29c3-4338-8f42-a4f118d08fb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01477714s
    STEP: Saw pod success 06/22/23 11:36:21.74
    Jun 22 11:36:21.741: INFO: Pod "downward-api-941a9678-29c3-4338-8f42-a4f118d08fb4" satisfied condition "Succeeded or Failed"
    Jun 22 11:36:21.746: INFO: Trying to get logs from node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k pod downward-api-941a9678-29c3-4338-8f42-a4f118d08fb4 container dapi-container: <nil>
    STEP: delete the pod 06/22/23 11:36:21.769
    Jun 22 11:36:21.784: INFO: Waiting for pod downward-api-941a9678-29c3-4338-8f42-a4f118d08fb4 to disappear
    Jun 22 11:36:21.789: INFO: Pod downward-api-941a9678-29c3-4338-8f42-a4f118d08fb4 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:36:21.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5502" for this suite. 06/22/23 11:36:21.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:36:21.819
Jun 22 11:36:21.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename replicaset 06/22/23 11:36:21.82
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:21.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:21.846
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 06/22/23 11:36:21.849
W0622 11:36:21.860823      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-adoption-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:36:21.860: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1193" to be "running and ready"
Jun 22 11:36:21.865: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.467356ms
Jun 22 11:36:21.865: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jun 22 11:36:23.872: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.011737396s
Jun 22 11:36:23.872: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jun 22 11:36:23.872: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 06/22/23 11:36:23.877
W0622 11:36:23.885311      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-adoption-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Then the orphan pod is adopted 06/22/23 11:36:23.885
STEP: When the matched label of one of its pods change 06/22/23 11:36:24.896
Jun 22 11:36:24.902: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 06/22/23 11:36:24.918
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jun 22 11:36:25.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1193" for this suite. 06/22/23 11:36:25.939
------------------------------
• [4.133 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:36:21.819
    Jun 22 11:36:21.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename replicaset 06/22/23 11:36:21.82
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:21.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:21.846
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 06/22/23 11:36:21.849
    W0622 11:36:21.860823      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-adoption-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:36:21.860: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1193" to be "running and ready"
    Jun 22 11:36:21.865: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.467356ms
    Jun 22 11:36:21.865: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 11:36:23.872: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.011737396s
    Jun 22 11:36:23.872: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jun 22 11:36:23.872: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 06/22/23 11:36:23.877
    W0622 11:36:23.885311      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-adoption-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Then the orphan pod is adopted 06/22/23 11:36:23.885
    STEP: When the matched label of one of its pods change 06/22/23 11:36:24.896
    Jun 22 11:36:24.902: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 06/22/23 11:36:24.918
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:36:25.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1193" for this suite. 06/22/23 11:36:25.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:36:25.954
Jun 22 11:36:25.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename pods 06/22/23 11:36:25.955
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:25.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:25.985
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 06/22/23 11:36:25.989
STEP: setting up watch 06/22/23 11:36:25.99
STEP: submitting the pod to kubernetes 06/22/23 11:36:26.096
STEP: verifying the pod is in kubernetes 06/22/23 11:36:26.11
STEP: verifying pod creation was observed 06/22/23 11:36:26.114
Jun 22 11:36:26.114: INFO: Waiting up to 5m0s for pod "pod-submit-remove-6c56b205-b54a-4809-8a6f-fb3274aba13b" in namespace "pods-9986" to be "running"
Jun 22 11:36:26.118: INFO: Pod "pod-submit-remove-6c56b205-b54a-4809-8a6f-fb3274aba13b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04035ms
Jun 22 11:36:28.124: INFO: Pod "pod-submit-remove-6c56b205-b54a-4809-8a6f-fb3274aba13b": Phase="Running", Reason="", readiness=true. Elapsed: 2.00975162s
Jun 22 11:36:28.124: INFO: Pod "pod-submit-remove-6c56b205-b54a-4809-8a6f-fb3274aba13b" satisfied condition "running"
STEP: deleting the pod gracefully 06/22/23 11:36:28.129
STEP: verifying pod deletion was observed 06/22/23 11:36:28.139
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun 22 11:36:30.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9986" for this suite. 06/22/23 11:36:30.387
------------------------------
• [4.442 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:36:25.954
    Jun 22 11:36:25.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename pods 06/22/23 11:36:25.955
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:25.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:25.985
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 06/22/23 11:36:25.989
    STEP: setting up watch 06/22/23 11:36:25.99
    STEP: submitting the pod to kubernetes 06/22/23 11:36:26.096
    STEP: verifying the pod is in kubernetes 06/22/23 11:36:26.11
    STEP: verifying pod creation was observed 06/22/23 11:36:26.114
    Jun 22 11:36:26.114: INFO: Waiting up to 5m0s for pod "pod-submit-remove-6c56b205-b54a-4809-8a6f-fb3274aba13b" in namespace "pods-9986" to be "running"
    Jun 22 11:36:26.118: INFO: Pod "pod-submit-remove-6c56b205-b54a-4809-8a6f-fb3274aba13b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04035ms
    Jun 22 11:36:28.124: INFO: Pod "pod-submit-remove-6c56b205-b54a-4809-8a6f-fb3274aba13b": Phase="Running", Reason="", readiness=true. Elapsed: 2.00975162s
    Jun 22 11:36:28.124: INFO: Pod "pod-submit-remove-6c56b205-b54a-4809-8a6f-fb3274aba13b" satisfied condition "running"
    STEP: deleting the pod gracefully 06/22/23 11:36:28.129
    STEP: verifying pod deletion was observed 06/22/23 11:36:28.139
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:36:30.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9986" for this suite. 06/22/23 11:36:30.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:36:30.398
Jun 22 11:36:30.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename resourcequota 06/22/23 11:36:30.399
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:30.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:30.424
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 06/22/23 11:36:30.428
STEP: Creating a ResourceQuota 06/22/23 11:36:35.434
STEP: Ensuring resource quota status is calculated 06/22/23 11:36:35.443
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun 22 11:36:37.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9195" for this suite. 06/22/23 11:36:37.458
------------------------------
• [SLOW TEST] [7.070 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:36:30.398
    Jun 22 11:36:30.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename resourcequota 06/22/23 11:36:30.399
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:30.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:30.424
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 06/22/23 11:36:30.428
    STEP: Creating a ResourceQuota 06/22/23 11:36:35.434
    STEP: Ensuring resource quota status is calculated 06/22/23 11:36:35.443
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:36:37.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9195" for this suite. 06/22/23 11:36:37.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:36:37.471
Jun 22 11:36:37.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename downward-api 06/22/23 11:36:37.472
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:37.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:37.498
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 06/22/23 11:36:37.502
W0622 11:36:37.516513      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:36:37.516: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0127a4f7-fdf4-47b2-8c82-5fdef13892e5" in namespace "downward-api-9382" to be "Succeeded or Failed"
Jun 22 11:36:37.520: INFO: Pod "downwardapi-volume-0127a4f7-fdf4-47b2-8c82-5fdef13892e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.208982ms
Jun 22 11:36:39.528: INFO: Pod "downwardapi-volume-0127a4f7-fdf4-47b2-8c82-5fdef13892e5": Phase="Running", Reason="", readiness=false. Elapsed: 2.012226665s
Jun 22 11:36:41.528: INFO: Pod "downwardapi-volume-0127a4f7-fdf4-47b2-8c82-5fdef13892e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011410443s
STEP: Saw pod success 06/22/23 11:36:41.528
Jun 22 11:36:41.528: INFO: Pod "downwardapi-volume-0127a4f7-fdf4-47b2-8c82-5fdef13892e5" satisfied condition "Succeeded or Failed"
Jun 22 11:36:41.533: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-0127a4f7-fdf4-47b2-8c82-5fdef13892e5 container client-container: <nil>
STEP: delete the pod 06/22/23 11:36:41.546
Jun 22 11:36:41.563: INFO: Waiting for pod downwardapi-volume-0127a4f7-fdf4-47b2-8c82-5fdef13892e5 to disappear
Jun 22 11:36:41.569: INFO: Pod downwardapi-volume-0127a4f7-fdf4-47b2-8c82-5fdef13892e5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun 22 11:36:41.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9382" for this suite. 06/22/23 11:36:41.577
------------------------------
• [4.115 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:36:37.471
    Jun 22 11:36:37.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename downward-api 06/22/23 11:36:37.472
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:37.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:37.498
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 06/22/23 11:36:37.502
    W0622 11:36:37.516513      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:36:37.516: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0127a4f7-fdf4-47b2-8c82-5fdef13892e5" in namespace "downward-api-9382" to be "Succeeded or Failed"
    Jun 22 11:36:37.520: INFO: Pod "downwardapi-volume-0127a4f7-fdf4-47b2-8c82-5fdef13892e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.208982ms
    Jun 22 11:36:39.528: INFO: Pod "downwardapi-volume-0127a4f7-fdf4-47b2-8c82-5fdef13892e5": Phase="Running", Reason="", readiness=false. Elapsed: 2.012226665s
    Jun 22 11:36:41.528: INFO: Pod "downwardapi-volume-0127a4f7-fdf4-47b2-8c82-5fdef13892e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011410443s
    STEP: Saw pod success 06/22/23 11:36:41.528
    Jun 22 11:36:41.528: INFO: Pod "downwardapi-volume-0127a4f7-fdf4-47b2-8c82-5fdef13892e5" satisfied condition "Succeeded or Failed"
    Jun 22 11:36:41.533: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-0127a4f7-fdf4-47b2-8c82-5fdef13892e5 container client-container: <nil>
    STEP: delete the pod 06/22/23 11:36:41.546
    Jun 22 11:36:41.563: INFO: Waiting for pod downwardapi-volume-0127a4f7-fdf4-47b2-8c82-5fdef13892e5 to disappear
    Jun 22 11:36:41.569: INFO: Pod downwardapi-volume-0127a4f7-fdf4-47b2-8c82-5fdef13892e5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:36:41.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9382" for this suite. 06/22/23 11:36:41.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:36:41.587
Jun 22 11:36:41.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename secrets 06/22/23 11:36:41.589
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:41.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:41.614
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 06/22/23 11:36:41.618
STEP: listing secrets in all namespaces to ensure that there are more than zero 06/22/23 11:36:41.624
STEP: patching the secret 06/22/23 11:36:41.63
STEP: deleting the secret using a LabelSelector 06/22/23 11:36:41.64
STEP: listing secrets in all namespaces, searching for label name and value in patch 06/22/23 11:36:41.651
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jun 22 11:36:41.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1661" for this suite. 06/22/23 11:36:41.663
------------------------------
• [0.085 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:36:41.587
    Jun 22 11:36:41.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename secrets 06/22/23 11:36:41.589
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:41.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:41.614
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 06/22/23 11:36:41.618
    STEP: listing secrets in all namespaces to ensure that there are more than zero 06/22/23 11:36:41.624
    STEP: patching the secret 06/22/23 11:36:41.63
    STEP: deleting the secret using a LabelSelector 06/22/23 11:36:41.64
    STEP: listing secrets in all namespaces, searching for label name and value in patch 06/22/23 11:36:41.651
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:36:41.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1661" for this suite. 06/22/23 11:36:41.663
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:36:41.673
Jun 22 11:36:41.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename downward-api 06/22/23 11:36:41.674
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:41.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:41.698
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 06/22/23 11:36:41.702
W0622 11:36:41.713044      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:36:41.713: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cccd1ea7-502a-4dbe-b84b-4dfb173578a0" in namespace "downward-api-3261" to be "Succeeded or Failed"
Jun 22 11:36:41.719: INFO: Pod "downwardapi-volume-cccd1ea7-502a-4dbe-b84b-4dfb173578a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.227607ms
Jun 22 11:36:43.726: INFO: Pod "downwardapi-volume-cccd1ea7-502a-4dbe-b84b-4dfb173578a0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013297727s
Jun 22 11:36:45.727: INFO: Pod "downwardapi-volume-cccd1ea7-502a-4dbe-b84b-4dfb173578a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014228196s
STEP: Saw pod success 06/22/23 11:36:45.727
Jun 22 11:36:45.727: INFO: Pod "downwardapi-volume-cccd1ea7-502a-4dbe-b84b-4dfb173578a0" satisfied condition "Succeeded or Failed"
Jun 22 11:36:45.732: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-cccd1ea7-502a-4dbe-b84b-4dfb173578a0 container client-container: <nil>
STEP: delete the pod 06/22/23 11:36:45.741
Jun 22 11:36:45.758: INFO: Waiting for pod downwardapi-volume-cccd1ea7-502a-4dbe-b84b-4dfb173578a0 to disappear
Jun 22 11:36:45.762: INFO: Pod downwardapi-volume-cccd1ea7-502a-4dbe-b84b-4dfb173578a0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun 22 11:36:45.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3261" for this suite. 06/22/23 11:36:45.769
------------------------------
• [4.105 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:36:41.673
    Jun 22 11:36:41.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename downward-api 06/22/23 11:36:41.674
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:41.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:41.698
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 06/22/23 11:36:41.702
    W0622 11:36:41.713044      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:36:41.713: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cccd1ea7-502a-4dbe-b84b-4dfb173578a0" in namespace "downward-api-3261" to be "Succeeded or Failed"
    Jun 22 11:36:41.719: INFO: Pod "downwardapi-volume-cccd1ea7-502a-4dbe-b84b-4dfb173578a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.227607ms
    Jun 22 11:36:43.726: INFO: Pod "downwardapi-volume-cccd1ea7-502a-4dbe-b84b-4dfb173578a0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013297727s
    Jun 22 11:36:45.727: INFO: Pod "downwardapi-volume-cccd1ea7-502a-4dbe-b84b-4dfb173578a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014228196s
    STEP: Saw pod success 06/22/23 11:36:45.727
    Jun 22 11:36:45.727: INFO: Pod "downwardapi-volume-cccd1ea7-502a-4dbe-b84b-4dfb173578a0" satisfied condition "Succeeded or Failed"
    Jun 22 11:36:45.732: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-cccd1ea7-502a-4dbe-b84b-4dfb173578a0 container client-container: <nil>
    STEP: delete the pod 06/22/23 11:36:45.741
    Jun 22 11:36:45.758: INFO: Waiting for pod downwardapi-volume-cccd1ea7-502a-4dbe-b84b-4dfb173578a0 to disappear
    Jun 22 11:36:45.762: INFO: Pod downwardapi-volume-cccd1ea7-502a-4dbe-b84b-4dfb173578a0 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:36:45.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3261" for this suite. 06/22/23 11:36:45.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:36:45.78
Jun 22 11:36:45.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename init-container 06/22/23 11:36:45.781
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:45.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:45.806
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 06/22/23 11:36:45.812
Jun 22 11:36:45.812: INFO: PodSpec: initContainers in spec.initContainers
W0622 11:36:45.824982      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "init1", "init2", "run1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "init1", "init2", "run1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "init1", "init2", "run1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "init1", "init2", "run1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:36:51.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1932" for this suite. 06/22/23 11:36:51.459
------------------------------
• [SLOW TEST] [5.690 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:36:45.78
    Jun 22 11:36:45.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename init-container 06/22/23 11:36:45.781
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:45.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:45.806
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 06/22/23 11:36:45.812
    Jun 22 11:36:45.812: INFO: PodSpec: initContainers in spec.initContainers
    W0622 11:36:45.824982      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "init1", "init2", "run1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "init1", "init2", "run1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "init1", "init2", "run1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "init1", "init2", "run1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:36:51.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1932" for this suite. 06/22/23 11:36:51.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:36:51.471
Jun 22 11:36:51.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename deployment 06/22/23 11:36:51.473
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:51.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:51.498
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 06/22/23 11:36:51.508
Jun 22 11:36:51.508: INFO: Creating simple deployment test-deployment-rsmbv
W0622 11:36:51.517607      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:36:51.527: INFO: deployment "test-deployment-rsmbv" doesn't have the required revision set
STEP: Getting /status 06/22/23 11:36:53.575
Jun 22 11:36:53.585: INFO: Deployment test-deployment-rsmbv has Conditions: [{Available True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rsmbv-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 06/22/23 11:36:53.585
Jun 22 11:36:53.618: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 36, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 36, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 36, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 36, 51, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-rsmbv-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 06/22/23 11:36:53.618
Jun 22 11:36:53.623: INFO: Observed &Deployment event: ADDED
Jun 22 11:36:53.623: INFO: Observed Deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rsmbv-54bc444df"}
Jun 22 11:36:53.623: INFO: Observed &Deployment event: MODIFIED
Jun 22 11:36:53.623: INFO: Observed Deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rsmbv-54bc444df"}
Jun 22 11:36:53.623: INFO: Observed Deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 22 11:36:53.623: INFO: Observed &Deployment event: MODIFIED
Jun 22 11:36:53.623: INFO: Observed Deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 22 11:36:53.623: INFO: Observed Deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-rsmbv-54bc444df" is progressing.}
Jun 22 11:36:53.624: INFO: Observed &Deployment event: MODIFIED
Jun 22 11:36:53.624: INFO: Observed Deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 22 11:36:53.624: INFO: Observed Deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rsmbv-54bc444df" has successfully progressed.}
Jun 22 11:36:53.624: INFO: Observed &Deployment event: MODIFIED
Jun 22 11:36:53.624: INFO: Observed Deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 22 11:36:53.624: INFO: Observed Deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rsmbv-54bc444df" has successfully progressed.}
Jun 22 11:36:53.624: INFO: Found Deployment test-deployment-rsmbv in namespace deployment-2703 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 22 11:36:53.624: INFO: Deployment test-deployment-rsmbv has an updated status
STEP: patching the Statefulset Status 06/22/23 11:36:53.624
Jun 22 11:36:53.624: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun 22 11:36:53.645: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 06/22/23 11:36:53.645
Jun 22 11:36:53.649: INFO: Observed &Deployment event: ADDED
Jun 22 11:36:53.649: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rsmbv-54bc444df"}
Jun 22 11:36:53.649: INFO: Observed &Deployment event: MODIFIED
Jun 22 11:36:53.649: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rsmbv-54bc444df"}
Jun 22 11:36:53.649: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 22 11:36:53.649: INFO: Observed &Deployment event: MODIFIED
Jun 22 11:36:53.650: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 22 11:36:53.650: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-rsmbv-54bc444df" is progressing.}
Jun 22 11:36:53.650: INFO: Observed &Deployment event: MODIFIED
Jun 22 11:36:53.650: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 22 11:36:53.650: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rsmbv-54bc444df" has successfully progressed.}
Jun 22 11:36:53.650: INFO: Observed &Deployment event: MODIFIED
Jun 22 11:36:53.650: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 22 11:36:53.650: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rsmbv-54bc444df" has successfully progressed.}
Jun 22 11:36:53.650: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 22 11:36:53.650: INFO: Observed &Deployment event: MODIFIED
Jun 22 11:36:53.650: INFO: Found deployment test-deployment-rsmbv in namespace deployment-2703 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jun 22 11:36:53.650: INFO: Deployment test-deployment-rsmbv has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 22 11:36:53.660: INFO: Deployment "test-deployment-rsmbv":
&Deployment{ObjectMeta:{test-deployment-rsmbv  deployment-2703  359eccf1-a889-4027-909c-044f8a562391 158328 1 2023-06-22 11:36:51 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-22 11:36:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-06-22 11:36:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-06-22 11:36:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d46488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-rsmbv-54bc444df",LastUpdateTime:2023-06-22 11:36:53 +0000 UTC,LastTransitionTime:2023-06-22 11:36:53 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 11:36:53.667: INFO: New ReplicaSet "test-deployment-rsmbv-54bc444df" of Deployment "test-deployment-rsmbv":
&ReplicaSet{ObjectMeta:{test-deployment-rsmbv-54bc444df  deployment-2703  9a7fa2f1-520a-4e59-829a-eff3da297cde 158321 1 2023-06-22 11:36:51 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-rsmbv 359eccf1-a889-4027-909c-044f8a562391 0xc004d46870 0xc004d46871}] [] [{kube-controller-manager Update apps/v1 2023-06-22 11:36:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"359eccf1-a889-4027-909c-044f8a562391\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 11:36:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d46918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 22 11:36:53.679: INFO: Pod "test-deployment-rsmbv-54bc444df-q9mpt" is available:
&Pod{ObjectMeta:{test-deployment-rsmbv-54bc444df-q9mpt test-deployment-rsmbv-54bc444df- deployment-2703  831d8fed-7b60-4a42-bf5d-24aa15329e8a 158320 0 2023-06-22 11:36:51 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-rsmbv-54bc444df 9a7fa2f1-520a-4e59-829a-eff3da297cde 0xc004d46cc0 0xc004d46cc1}] [] [{kube-controller-manager Update v1 2023-06-22 11:36:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a7fa2f1-520a-4e59-829a-eff3da297cde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 11:36:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pxdb4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pxdb4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 11:36:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 11:36:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 11:36:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 11:36:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.219,StartTime:2023-06-22 11:36:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 11:36:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0c44049676bb58fb82f6195083198c329d1f8d822afc61b8658c41c5e083ee16,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun 22 11:36:53.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2703" for this suite. 06/22/23 11:36:53.694
------------------------------
• [2.240 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:36:51.471
    Jun 22 11:36:51.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename deployment 06/22/23 11:36:51.473
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:51.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:51.498
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 06/22/23 11:36:51.508
    Jun 22 11:36:51.508: INFO: Creating simple deployment test-deployment-rsmbv
    W0622 11:36:51.517607      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:36:51.527: INFO: deployment "test-deployment-rsmbv" doesn't have the required revision set
    STEP: Getting /status 06/22/23 11:36:53.575
    Jun 22 11:36:53.585: INFO: Deployment test-deployment-rsmbv has Conditions: [{Available True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rsmbv-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 06/22/23 11:36:53.585
    Jun 22 11:36:53.618: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 36, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 36, 52, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 22, 11, 36, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 22, 11, 36, 51, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-rsmbv-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 06/22/23 11:36:53.618
    Jun 22 11:36:53.623: INFO: Observed &Deployment event: ADDED
    Jun 22 11:36:53.623: INFO: Observed Deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rsmbv-54bc444df"}
    Jun 22 11:36:53.623: INFO: Observed &Deployment event: MODIFIED
    Jun 22 11:36:53.623: INFO: Observed Deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rsmbv-54bc444df"}
    Jun 22 11:36:53.623: INFO: Observed Deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun 22 11:36:53.623: INFO: Observed &Deployment event: MODIFIED
    Jun 22 11:36:53.623: INFO: Observed Deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun 22 11:36:53.623: INFO: Observed Deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-rsmbv-54bc444df" is progressing.}
    Jun 22 11:36:53.624: INFO: Observed &Deployment event: MODIFIED
    Jun 22 11:36:53.624: INFO: Observed Deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun 22 11:36:53.624: INFO: Observed Deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rsmbv-54bc444df" has successfully progressed.}
    Jun 22 11:36:53.624: INFO: Observed &Deployment event: MODIFIED
    Jun 22 11:36:53.624: INFO: Observed Deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun 22 11:36:53.624: INFO: Observed Deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rsmbv-54bc444df" has successfully progressed.}
    Jun 22 11:36:53.624: INFO: Found Deployment test-deployment-rsmbv in namespace deployment-2703 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun 22 11:36:53.624: INFO: Deployment test-deployment-rsmbv has an updated status
    STEP: patching the Statefulset Status 06/22/23 11:36:53.624
    Jun 22 11:36:53.624: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jun 22 11:36:53.645: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 06/22/23 11:36:53.645
    Jun 22 11:36:53.649: INFO: Observed &Deployment event: ADDED
    Jun 22 11:36:53.649: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rsmbv-54bc444df"}
    Jun 22 11:36:53.649: INFO: Observed &Deployment event: MODIFIED
    Jun 22 11:36:53.649: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rsmbv-54bc444df"}
    Jun 22 11:36:53.649: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun 22 11:36:53.649: INFO: Observed &Deployment event: MODIFIED
    Jun 22 11:36:53.650: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun 22 11:36:53.650: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:51 +0000 UTC 2023-06-22 11:36:51 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-rsmbv-54bc444df" is progressing.}
    Jun 22 11:36:53.650: INFO: Observed &Deployment event: MODIFIED
    Jun 22 11:36:53.650: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun 22 11:36:53.650: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rsmbv-54bc444df" has successfully progressed.}
    Jun 22 11:36:53.650: INFO: Observed &Deployment event: MODIFIED
    Jun 22 11:36:53.650: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun 22 11:36:53.650: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-22 11:36:52 +0000 UTC 2023-06-22 11:36:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rsmbv-54bc444df" has successfully progressed.}
    Jun 22 11:36:53.650: INFO: Observed deployment test-deployment-rsmbv in namespace deployment-2703 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun 22 11:36:53.650: INFO: Observed &Deployment event: MODIFIED
    Jun 22 11:36:53.650: INFO: Found deployment test-deployment-rsmbv in namespace deployment-2703 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jun 22 11:36:53.650: INFO: Deployment test-deployment-rsmbv has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 22 11:36:53.660: INFO: Deployment "test-deployment-rsmbv":
    &Deployment{ObjectMeta:{test-deployment-rsmbv  deployment-2703  359eccf1-a889-4027-909c-044f8a562391 158328 1 2023-06-22 11:36:51 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-22 11:36:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-06-22 11:36:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-06-22 11:36:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d46488 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-rsmbv-54bc444df",LastUpdateTime:2023-06-22 11:36:53 +0000 UTC,LastTransitionTime:2023-06-22 11:36:53 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun 22 11:36:53.667: INFO: New ReplicaSet "test-deployment-rsmbv-54bc444df" of Deployment "test-deployment-rsmbv":
    &ReplicaSet{ObjectMeta:{test-deployment-rsmbv-54bc444df  deployment-2703  9a7fa2f1-520a-4e59-829a-eff3da297cde 158321 1 2023-06-22 11:36:51 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-rsmbv 359eccf1-a889-4027-909c-044f8a562391 0xc004d46870 0xc004d46871}] [] [{kube-controller-manager Update apps/v1 2023-06-22 11:36:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"359eccf1-a889-4027-909c-044f8a562391\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 11:36:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d46918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 22 11:36:53.679: INFO: Pod "test-deployment-rsmbv-54bc444df-q9mpt" is available:
    &Pod{ObjectMeta:{test-deployment-rsmbv-54bc444df-q9mpt test-deployment-rsmbv-54bc444df- deployment-2703  831d8fed-7b60-4a42-bf5d-24aa15329e8a 158320 0 2023-06-22 11:36:51 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-rsmbv-54bc444df 9a7fa2f1-520a-4e59-829a-eff3da297cde 0xc004d46cc0 0xc004d46cc1}] [] [{kube-controller-manager Update v1 2023-06-22 11:36:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a7fa2f1-520a-4e59-829a-eff3da297cde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 11:36:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"100.96.3.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pxdb4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pxdb4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 11:36:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 11:36:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 11:36:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 11:36:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:100.96.3.219,StartTime:2023-06-22 11:36:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-22 11:36:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0c44049676bb58fb82f6195083198c329d1f8d822afc61b8658c41c5e083ee16,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.96.3.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:36:53.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2703" for this suite. 06/22/23 11:36:53.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:36:53.725
Jun 22 11:36:53.725: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename csiinlinevolumes 06/22/23 11:36:53.726
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:53.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:53.766
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 06/22/23 11:36:53.772
W0622 11:36:53.801482      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-csi-inline-volumes" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-csi-inline-volumes" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-csi-inline-volumes" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-csi-inline-volumes" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0622 11:36:53.843973      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-csi-inline-volumes" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-csi-inline-volumes" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-csi-inline-volumes" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-csi-inline-volumes" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: getting 06/22/23 11:36:53.844
STEP: listing in namespace 06/22/23 11:36:53.867
STEP: patching 06/22/23 11:36:53.874
STEP: deleting 06/22/23 11:36:53.891
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jun 22 11:36:53.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-931" for this suite. 06/22/23 11:36:53.92
------------------------------
• [0.209 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:36:53.725
    Jun 22 11:36:53.725: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename csiinlinevolumes 06/22/23 11:36:53.726
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:53.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:53.766
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 06/22/23 11:36:53.772
    W0622 11:36:53.801482      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-csi-inline-volumes" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-csi-inline-volumes" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-csi-inline-volumes" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-csi-inline-volumes" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0622 11:36:53.843973      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pod-csi-inline-volumes" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-csi-inline-volumes" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-csi-inline-volumes" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-csi-inline-volumes" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: getting 06/22/23 11:36:53.844
    STEP: listing in namespace 06/22/23 11:36:53.867
    STEP: patching 06/22/23 11:36:53.874
    STEP: deleting 06/22/23 11:36:53.891
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:36:53.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-931" for this suite. 06/22/23 11:36:53.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:36:53.936
Jun 22 11:36:53.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename endpointslicemirroring 06/22/23 11:36:53.937
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:53.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:53.971
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 06/22/23 11:36:54.02
Jun 22 11:36:54.043: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 06/22/23 11:36:56.053
STEP: mirroring deletion of a custom Endpoint 06/22/23 11:36:56.07
Jun 22 11:36:56.091: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Jun 22 11:36:58.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-2742" for this suite. 06/22/23 11:36:58.109
------------------------------
• [4.185 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:36:53.936
    Jun 22 11:36:53.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename endpointslicemirroring 06/22/23 11:36:53.937
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:53.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:53.971
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 06/22/23 11:36:54.02
    Jun 22 11:36:54.043: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 06/22/23 11:36:56.053
    STEP: mirroring deletion of a custom Endpoint 06/22/23 11:36:56.07
    Jun 22 11:36:56.091: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:36:58.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-2742" for this suite. 06/22/23 11:36:58.109
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:36:58.121
Jun 22 11:36:58.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename watch 06/22/23 11:36:58.122
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:58.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:58.152
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 06/22/23 11:36:58.156
STEP: creating a new configmap 06/22/23 11:36:58.159
STEP: modifying the configmap once 06/22/23 11:36:58.167
STEP: closing the watch once it receives two notifications 06/22/23 11:36:58.18
Jun 22 11:36:58.180: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9535  22f7fbb2-d0c2-4add-8bc5-96d009931249 158388 0 2023-06-22 11:36:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-22 11:36:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 11:36:58.180: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9535  22f7fbb2-d0c2-4add-8bc5-96d009931249 158389 0 2023-06-22 11:36:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-22 11:36:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 06/22/23 11:36:58.18
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 06/22/23 11:36:58.194
STEP: deleting the configmap 06/22/23 11:36:58.196
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 06/22/23 11:36:58.209
Jun 22 11:36:58.209: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9535  22f7fbb2-d0c2-4add-8bc5-96d009931249 158390 0 2023-06-22 11:36:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-22 11:36:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 22 11:36:58.209: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9535  22f7fbb2-d0c2-4add-8bc5-96d009931249 158391 0 2023-06-22 11:36:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-22 11:36:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jun 22 11:36:58.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9535" for this suite. 06/22/23 11:36:58.218
------------------------------
• [0.107 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:36:58.121
    Jun 22 11:36:58.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename watch 06/22/23 11:36:58.122
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:58.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:58.152
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 06/22/23 11:36:58.156
    STEP: creating a new configmap 06/22/23 11:36:58.159
    STEP: modifying the configmap once 06/22/23 11:36:58.167
    STEP: closing the watch once it receives two notifications 06/22/23 11:36:58.18
    Jun 22 11:36:58.180: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9535  22f7fbb2-d0c2-4add-8bc5-96d009931249 158388 0 2023-06-22 11:36:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-22 11:36:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 22 11:36:58.180: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9535  22f7fbb2-d0c2-4add-8bc5-96d009931249 158389 0 2023-06-22 11:36:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-22 11:36:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 06/22/23 11:36:58.18
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 06/22/23 11:36:58.194
    STEP: deleting the configmap 06/22/23 11:36:58.196
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 06/22/23 11:36:58.209
    Jun 22 11:36:58.209: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9535  22f7fbb2-d0c2-4add-8bc5-96d009931249 158390 0 2023-06-22 11:36:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-22 11:36:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 22 11:36:58.209: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9535  22f7fbb2-d0c2-4add-8bc5-96d009931249 158391 0 2023-06-22 11:36:58 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-22 11:36:58 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:36:58.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9535" for this suite. 06/22/23 11:36:58.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:36:58.229
Jun 22 11:36:58.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename crd-publish-openapi 06/22/23 11:36:58.23
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:58.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:58.258
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 06/22/23 11:36:58.262
Jun 22 11:36:58.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 11:37:01.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:37:11.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6581" for this suite. 06/22/23 11:37:11.716
------------------------------
• [SLOW TEST] [13.497 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:36:58.229
    Jun 22 11:36:58.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename crd-publish-openapi 06/22/23 11:36:58.23
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:36:58.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:36:58.258
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 06/22/23 11:36:58.262
    Jun 22 11:36:58.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 11:37:01.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:37:11.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6581" for this suite. 06/22/23 11:37:11.716
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:37:11.729
Jun 22 11:37:11.729: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename pods 06/22/23 11:37:11.73
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:37:11.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:37:11.756
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 06/22/23 11:37:11.759
Jun 22 11:37:11.772: INFO: created test-pod-1
Jun 22 11:37:11.780: INFO: created test-pod-2
Jun 22 11:37:11.788: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 06/22/23 11:37:11.788
Jun 22 11:37:11.788: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-9401' to be running and ready
Jun 22 11:37:11.806: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 22 11:37:11.806: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 22 11:37:11.806: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 22 11:37:11.806: INFO: 0 / 3 pods in namespace 'pods-9401' are running and ready (0 seconds elapsed)
Jun 22 11:37:11.806: INFO: expected 0 pod replicas in namespace 'pods-9401', 0 are Running and Ready.
Jun 22 11:37:11.806: INFO: POD         NODE                                         PHASE    GRACE  CONDITIONS
Jun 22 11:37:11.806: INFO: test-pod-1  wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:37:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:37:11 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:37:11 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:37:11 +0000 UTC  }]
Jun 22 11:37:11.806: INFO: test-pod-2  wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:37:11 +0000 UTC  }]
Jun 22 11:37:11.806: INFO: test-pod-3  wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:37:11 +0000 UTC  }]
Jun 22 11:37:11.806: INFO: 
Jun 22 11:37:13.821: INFO: 3 / 3 pods in namespace 'pods-9401' are running and ready (2 seconds elapsed)
Jun 22 11:37:13.821: INFO: expected 0 pod replicas in namespace 'pods-9401', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 06/22/23 11:37:14.018
Jun 22 11:37:14.023: INFO: Pod quantity 3 is different from expected quantity 0
Jun 22 11:37:15.032: INFO: Pod quantity 3 is different from expected quantity 0
Jun 22 11:37:16.031: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jun 22 11:37:17.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9401" for this suite. 06/22/23 11:37:17.037
------------------------------
• [SLOW TEST] [5.317 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:37:11.729
    Jun 22 11:37:11.729: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename pods 06/22/23 11:37:11.73
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:37:11.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:37:11.756
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 06/22/23 11:37:11.759
    Jun 22 11:37:11.772: INFO: created test-pod-1
    Jun 22 11:37:11.780: INFO: created test-pod-2
    Jun 22 11:37:11.788: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 06/22/23 11:37:11.788
    Jun 22 11:37:11.788: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-9401' to be running and ready
    Jun 22 11:37:11.806: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun 22 11:37:11.806: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun 22 11:37:11.806: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun 22 11:37:11.806: INFO: 0 / 3 pods in namespace 'pods-9401' are running and ready (0 seconds elapsed)
    Jun 22 11:37:11.806: INFO: expected 0 pod replicas in namespace 'pods-9401', 0 are Running and Ready.
    Jun 22 11:37:11.806: INFO: POD         NODE                                         PHASE    GRACE  CONDITIONS
    Jun 22 11:37:11.806: INFO: test-pod-1  wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:37:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:37:11 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:37:11 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:37:11 +0000 UTC  }]
    Jun 22 11:37:11.806: INFO: test-pod-2  wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:37:11 +0000 UTC  }]
    Jun 22 11:37:11.806: INFO: test-pod-3  wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-22 11:37:11 +0000 UTC  }]
    Jun 22 11:37:11.806: INFO: 
    Jun 22 11:37:13.821: INFO: 3 / 3 pods in namespace 'pods-9401' are running and ready (2 seconds elapsed)
    Jun 22 11:37:13.821: INFO: expected 0 pod replicas in namespace 'pods-9401', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 06/22/23 11:37:14.018
    Jun 22 11:37:14.023: INFO: Pod quantity 3 is different from expected quantity 0
    Jun 22 11:37:15.032: INFO: Pod quantity 3 is different from expected quantity 0
    Jun 22 11:37:16.031: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:37:17.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9401" for this suite. 06/22/23 11:37:17.037
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:37:17.046
Jun 22 11:37:17.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename gc 06/22/23 11:37:17.047
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:37:17.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:37:17.072
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 06/22/23 11:37:17.082
W0622 11:37:17.091010      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: create the rc2 06/22/23 11:37:17.091
W0622 11:37:17.098468      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 06/22/23 11:37:22.106
STEP: delete the rc simpletest-rc-to-be-deleted 06/22/23 11:37:22.694
STEP: wait for the rc to be deleted 06/22/23 11:37:22.706
Jun 22 11:37:27.727: INFO: 70 pods remaining
Jun 22 11:37:27.727: INFO: 70 pods has nil DeletionTimestamp
Jun 22 11:37:27.727: INFO: 
STEP: Gathering metrics 06/22/23 11:37:32.724
Jun 22 11:37:32.765: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" in namespace "kube-system" to be "running and ready"
Jun 22 11:37:32.770: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v": Phase="Running", Reason="", readiness=true. Elapsed: 4.676106ms
Jun 22 11:37:32.770: INFO: The phase of Pod kube-controller-manager-wl-antrea-prdzc-k4b4v is Running (Ready = true)
Jun 22 11:37:32.770: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" satisfied condition "running and ready"
Jun 22 11:37:32.862: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun 22 11:37:32.862: INFO: Deleting pod "simpletest-rc-to-be-deleted-22n2h" in namespace "gc-3485"
Jun 22 11:37:32.883: INFO: Deleting pod "simpletest-rc-to-be-deleted-26524" in namespace "gc-3485"
Jun 22 11:37:32.900: INFO: Deleting pod "simpletest-rc-to-be-deleted-28tts" in namespace "gc-3485"
Jun 22 11:37:32.917: INFO: Deleting pod "simpletest-rc-to-be-deleted-2br5n" in namespace "gc-3485"
Jun 22 11:37:32.939: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gxpv" in namespace "gc-3485"
Jun 22 11:37:32.953: INFO: Deleting pod "simpletest-rc-to-be-deleted-46xxw" in namespace "gc-3485"
Jun 22 11:37:32.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ttvl" in namespace "gc-3485"
Jun 22 11:37:32.991: INFO: Deleting pod "simpletest-rc-to-be-deleted-5c2mm" in namespace "gc-3485"
Jun 22 11:37:33.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-72zl9" in namespace "gc-3485"
Jun 22 11:37:33.028: INFO: Deleting pod "simpletest-rc-to-be-deleted-75dcb" in namespace "gc-3485"
Jun 22 11:37:33.045: INFO: Deleting pod "simpletest-rc-to-be-deleted-78xz9" in namespace "gc-3485"
Jun 22 11:37:33.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bftp" in namespace "gc-3485"
Jun 22 11:37:33.072: INFO: Deleting pod "simpletest-rc-to-be-deleted-97whg" in namespace "gc-3485"
Jun 22 11:37:33.088: INFO: Deleting pod "simpletest-rc-to-be-deleted-9tddj" in namespace "gc-3485"
Jun 22 11:37:33.102: INFO: Deleting pod "simpletest-rc-to-be-deleted-9v9l6" in namespace "gc-3485"
Jun 22 11:37:33.118: INFO: Deleting pod "simpletest-rc-to-be-deleted-blkh4" in namespace "gc-3485"
Jun 22 11:37:33.149: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvw2q" in namespace "gc-3485"
Jun 22 11:37:33.172: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9smp" in namespace "gc-3485"
Jun 22 11:37:33.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-cb997" in namespace "gc-3485"
Jun 22 11:37:33.203: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgzqt" in namespace "gc-3485"
Jun 22 11:37:33.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnc4x" in namespace "gc-3485"
Jun 22 11:37:33.233: INFO: Deleting pod "simpletest-rc-to-be-deleted-cpgcg" in namespace "gc-3485"
Jun 22 11:37:33.251: INFO: Deleting pod "simpletest-rc-to-be-deleted-crxv4" in namespace "gc-3485"
Jun 22 11:37:33.271: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpnr7" in namespace "gc-3485"
Jun 22 11:37:33.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-drj5c" in namespace "gc-3485"
Jun 22 11:37:33.306: INFO: Deleting pod "simpletest-rc-to-be-deleted-f552x" in namespace "gc-3485"
Jun 22 11:37:33.324: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjvjl" in namespace "gc-3485"
Jun 22 11:37:33.338: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfgks" in namespace "gc-3485"
Jun 22 11:37:33.354: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmchq" in namespace "gc-3485"
Jun 22 11:37:33.382: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnkc6" in namespace "gc-3485"
Jun 22 11:37:33.398: INFO: Deleting pod "simpletest-rc-to-be-deleted-grz6k" in namespace "gc-3485"
Jun 22 11:37:33.419: INFO: Deleting pod "simpletest-rc-to-be-deleted-gskgj" in namespace "gc-3485"
Jun 22 11:37:33.439: INFO: Deleting pod "simpletest-rc-to-be-deleted-gttzs" in namespace "gc-3485"
Jun 22 11:37:33.463: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwmjq" in namespace "gc-3485"
Jun 22 11:37:33.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-hg77w" in namespace "gc-3485"
Jun 22 11:37:33.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgb2t" in namespace "gc-3485"
Jun 22 11:37:33.523: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhx94" in namespace "gc-3485"
Jun 22 11:37:33.545: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmb4t" in namespace "gc-3485"
Jun 22 11:37:33.563: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6grd" in namespace "gc-3485"
Jun 22 11:37:33.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6p5z" in namespace "gc-3485"
Jun 22 11:37:33.610: INFO: Deleting pod "simpletest-rc-to-be-deleted-jcfn6" in namespace "gc-3485"
Jun 22 11:37:33.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-jcvsb" in namespace "gc-3485"
Jun 22 11:37:33.652: INFO: Deleting pod "simpletest-rc-to-be-deleted-jfbb2" in namespace "gc-3485"
Jun 22 11:37:33.670: INFO: Deleting pod "simpletest-rc-to-be-deleted-jp5rd" in namespace "gc-3485"
Jun 22 11:37:33.689: INFO: Deleting pod "simpletest-rc-to-be-deleted-k4sg9" in namespace "gc-3485"
Jun 22 11:37:33.710: INFO: Deleting pod "simpletest-rc-to-be-deleted-k9qt7" in namespace "gc-3485"
Jun 22 11:37:33.731: INFO: Deleting pod "simpletest-rc-to-be-deleted-l75vw" in namespace "gc-3485"
Jun 22 11:37:33.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-lcf9x" in namespace "gc-3485"
Jun 22 11:37:33.770: INFO: Deleting pod "simpletest-rc-to-be-deleted-lsgw8" in namespace "gc-3485"
Jun 22 11:37:33.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-ml98j" in namespace "gc-3485"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun 22 11:37:33.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3485" for this suite. 06/22/23 11:37:33.821
------------------------------
• [SLOW TEST] [16.787 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:37:17.046
    Jun 22 11:37:17.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename gc 06/22/23 11:37:17.047
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:37:17.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:37:17.072
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 06/22/23 11:37:17.082
    W0622 11:37:17.091010      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: create the rc2 06/22/23 11:37:17.091
    W0622 11:37:17.098468      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 06/22/23 11:37:22.106
    STEP: delete the rc simpletest-rc-to-be-deleted 06/22/23 11:37:22.694
    STEP: wait for the rc to be deleted 06/22/23 11:37:22.706
    Jun 22 11:37:27.727: INFO: 70 pods remaining
    Jun 22 11:37:27.727: INFO: 70 pods has nil DeletionTimestamp
    Jun 22 11:37:27.727: INFO: 
    STEP: Gathering metrics 06/22/23 11:37:32.724
    Jun 22 11:37:32.765: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" in namespace "kube-system" to be "running and ready"
    Jun 22 11:37:32.770: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v": Phase="Running", Reason="", readiness=true. Elapsed: 4.676106ms
    Jun 22 11:37:32.770: INFO: The phase of Pod kube-controller-manager-wl-antrea-prdzc-k4b4v is Running (Ready = true)
    Jun 22 11:37:32.770: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" satisfied condition "running and ready"
    Jun 22 11:37:32.862: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jun 22 11:37:32.862: INFO: Deleting pod "simpletest-rc-to-be-deleted-22n2h" in namespace "gc-3485"
    Jun 22 11:37:32.883: INFO: Deleting pod "simpletest-rc-to-be-deleted-26524" in namespace "gc-3485"
    Jun 22 11:37:32.900: INFO: Deleting pod "simpletest-rc-to-be-deleted-28tts" in namespace "gc-3485"
    Jun 22 11:37:32.917: INFO: Deleting pod "simpletest-rc-to-be-deleted-2br5n" in namespace "gc-3485"
    Jun 22 11:37:32.939: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gxpv" in namespace "gc-3485"
    Jun 22 11:37:32.953: INFO: Deleting pod "simpletest-rc-to-be-deleted-46xxw" in namespace "gc-3485"
    Jun 22 11:37:32.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ttvl" in namespace "gc-3485"
    Jun 22 11:37:32.991: INFO: Deleting pod "simpletest-rc-to-be-deleted-5c2mm" in namespace "gc-3485"
    Jun 22 11:37:33.012: INFO: Deleting pod "simpletest-rc-to-be-deleted-72zl9" in namespace "gc-3485"
    Jun 22 11:37:33.028: INFO: Deleting pod "simpletest-rc-to-be-deleted-75dcb" in namespace "gc-3485"
    Jun 22 11:37:33.045: INFO: Deleting pod "simpletest-rc-to-be-deleted-78xz9" in namespace "gc-3485"
    Jun 22 11:37:33.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-8bftp" in namespace "gc-3485"
    Jun 22 11:37:33.072: INFO: Deleting pod "simpletest-rc-to-be-deleted-97whg" in namespace "gc-3485"
    Jun 22 11:37:33.088: INFO: Deleting pod "simpletest-rc-to-be-deleted-9tddj" in namespace "gc-3485"
    Jun 22 11:37:33.102: INFO: Deleting pod "simpletest-rc-to-be-deleted-9v9l6" in namespace "gc-3485"
    Jun 22 11:37:33.118: INFO: Deleting pod "simpletest-rc-to-be-deleted-blkh4" in namespace "gc-3485"
    Jun 22 11:37:33.149: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvw2q" in namespace "gc-3485"
    Jun 22 11:37:33.172: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9smp" in namespace "gc-3485"
    Jun 22 11:37:33.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-cb997" in namespace "gc-3485"
    Jun 22 11:37:33.203: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgzqt" in namespace "gc-3485"
    Jun 22 11:37:33.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnc4x" in namespace "gc-3485"
    Jun 22 11:37:33.233: INFO: Deleting pod "simpletest-rc-to-be-deleted-cpgcg" in namespace "gc-3485"
    Jun 22 11:37:33.251: INFO: Deleting pod "simpletest-rc-to-be-deleted-crxv4" in namespace "gc-3485"
    Jun 22 11:37:33.271: INFO: Deleting pod "simpletest-rc-to-be-deleted-dpnr7" in namespace "gc-3485"
    Jun 22 11:37:33.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-drj5c" in namespace "gc-3485"
    Jun 22 11:37:33.306: INFO: Deleting pod "simpletest-rc-to-be-deleted-f552x" in namespace "gc-3485"
    Jun 22 11:37:33.324: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjvjl" in namespace "gc-3485"
    Jun 22 11:37:33.338: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfgks" in namespace "gc-3485"
    Jun 22 11:37:33.354: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmchq" in namespace "gc-3485"
    Jun 22 11:37:33.382: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnkc6" in namespace "gc-3485"
    Jun 22 11:37:33.398: INFO: Deleting pod "simpletest-rc-to-be-deleted-grz6k" in namespace "gc-3485"
    Jun 22 11:37:33.419: INFO: Deleting pod "simpletest-rc-to-be-deleted-gskgj" in namespace "gc-3485"
    Jun 22 11:37:33.439: INFO: Deleting pod "simpletest-rc-to-be-deleted-gttzs" in namespace "gc-3485"
    Jun 22 11:37:33.463: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwmjq" in namespace "gc-3485"
    Jun 22 11:37:33.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-hg77w" in namespace "gc-3485"
    Jun 22 11:37:33.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgb2t" in namespace "gc-3485"
    Jun 22 11:37:33.523: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhx94" in namespace "gc-3485"
    Jun 22 11:37:33.545: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmb4t" in namespace "gc-3485"
    Jun 22 11:37:33.563: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6grd" in namespace "gc-3485"
    Jun 22 11:37:33.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6p5z" in namespace "gc-3485"
    Jun 22 11:37:33.610: INFO: Deleting pod "simpletest-rc-to-be-deleted-jcfn6" in namespace "gc-3485"
    Jun 22 11:37:33.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-jcvsb" in namespace "gc-3485"
    Jun 22 11:37:33.652: INFO: Deleting pod "simpletest-rc-to-be-deleted-jfbb2" in namespace "gc-3485"
    Jun 22 11:37:33.670: INFO: Deleting pod "simpletest-rc-to-be-deleted-jp5rd" in namespace "gc-3485"
    Jun 22 11:37:33.689: INFO: Deleting pod "simpletest-rc-to-be-deleted-k4sg9" in namespace "gc-3485"
    Jun 22 11:37:33.710: INFO: Deleting pod "simpletest-rc-to-be-deleted-k9qt7" in namespace "gc-3485"
    Jun 22 11:37:33.731: INFO: Deleting pod "simpletest-rc-to-be-deleted-l75vw" in namespace "gc-3485"
    Jun 22 11:37:33.753: INFO: Deleting pod "simpletest-rc-to-be-deleted-lcf9x" in namespace "gc-3485"
    Jun 22 11:37:33.770: INFO: Deleting pod "simpletest-rc-to-be-deleted-lsgw8" in namespace "gc-3485"
    Jun 22 11:37:33.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-ml98j" in namespace "gc-3485"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:37:33.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3485" for this suite. 06/22/23 11:37:33.821
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:37:33.834
Jun 22 11:37:33.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename downward-api 06/22/23 11:37:33.836
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:37:33.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:37:33.866
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 06/22/23 11:37:33.87
W0622 11:37:33.883622      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:37:33.883: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c4e8b4e-a72b-4d28-b2b4-77ca7f227cd5" in namespace "downward-api-2243" to be "Succeeded or Failed"
Jun 22 11:37:33.892: INFO: Pod "downwardapi-volume-7c4e8b4e-a72b-4d28-b2b4-77ca7f227cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.870598ms
Jun 22 11:37:35.900: INFO: Pod "downwardapi-volume-7c4e8b4e-a72b-4d28-b2b4-77ca7f227cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016733058s
Jun 22 11:37:37.898: INFO: Pod "downwardapi-volume-7c4e8b4e-a72b-4d28-b2b4-77ca7f227cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014476399s
Jun 22 11:37:39.899: INFO: Pod "downwardapi-volume-7c4e8b4e-a72b-4d28-b2b4-77ca7f227cd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015406542s
STEP: Saw pod success 06/22/23 11:37:39.899
Jun 22 11:37:39.899: INFO: Pod "downwardapi-volume-7c4e8b4e-a72b-4d28-b2b4-77ca7f227cd5" satisfied condition "Succeeded or Failed"
Jun 22 11:37:39.904: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-7c4e8b4e-a72b-4d28-b2b4-77ca7f227cd5 container client-container: <nil>
STEP: delete the pod 06/22/23 11:37:39.913
Jun 22 11:37:39.930: INFO: Waiting for pod downwardapi-volume-7c4e8b4e-a72b-4d28-b2b4-77ca7f227cd5 to disappear
Jun 22 11:37:39.934: INFO: Pod downwardapi-volume-7c4e8b4e-a72b-4d28-b2b4-77ca7f227cd5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jun 22 11:37:39.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2243" for this suite. 06/22/23 11:37:39.941
------------------------------
• [SLOW TEST] [6.115 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:37:33.834
    Jun 22 11:37:33.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename downward-api 06/22/23 11:37:33.836
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:37:33.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:37:33.866
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 06/22/23 11:37:33.87
    W0622 11:37:33.883622      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:37:33.883: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c4e8b4e-a72b-4d28-b2b4-77ca7f227cd5" in namespace "downward-api-2243" to be "Succeeded or Failed"
    Jun 22 11:37:33.892: INFO: Pod "downwardapi-volume-7c4e8b4e-a72b-4d28-b2b4-77ca7f227cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.870598ms
    Jun 22 11:37:35.900: INFO: Pod "downwardapi-volume-7c4e8b4e-a72b-4d28-b2b4-77ca7f227cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016733058s
    Jun 22 11:37:37.898: INFO: Pod "downwardapi-volume-7c4e8b4e-a72b-4d28-b2b4-77ca7f227cd5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014476399s
    Jun 22 11:37:39.899: INFO: Pod "downwardapi-volume-7c4e8b4e-a72b-4d28-b2b4-77ca7f227cd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015406542s
    STEP: Saw pod success 06/22/23 11:37:39.899
    Jun 22 11:37:39.899: INFO: Pod "downwardapi-volume-7c4e8b4e-a72b-4d28-b2b4-77ca7f227cd5" satisfied condition "Succeeded or Failed"
    Jun 22 11:37:39.904: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-7c4e8b4e-a72b-4d28-b2b4-77ca7f227cd5 container client-container: <nil>
    STEP: delete the pod 06/22/23 11:37:39.913
    Jun 22 11:37:39.930: INFO: Waiting for pod downwardapi-volume-7c4e8b4e-a72b-4d28-b2b4-77ca7f227cd5 to disappear
    Jun 22 11:37:39.934: INFO: Pod downwardapi-volume-7c4e8b4e-a72b-4d28-b2b4-77ca7f227cd5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:37:39.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2243" for this suite. 06/22/23 11:37:39.941
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:37:39.95
Jun 22 11:37:39.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename replicaset 06/22/23 11:37:39.951
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:37:39.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:37:39.976
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jun 22 11:37:39.979: INFO: Creating ReplicaSet my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b
W0622 11:37:39.986371      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:37:39.992: INFO: Pod name my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b: Found 0 pods out of 1
Jun 22 11:37:44.998: INFO: Pod name my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b: Found 1 pods out of 1
Jun 22 11:37:44.998: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b" is running
Jun 22 11:37:44.998: INFO: Waiting up to 5m0s for pod "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b-f5mrk" in namespace "replicaset-4502" to be "running"
Jun 22 11:37:45.002: INFO: Pod "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b-f5mrk": Phase="Running", Reason="", readiness=true. Elapsed: 4.377542ms
Jun 22 11:37:45.002: INFO: Pod "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b-f5mrk" satisfied condition "running"
Jun 22 11:37:45.002: INFO: Pod "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b-f5mrk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-22 11:37:40 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-22 11:37:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-22 11:37:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-22 11:37:40 +0000 UTC Reason: Message:}])
Jun 22 11:37:45.002: INFO: Trying to dial the pod
Jun 22 11:37:50.027: INFO: Controller my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b: Got expected result from replica 1 [my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b-f5mrk]: "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b-f5mrk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jun 22 11:37:50.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4502" for this suite. 06/22/23 11:37:50.036
------------------------------
• [SLOW TEST] [10.096 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:37:39.95
    Jun 22 11:37:39.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename replicaset 06/22/23 11:37:39.951
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:37:39.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:37:39.976
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jun 22 11:37:39.979: INFO: Creating ReplicaSet my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b
    W0622 11:37:39.986371      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:37:39.992: INFO: Pod name my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b: Found 0 pods out of 1
    Jun 22 11:37:44.998: INFO: Pod name my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b: Found 1 pods out of 1
    Jun 22 11:37:44.998: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b" is running
    Jun 22 11:37:44.998: INFO: Waiting up to 5m0s for pod "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b-f5mrk" in namespace "replicaset-4502" to be "running"
    Jun 22 11:37:45.002: INFO: Pod "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b-f5mrk": Phase="Running", Reason="", readiness=true. Elapsed: 4.377542ms
    Jun 22 11:37:45.002: INFO: Pod "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b-f5mrk" satisfied condition "running"
    Jun 22 11:37:45.002: INFO: Pod "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b-f5mrk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-22 11:37:40 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-22 11:37:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-22 11:37:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-22 11:37:40 +0000 UTC Reason: Message:}])
    Jun 22 11:37:45.002: INFO: Trying to dial the pod
    Jun 22 11:37:50.027: INFO: Controller my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b: Got expected result from replica 1 [my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b-f5mrk]: "my-hostname-basic-c9938290-605f-45da-affc-c00d9e006e8b-f5mrk", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:37:50.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4502" for this suite. 06/22/23 11:37:50.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:37:50.046
Jun 22 11:37:50.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename emptydir 06/22/23 11:37:50.047
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:37:50.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:37:50.074
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 06/22/23 11:37:50.078
W0622 11:37:50.089391      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:37:50.089: INFO: Waiting up to 5m0s for pod "pod-694d17a9-4ec4-4388-99e3-f872f2486643" in namespace "emptydir-29" to be "Succeeded or Failed"
Jun 22 11:37:50.096: INFO: Pod "pod-694d17a9-4ec4-4388-99e3-f872f2486643": Phase="Pending", Reason="", readiness=false. Elapsed: 7.261889ms
Jun 22 11:37:52.104: INFO: Pod "pod-694d17a9-4ec4-4388-99e3-f872f2486643": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014529743s
Jun 22 11:37:54.102: INFO: Pod "pod-694d17a9-4ec4-4388-99e3-f872f2486643": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013081515s
STEP: Saw pod success 06/22/23 11:37:54.102
Jun 22 11:37:54.102: INFO: Pod "pod-694d17a9-4ec4-4388-99e3-f872f2486643" satisfied condition "Succeeded or Failed"
Jun 22 11:37:54.107: INFO: Trying to get logs from node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k pod pod-694d17a9-4ec4-4388-99e3-f872f2486643 container test-container: <nil>
STEP: delete the pod 06/22/23 11:37:54.124
Jun 22 11:37:54.140: INFO: Waiting for pod pod-694d17a9-4ec4-4388-99e3-f872f2486643 to disappear
Jun 22 11:37:54.145: INFO: Pod pod-694d17a9-4ec4-4388-99e3-f872f2486643 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun 22 11:37:54.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-29" for this suite. 06/22/23 11:37:54.152
------------------------------
• [4.113 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:37:50.046
    Jun 22 11:37:50.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename emptydir 06/22/23 11:37:50.047
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:37:50.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:37:50.074
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 06/22/23 11:37:50.078
    W0622 11:37:50.089391      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:37:50.089: INFO: Waiting up to 5m0s for pod "pod-694d17a9-4ec4-4388-99e3-f872f2486643" in namespace "emptydir-29" to be "Succeeded or Failed"
    Jun 22 11:37:50.096: INFO: Pod "pod-694d17a9-4ec4-4388-99e3-f872f2486643": Phase="Pending", Reason="", readiness=false. Elapsed: 7.261889ms
    Jun 22 11:37:52.104: INFO: Pod "pod-694d17a9-4ec4-4388-99e3-f872f2486643": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014529743s
    Jun 22 11:37:54.102: INFO: Pod "pod-694d17a9-4ec4-4388-99e3-f872f2486643": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013081515s
    STEP: Saw pod success 06/22/23 11:37:54.102
    Jun 22 11:37:54.102: INFO: Pod "pod-694d17a9-4ec4-4388-99e3-f872f2486643" satisfied condition "Succeeded or Failed"
    Jun 22 11:37:54.107: INFO: Trying to get logs from node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k pod pod-694d17a9-4ec4-4388-99e3-f872f2486643 container test-container: <nil>
    STEP: delete the pod 06/22/23 11:37:54.124
    Jun 22 11:37:54.140: INFO: Waiting for pod pod-694d17a9-4ec4-4388-99e3-f872f2486643 to disappear
    Jun 22 11:37:54.145: INFO: Pod pod-694d17a9-4ec4-4388-99e3-f872f2486643 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:37:54.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-29" for this suite. 06/22/23 11:37:54.152
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:37:54.161
Jun 22 11:37:54.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename services 06/22/23 11:37:54.162
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:37:54.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:37:54.19
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 06/22/23 11:37:54.199
STEP: watching for the Service to be added 06/22/23 11:37:54.22
Jun 22 11:37:54.223: INFO: Found Service test-service-wmzw6 in namespace services-995 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jun 22 11:37:54.224: INFO: Service test-service-wmzw6 created
STEP: Getting /status 06/22/23 11:37:54.224
Jun 22 11:37:54.231: INFO: Service test-service-wmzw6 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 06/22/23 11:37:54.231
STEP: watching for the Service to be patched 06/22/23 11:37:54.238
Jun 22 11:37:54.241: INFO: observed Service test-service-wmzw6 in namespace services-995 with annotations: map[] & LoadBalancer: {[]}
Jun 22 11:37:54.241: INFO: Found Service test-service-wmzw6 in namespace services-995 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jun 22 11:37:54.241: INFO: Service test-service-wmzw6 has service status patched
STEP: updating the ServiceStatus 06/22/23 11:37:54.241
Jun 22 11:37:54.256: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 06/22/23 11:37:54.256
Jun 22 11:37:54.259: INFO: Observed Service test-service-wmzw6 in namespace services-995 with annotations: map[] & Conditions: {[]}
Jun 22 11:37:54.259: INFO: Observed event: &Service{ObjectMeta:{test-service-wmzw6  services-995  876b6bac-6617-4127-85a2-2df0f225e663 160227 0 2023-06-22 11:37:54 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-06-22 11:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-06-22 11:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:100.64.220.147,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[100.64.220.147],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jun 22 11:37:54.260: INFO: Found Service test-service-wmzw6 in namespace services-995 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 22 11:37:54.260: INFO: Service test-service-wmzw6 has service status updated
STEP: patching the service 06/22/23 11:37:54.26
STEP: watching for the Service to be patched 06/22/23 11:37:54.268
Jun 22 11:37:54.272: INFO: observed Service test-service-wmzw6 in namespace services-995 with labels: map[test-service-static:true]
Jun 22 11:37:54.272: INFO: observed Service test-service-wmzw6 in namespace services-995 with labels: map[test-service-static:true]
Jun 22 11:37:54.272: INFO: observed Service test-service-wmzw6 in namespace services-995 with labels: map[test-service-static:true]
Jun 22 11:37:54.272: INFO: Found Service test-service-wmzw6 in namespace services-995 with labels: map[test-service:patched test-service-static:true]
Jun 22 11:37:54.272: INFO: Service test-service-wmzw6 patched
STEP: deleting the service 06/22/23 11:37:54.272
STEP: watching for the Service to be deleted 06/22/23 11:37:54.303
Jun 22 11:37:54.305: INFO: Observed event: ADDED
Jun 22 11:37:54.305: INFO: Observed event: MODIFIED
Jun 22 11:37:54.305: INFO: Observed event: MODIFIED
Jun 22 11:37:54.305: INFO: Observed event: MODIFIED
Jun 22 11:37:54.306: INFO: Found Service test-service-wmzw6 in namespace services-995 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jun 22 11:37:54.306: INFO: Service test-service-wmzw6 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun 22 11:37:54.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-995" for this suite. 06/22/23 11:37:54.314
------------------------------
• [0.166 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:37:54.161
    Jun 22 11:37:54.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename services 06/22/23 11:37:54.162
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:37:54.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:37:54.19
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 06/22/23 11:37:54.199
    STEP: watching for the Service to be added 06/22/23 11:37:54.22
    Jun 22 11:37:54.223: INFO: Found Service test-service-wmzw6 in namespace services-995 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jun 22 11:37:54.224: INFO: Service test-service-wmzw6 created
    STEP: Getting /status 06/22/23 11:37:54.224
    Jun 22 11:37:54.231: INFO: Service test-service-wmzw6 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 06/22/23 11:37:54.231
    STEP: watching for the Service to be patched 06/22/23 11:37:54.238
    Jun 22 11:37:54.241: INFO: observed Service test-service-wmzw6 in namespace services-995 with annotations: map[] & LoadBalancer: {[]}
    Jun 22 11:37:54.241: INFO: Found Service test-service-wmzw6 in namespace services-995 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jun 22 11:37:54.241: INFO: Service test-service-wmzw6 has service status patched
    STEP: updating the ServiceStatus 06/22/23 11:37:54.241
    Jun 22 11:37:54.256: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 06/22/23 11:37:54.256
    Jun 22 11:37:54.259: INFO: Observed Service test-service-wmzw6 in namespace services-995 with annotations: map[] & Conditions: {[]}
    Jun 22 11:37:54.259: INFO: Observed event: &Service{ObjectMeta:{test-service-wmzw6  services-995  876b6bac-6617-4127-85a2-2df0f225e663 160227 0 2023-06-22 11:37:54 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-06-22 11:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-06-22 11:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:100.64.220.147,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[100.64.220.147],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jun 22 11:37:54.260: INFO: Found Service test-service-wmzw6 in namespace services-995 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun 22 11:37:54.260: INFO: Service test-service-wmzw6 has service status updated
    STEP: patching the service 06/22/23 11:37:54.26
    STEP: watching for the Service to be patched 06/22/23 11:37:54.268
    Jun 22 11:37:54.272: INFO: observed Service test-service-wmzw6 in namespace services-995 with labels: map[test-service-static:true]
    Jun 22 11:37:54.272: INFO: observed Service test-service-wmzw6 in namespace services-995 with labels: map[test-service-static:true]
    Jun 22 11:37:54.272: INFO: observed Service test-service-wmzw6 in namespace services-995 with labels: map[test-service-static:true]
    Jun 22 11:37:54.272: INFO: Found Service test-service-wmzw6 in namespace services-995 with labels: map[test-service:patched test-service-static:true]
    Jun 22 11:37:54.272: INFO: Service test-service-wmzw6 patched
    STEP: deleting the service 06/22/23 11:37:54.272
    STEP: watching for the Service to be deleted 06/22/23 11:37:54.303
    Jun 22 11:37:54.305: INFO: Observed event: ADDED
    Jun 22 11:37:54.305: INFO: Observed event: MODIFIED
    Jun 22 11:37:54.305: INFO: Observed event: MODIFIED
    Jun 22 11:37:54.305: INFO: Observed event: MODIFIED
    Jun 22 11:37:54.306: INFO: Found Service test-service-wmzw6 in namespace services-995 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jun 22 11:37:54.306: INFO: Service test-service-wmzw6 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:37:54.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-995" for this suite. 06/22/23 11:37:54.314
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:37:54.329
Jun 22 11:37:54.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename runtimeclass 06/22/23 11:37:54.33
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:37:54.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:37:54.356
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-4806-delete-me 06/22/23 11:37:54.371
STEP: Waiting for the RuntimeClass to disappear 06/22/23 11:37:54.382
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jun 22 11:37:54.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4806" for this suite. 06/22/23 11:37:54.409
------------------------------
• [0.092 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:37:54.329
    Jun 22 11:37:54.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename runtimeclass 06/22/23 11:37:54.33
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:37:54.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:37:54.356
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-4806-delete-me 06/22/23 11:37:54.371
    STEP: Waiting for the RuntimeClass to disappear 06/22/23 11:37:54.382
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:37:54.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4806" for this suite. 06/22/23 11:37:54.409
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:37:54.422
Jun 22 11:37:54.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename pod-network-test 06/22/23 11:37:54.422
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:37:54.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:37:54.449
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-9838 06/22/23 11:37:54.453
STEP: creating a selector 06/22/23 11:37:54.453
STEP: Creating the service pods in kubernetes 06/22/23 11:37:54.453
Jun 22 11:37:54.453: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
W0622 11:37:54.479291      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0622 11:37:54.489002      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0622 11:37:54.503767      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:37:54.503: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9838" to be "running and ready"
Jun 22 11:37:54.511: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.09697ms
Jun 22 11:37:54.511: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 11:37:56.518: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.014403491s
Jun 22 11:37:56.518: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:37:58.518: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014141639s
Jun 22 11:37:58.518: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:38:00.518: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014692241s
Jun 22 11:38:00.518: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:38:02.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013721001s
Jun 22 11:38:02.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:38:04.518: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014505202s
Jun 22 11:38:04.518: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:38:06.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.01375463s
Jun 22 11:38:06.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:38:08.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013804905s
Jun 22 11:38:08.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:38:10.518: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01471076s
Jun 22 11:38:10.518: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:38:12.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.013434159s
Jun 22 11:38:12.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:38:14.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013274368s
Jun 22 11:38:14.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 22 11:38:16.518: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014165787s
Jun 22 11:38:16.518: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun 22 11:38:16.518: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun 22 11:38:16.523: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9838" to be "running and ready"
Jun 22 11:38:16.527: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.619103ms
Jun 22 11:38:16.527: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun 22 11:38:16.527: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun 22 11:38:16.554: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9838" to be "running and ready"
Jun 22 11:38:16.562: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.382978ms
Jun 22 11:38:16.562: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun 22 11:38:16.562: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/22/23 11:38:16.567
W0622 11:38:16.574123      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:38:16.574: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9838" to be "running"
Jun 22 11:38:16.578: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.741153ms
Jun 22 11:38:18.584: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010713259s
Jun 22 11:38:18.584: INFO: Pod "test-container-pod" satisfied condition "running"
Jun 22 11:38:18.592: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 22 11:38:18.592: INFO: Breadth first check of 100.96.4.55 on host 10.92.224.179...
Jun 22 11:38:18.598: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.6:9080/dial?request=hostname&protocol=udp&host=100.96.4.55&port=8081&tries=1'] Namespace:pod-network-test-9838 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 11:38:18.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 11:38:18.599: INFO: ExecWithOptions: Clientset creation
Jun 22 11:38:18.599: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-9838/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.6%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.96.4.55%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 22 11:38:18.702: INFO: Waiting for responses: map[]
Jun 22 11:38:18.702: INFO: reached 100.96.4.55 after 0/1 tries
Jun 22 11:38:18.702: INFO: Breadth first check of 100.96.3.5 on host 10.92.226.162...
Jun 22 11:38:18.707: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.6:9080/dial?request=hostname&protocol=udp&host=100.96.3.5&port=8081&tries=1'] Namespace:pod-network-test-9838 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 11:38:18.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 11:38:18.708: INFO: ExecWithOptions: Clientset creation
Jun 22 11:38:18.708: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-9838/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.6%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.96.3.5%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 22 11:38:18.796: INFO: Waiting for responses: map[]
Jun 22 11:38:18.796: INFO: reached 100.96.3.5 after 0/1 tries
Jun 22 11:38:18.797: INFO: Breadth first check of 100.96.2.40 on host 10.92.224.62...
Jun 22 11:38:18.802: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.6:9080/dial?request=hostname&protocol=udp&host=100.96.2.40&port=8081&tries=1'] Namespace:pod-network-test-9838 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 11:38:18.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 11:38:18.803: INFO: ExecWithOptions: Clientset creation
Jun 22 11:38:18.804: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-9838/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.6%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.96.2.40%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 22 11:38:18.898: INFO: Waiting for responses: map[]
Jun 22 11:38:18.898: INFO: reached 100.96.2.40 after 0/1 tries
Jun 22 11:38:18.898: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jun 22 11:38:18.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9838" for this suite. 06/22/23 11:38:18.906
------------------------------
• [SLOW TEST] [24.493 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:37:54.422
    Jun 22 11:37:54.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename pod-network-test 06/22/23 11:37:54.422
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:37:54.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:37:54.449
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-9838 06/22/23 11:37:54.453
    STEP: creating a selector 06/22/23 11:37:54.453
    STEP: Creating the service pods in kubernetes 06/22/23 11:37:54.453
    Jun 22 11:37:54.453: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    W0622 11:37:54.479291      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0622 11:37:54.489002      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0622 11:37:54.503767      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:37:54.503: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9838" to be "running and ready"
    Jun 22 11:37:54.511: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.09697ms
    Jun 22 11:37:54.511: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 11:37:56.518: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.014403491s
    Jun 22 11:37:56.518: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:37:58.518: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014141639s
    Jun 22 11:37:58.518: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:38:00.518: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014692241s
    Jun 22 11:38:00.518: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:38:02.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013721001s
    Jun 22 11:38:02.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:38:04.518: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014505202s
    Jun 22 11:38:04.518: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:38:06.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.01375463s
    Jun 22 11:38:06.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:38:08.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013804905s
    Jun 22 11:38:08.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:38:10.518: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01471076s
    Jun 22 11:38:10.518: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:38:12.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.013434159s
    Jun 22 11:38:12.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:38:14.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013274368s
    Jun 22 11:38:14.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 22 11:38:16.518: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014165787s
    Jun 22 11:38:16.518: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun 22 11:38:16.518: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun 22 11:38:16.523: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9838" to be "running and ready"
    Jun 22 11:38:16.527: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.619103ms
    Jun 22 11:38:16.527: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun 22 11:38:16.527: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun 22 11:38:16.554: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9838" to be "running and ready"
    Jun 22 11:38:16.562: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.382978ms
    Jun 22 11:38:16.562: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun 22 11:38:16.562: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/22/23 11:38:16.567
    W0622 11:38:16.574123      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:38:16.574: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9838" to be "running"
    Jun 22 11:38:16.578: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.741153ms
    Jun 22 11:38:18.584: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010713259s
    Jun 22 11:38:18.584: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun 22 11:38:18.592: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun 22 11:38:18.592: INFO: Breadth first check of 100.96.4.55 on host 10.92.224.179...
    Jun 22 11:38:18.598: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.6:9080/dial?request=hostname&protocol=udp&host=100.96.4.55&port=8081&tries=1'] Namespace:pod-network-test-9838 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 11:38:18.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 11:38:18.599: INFO: ExecWithOptions: Clientset creation
    Jun 22 11:38:18.599: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-9838/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.6%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.96.4.55%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 22 11:38:18.702: INFO: Waiting for responses: map[]
    Jun 22 11:38:18.702: INFO: reached 100.96.4.55 after 0/1 tries
    Jun 22 11:38:18.702: INFO: Breadth first check of 100.96.3.5 on host 10.92.226.162...
    Jun 22 11:38:18.707: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.6:9080/dial?request=hostname&protocol=udp&host=100.96.3.5&port=8081&tries=1'] Namespace:pod-network-test-9838 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 11:38:18.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 11:38:18.708: INFO: ExecWithOptions: Clientset creation
    Jun 22 11:38:18.708: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-9838/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.6%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.96.3.5%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 22 11:38:18.796: INFO: Waiting for responses: map[]
    Jun 22 11:38:18.796: INFO: reached 100.96.3.5 after 0/1 tries
    Jun 22 11:38:18.797: INFO: Breadth first check of 100.96.2.40 on host 10.92.224.62...
    Jun 22 11:38:18.802: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.3.6:9080/dial?request=hostname&protocol=udp&host=100.96.2.40&port=8081&tries=1'] Namespace:pod-network-test-9838 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 11:38:18.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 11:38:18.803: INFO: ExecWithOptions: Clientset creation
    Jun 22 11:38:18.804: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/pod-network-test-9838/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F100.96.3.6%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D100.96.2.40%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 22 11:38:18.898: INFO: Waiting for responses: map[]
    Jun 22 11:38:18.898: INFO: reached 100.96.2.40 after 0/1 tries
    Jun 22 11:38:18.898: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:38:18.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9838" for this suite. 06/22/23 11:38:18.906
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:38:18.915
Jun 22 11:38:18.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubectl 06/22/23 11:38:18.916
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:38:18.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:38:18.943
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 06/22/23 11:38:18.948
Jun 22 11:38:18.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 create -f -'
Jun 22 11:38:20.241: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"update-demo\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"update-demo\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"update-demo\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"update-demo\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Jun 22 11:38:20.241: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/22/23 11:38:20.242
Jun 22 11:38:20.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 11:38:20.333: INFO: stderr: ""
Jun 22 11:38:20.333: INFO: stdout: "update-demo-nautilus-5gd5j update-demo-nautilus-nsdlc "
Jun 22 11:38:20.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 get pods update-demo-nautilus-5gd5j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 11:38:20.413: INFO: stderr: ""
Jun 22 11:38:20.413: INFO: stdout: ""
Jun 22 11:38:20.413: INFO: update-demo-nautilus-5gd5j is created but not running
Jun 22 11:38:25.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 22 11:38:25.496: INFO: stderr: ""
Jun 22 11:38:25.496: INFO: stdout: "update-demo-nautilus-5gd5j update-demo-nautilus-nsdlc "
Jun 22 11:38:25.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 get pods update-demo-nautilus-5gd5j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 11:38:25.578: INFO: stderr: ""
Jun 22 11:38:25.578: INFO: stdout: "true"
Jun 22 11:38:25.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 get pods update-demo-nautilus-5gd5j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 11:38:25.657: INFO: stderr: ""
Jun 22 11:38:25.657: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun 22 11:38:25.657: INFO: validating pod update-demo-nautilus-5gd5j
Jun 22 11:38:25.669: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 11:38:25.669: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 11:38:25.669: INFO: update-demo-nautilus-5gd5j is verified up and running
Jun 22 11:38:25.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 get pods update-demo-nautilus-nsdlc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 22 11:38:25.747: INFO: stderr: ""
Jun 22 11:38:25.747: INFO: stdout: "true"
Jun 22 11:38:25.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 get pods update-demo-nautilus-nsdlc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 22 11:38:25.827: INFO: stderr: ""
Jun 22 11:38:25.827: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jun 22 11:38:25.827: INFO: validating pod update-demo-nautilus-nsdlc
Jun 22 11:38:25.839: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 11:38:25.840: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 11:38:25.840: INFO: update-demo-nautilus-nsdlc is verified up and running
STEP: using delete to clean up resources 06/22/23 11:38:25.84
Jun 22 11:38:25.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 delete --grace-period=0 --force -f -'
Jun 22 11:38:25.930: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 11:38:25.930: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 22 11:38:25.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 get rc,svc -l name=update-demo --no-headers'
Jun 22 11:38:26.027: INFO: stderr: "No resources found in kubectl-5585 namespace.\n"
Jun 22 11:38:26.027: INFO: stdout: ""
Jun 22 11:38:26.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 11:38:26.110: INFO: stderr: ""
Jun 22 11:38:26.110: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun 22 11:38:26.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5585" for this suite. 06/22/23 11:38:26.117
------------------------------
• [SLOW TEST] [7.213 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:38:18.915
    Jun 22 11:38:18.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubectl 06/22/23 11:38:18.916
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:38:18.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:38:18.943
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 06/22/23 11:38:18.948
    Jun 22 11:38:18.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 create -f -'
    Jun 22 11:38:20.241: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"update-demo\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"update-demo\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"update-demo\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"update-demo\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
    Jun 22 11:38:20.241: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/22/23 11:38:20.242
    Jun 22 11:38:20.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 22 11:38:20.333: INFO: stderr: ""
    Jun 22 11:38:20.333: INFO: stdout: "update-demo-nautilus-5gd5j update-demo-nautilus-nsdlc "
    Jun 22 11:38:20.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 get pods update-demo-nautilus-5gd5j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 22 11:38:20.413: INFO: stderr: ""
    Jun 22 11:38:20.413: INFO: stdout: ""
    Jun 22 11:38:20.413: INFO: update-demo-nautilus-5gd5j is created but not running
    Jun 22 11:38:25.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 22 11:38:25.496: INFO: stderr: ""
    Jun 22 11:38:25.496: INFO: stdout: "update-demo-nautilus-5gd5j update-demo-nautilus-nsdlc "
    Jun 22 11:38:25.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 get pods update-demo-nautilus-5gd5j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 22 11:38:25.578: INFO: stderr: ""
    Jun 22 11:38:25.578: INFO: stdout: "true"
    Jun 22 11:38:25.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 get pods update-demo-nautilus-5gd5j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 22 11:38:25.657: INFO: stderr: ""
    Jun 22 11:38:25.657: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun 22 11:38:25.657: INFO: validating pod update-demo-nautilus-5gd5j
    Jun 22 11:38:25.669: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 22 11:38:25.669: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 22 11:38:25.669: INFO: update-demo-nautilus-5gd5j is verified up and running
    Jun 22 11:38:25.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 get pods update-demo-nautilus-nsdlc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 22 11:38:25.747: INFO: stderr: ""
    Jun 22 11:38:25.747: INFO: stdout: "true"
    Jun 22 11:38:25.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 get pods update-demo-nautilus-nsdlc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 22 11:38:25.827: INFO: stderr: ""
    Jun 22 11:38:25.827: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jun 22 11:38:25.827: INFO: validating pod update-demo-nautilus-nsdlc
    Jun 22 11:38:25.839: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 22 11:38:25.840: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 22 11:38:25.840: INFO: update-demo-nautilus-nsdlc is verified up and running
    STEP: using delete to clean up resources 06/22/23 11:38:25.84
    Jun 22 11:38:25.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 delete --grace-period=0 --force -f -'
    Jun 22 11:38:25.930: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 22 11:38:25.930: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jun 22 11:38:25.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 get rc,svc -l name=update-demo --no-headers'
    Jun 22 11:38:26.027: INFO: stderr: "No resources found in kubectl-5585 namespace.\n"
    Jun 22 11:38:26.027: INFO: stdout: ""
    Jun 22 11:38:26.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-5585 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jun 22 11:38:26.110: INFO: stderr: ""
    Jun 22 11:38:26.110: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:38:26.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5585" for this suite. 06/22/23 11:38:26.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:38:26.129
Jun 22 11:38:26.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubectl 06/22/23 11:38:26.13
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:38:26.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:38:26.158
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 06/22/23 11:38:26.162
Jun 22 11:38:26.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-8130 create -f -'
Jun 22 11:38:27.064: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"httpd\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"httpd\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"httpd\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"httpd\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Jun 22 11:38:27.064: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 06/22/23 11:38:27.064
Jun 22 11:38:27.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-8130 diff -f -'
Jun 22 11:38:27.934: INFO: rc: 1
Jun 22 11:38:27.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-8130 delete -f -'
Jun 22 11:38:28.018: INFO: stderr: ""
Jun 22 11:38:28.018: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun 22 11:38:28.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8130" for this suite. 06/22/23 11:38:28.027
------------------------------
• [1.907 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:38:26.129
    Jun 22 11:38:26.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubectl 06/22/23 11:38:26.13
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:38:26.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:38:26.158
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 06/22/23 11:38:26.162
    Jun 22 11:38:26.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-8130 create -f -'
    Jun 22 11:38:27.064: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"httpd\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"httpd\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"httpd\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"httpd\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
    Jun 22 11:38:27.064: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 06/22/23 11:38:27.064
    Jun 22 11:38:27.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-8130 diff -f -'
    Jun 22 11:38:27.934: INFO: rc: 1
    Jun 22 11:38:27.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-8130 delete -f -'
    Jun 22 11:38:28.018: INFO: stderr: ""
    Jun 22 11:38:28.018: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:38:28.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8130" for this suite. 06/22/23 11:38:28.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:38:28.036
Jun 22 11:38:28.037: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename taint-single-pod 06/22/23 11:38:28.038
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:38:28.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:38:28.066
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Jun 22 11:38:28.070: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 22 11:39:28.123: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Jun 22 11:39:28.129: INFO: Starting informer...
STEP: Starting pod... 06/22/23 11:39:28.129
W0622 11:39:28.144093      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:39:28.349: INFO: Pod is running on wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw. Tainting Node
STEP: Trying to apply a taint on the Node 06/22/23 11:39:28.349
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/22/23 11:39:28.371
STEP: Waiting short time to make sure Pod is queued for deletion 06/22/23 11:39:28.376
Jun 22 11:39:28.377: INFO: Pod wasn't evicted. Proceeding
Jun 22 11:39:28.377: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/22/23 11:39:28.394
STEP: Waiting some time to make sure that toleration time passed. 06/22/23 11:39:28.4
Jun 22 11:40:43.405: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:40:43.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-1863" for this suite. 06/22/23 11:40:43.414
------------------------------
• [SLOW TEST] [135.391 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:38:28.036
    Jun 22 11:38:28.037: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename taint-single-pod 06/22/23 11:38:28.038
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:38:28.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:38:28.066
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Jun 22 11:38:28.070: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 22 11:39:28.123: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Jun 22 11:39:28.129: INFO: Starting informer...
    STEP: Starting pod... 06/22/23 11:39:28.129
    W0622 11:39:28.144093      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:39:28.349: INFO: Pod is running on wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw. Tainting Node
    STEP: Trying to apply a taint on the Node 06/22/23 11:39:28.349
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/22/23 11:39:28.371
    STEP: Waiting short time to make sure Pod is queued for deletion 06/22/23 11:39:28.376
    Jun 22 11:39:28.377: INFO: Pod wasn't evicted. Proceeding
    Jun 22 11:39:28.377: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/22/23 11:39:28.394
    STEP: Waiting some time to make sure that toleration time passed. 06/22/23 11:39:28.4
    Jun 22 11:40:43.405: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:40:43.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-1863" for this suite. 06/22/23 11:40:43.414
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:40:43.428
Jun 22 11:40:43.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename events 06/22/23 11:40:43.43
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:40:43.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:40:43.454
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 06/22/23 11:40:43.457
STEP: get a list of Events with a label in the current namespace 06/22/23 11:40:43.478
STEP: delete a list of events 06/22/23 11:40:43.483
Jun 22 11:40:43.483: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 06/22/23 11:40:43.512
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jun 22 11:40:43.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9309" for this suite. 06/22/23 11:40:43.525
------------------------------
• [0.105 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:40:43.428
    Jun 22 11:40:43.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename events 06/22/23 11:40:43.43
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:40:43.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:40:43.454
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 06/22/23 11:40:43.457
    STEP: get a list of Events with a label in the current namespace 06/22/23 11:40:43.478
    STEP: delete a list of events 06/22/23 11:40:43.483
    Jun 22 11:40:43.483: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 06/22/23 11:40:43.512
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:40:43.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9309" for this suite. 06/22/23 11:40:43.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:40:43.535
Jun 22 11:40:43.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename gc 06/22/23 11:40:43.536
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:40:43.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:40:43.557
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 06/22/23 11:40:43.56
W0622 11:40:43.569702      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the Deployment to create new ReplicaSet 06/22/23 11:40:43.569
STEP: delete the deployment 06/22/23 11:40:44.085
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 06/22/23 11:40:44.093
STEP: Gathering metrics 06/22/23 11:40:44.626
Jun 22 11:40:44.656: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" in namespace "kube-system" to be "running and ready"
Jun 22 11:40:44.661: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v": Phase="Running", Reason="", readiness=true. Elapsed: 4.847257ms
Jun 22 11:40:44.661: INFO: The phase of Pod kube-controller-manager-wl-antrea-prdzc-k4b4v is Running (Ready = true)
Jun 22 11:40:44.661: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" satisfied condition "running and ready"
Jun 22 11:40:44.732: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun 22 11:40:44.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7235" for this suite. 06/22/23 11:40:44.74
------------------------------
• [1.215 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:40:43.535
    Jun 22 11:40:43.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename gc 06/22/23 11:40:43.536
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:40:43.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:40:43.557
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 06/22/23 11:40:43.56
    W0622 11:40:43.569702      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the Deployment to create new ReplicaSet 06/22/23 11:40:43.569
    STEP: delete the deployment 06/22/23 11:40:44.085
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 06/22/23 11:40:44.093
    STEP: Gathering metrics 06/22/23 11:40:44.626
    Jun 22 11:40:44.656: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" in namespace "kube-system" to be "running and ready"
    Jun 22 11:40:44.661: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v": Phase="Running", Reason="", readiness=true. Elapsed: 4.847257ms
    Jun 22 11:40:44.661: INFO: The phase of Pod kube-controller-manager-wl-antrea-prdzc-k4b4v is Running (Ready = true)
    Jun 22 11:40:44.661: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" satisfied condition "running and ready"
    Jun 22 11:40:44.732: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:40:44.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7235" for this suite. 06/22/23 11:40:44.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:40:44.753
Jun 22 11:40:44.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 11:40:44.754
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:40:44.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:40:44.778
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-d1808ddd-0f1d-429d-b85b-70e5719c51e4 06/22/23 11:40:44.782
STEP: Creating a pod to test consume secrets 06/22/23 11:40:44.788
W0622 11:40:44.800879      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:40:44.801: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-669ae5dc-932b-434a-a9f8-74f16404f978" in namespace "projected-43" to be "Succeeded or Failed"
Jun 22 11:40:44.806: INFO: Pod "pod-projected-secrets-669ae5dc-932b-434a-a9f8-74f16404f978": Phase="Pending", Reason="", readiness=false. Elapsed: 5.014915ms
Jun 22 11:40:46.813: INFO: Pod "pod-projected-secrets-669ae5dc-932b-434a-a9f8-74f16404f978": Phase="Running", Reason="", readiness=false. Elapsed: 2.012723778s
Jun 22 11:40:48.812: INFO: Pod "pod-projected-secrets-669ae5dc-932b-434a-a9f8-74f16404f978": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011529416s
STEP: Saw pod success 06/22/23 11:40:48.812
Jun 22 11:40:48.812: INFO: Pod "pod-projected-secrets-669ae5dc-932b-434a-a9f8-74f16404f978" satisfied condition "Succeeded or Failed"
Jun 22 11:40:48.818: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-secrets-669ae5dc-932b-434a-a9f8-74f16404f978 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/22/23 11:40:48.841
Jun 22 11:40:48.860: INFO: Waiting for pod pod-projected-secrets-669ae5dc-932b-434a-a9f8-74f16404f978 to disappear
Jun 22 11:40:48.864: INFO: Pod pod-projected-secrets-669ae5dc-932b-434a-a9f8-74f16404f978 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jun 22 11:40:48.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-43" for this suite. 06/22/23 11:40:48.871
------------------------------
• [4.126 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:40:44.753
    Jun 22 11:40:44.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 11:40:44.754
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:40:44.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:40:44.778
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-d1808ddd-0f1d-429d-b85b-70e5719c51e4 06/22/23 11:40:44.782
    STEP: Creating a pod to test consume secrets 06/22/23 11:40:44.788
    W0622 11:40:44.800879      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:40:44.801: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-669ae5dc-932b-434a-a9f8-74f16404f978" in namespace "projected-43" to be "Succeeded or Failed"
    Jun 22 11:40:44.806: INFO: Pod "pod-projected-secrets-669ae5dc-932b-434a-a9f8-74f16404f978": Phase="Pending", Reason="", readiness=false. Elapsed: 5.014915ms
    Jun 22 11:40:46.813: INFO: Pod "pod-projected-secrets-669ae5dc-932b-434a-a9f8-74f16404f978": Phase="Running", Reason="", readiness=false. Elapsed: 2.012723778s
    Jun 22 11:40:48.812: INFO: Pod "pod-projected-secrets-669ae5dc-932b-434a-a9f8-74f16404f978": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011529416s
    STEP: Saw pod success 06/22/23 11:40:48.812
    Jun 22 11:40:48.812: INFO: Pod "pod-projected-secrets-669ae5dc-932b-434a-a9f8-74f16404f978" satisfied condition "Succeeded or Failed"
    Jun 22 11:40:48.818: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-secrets-669ae5dc-932b-434a-a9f8-74f16404f978 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/22/23 11:40:48.841
    Jun 22 11:40:48.860: INFO: Waiting for pod pod-projected-secrets-669ae5dc-932b-434a-a9f8-74f16404f978 to disappear
    Jun 22 11:40:48.864: INFO: Pod pod-projected-secrets-669ae5dc-932b-434a-a9f8-74f16404f978 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:40:48.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-43" for this suite. 06/22/23 11:40:48.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:40:48.879
Jun 22 11:40:48.879: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename webhook 06/22/23 11:40:48.88
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:40:48.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:40:48.915
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/22/23 11:40:48.934
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:40:49.448
STEP: Deploying the webhook pod 06/22/23 11:40:49.462
W0622 11:40:49.479050      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 11:40:49.479
Jun 22 11:40:49.489: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/22/23 11:40:51.505
STEP: Verifying the service has paired with the endpoint 06/22/23 11:40:51.527
Jun 22 11:40:52.530: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 06/22/23 11:40:52.66
STEP: Creating a configMap that should be mutated 06/22/23 11:40:52.709
STEP: Deleting the collection of validation webhooks 06/22/23 11:40:52.75
STEP: Creating a configMap that should not be mutated 06/22/23 11:40:52.823
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:40:52.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9708" for this suite. 06/22/23 11:40:52.934
STEP: Destroying namespace "webhook-9708-markers" for this suite. 06/22/23 11:40:52.945
------------------------------
• [4.080 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:40:48.879
    Jun 22 11:40:48.879: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename webhook 06/22/23 11:40:48.88
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:40:48.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:40:48.915
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/22/23 11:40:48.934
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:40:49.448
    STEP: Deploying the webhook pod 06/22/23 11:40:49.462
    W0622 11:40:49.479050      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 11:40:49.479
    Jun 22 11:40:49.489: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/22/23 11:40:51.505
    STEP: Verifying the service has paired with the endpoint 06/22/23 11:40:51.527
    Jun 22 11:40:52.530: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 06/22/23 11:40:52.66
    STEP: Creating a configMap that should be mutated 06/22/23 11:40:52.709
    STEP: Deleting the collection of validation webhooks 06/22/23 11:40:52.75
    STEP: Creating a configMap that should not be mutated 06/22/23 11:40:52.823
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:40:52.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9708" for this suite. 06/22/23 11:40:52.934
    STEP: Destroying namespace "webhook-9708-markers" for this suite. 06/22/23 11:40:52.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:40:52.965
Jun 22 11:40:52.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename downward-api 06/22/23 11:40:52.976
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:40:53.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:40:53.027
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 06/22/23 11:40:53.032
W0622 11:40:53.045835      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:40:53.046: INFO: Waiting up to 5m0s for pod "downward-api-4e937e0d-9a63-4293-aa32-7f21ff1bb02a" in namespace "downward-api-3131" to be "Succeeded or Failed"
Jun 22 11:40:53.056: INFO: Pod "downward-api-4e937e0d-9a63-4293-aa32-7f21ff1bb02a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.396492ms
Jun 22 11:40:55.064: INFO: Pod "downward-api-4e937e0d-9a63-4293-aa32-7f21ff1bb02a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018014634s
Jun 22 11:40:57.063: INFO: Pod "downward-api-4e937e0d-9a63-4293-aa32-7f21ff1bb02a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016913823s
STEP: Saw pod success 06/22/23 11:40:57.063
Jun 22 11:40:57.064: INFO: Pod "downward-api-4e937e0d-9a63-4293-aa32-7f21ff1bb02a" satisfied condition "Succeeded or Failed"
Jun 22 11:40:57.069: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downward-api-4e937e0d-9a63-4293-aa32-7f21ff1bb02a container dapi-container: <nil>
STEP: delete the pod 06/22/23 11:40:57.082
Jun 22 11:40:57.101: INFO: Waiting for pod downward-api-4e937e0d-9a63-4293-aa32-7f21ff1bb02a to disappear
Jun 22 11:40:57.106: INFO: Pod downward-api-4e937e0d-9a63-4293-aa32-7f21ff1bb02a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jun 22 11:40:57.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3131" for this suite. 06/22/23 11:40:57.115
------------------------------
• [4.161 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:40:52.965
    Jun 22 11:40:52.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename downward-api 06/22/23 11:40:52.976
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:40:53.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:40:53.027
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 06/22/23 11:40:53.032
    W0622 11:40:53.045835      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:40:53.046: INFO: Waiting up to 5m0s for pod "downward-api-4e937e0d-9a63-4293-aa32-7f21ff1bb02a" in namespace "downward-api-3131" to be "Succeeded or Failed"
    Jun 22 11:40:53.056: INFO: Pod "downward-api-4e937e0d-9a63-4293-aa32-7f21ff1bb02a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.396492ms
    Jun 22 11:40:55.064: INFO: Pod "downward-api-4e937e0d-9a63-4293-aa32-7f21ff1bb02a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018014634s
    Jun 22 11:40:57.063: INFO: Pod "downward-api-4e937e0d-9a63-4293-aa32-7f21ff1bb02a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016913823s
    STEP: Saw pod success 06/22/23 11:40:57.063
    Jun 22 11:40:57.064: INFO: Pod "downward-api-4e937e0d-9a63-4293-aa32-7f21ff1bb02a" satisfied condition "Succeeded or Failed"
    Jun 22 11:40:57.069: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downward-api-4e937e0d-9a63-4293-aa32-7f21ff1bb02a container dapi-container: <nil>
    STEP: delete the pod 06/22/23 11:40:57.082
    Jun 22 11:40:57.101: INFO: Waiting for pod downward-api-4e937e0d-9a63-4293-aa32-7f21ff1bb02a to disappear
    Jun 22 11:40:57.106: INFO: Pod downward-api-4e937e0d-9a63-4293-aa32-7f21ff1bb02a no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:40:57.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3131" for this suite. 06/22/23 11:40:57.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:40:57.127
Jun 22 11:40:57.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename emptydir 06/22/23 11:40:57.129
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:40:57.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:40:57.157
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 06/22/23 11:40:57.161
W0622 11:40:57.175025      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:40:57.175: INFO: Waiting up to 5m0s for pod "pod-e54ca478-b29a-472a-94a5-a07494354c24" in namespace "emptydir-7788" to be "Succeeded or Failed"
Jun 22 11:40:57.179: INFO: Pod "pod-e54ca478-b29a-472a-94a5-a07494354c24": Phase="Pending", Reason="", readiness=false. Elapsed: 4.474674ms
Jun 22 11:40:59.187: INFO: Pod "pod-e54ca478-b29a-472a-94a5-a07494354c24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011795535s
Jun 22 11:41:01.187: INFO: Pod "pod-e54ca478-b29a-472a-94a5-a07494354c24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012090364s
STEP: Saw pod success 06/22/23 11:41:01.187
Jun 22 11:41:01.187: INFO: Pod "pod-e54ca478-b29a-472a-94a5-a07494354c24" satisfied condition "Succeeded or Failed"
Jun 22 11:41:01.193: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-e54ca478-b29a-472a-94a5-a07494354c24 container test-container: <nil>
STEP: delete the pod 06/22/23 11:41:01.204
Jun 22 11:41:01.219: INFO: Waiting for pod pod-e54ca478-b29a-472a-94a5-a07494354c24 to disappear
Jun 22 11:41:01.224: INFO: Pod pod-e54ca478-b29a-472a-94a5-a07494354c24 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun 22 11:41:01.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7788" for this suite. 06/22/23 11:41:01.234
------------------------------
• [4.115 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:40:57.127
    Jun 22 11:40:57.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename emptydir 06/22/23 11:40:57.129
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:40:57.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:40:57.157
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 06/22/23 11:40:57.161
    W0622 11:40:57.175025      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:40:57.175: INFO: Waiting up to 5m0s for pod "pod-e54ca478-b29a-472a-94a5-a07494354c24" in namespace "emptydir-7788" to be "Succeeded or Failed"
    Jun 22 11:40:57.179: INFO: Pod "pod-e54ca478-b29a-472a-94a5-a07494354c24": Phase="Pending", Reason="", readiness=false. Elapsed: 4.474674ms
    Jun 22 11:40:59.187: INFO: Pod "pod-e54ca478-b29a-472a-94a5-a07494354c24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011795535s
    Jun 22 11:41:01.187: INFO: Pod "pod-e54ca478-b29a-472a-94a5-a07494354c24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012090364s
    STEP: Saw pod success 06/22/23 11:41:01.187
    Jun 22 11:41:01.187: INFO: Pod "pod-e54ca478-b29a-472a-94a5-a07494354c24" satisfied condition "Succeeded or Failed"
    Jun 22 11:41:01.193: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-e54ca478-b29a-472a-94a5-a07494354c24 container test-container: <nil>
    STEP: delete the pod 06/22/23 11:41:01.204
    Jun 22 11:41:01.219: INFO: Waiting for pod pod-e54ca478-b29a-472a-94a5-a07494354c24 to disappear
    Jun 22 11:41:01.224: INFO: Pod pod-e54ca478-b29a-472a-94a5-a07494354c24 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:41:01.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7788" for this suite. 06/22/23 11:41:01.234
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:41:01.247
Jun 22 11:41:01.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename dns 06/22/23 11:41:01.249
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:01.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:01.278
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6140.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6140.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 06/22/23 11:41:01.282
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6140.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6140.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 06/22/23 11:41:01.283
STEP: creating a pod to probe /etc/hosts 06/22/23 11:41:01.283
STEP: submitting the pod to kubernetes 06/22/23 11:41:01.283
W0622 11:41:01.297234      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:41:01.297: INFO: Waiting up to 15m0s for pod "dns-test-2580113e-66e8-43b7-b368-8222cda0abc0" in namespace "dns-6140" to be "running"
Jun 22 11:41:01.302: INFO: Pod "dns-test-2580113e-66e8-43b7-b368-8222cda0abc0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.135105ms
Jun 22 11:41:03.308: INFO: Pod "dns-test-2580113e-66e8-43b7-b368-8222cda0abc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.010828733s
Jun 22 11:41:03.308: INFO: Pod "dns-test-2580113e-66e8-43b7-b368-8222cda0abc0" satisfied condition "running"
STEP: retrieving the pod 06/22/23 11:41:03.308
STEP: looking for the results for each expected name from probers 06/22/23 11:41:03.314
Jun 22 11:41:03.343: INFO: DNS probes using dns-6140/dns-test-2580113e-66e8-43b7-b368-8222cda0abc0 succeeded

STEP: deleting the pod 06/22/23 11:41:03.343
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun 22 11:41:03.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6140" for this suite. 06/22/23 11:41:03.371
------------------------------
• [2.136 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:41:01.247
    Jun 22 11:41:01.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename dns 06/22/23 11:41:01.249
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:01.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:01.278
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6140.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6140.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     06/22/23 11:41:01.282
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6140.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6140.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     06/22/23 11:41:01.283
    STEP: creating a pod to probe /etc/hosts 06/22/23 11:41:01.283
    STEP: submitting the pod to kubernetes 06/22/23 11:41:01.283
    W0622 11:41:01.297234      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:41:01.297: INFO: Waiting up to 15m0s for pod "dns-test-2580113e-66e8-43b7-b368-8222cda0abc0" in namespace "dns-6140" to be "running"
    Jun 22 11:41:01.302: INFO: Pod "dns-test-2580113e-66e8-43b7-b368-8222cda0abc0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.135105ms
    Jun 22 11:41:03.308: INFO: Pod "dns-test-2580113e-66e8-43b7-b368-8222cda0abc0": Phase="Running", Reason="", readiness=true. Elapsed: 2.010828733s
    Jun 22 11:41:03.308: INFO: Pod "dns-test-2580113e-66e8-43b7-b368-8222cda0abc0" satisfied condition "running"
    STEP: retrieving the pod 06/22/23 11:41:03.308
    STEP: looking for the results for each expected name from probers 06/22/23 11:41:03.314
    Jun 22 11:41:03.343: INFO: DNS probes using dns-6140/dns-test-2580113e-66e8-43b7-b368-8222cda0abc0 succeeded

    STEP: deleting the pod 06/22/23 11:41:03.343
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:41:03.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6140" for this suite. 06/22/23 11:41:03.371
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:41:03.384
Jun 22 11:41:03.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename deployment 06/22/23 11:41:03.386
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:03.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:03.411
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jun 22 11:41:03.415: INFO: Creating deployment "test-recreate-deployment"
W0622 11:41:03.426196      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:41:03.426: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 22 11:41:03.437: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jun 22 11:41:05.451: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 22 11:41:05.461: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
W0622 11:41:05.479699      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:41:05.479: INFO: Updating deployment test-recreate-deployment
Jun 22 11:41:05.479: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 22 11:41:05.580: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9929  b93b39ae-c72c-46be-bcd2-73dc72e3eaae 161538 2 2023-06-22 11:41:03 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-22 11:41:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 11:41:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053f5168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-22 11:41:05 +0000 UTC,LastTransitionTime:2023-06-22 11:41:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-06-22 11:41:05 +0000 UTC,LastTransitionTime:2023-06-22 11:41:03 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun 22 11:41:05.588: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9929  7868018e-01ba-4c48-b08f-500ff4437c9f 161534 1 2023-06-22 11:41:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b93b39ae-c72c-46be-bcd2-73dc72e3eaae 0xc0053f5630 0xc0053f5631}] [] [{kube-controller-manager Update apps/v1 2023-06-22 11:41:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b93b39ae-c72c-46be-bcd2-73dc72e3eaae\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 11:41:05 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053f56c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 11:41:05.588: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 22 11:41:05.588: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9929  f9bf9d87-be1b-49e6-adeb-0545135856af 161526 2 2023-06-22 11:41:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b93b39ae-c72c-46be-bcd2-73dc72e3eaae 0xc0053f5517 0xc0053f5518}] [] [{kube-controller-manager Update apps/v1 2023-06-22 11:41:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b93b39ae-c72c-46be-bcd2-73dc72e3eaae\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 11:41:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053f55c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 22 11:41:05.595: INFO: Pod "test-recreate-deployment-cff6dc657-bckk8" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-bckk8 test-recreate-deployment-cff6dc657- deployment-9929  b30d8a54-8c10-4539-91b6-afa5ff46e4fa 161539 0 2023-06-22 11:41:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 7868018e-01ba-4c48-b08f-500ff4437c9f 0xc0053f5b40 0xc0053f5b41}] [] [{kube-controller-manager Update v1 2023-06-22 11:41:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7868018e-01ba-4c48-b08f-500ff4437c9f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 11:41:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t6jgb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t6jgb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 11:41:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 11:41:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 11:41:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 11:41:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:,StartTime:2023-06-22 11:41:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jun 22 11:41:05.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9929" for this suite. 06/22/23 11:41:05.605
------------------------------
• [2.232 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:41:03.384
    Jun 22 11:41:03.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename deployment 06/22/23 11:41:03.386
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:03.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:03.411
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jun 22 11:41:03.415: INFO: Creating deployment "test-recreate-deployment"
    W0622 11:41:03.426196      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:41:03.426: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jun 22 11:41:03.437: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
    Jun 22 11:41:05.451: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jun 22 11:41:05.461: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    W0622 11:41:05.479699      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:41:05.479: INFO: Updating deployment test-recreate-deployment
    Jun 22 11:41:05.479: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 22 11:41:05.580: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9929  b93b39ae-c72c-46be-bcd2-73dc72e3eaae 161538 2 2023-06-22 11:41:03 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-22 11:41:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 11:41:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053f5168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-22 11:41:05 +0000 UTC,LastTransitionTime:2023-06-22 11:41:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-06-22 11:41:05 +0000 UTC,LastTransitionTime:2023-06-22 11:41:03 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jun 22 11:41:05.588: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9929  7868018e-01ba-4c48-b08f-500ff4437c9f 161534 1 2023-06-22 11:41:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b93b39ae-c72c-46be-bcd2-73dc72e3eaae 0xc0053f5630 0xc0053f5631}] [] [{kube-controller-manager Update apps/v1 2023-06-22 11:41:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b93b39ae-c72c-46be-bcd2-73dc72e3eaae\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 11:41:05 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053f56c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 22 11:41:05.588: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jun 22 11:41:05.588: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9929  f9bf9d87-be1b-49e6-adeb-0545135856af 161526 2 2023-06-22 11:41:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b93b39ae-c72c-46be-bcd2-73dc72e3eaae 0xc0053f5517 0xc0053f5518}] [] [{kube-controller-manager Update apps/v1 2023-06-22 11:41:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b93b39ae-c72c-46be-bcd2-73dc72e3eaae\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-22 11:41:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053f55c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 22 11:41:05.595: INFO: Pod "test-recreate-deployment-cff6dc657-bckk8" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-bckk8 test-recreate-deployment-cff6dc657- deployment-9929  b30d8a54-8c10-4539-91b6-afa5ff46e4fa 161539 0 2023-06-22 11:41:05 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 7868018e-01ba-4c48-b08f-500ff4437c9f 0xc0053f5b40 0xc0053f5b41}] [] [{kube-controller-manager Update v1 2023-06-22 11:41:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7868018e-01ba-4c48-b08f-500ff4437c9f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-22 11:41:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t6jgb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t6jgb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 11:41:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 11:41:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 11:41:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-22 11:41:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.92.226.162,PodIP:,StartTime:2023-06-22 11:41:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:41:05.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9929" for this suite. 06/22/23 11:41:05.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:41:05.619
Jun 22 11:41:05.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename container-runtime 06/22/23 11:41:05.62
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:05.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:05.644
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 06/22/23 11:41:05.648
W0622 11:41:05.659026      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: wait for the container to reach Succeeded 06/22/23 11:41:05.659
STEP: get the container status 06/22/23 11:41:08.685
STEP: the container should be terminated 06/22/23 11:41:08.69
STEP: the termination message should be set 06/22/23 11:41:08.691
Jun 22 11:41:08.691: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 06/22/23 11:41:08.691
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jun 22 11:41:08.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2246" for this suite. 06/22/23 11:41:08.721
------------------------------
• [3.111 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:41:05.619
    Jun 22 11:41:05.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename container-runtime 06/22/23 11:41:05.62
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:05.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:05.644
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 06/22/23 11:41:05.648
    W0622 11:41:05.659026      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: wait for the container to reach Succeeded 06/22/23 11:41:05.659
    STEP: get the container status 06/22/23 11:41:08.685
    STEP: the container should be terminated 06/22/23 11:41:08.69
    STEP: the termination message should be set 06/22/23 11:41:08.691
    Jun 22 11:41:08.691: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 06/22/23 11:41:08.691
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:41:08.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2246" for this suite. 06/22/23 11:41:08.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:41:08.733
Jun 22 11:41:08.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename csiinlinevolumes 06/22/23 11:41:08.735
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:08.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:08.76
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 06/22/23 11:41:08.764
STEP: getting 06/22/23 11:41:08.79
STEP: listing 06/22/23 11:41:08.8
STEP: deleting 06/22/23 11:41:08.805
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jun 22 11:41:08.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-6473" for this suite. 06/22/23 11:41:08.838
------------------------------
• [0.114 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:41:08.733
    Jun 22 11:41:08.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename csiinlinevolumes 06/22/23 11:41:08.735
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:08.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:08.76
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 06/22/23 11:41:08.764
    STEP: getting 06/22/23 11:41:08.79
    STEP: listing 06/22/23 11:41:08.8
    STEP: deleting 06/22/23 11:41:08.805
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:41:08.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-6473" for this suite. 06/22/23 11:41:08.838
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:41:08.855
Jun 22 11:41:08.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename services 06/22/23 11:41:08.856
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:08.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:08.878
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-99 06/22/23 11:41:08.882
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/22/23 11:41:08.901
STEP: creating service externalsvc in namespace services-99 06/22/23 11:41:08.902
STEP: creating replication controller externalsvc in namespace services-99 06/22/23 11:41:08.922
W0622 11:41:08.932588      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "externalsvc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "externalsvc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "externalsvc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "externalsvc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0622 11:41:08.933231      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-99, replica count: 2
I0622 11:41:11.985087      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 06/22/23 11:41:11.992
Jun 22 11:41:12.026: INFO: Creating new exec pod
W0622 11:41:12.040139      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:41:12.040: INFO: Waiting up to 5m0s for pod "execpodh7h9s" in namespace "services-99" to be "running"
Jun 22 11:41:12.045: INFO: Pod "execpodh7h9s": Phase="Pending", Reason="", readiness=false. Elapsed: 5.244001ms
Jun 22 11:41:14.051: INFO: Pod "execpodh7h9s": Phase="Running", Reason="", readiness=true. Elapsed: 2.011292371s
Jun 22 11:41:14.051: INFO: Pod "execpodh7h9s" satisfied condition "running"
Jun 22 11:41:14.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-99 exec execpodh7h9s -- /bin/sh -x -c nslookup clusterip-service.services-99.svc.cluster.local'
Jun 22 11:41:14.389: INFO: stderr: "+ nslookup clusterip-service.services-99.svc.cluster.local\n"
Jun 22 11:41:14.389: INFO: stdout: "Server:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nclusterip-service.services-99.svc.cluster.local\tcanonical name = externalsvc.services-99.svc.cluster.local.\nName:\texternalsvc.services-99.svc.cluster.local\nAddress: 100.66.73.44\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-99, will wait for the garbage collector to delete the pods 06/22/23 11:41:14.389
Jun 22 11:41:14.455: INFO: Deleting ReplicationController externalsvc took: 9.846209ms
Jun 22 11:41:14.556: INFO: Terminating ReplicationController externalsvc pods took: 100.68812ms
Jun 22 11:41:16.988: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun 22 11:41:17.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-99" for this suite. 06/22/23 11:41:17.01
------------------------------
• [SLOW TEST] [8.163 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:41:08.855
    Jun 22 11:41:08.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename services 06/22/23 11:41:08.856
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:08.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:08.878
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-99 06/22/23 11:41:08.882
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/22/23 11:41:08.901
    STEP: creating service externalsvc in namespace services-99 06/22/23 11:41:08.902
    STEP: creating replication controller externalsvc in namespace services-99 06/22/23 11:41:08.922
    W0622 11:41:08.932588      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "externalsvc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "externalsvc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "externalsvc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "externalsvc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0622 11:41:08.933231      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-99, replica count: 2
    I0622 11:41:11.985087      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 06/22/23 11:41:11.992
    Jun 22 11:41:12.026: INFO: Creating new exec pod
    W0622 11:41:12.040139      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:41:12.040: INFO: Waiting up to 5m0s for pod "execpodh7h9s" in namespace "services-99" to be "running"
    Jun 22 11:41:12.045: INFO: Pod "execpodh7h9s": Phase="Pending", Reason="", readiness=false. Elapsed: 5.244001ms
    Jun 22 11:41:14.051: INFO: Pod "execpodh7h9s": Phase="Running", Reason="", readiness=true. Elapsed: 2.011292371s
    Jun 22 11:41:14.051: INFO: Pod "execpodh7h9s" satisfied condition "running"
    Jun 22 11:41:14.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-99 exec execpodh7h9s -- /bin/sh -x -c nslookup clusterip-service.services-99.svc.cluster.local'
    Jun 22 11:41:14.389: INFO: stderr: "+ nslookup clusterip-service.services-99.svc.cluster.local\n"
    Jun 22 11:41:14.389: INFO: stdout: "Server:\t\t100.64.0.10\nAddress:\t100.64.0.10#53\n\nclusterip-service.services-99.svc.cluster.local\tcanonical name = externalsvc.services-99.svc.cluster.local.\nName:\texternalsvc.services-99.svc.cluster.local\nAddress: 100.66.73.44\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-99, will wait for the garbage collector to delete the pods 06/22/23 11:41:14.389
    Jun 22 11:41:14.455: INFO: Deleting ReplicationController externalsvc took: 9.846209ms
    Jun 22 11:41:14.556: INFO: Terminating ReplicationController externalsvc pods took: 100.68812ms
    Jun 22 11:41:16.988: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:41:17.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-99" for this suite. 06/22/23 11:41:17.01
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:41:17.018
Jun 22 11:41:17.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename crd-publish-openapi 06/22/23 11:41:17.019
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:17.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:17.043
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 06/22/23 11:41:17.047
Jun 22 11:41:17.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: mark a version not serverd 06/22/23 11:41:22.363
STEP: check the unserved version gets removed 06/22/23 11:41:22.389
STEP: check the other version is not changed 06/22/23 11:41:24.925
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:41:29.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5547" for this suite. 06/22/23 11:41:29.419
------------------------------
• [SLOW TEST] [12.410 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:41:17.018
    Jun 22 11:41:17.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename crd-publish-openapi 06/22/23 11:41:17.019
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:17.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:17.043
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 06/22/23 11:41:17.047
    Jun 22 11:41:17.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: mark a version not serverd 06/22/23 11:41:22.363
    STEP: check the unserved version gets removed 06/22/23 11:41:22.389
    STEP: check the other version is not changed 06/22/23 11:41:24.925
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:41:29.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5547" for this suite. 06/22/23 11:41:29.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:41:29.43
Jun 22 11:41:29.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename containers 06/22/23 11:41:29.431
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:29.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:29.455
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 06/22/23 11:41:29.459
W0622 11:41:29.469583      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:41:29.469: INFO: Waiting up to 5m0s for pod "client-containers-014b35c5-4034-4ceb-9335-aee4824bbb31" in namespace "containers-9362" to be "Succeeded or Failed"
Jun 22 11:41:29.473: INFO: Pod "client-containers-014b35c5-4034-4ceb-9335-aee4824bbb31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.099981ms
Jun 22 11:41:31.480: INFO: Pod "client-containers-014b35c5-4034-4ceb-9335-aee4824bbb31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011041843s
Jun 22 11:41:33.481: INFO: Pod "client-containers-014b35c5-4034-4ceb-9335-aee4824bbb31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011511711s
STEP: Saw pod success 06/22/23 11:41:33.481
Jun 22 11:41:33.481: INFO: Pod "client-containers-014b35c5-4034-4ceb-9335-aee4824bbb31" satisfied condition "Succeeded or Failed"
Jun 22 11:41:33.486: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod client-containers-014b35c5-4034-4ceb-9335-aee4824bbb31 container agnhost-container: <nil>
STEP: delete the pod 06/22/23 11:41:33.499
Jun 22 11:41:33.518: INFO: Waiting for pod client-containers-014b35c5-4034-4ceb-9335-aee4824bbb31 to disappear
Jun 22 11:41:33.522: INFO: Pod client-containers-014b35c5-4034-4ceb-9335-aee4824bbb31 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jun 22 11:41:33.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9362" for this suite. 06/22/23 11:41:33.53
------------------------------
• [4.110 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:41:29.43
    Jun 22 11:41:29.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename containers 06/22/23 11:41:29.431
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:29.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:29.455
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 06/22/23 11:41:29.459
    W0622 11:41:29.469583      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:41:29.469: INFO: Waiting up to 5m0s for pod "client-containers-014b35c5-4034-4ceb-9335-aee4824bbb31" in namespace "containers-9362" to be "Succeeded or Failed"
    Jun 22 11:41:29.473: INFO: Pod "client-containers-014b35c5-4034-4ceb-9335-aee4824bbb31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.099981ms
    Jun 22 11:41:31.480: INFO: Pod "client-containers-014b35c5-4034-4ceb-9335-aee4824bbb31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011041843s
    Jun 22 11:41:33.481: INFO: Pod "client-containers-014b35c5-4034-4ceb-9335-aee4824bbb31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011511711s
    STEP: Saw pod success 06/22/23 11:41:33.481
    Jun 22 11:41:33.481: INFO: Pod "client-containers-014b35c5-4034-4ceb-9335-aee4824bbb31" satisfied condition "Succeeded or Failed"
    Jun 22 11:41:33.486: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod client-containers-014b35c5-4034-4ceb-9335-aee4824bbb31 container agnhost-container: <nil>
    STEP: delete the pod 06/22/23 11:41:33.499
    Jun 22 11:41:33.518: INFO: Waiting for pod client-containers-014b35c5-4034-4ceb-9335-aee4824bbb31 to disappear
    Jun 22 11:41:33.522: INFO: Pod client-containers-014b35c5-4034-4ceb-9335-aee4824bbb31 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:41:33.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9362" for this suite. 06/22/23 11:41:33.53
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:41:33.541
Jun 22 11:41:33.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename downward-api 06/22/23 11:41:33.542
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:33.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:33.565
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 06/22/23 11:41:33.57
W0622 11:41:33.582928      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:41:33.583: INFO: Waiting up to 5m0s for pod "downward-api-d4339779-6a23-4267-ab88-1ed2a980a98c" in namespace "downward-api-7590" to be "Succeeded or Failed"
Jun 22 11:41:33.587: INFO: Pod "downward-api-d4339779-6a23-4267-ab88-1ed2a980a98c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06713ms
Jun 22 11:41:35.593: INFO: Pod "downward-api-d4339779-6a23-4267-ab88-1ed2a980a98c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009982322s
Jun 22 11:41:37.595: INFO: Pod "downward-api-d4339779-6a23-4267-ab88-1ed2a980a98c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012098405s
STEP: Saw pod success 06/22/23 11:41:37.595
Jun 22 11:41:37.595: INFO: Pod "downward-api-d4339779-6a23-4267-ab88-1ed2a980a98c" satisfied condition "Succeeded or Failed"
Jun 22 11:41:37.600: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downward-api-d4339779-6a23-4267-ab88-1ed2a980a98c container dapi-container: <nil>
STEP: delete the pod 06/22/23 11:41:37.611
Jun 22 11:41:37.625: INFO: Waiting for pod downward-api-d4339779-6a23-4267-ab88-1ed2a980a98c to disappear
Jun 22 11:41:37.629: INFO: Pod downward-api-d4339779-6a23-4267-ab88-1ed2a980a98c no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jun 22 11:41:37.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7590" for this suite. 06/22/23 11:41:37.636
------------------------------
• [4.103 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:41:33.541
    Jun 22 11:41:33.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename downward-api 06/22/23 11:41:33.542
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:33.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:33.565
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 06/22/23 11:41:33.57
    W0622 11:41:33.582928      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:41:33.583: INFO: Waiting up to 5m0s for pod "downward-api-d4339779-6a23-4267-ab88-1ed2a980a98c" in namespace "downward-api-7590" to be "Succeeded or Failed"
    Jun 22 11:41:33.587: INFO: Pod "downward-api-d4339779-6a23-4267-ab88-1ed2a980a98c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06713ms
    Jun 22 11:41:35.593: INFO: Pod "downward-api-d4339779-6a23-4267-ab88-1ed2a980a98c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009982322s
    Jun 22 11:41:37.595: INFO: Pod "downward-api-d4339779-6a23-4267-ab88-1ed2a980a98c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012098405s
    STEP: Saw pod success 06/22/23 11:41:37.595
    Jun 22 11:41:37.595: INFO: Pod "downward-api-d4339779-6a23-4267-ab88-1ed2a980a98c" satisfied condition "Succeeded or Failed"
    Jun 22 11:41:37.600: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downward-api-d4339779-6a23-4267-ab88-1ed2a980a98c container dapi-container: <nil>
    STEP: delete the pod 06/22/23 11:41:37.611
    Jun 22 11:41:37.625: INFO: Waiting for pod downward-api-d4339779-6a23-4267-ab88-1ed2a980a98c to disappear
    Jun 22 11:41:37.629: INFO: Pod downward-api-d4339779-6a23-4267-ab88-1ed2a980a98c no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:41:37.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7590" for this suite. 06/22/23 11:41:37.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:41:37.646
Jun 22 11:41:37.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename webhook 06/22/23 11:41:37.647
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:37.668
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:37.672
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/22/23 11:41:37.689
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:41:37.878
STEP: Deploying the webhook pod 06/22/23 11:41:37.89
W0622 11:41:37.906162      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 11:41:37.906
Jun 22 11:41:37.916: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/22/23 11:41:39.932
STEP: Verifying the service has paired with the endpoint 06/22/23 11:41:39.95
Jun 22 11:41:40.950: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 06/22/23 11:41:40.956
STEP: Registering slow webhook via the AdmissionRegistration API 06/22/23 11:41:40.956
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 06/22/23 11:41:40.982
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 06/22/23 11:41:41.996
STEP: Registering slow webhook via the AdmissionRegistration API 06/22/23 11:41:41.996
STEP: Having no error when timeout is longer than webhook latency 06/22/23 11:41:43.033
STEP: Registering slow webhook via the AdmissionRegistration API 06/22/23 11:41:43.033
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 06/22/23 11:41:48.074
STEP: Registering slow webhook via the AdmissionRegistration API 06/22/23 11:41:48.074
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:41:53.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5669" for this suite. 06/22/23 11:41:53.165
STEP: Destroying namespace "webhook-5669-markers" for this suite. 06/22/23 11:41:53.175
------------------------------
• [SLOW TEST] [15.537 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:41:37.646
    Jun 22 11:41:37.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename webhook 06/22/23 11:41:37.647
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:37.668
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:37.672
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/22/23 11:41:37.689
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:41:37.878
    STEP: Deploying the webhook pod 06/22/23 11:41:37.89
    W0622 11:41:37.906162      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 11:41:37.906
    Jun 22 11:41:37.916: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/22/23 11:41:39.932
    STEP: Verifying the service has paired with the endpoint 06/22/23 11:41:39.95
    Jun 22 11:41:40.950: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 06/22/23 11:41:40.956
    STEP: Registering slow webhook via the AdmissionRegistration API 06/22/23 11:41:40.956
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 06/22/23 11:41:40.982
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 06/22/23 11:41:41.996
    STEP: Registering slow webhook via the AdmissionRegistration API 06/22/23 11:41:41.996
    STEP: Having no error when timeout is longer than webhook latency 06/22/23 11:41:43.033
    STEP: Registering slow webhook via the AdmissionRegistration API 06/22/23 11:41:43.033
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 06/22/23 11:41:48.074
    STEP: Registering slow webhook via the AdmissionRegistration API 06/22/23 11:41:48.074
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:41:53.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5669" for this suite. 06/22/23 11:41:53.165
    STEP: Destroying namespace "webhook-5669-markers" for this suite. 06/22/23 11:41:53.175
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:41:53.186
Jun 22 11:41:53.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename daemonsets 06/22/23 11:41:53.187
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:53.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:53.21
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 06/22/23 11:41:53.245
W0622 11:41:53.253225      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Check that daemon pods launch on every node of the cluster. 06/22/23 11:41:53.253
Jun 22 11:41:53.259: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:53.259: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:53.259: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:53.263: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 11:41:53.263: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
Jun 22 11:41:54.275: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:54.275: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:54.275: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:54.281: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 22 11:41:54.281: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
Jun 22 11:41:55.285: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:55.285: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:55.285: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:55.298: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 22 11:41:55.298: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 06/22/23 11:41:55.303
Jun 22 11:41:55.366: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:55.366: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:55.366: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:55.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 22 11:41:55.372: INFO: Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 is running 0 daemon pod, expected 1
Jun 22 11:41:56.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:56.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:56.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:56.386: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 22 11:41:56.386: INFO: Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 is running 0 daemon pod, expected 1
Jun 22 11:41:57.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:57.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:57.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:57.386: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 22 11:41:57.386: INFO: Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 is running 0 daemon pod, expected 1
Jun 22 11:41:58.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:58.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:58.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:58.386: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 22 11:41:58.386: INFO: Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 is running 0 daemon pod, expected 1
Jun 22 11:41:59.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:59.382: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:59.382: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 11:41:59.387: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 22 11:41:59.387: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 06/22/23 11:41:59.392
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6469, will wait for the garbage collector to delete the pods 06/22/23 11:41:59.392
Jun 22 11:41:59.457: INFO: Deleting DaemonSet.extensions daemon-set took: 10.285072ms
Jun 22 11:41:59.558: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.572446ms
Jun 22 11:42:01.664: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 22 11:42:01.664: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 22 11:42:01.668: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"162165"},"items":null}

Jun 22 11:42:01.672: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"162165"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:42:01.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6469" for this suite. 06/22/23 11:42:01.699
------------------------------
• [SLOW TEST] [8.524 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:41:53.186
    Jun 22 11:41:53.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename daemonsets 06/22/23 11:41:53.187
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:41:53.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:41:53.21
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 06/22/23 11:41:53.245
    W0622 11:41:53.253225      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Check that daemon pods launch on every node of the cluster. 06/22/23 11:41:53.253
    Jun 22 11:41:53.259: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:53.259: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:53.259: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:53.263: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 11:41:53.263: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
    Jun 22 11:41:54.275: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:54.275: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:54.275: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:54.281: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 22 11:41:54.281: INFO: Node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k is running 0 daemon pod, expected 1
    Jun 22 11:41:55.285: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:55.285: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:55.285: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:55.298: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 22 11:41:55.298: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 06/22/23 11:41:55.303
    Jun 22 11:41:55.366: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:55.366: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:55.366: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:55.372: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 22 11:41:55.372: INFO: Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 is running 0 daemon pod, expected 1
    Jun 22 11:41:56.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:56.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:56.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:56.386: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 22 11:41:56.386: INFO: Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 is running 0 daemon pod, expected 1
    Jun 22 11:41:57.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:57.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:57.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:57.386: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 22 11:41:57.386: INFO: Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 is running 0 daemon pod, expected 1
    Jun 22 11:41:58.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:58.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:58.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:58.386: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 22 11:41:58.386: INFO: Node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 is running 0 daemon pod, expected 1
    Jun 22 11:41:59.381: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-46jvd with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:59.382: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-d8vnc with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:59.382: INFO: DaemonSet pods can't tolerate node wl-antrea-prdzc-k4b4v with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 22 11:41:59.387: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 22 11:41:59.387: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 06/22/23 11:41:59.392
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6469, will wait for the garbage collector to delete the pods 06/22/23 11:41:59.392
    Jun 22 11:41:59.457: INFO: Deleting DaemonSet.extensions daemon-set took: 10.285072ms
    Jun 22 11:41:59.558: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.572446ms
    Jun 22 11:42:01.664: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 22 11:42:01.664: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 22 11:42:01.668: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"162165"},"items":null}

    Jun 22 11:42:01.672: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"162165"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:42:01.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6469" for this suite. 06/22/23 11:42:01.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:42:01.711
Jun 22 11:42:01.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename svc-latency 06/22/23 11:42:01.712
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:42:01.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:42:01.735
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jun 22 11:42:01.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7159 06/22/23 11:42:01.74
W0622 11:42:01.749468      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "svc-latency-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "svc-latency-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "svc-latency-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "svc-latency-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0622 11:42:01.749693      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7159, replica count: 1
I0622 11:42:02.801901      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 11:42:03.802932      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 11:42:03.926: INFO: Created: latency-svc-w9s66
Jun 22 11:42:03.936: INFO: Got endpoints: latency-svc-w9s66 [33.197543ms]
Jun 22 11:42:03.960: INFO: Created: latency-svc-f7f6x
Jun 22 11:42:03.965: INFO: Got endpoints: latency-svc-f7f6x [28.297329ms]
Jun 22 11:42:03.975: INFO: Created: latency-svc-xtb28
Jun 22 11:42:03.990: INFO: Got endpoints: latency-svc-xtb28 [51.96303ms]
Jun 22 11:42:03.995: INFO: Created: latency-svc-bg44x
Jun 22 11:42:04.002: INFO: Got endpoints: latency-svc-bg44x [63.651829ms]
Jun 22 11:42:04.016: INFO: Created: latency-svc-spldl
Jun 22 11:42:04.025: INFO: Got endpoints: latency-svc-spldl [86.630578ms]
Jun 22 11:42:04.029: INFO: Created: latency-svc-5djfc
Jun 22 11:42:04.037: INFO: Got endpoints: latency-svc-5djfc [98.129412ms]
Jun 22 11:42:04.047: INFO: Created: latency-svc-xs469
Jun 22 11:42:04.053: INFO: Got endpoints: latency-svc-xs469 [114.708433ms]
Jun 22 11:42:04.058: INFO: Created: latency-svc-4k85r
Jun 22 11:42:04.069: INFO: Got endpoints: latency-svc-4k85r [130.353939ms]
Jun 22 11:42:04.074: INFO: Created: latency-svc-9wrsh
Jun 22 11:42:04.083: INFO: Got endpoints: latency-svc-9wrsh [143.828674ms]
Jun 22 11:42:04.090: INFO: Created: latency-svc-7h9lg
Jun 22 11:42:04.097: INFO: Got endpoints: latency-svc-7h9lg [157.26967ms]
Jun 22 11:42:04.102: INFO: Created: latency-svc-6p54n
Jun 22 11:42:04.117: INFO: Got endpoints: latency-svc-6p54n [177.141908ms]
Jun 22 11:42:04.118: INFO: Created: latency-svc-nd4vv
Jun 22 11:42:04.123: INFO: Got endpoints: latency-svc-nd4vv [183.050153ms]
Jun 22 11:42:04.135: INFO: Created: latency-svc-4cntf
Jun 22 11:42:04.145: INFO: Got endpoints: latency-svc-4cntf [205.18056ms]
Jun 22 11:42:04.151: INFO: Created: latency-svc-j5dsk
Jun 22 11:42:04.168: INFO: Created: latency-svc-vtrmh
Jun 22 11:42:04.177: INFO: Got endpoints: latency-svc-j5dsk [237.048782ms]
Jun 22 11:42:04.179: INFO: Got endpoints: latency-svc-vtrmh [238.65701ms]
Jun 22 11:42:04.187: INFO: Created: latency-svc-f8ts7
Jun 22 11:42:04.195: INFO: Got endpoints: latency-svc-f8ts7 [254.818332ms]
Jun 22 11:42:04.199: INFO: Created: latency-svc-rdlwp
Jun 22 11:42:04.209: INFO: Got endpoints: latency-svc-rdlwp [243.404131ms]
Jun 22 11:42:04.213: INFO: Created: latency-svc-xd6q7
Jun 22 11:42:04.222: INFO: Got endpoints: latency-svc-xd6q7 [232.316355ms]
Jun 22 11:42:04.227: INFO: Created: latency-svc-mdgbc
Jun 22 11:42:04.236: INFO: Got endpoints: latency-svc-mdgbc [233.852678ms]
Jun 22 11:42:04.241: INFO: Created: latency-svc-j786s
Jun 22 11:42:04.248: INFO: Got endpoints: latency-svc-j786s [222.52898ms]
Jun 22 11:42:04.255: INFO: Created: latency-svc-tx6sr
Jun 22 11:42:04.263: INFO: Got endpoints: latency-svc-tx6sr [225.99081ms]
Jun 22 11:42:04.266: INFO: Created: latency-svc-k6t6b
Jun 22 11:42:04.275: INFO: Got endpoints: latency-svc-k6t6b [221.072229ms]
Jun 22 11:42:04.285: INFO: Created: latency-svc-j9c45
Jun 22 11:42:04.289: INFO: Got endpoints: latency-svc-j9c45 [219.761451ms]
Jun 22 11:42:04.300: INFO: Created: latency-svc-xnshp
Jun 22 11:42:04.314: INFO: Got endpoints: latency-svc-xnshp [230.687116ms]
Jun 22 11:42:04.321: INFO: Created: latency-svc-sxf9j
Jun 22 11:42:04.330: INFO: Got endpoints: latency-svc-sxf9j [232.963102ms]
Jun 22 11:42:04.335: INFO: Created: latency-svc-8lq9g
Jun 22 11:42:04.344: INFO: Got endpoints: latency-svc-8lq9g [227.441259ms]
Jun 22 11:42:04.350: INFO: Created: latency-svc-nxbxp
Jun 22 11:42:04.361: INFO: Got endpoints: latency-svc-nxbxp [238.291509ms]
Jun 22 11:42:04.366: INFO: Created: latency-svc-2ht9k
Jun 22 11:42:04.379: INFO: Got endpoints: latency-svc-2ht9k [233.78428ms]
Jun 22 11:42:04.384: INFO: Created: latency-svc-v7bzs
Jun 22 11:42:04.392: INFO: Got endpoints: latency-svc-v7bzs [215.144198ms]
Jun 22 11:42:04.399: INFO: Created: latency-svc-rl4jz
Jun 22 11:42:04.409: INFO: Got endpoints: latency-svc-rl4jz [230.857596ms]
Jun 22 11:42:04.418: INFO: Created: latency-svc-7c6hm
Jun 22 11:42:04.434: INFO: Got endpoints: latency-svc-7c6hm [238.683499ms]
Jun 22 11:42:04.434: INFO: Created: latency-svc-rckpq
Jun 22 11:42:04.445: INFO: Got endpoints: latency-svc-rckpq [236.355948ms]
Jun 22 11:42:04.452: INFO: Created: latency-svc-dsrt5
Jun 22 11:42:04.464: INFO: Got endpoints: latency-svc-dsrt5 [241.861966ms]
Jun 22 11:42:04.469: INFO: Created: latency-svc-799nr
Jun 22 11:42:04.475: INFO: Got endpoints: latency-svc-799nr [238.922946ms]
Jun 22 11:42:04.486: INFO: Created: latency-svc-mdscm
Jun 22 11:42:04.498: INFO: Got endpoints: latency-svc-mdscm [250.097157ms]
Jun 22 11:42:04.506: INFO: Created: latency-svc-dvjzl
Jun 22 11:42:04.513: INFO: Got endpoints: latency-svc-dvjzl [249.487071ms]
Jun 22 11:42:04.522: INFO: Created: latency-svc-6vxqp
Jun 22 11:42:04.530: INFO: Got endpoints: latency-svc-6vxqp [255.140507ms]
Jun 22 11:42:04.538: INFO: Created: latency-svc-5kqp8
Jun 22 11:42:04.542: INFO: Got endpoints: latency-svc-5kqp8 [252.551549ms]
Jun 22 11:42:04.554: INFO: Created: latency-svc-gl6hz
Jun 22 11:42:04.564: INFO: Got endpoints: latency-svc-gl6hz [249.623126ms]
Jun 22 11:42:04.567: INFO: Created: latency-svc-f4rb8
Jun 22 11:42:04.575: INFO: Got endpoints: latency-svc-f4rb8 [244.936124ms]
Jun 22 11:42:04.581: INFO: Created: latency-svc-6w9g8
Jun 22 11:42:04.591: INFO: Got endpoints: latency-svc-6w9g8 [247.073406ms]
Jun 22 11:42:04.597: INFO: Created: latency-svc-fmm8c
Jun 22 11:42:04.606: INFO: Got endpoints: latency-svc-fmm8c [244.797684ms]
Jun 22 11:42:04.615: INFO: Created: latency-svc-srrrj
Jun 22 11:42:04.618: INFO: Got endpoints: latency-svc-srrrj [239.349738ms]
Jun 22 11:42:04.628: INFO: Created: latency-svc-vzbqm
Jun 22 11:42:04.637: INFO: Got endpoints: latency-svc-vzbqm [245.195053ms]
Jun 22 11:42:04.641: INFO: Created: latency-svc-gmxhj
Jun 22 11:42:04.658: INFO: Created: latency-svc-srjn7
Jun 22 11:42:04.678: INFO: Created: latency-svc-zftrw
Jun 22 11:42:04.687: INFO: Got endpoints: latency-svc-gmxhj [277.432088ms]
Jun 22 11:42:04.692: INFO: Created: latency-svc-sm4nx
Jun 22 11:42:04.705: INFO: Created: latency-svc-cnph6
Jun 22 11:42:04.717: INFO: Created: latency-svc-2q826
Jun 22 11:42:04.735: INFO: Got endpoints: latency-svc-srjn7 [301.48944ms]
Jun 22 11:42:04.737: INFO: Created: latency-svc-tmwcn
Jun 22 11:42:04.752: INFO: Created: latency-svc-h5z5m
Jun 22 11:42:04.765: INFO: Created: latency-svc-t9p4h
Jun 22 11:42:04.780: INFO: Created: latency-svc-nn5lb
Jun 22 11:42:04.783: INFO: Got endpoints: latency-svc-zftrw [336.859284ms]
Jun 22 11:42:04.798: INFO: Created: latency-svc-mwbx6
Jun 22 11:42:04.821: INFO: Created: latency-svc-mfd8t
Jun 22 11:42:04.833: INFO: Created: latency-svc-h7kmg
Jun 22 11:42:04.840: INFO: Got endpoints: latency-svc-sm4nx [376.069873ms]
Jun 22 11:42:04.845: INFO: Created: latency-svc-pbrgr
Jun 22 11:42:04.864: INFO: Created: latency-svc-c4j4h
Jun 22 11:42:04.875: INFO: Created: latency-svc-pgh5s
Jun 22 11:42:04.881: INFO: Got endpoints: latency-svc-cnph6 [406.321517ms]
Jun 22 11:42:04.894: INFO: Created: latency-svc-8zh6c
Jun 22 11:42:04.911: INFO: Created: latency-svc-brzlk
Jun 22 11:42:04.932: INFO: Created: latency-svc-8nzwb
Jun 22 11:42:04.938: INFO: Got endpoints: latency-svc-2q826 [440.533256ms]
Jun 22 11:42:04.955: INFO: Created: latency-svc-9g4ch
Jun 22 11:42:04.966: INFO: Created: latency-svc-s55dk
Jun 22 11:42:04.986: INFO: Got endpoints: latency-svc-tmwcn [473.023244ms]
Jun 22 11:42:05.013: INFO: Created: latency-svc-z69nc
Jun 22 11:42:05.031: INFO: Got endpoints: latency-svc-h5z5m [501.219876ms]
Jun 22 11:42:05.052: INFO: Created: latency-svc-kq4jd
Jun 22 11:42:05.087: INFO: Got endpoints: latency-svc-t9p4h [545.105776ms]
Jun 22 11:42:05.107: INFO: Created: latency-svc-nncnj
Jun 22 11:42:05.135: INFO: Got endpoints: latency-svc-nn5lb [571.433757ms]
Jun 22 11:42:05.157: INFO: Created: latency-svc-fdw2p
Jun 22 11:42:05.186: INFO: Got endpoints: latency-svc-mwbx6 [610.464556ms]
Jun 22 11:42:05.207: INFO: Created: latency-svc-zkgzd
Jun 22 11:42:05.234: INFO: Got endpoints: latency-svc-mfd8t [642.932844ms]
Jun 22 11:42:05.257: INFO: Created: latency-svc-gk9cw
Jun 22 11:42:05.282: INFO: Got endpoints: latency-svc-h7kmg [675.772407ms]
Jun 22 11:42:05.302: INFO: Created: latency-svc-h6xgj
Jun 22 11:42:05.336: INFO: Got endpoints: latency-svc-pbrgr [717.758717ms]
Jun 22 11:42:05.370: INFO: Created: latency-svc-rjgcw
Jun 22 11:42:05.382: INFO: Got endpoints: latency-svc-c4j4h [744.480208ms]
Jun 22 11:42:05.403: INFO: Created: latency-svc-5zsqz
Jun 22 11:42:05.437: INFO: Got endpoints: latency-svc-pgh5s [749.508876ms]
Jun 22 11:42:05.466: INFO: Created: latency-svc-jdkpw
Jun 22 11:42:05.482: INFO: Got endpoints: latency-svc-8zh6c [746.934062ms]
Jun 22 11:42:05.504: INFO: Created: latency-svc-b2pll
Jun 22 11:42:05.532: INFO: Got endpoints: latency-svc-brzlk [748.682111ms]
Jun 22 11:42:05.553: INFO: Created: latency-svc-ckpbr
Jun 22 11:42:05.584: INFO: Got endpoints: latency-svc-8nzwb [743.655051ms]
Jun 22 11:42:05.603: INFO: Created: latency-svc-7v6cx
Jun 22 11:42:05.633: INFO: Got endpoints: latency-svc-9g4ch [751.103611ms]
Jun 22 11:42:05.653: INFO: Created: latency-svc-dr5kb
Jun 22 11:42:05.681: INFO: Got endpoints: latency-svc-s55dk [743.018448ms]
Jun 22 11:42:05.700: INFO: Created: latency-svc-sv6r8
Jun 22 11:42:05.733: INFO: Got endpoints: latency-svc-z69nc [747.395768ms]
Jun 22 11:42:05.752: INFO: Created: latency-svc-dx2zm
Jun 22 11:42:05.781: INFO: Got endpoints: latency-svc-kq4jd [750.026035ms]
Jun 22 11:42:05.805: INFO: Created: latency-svc-j9n7f
Jun 22 11:42:05.835: INFO: Got endpoints: latency-svc-nncnj [747.844267ms]
Jun 22 11:42:05.859: INFO: Created: latency-svc-8q7sh
Jun 22 11:42:05.883: INFO: Got endpoints: latency-svc-fdw2p [747.530679ms]
Jun 22 11:42:05.906: INFO: Created: latency-svc-2r9x7
Jun 22 11:42:05.934: INFO: Got endpoints: latency-svc-zkgzd [747.865277ms]
Jun 22 11:42:05.958: INFO: Created: latency-svc-wsnkk
Jun 22 11:42:05.984: INFO: Got endpoints: latency-svc-gk9cw [749.553553ms]
Jun 22 11:42:06.006: INFO: Created: latency-svc-l6q95
Jun 22 11:42:06.038: INFO: Got endpoints: latency-svc-h6xgj [756.531867ms]
Jun 22 11:42:06.062: INFO: Created: latency-svc-cn67g
Jun 22 11:42:06.084: INFO: Got endpoints: latency-svc-rjgcw [748.071056ms]
Jun 22 11:42:06.110: INFO: Created: latency-svc-szvc7
Jun 22 11:42:06.135: INFO: Got endpoints: latency-svc-5zsqz [752.548879ms]
Jun 22 11:42:06.161: INFO: Created: latency-svc-wzxl5
Jun 22 11:42:06.185: INFO: Got endpoints: latency-svc-jdkpw [748.267099ms]
Jun 22 11:42:06.205: INFO: Created: latency-svc-kkz2d
Jun 22 11:42:06.234: INFO: Got endpoints: latency-svc-b2pll [751.088765ms]
Jun 22 11:42:06.260: INFO: Created: latency-svc-8cstz
Jun 22 11:42:06.290: INFO: Got endpoints: latency-svc-ckpbr [757.915954ms]
Jun 22 11:42:06.308: INFO: Created: latency-svc-n8nhr
Jun 22 11:42:06.331: INFO: Got endpoints: latency-svc-7v6cx [747.24196ms]
Jun 22 11:42:06.349: INFO: Created: latency-svc-gm8gw
Jun 22 11:42:06.386: INFO: Got endpoints: latency-svc-dr5kb [753.475784ms]
Jun 22 11:42:06.406: INFO: Created: latency-svc-85s2f
Jun 22 11:42:06.433: INFO: Got endpoints: latency-svc-sv6r8 [751.615779ms]
Jun 22 11:42:06.451: INFO: Created: latency-svc-x9jm2
Jun 22 11:42:06.483: INFO: Got endpoints: latency-svc-dx2zm [750.139087ms]
Jun 22 11:42:06.503: INFO: Created: latency-svc-4kgzs
Jun 22 11:42:06.533: INFO: Got endpoints: latency-svc-j9n7f [751.527676ms]
Jun 22 11:42:06.554: INFO: Created: latency-svc-klb79
Jun 22 11:42:06.582: INFO: Got endpoints: latency-svc-8q7sh [747.267113ms]
Jun 22 11:42:06.606: INFO: Created: latency-svc-zbsdl
Jun 22 11:42:06.633: INFO: Got endpoints: latency-svc-2r9x7 [749.872689ms]
Jun 22 11:42:06.653: INFO: Created: latency-svc-mt825
Jun 22 11:42:06.685: INFO: Got endpoints: latency-svc-wsnkk [751.225869ms]
Jun 22 11:42:06.705: INFO: Created: latency-svc-pzszz
Jun 22 11:42:06.732: INFO: Got endpoints: latency-svc-l6q95 [748.097538ms]
Jun 22 11:42:06.756: INFO: Created: latency-svc-hps97
Jun 22 11:42:06.783: INFO: Got endpoints: latency-svc-cn67g [744.111596ms]
Jun 22 11:42:06.800: INFO: Created: latency-svc-gbt4f
Jun 22 11:42:06.833: INFO: Got endpoints: latency-svc-szvc7 [748.4292ms]
Jun 22 11:42:06.853: INFO: Created: latency-svc-lh5z4
Jun 22 11:42:06.881: INFO: Got endpoints: latency-svc-wzxl5 [746.262248ms]
Jun 22 11:42:06.898: INFO: Created: latency-svc-xr7dl
Jun 22 11:42:06.933: INFO: Got endpoints: latency-svc-kkz2d [748.432392ms]
Jun 22 11:42:06.954: INFO: Created: latency-svc-tlhhb
Jun 22 11:42:06.988: INFO: Got endpoints: latency-svc-8cstz [754.458526ms]
Jun 22 11:42:07.011: INFO: Created: latency-svc-s6b8v
Jun 22 11:42:07.039: INFO: Got endpoints: latency-svc-n8nhr [748.960661ms]
Jun 22 11:42:07.061: INFO: Created: latency-svc-s8njh
Jun 22 11:42:07.084: INFO: Got endpoints: latency-svc-gm8gw [752.390895ms]
Jun 22 11:42:07.106: INFO: Created: latency-svc-t6rm8
Jun 22 11:42:07.132: INFO: Got endpoints: latency-svc-85s2f [745.399071ms]
Jun 22 11:42:07.156: INFO: Created: latency-svc-jj22g
Jun 22 11:42:07.183: INFO: Got endpoints: latency-svc-x9jm2 [750.202223ms]
Jun 22 11:42:07.206: INFO: Created: latency-svc-lcfm9
Jun 22 11:42:07.233: INFO: Got endpoints: latency-svc-4kgzs [749.819707ms]
Jun 22 11:42:07.254: INFO: Created: latency-svc-lrmnm
Jun 22 11:42:07.282: INFO: Got endpoints: latency-svc-klb79 [749.594042ms]
Jun 22 11:42:07.304: INFO: Created: latency-svc-cngsg
Jun 22 11:42:07.335: INFO: Got endpoints: latency-svc-zbsdl [752.725468ms]
Jun 22 11:42:07.358: INFO: Created: latency-svc-2svpd
Jun 22 11:42:07.383: INFO: Got endpoints: latency-svc-mt825 [749.900234ms]
Jun 22 11:42:07.402: INFO: Created: latency-svc-lqdkh
Jun 22 11:42:07.434: INFO: Got endpoints: latency-svc-pzszz [748.576072ms]
Jun 22 11:42:07.452: INFO: Created: latency-svc-km5gd
Jun 22 11:42:07.485: INFO: Got endpoints: latency-svc-hps97 [752.507811ms]
Jun 22 11:42:07.507: INFO: Created: latency-svc-5snft
Jun 22 11:42:07.531: INFO: Got endpoints: latency-svc-gbt4f [748.016512ms]
Jun 22 11:42:07.555: INFO: Created: latency-svc-x665w
Jun 22 11:42:07.583: INFO: Got endpoints: latency-svc-lh5z4 [750.331496ms]
Jun 22 11:42:07.618: INFO: Created: latency-svc-8kmxb
Jun 22 11:42:07.633: INFO: Got endpoints: latency-svc-xr7dl [751.735285ms]
Jun 22 11:42:07.651: INFO: Created: latency-svc-tzrhr
Jun 22 11:42:07.683: INFO: Got endpoints: latency-svc-tlhhb [749.822745ms]
Jun 22 11:42:07.701: INFO: Created: latency-svc-clqsl
Jun 22 11:42:07.733: INFO: Got endpoints: latency-svc-s6b8v [744.830991ms]
Jun 22 11:42:07.753: INFO: Created: latency-svc-4fm24
Jun 22 11:42:07.783: INFO: Got endpoints: latency-svc-s8njh [744.595013ms]
Jun 22 11:42:07.805: INFO: Created: latency-svc-wg7r8
Jun 22 11:42:07.833: INFO: Got endpoints: latency-svc-t6rm8 [749.405427ms]
Jun 22 11:42:07.856: INFO: Created: latency-svc-kcmd7
Jun 22 11:42:07.884: INFO: Got endpoints: latency-svc-jj22g [751.66409ms]
Jun 22 11:42:07.906: INFO: Created: latency-svc-lttxt
Jun 22 11:42:07.930: INFO: Got endpoints: latency-svc-lcfm9 [746.847695ms]
Jun 22 11:42:07.963: INFO: Created: latency-svc-2m88p
Jun 22 11:42:07.984: INFO: Got endpoints: latency-svc-lrmnm [750.917045ms]
Jun 22 11:42:08.009: INFO: Created: latency-svc-fbncw
Jun 22 11:42:08.038: INFO: Got endpoints: latency-svc-cngsg [755.119364ms]
Jun 22 11:42:08.062: INFO: Created: latency-svc-6vwwl
Jun 22 11:42:08.083: INFO: Got endpoints: latency-svc-2svpd [747.797061ms]
Jun 22 11:42:08.108: INFO: Created: latency-svc-grk7g
Jun 22 11:42:08.135: INFO: Got endpoints: latency-svc-lqdkh [752.234729ms]
Jun 22 11:42:08.163: INFO: Created: latency-svc-9kk9r
Jun 22 11:42:08.181: INFO: Got endpoints: latency-svc-km5gd [746.774079ms]
Jun 22 11:42:08.203: INFO: Created: latency-svc-bfqws
Jun 22 11:42:08.233: INFO: Got endpoints: latency-svc-5snft [747.73005ms]
Jun 22 11:42:08.254: INFO: Created: latency-svc-vkqsp
Jun 22 11:42:08.286: INFO: Got endpoints: latency-svc-x665w [754.924288ms]
Jun 22 11:42:08.308: INFO: Created: latency-svc-2zbsp
Jun 22 11:42:08.334: INFO: Got endpoints: latency-svc-8kmxb [750.708493ms]
Jun 22 11:42:08.355: INFO: Created: latency-svc-pvt77
Jun 22 11:42:08.383: INFO: Got endpoints: latency-svc-tzrhr [750.197973ms]
Jun 22 11:42:08.404: INFO: Created: latency-svc-nwf99
Jun 22 11:42:08.434: INFO: Got endpoints: latency-svc-clqsl [750.426513ms]
Jun 22 11:42:08.476: INFO: Created: latency-svc-4vd59
Jun 22 11:42:08.481: INFO: Got endpoints: latency-svc-4fm24 [748.151477ms]
Jun 22 11:42:08.499: INFO: Created: latency-svc-mfz4k
Jun 22 11:42:08.534: INFO: Got endpoints: latency-svc-wg7r8 [750.341717ms]
Jun 22 11:42:08.554: INFO: Created: latency-svc-d7xss
Jun 22 11:42:08.582: INFO: Got endpoints: latency-svc-kcmd7 [749.021826ms]
Jun 22 11:42:08.616: INFO: Created: latency-svc-fg2tj
Jun 22 11:42:08.631: INFO: Got endpoints: latency-svc-lttxt [747.473968ms]
Jun 22 11:42:08.650: INFO: Created: latency-svc-lnc4s
Jun 22 11:42:08.685: INFO: Got endpoints: latency-svc-2m88p [754.36831ms]
Jun 22 11:42:08.704: INFO: Created: latency-svc-wbj6j
Jun 22 11:42:08.732: INFO: Got endpoints: latency-svc-fbncw [747.317347ms]
Jun 22 11:42:08.755: INFO: Created: latency-svc-gtw7f
Jun 22 11:42:09.034: INFO: Got endpoints: latency-svc-6vwwl [996.521158ms]
Jun 22 11:42:09.043: INFO: Got endpoints: latency-svc-bfqws [862.078084ms]
Jun 22 11:42:09.043: INFO: Got endpoints: latency-svc-grk7g [960.219737ms]
Jun 22 11:42:09.043: INFO: Got endpoints: latency-svc-9kk9r [908.027406ms]
Jun 22 11:42:09.044: INFO: Got endpoints: latency-svc-2zbsp [758.041608ms]
Jun 22 11:42:09.044: INFO: Got endpoints: latency-svc-vkqsp [811.244923ms]
Jun 22 11:42:09.066: INFO: Created: latency-svc-ckkh6
Jun 22 11:42:09.084: INFO: Got endpoints: latency-svc-pvt77 [749.953614ms]
Jun 22 11:42:09.097: INFO: Created: latency-svc-k7ttb
Jun 22 11:42:09.116: INFO: Created: latency-svc-8zlsm
Jun 22 11:42:09.135: INFO: Created: latency-svc-ng4g4
Jun 22 11:42:09.155: INFO: Created: latency-svc-kx7sl
Jun 22 11:42:09.168: INFO: Got endpoints: latency-svc-nwf99 [785.472671ms]
Jun 22 11:42:09.175: INFO: Created: latency-svc-4mpsj
Jun 22 11:42:09.200: INFO: Got endpoints: latency-svc-4vd59 [765.919389ms]
Jun 22 11:42:09.213: INFO: Created: latency-svc-29knh
Jun 22 11:42:09.229: INFO: Created: latency-svc-jdtb5
Jun 22 11:42:09.233: INFO: Got endpoints: latency-svc-mfz4k [751.388375ms]
Jun 22 11:42:09.256: INFO: Created: latency-svc-6jt97
Jun 22 11:42:09.275: INFO: Created: latency-svc-hwcxk
Jun 22 11:42:09.283: INFO: Got endpoints: latency-svc-d7xss [749.428717ms]
Jun 22 11:42:09.299: INFO: Created: latency-svc-x2qk9
Jun 22 11:42:09.333: INFO: Got endpoints: latency-svc-fg2tj [750.879451ms]
Jun 22 11:42:09.360: INFO: Created: latency-svc-8nclf
Jun 22 11:42:09.382: INFO: Got endpoints: latency-svc-lnc4s [750.897009ms]
Jun 22 11:42:09.402: INFO: Created: latency-svc-qvls5
Jun 22 11:42:09.437: INFO: Got endpoints: latency-svc-wbj6j [752.318849ms]
Jun 22 11:42:09.471: INFO: Created: latency-svc-9wj82
Jun 22 11:42:09.483: INFO: Got endpoints: latency-svc-gtw7f [751.193236ms]
Jun 22 11:42:09.503: INFO: Created: latency-svc-xlw4m
Jun 22 11:42:09.533: INFO: Got endpoints: latency-svc-ckkh6 [498.678332ms]
Jun 22 11:42:09.553: INFO: Created: latency-svc-qxkgs
Jun 22 11:42:09.586: INFO: Got endpoints: latency-svc-k7ttb [542.854336ms]
Jun 22 11:42:09.608: INFO: Created: latency-svc-l4x9s
Jun 22 11:42:09.633: INFO: Got endpoints: latency-svc-8zlsm [589.649906ms]
Jun 22 11:42:09.652: INFO: Created: latency-svc-x7zzg
Jun 22 11:42:09.684: INFO: Got endpoints: latency-svc-ng4g4 [640.361251ms]
Jun 22 11:42:09.718: INFO: Created: latency-svc-w94wg
Jun 22 11:42:09.736: INFO: Got endpoints: latency-svc-kx7sl [692.385147ms]
Jun 22 11:42:09.755: INFO: Created: latency-svc-tfmwp
Jun 22 11:42:09.783: INFO: Got endpoints: latency-svc-4mpsj [739.24873ms]
Jun 22 11:42:09.802: INFO: Created: latency-svc-hrj7x
Jun 22 11:42:09.834: INFO: Got endpoints: latency-svc-29knh [750.131539ms]
Jun 22 11:42:09.867: INFO: Created: latency-svc-b2n5f
Jun 22 11:42:09.883: INFO: Got endpoints: latency-svc-jdtb5 [714.171264ms]
Jun 22 11:42:09.903: INFO: Created: latency-svc-96htd
Jun 22 11:42:09.933: INFO: Got endpoints: latency-svc-6jt97 [733.034383ms]
Jun 22 11:42:09.960: INFO: Created: latency-svc-q7cpp
Jun 22 11:42:09.989: INFO: Got endpoints: latency-svc-hwcxk [755.685019ms]
Jun 22 11:42:10.012: INFO: Created: latency-svc-fbdbc
Jun 22 11:42:10.033: INFO: Got endpoints: latency-svc-x2qk9 [749.717746ms]
Jun 22 11:42:10.059: INFO: Created: latency-svc-d2s7k
Jun 22 11:42:10.083: INFO: Got endpoints: latency-svc-8nclf [749.736929ms]
Jun 22 11:42:10.110: INFO: Created: latency-svc-7ppd8
Jun 22 11:42:10.135: INFO: Got endpoints: latency-svc-qvls5 [752.580767ms]
Jun 22 11:42:10.156: INFO: Created: latency-svc-zzqlb
Jun 22 11:42:10.182: INFO: Got endpoints: latency-svc-9wj82 [745.057837ms]
Jun 22 11:42:10.203: INFO: Created: latency-svc-q2688
Jun 22 11:42:10.234: INFO: Got endpoints: latency-svc-xlw4m [751.200927ms]
Jun 22 11:42:10.257: INFO: Created: latency-svc-fz6ft
Jun 22 11:42:10.284: INFO: Got endpoints: latency-svc-qxkgs [750.738045ms]
Jun 22 11:42:10.307: INFO: Created: latency-svc-fxwpf
Jun 22 11:42:10.334: INFO: Got endpoints: latency-svc-l4x9s [748.178415ms]
Jun 22 11:42:10.357: INFO: Created: latency-svc-2shq8
Jun 22 11:42:10.383: INFO: Got endpoints: latency-svc-x7zzg [750.395148ms]
Jun 22 11:42:10.407: INFO: Created: latency-svc-ltdxd
Jun 22 11:42:10.434: INFO: Got endpoints: latency-svc-w94wg [749.952656ms]
Jun 22 11:42:10.456: INFO: Created: latency-svc-bdj4r
Jun 22 11:42:10.482: INFO: Got endpoints: latency-svc-tfmwp [745.802545ms]
Jun 22 11:42:10.506: INFO: Created: latency-svc-9xq6k
Jun 22 11:42:10.533: INFO: Got endpoints: latency-svc-hrj7x [749.979667ms]
Jun 22 11:42:10.556: INFO: Created: latency-svc-n992x
Jun 22 11:42:10.583: INFO: Got endpoints: latency-svc-b2n5f [748.889551ms]
Jun 22 11:42:10.604: INFO: Created: latency-svc-456tt
Jun 22 11:42:10.633: INFO: Got endpoints: latency-svc-96htd [749.79916ms]
Jun 22 11:42:10.657: INFO: Created: latency-svc-vvrrl
Jun 22 11:42:10.682: INFO: Got endpoints: latency-svc-q7cpp [749.532042ms]
Jun 22 11:42:10.703: INFO: Created: latency-svc-hctrf
Jun 22 11:42:10.731: INFO: Got endpoints: latency-svc-fbdbc [742.114111ms]
Jun 22 11:42:10.753: INFO: Created: latency-svc-h2kmr
Jun 22 11:42:10.782: INFO: Got endpoints: latency-svc-d2s7k [748.894078ms]
Jun 22 11:42:10.806: INFO: Created: latency-svc-2bb4c
Jun 22 11:42:10.835: INFO: Got endpoints: latency-svc-7ppd8 [752.207313ms]
Jun 22 11:42:10.861: INFO: Created: latency-svc-bksxm
Jun 22 11:42:10.882: INFO: Got endpoints: latency-svc-zzqlb [746.7908ms]
Jun 22 11:42:10.907: INFO: Created: latency-svc-s56xw
Jun 22 11:42:10.933: INFO: Got endpoints: latency-svc-q2688 [750.462318ms]
Jun 22 11:42:10.960: INFO: Created: latency-svc-xvqzd
Jun 22 11:42:10.987: INFO: Got endpoints: latency-svc-fz6ft [752.593542ms]
Jun 22 11:42:11.014: INFO: Created: latency-svc-qvjmg
Jun 22 11:42:11.037: INFO: Got endpoints: latency-svc-fxwpf [753.412401ms]
Jun 22 11:42:11.084: INFO: Created: latency-svc-r72cc
Jun 22 11:42:11.141: INFO: Got endpoints: latency-svc-2shq8 [806.892945ms]
Jun 22 11:42:11.144: INFO: Got endpoints: latency-svc-ltdxd [761.150471ms]
Jun 22 11:42:11.162: INFO: Created: latency-svc-l5ttl
Jun 22 11:42:11.178: INFO: Created: latency-svc-2wr9r
Jun 22 11:42:11.184: INFO: Got endpoints: latency-svc-bdj4r [750.500925ms]
Jun 22 11:42:11.205: INFO: Created: latency-svc-qvq5s
Jun 22 11:42:11.233: INFO: Got endpoints: latency-svc-9xq6k [750.645843ms]
Jun 22 11:42:11.252: INFO: Created: latency-svc-lsgbl
Jun 22 11:42:11.282: INFO: Got endpoints: latency-svc-n992x [748.503861ms]
Jun 22 11:42:11.301: INFO: Created: latency-svc-kdjvv
Jun 22 11:42:11.334: INFO: Got endpoints: latency-svc-456tt [750.569032ms]
Jun 22 11:42:11.355: INFO: Created: latency-svc-gfk9k
Jun 22 11:42:11.383: INFO: Got endpoints: latency-svc-vvrrl [750.844754ms]
Jun 22 11:42:11.405: INFO: Created: latency-svc-x2jqh
Jun 22 11:42:11.442: INFO: Got endpoints: latency-svc-hctrf [759.74576ms]
Jun 22 11:42:11.465: INFO: Created: latency-svc-q2hlp
Jun 22 11:42:11.484: INFO: Got endpoints: latency-svc-h2kmr [753.147767ms]
Jun 22 11:42:11.503: INFO: Created: latency-svc-bprgt
Jun 22 11:42:11.533: INFO: Got endpoints: latency-svc-2bb4c [750.736062ms]
Jun 22 11:42:11.561: INFO: Created: latency-svc-2jrnv
Jun 22 11:42:11.587: INFO: Got endpoints: latency-svc-bksxm [751.408636ms]
Jun 22 11:42:11.609: INFO: Created: latency-svc-jlrtp
Jun 22 11:42:11.633: INFO: Got endpoints: latency-svc-s56xw [751.385014ms]
Jun 22 11:42:11.660: INFO: Created: latency-svc-46mv6
Jun 22 11:42:11.683: INFO: Got endpoints: latency-svc-xvqzd [750.642008ms]
Jun 22 11:42:11.704: INFO: Created: latency-svc-wxkph
Jun 22 11:42:11.733: INFO: Got endpoints: latency-svc-qvjmg [746.434582ms]
Jun 22 11:42:11.757: INFO: Created: latency-svc-csxn8
Jun 22 11:42:11.782: INFO: Got endpoints: latency-svc-r72cc [744.783948ms]
Jun 22 11:42:11.834: INFO: Got endpoints: latency-svc-l5ttl [693.214398ms]
Jun 22 11:42:11.883: INFO: Got endpoints: latency-svc-2wr9r [738.847454ms]
Jun 22 11:42:11.935: INFO: Got endpoints: latency-svc-qvq5s [750.781866ms]
Jun 22 11:42:11.983: INFO: Got endpoints: latency-svc-lsgbl [750.549683ms]
Jun 22 11:42:12.035: INFO: Got endpoints: latency-svc-kdjvv [753.098807ms]
Jun 22 11:42:12.085: INFO: Got endpoints: latency-svc-gfk9k [751.077076ms]
Jun 22 11:42:12.134: INFO: Got endpoints: latency-svc-x2jqh [750.27167ms]
Jun 22 11:42:12.188: INFO: Got endpoints: latency-svc-q2hlp [745.442438ms]
Jun 22 11:42:12.234: INFO: Got endpoints: latency-svc-bprgt [749.575718ms]
Jun 22 11:42:12.284: INFO: Got endpoints: latency-svc-2jrnv [751.157846ms]
Jun 22 11:42:12.333: INFO: Got endpoints: latency-svc-jlrtp [745.857392ms]
Jun 22 11:42:12.383: INFO: Got endpoints: latency-svc-46mv6 [750.068199ms]
Jun 22 11:42:12.434: INFO: Got endpoints: latency-svc-wxkph [750.35109ms]
Jun 22 11:42:12.485: INFO: Got endpoints: latency-svc-csxn8 [751.714428ms]
Jun 22 11:42:12.485: INFO: Latencies: [28.297329ms 51.96303ms 63.651829ms 86.630578ms 98.129412ms 114.708433ms 130.353939ms 143.828674ms 157.26967ms 177.141908ms 183.050153ms 205.18056ms 215.144198ms 219.761451ms 221.072229ms 222.52898ms 225.99081ms 227.441259ms 230.687116ms 230.857596ms 232.316355ms 232.963102ms 233.78428ms 233.852678ms 236.355948ms 237.048782ms 238.291509ms 238.65701ms 238.683499ms 238.922946ms 239.349738ms 241.861966ms 243.404131ms 244.797684ms 244.936124ms 245.195053ms 247.073406ms 249.487071ms 249.623126ms 250.097157ms 252.551549ms 254.818332ms 255.140507ms 277.432088ms 301.48944ms 336.859284ms 376.069873ms 406.321517ms 440.533256ms 473.023244ms 498.678332ms 501.219876ms 542.854336ms 545.105776ms 571.433757ms 589.649906ms 610.464556ms 640.361251ms 642.932844ms 675.772407ms 692.385147ms 693.214398ms 714.171264ms 717.758717ms 733.034383ms 738.847454ms 739.24873ms 742.114111ms 743.018448ms 743.655051ms 744.111596ms 744.480208ms 744.595013ms 744.783948ms 744.830991ms 745.057837ms 745.399071ms 745.442438ms 745.802545ms 745.857392ms 746.262248ms 746.434582ms 746.774079ms 746.7908ms 746.847695ms 746.934062ms 747.24196ms 747.267113ms 747.317347ms 747.395768ms 747.473968ms 747.530679ms 747.73005ms 747.797061ms 747.844267ms 747.865277ms 748.016512ms 748.071056ms 748.097538ms 748.151477ms 748.178415ms 748.267099ms 748.4292ms 748.432392ms 748.503861ms 748.576072ms 748.682111ms 748.889551ms 748.894078ms 748.960661ms 749.021826ms 749.405427ms 749.428717ms 749.508876ms 749.532042ms 749.553553ms 749.575718ms 749.594042ms 749.717746ms 749.736929ms 749.79916ms 749.819707ms 749.822745ms 749.872689ms 749.900234ms 749.952656ms 749.953614ms 749.979667ms 750.026035ms 750.068199ms 750.131539ms 750.139087ms 750.197973ms 750.202223ms 750.27167ms 750.331496ms 750.341717ms 750.35109ms 750.395148ms 750.426513ms 750.462318ms 750.500925ms 750.549683ms 750.569032ms 750.642008ms 750.645843ms 750.708493ms 750.736062ms 750.738045ms 750.781866ms 750.844754ms 750.879451ms 750.897009ms 750.917045ms 751.077076ms 751.088765ms 751.103611ms 751.157846ms 751.193236ms 751.200927ms 751.225869ms 751.385014ms 751.388375ms 751.408636ms 751.527676ms 751.615779ms 751.66409ms 751.714428ms 751.735285ms 752.207313ms 752.234729ms 752.318849ms 752.390895ms 752.507811ms 752.548879ms 752.580767ms 752.593542ms 752.725468ms 753.098807ms 753.147767ms 753.412401ms 753.475784ms 754.36831ms 754.458526ms 754.924288ms 755.119364ms 755.685019ms 756.531867ms 757.915954ms 758.041608ms 759.74576ms 761.150471ms 765.919389ms 785.472671ms 806.892945ms 811.244923ms 862.078084ms 908.027406ms 960.219737ms 996.521158ms]
Jun 22 11:42:12.485: INFO: 50 %ile: 748.178415ms
Jun 22 11:42:12.485: INFO: 90 %ile: 753.412401ms
Jun 22 11:42:12.485: INFO: 99 %ile: 960.219737ms
Jun 22 11:42:12.485: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Jun 22 11:42:12.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-7159" for this suite. 06/22/23 11:42:12.495
------------------------------
• [SLOW TEST] [10.793 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:42:01.711
    Jun 22 11:42:01.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename svc-latency 06/22/23 11:42:01.712
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:42:01.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:42:01.735
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jun 22 11:42:01.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-7159 06/22/23 11:42:01.74
    W0622 11:42:01.749468      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "svc-latency-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "svc-latency-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "svc-latency-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "svc-latency-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0622 11:42:01.749693      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7159, replica count: 1
    I0622 11:42:02.801901      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0622 11:42:03.802932      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 22 11:42:03.926: INFO: Created: latency-svc-w9s66
    Jun 22 11:42:03.936: INFO: Got endpoints: latency-svc-w9s66 [33.197543ms]
    Jun 22 11:42:03.960: INFO: Created: latency-svc-f7f6x
    Jun 22 11:42:03.965: INFO: Got endpoints: latency-svc-f7f6x [28.297329ms]
    Jun 22 11:42:03.975: INFO: Created: latency-svc-xtb28
    Jun 22 11:42:03.990: INFO: Got endpoints: latency-svc-xtb28 [51.96303ms]
    Jun 22 11:42:03.995: INFO: Created: latency-svc-bg44x
    Jun 22 11:42:04.002: INFO: Got endpoints: latency-svc-bg44x [63.651829ms]
    Jun 22 11:42:04.016: INFO: Created: latency-svc-spldl
    Jun 22 11:42:04.025: INFO: Got endpoints: latency-svc-spldl [86.630578ms]
    Jun 22 11:42:04.029: INFO: Created: latency-svc-5djfc
    Jun 22 11:42:04.037: INFO: Got endpoints: latency-svc-5djfc [98.129412ms]
    Jun 22 11:42:04.047: INFO: Created: latency-svc-xs469
    Jun 22 11:42:04.053: INFO: Got endpoints: latency-svc-xs469 [114.708433ms]
    Jun 22 11:42:04.058: INFO: Created: latency-svc-4k85r
    Jun 22 11:42:04.069: INFO: Got endpoints: latency-svc-4k85r [130.353939ms]
    Jun 22 11:42:04.074: INFO: Created: latency-svc-9wrsh
    Jun 22 11:42:04.083: INFO: Got endpoints: latency-svc-9wrsh [143.828674ms]
    Jun 22 11:42:04.090: INFO: Created: latency-svc-7h9lg
    Jun 22 11:42:04.097: INFO: Got endpoints: latency-svc-7h9lg [157.26967ms]
    Jun 22 11:42:04.102: INFO: Created: latency-svc-6p54n
    Jun 22 11:42:04.117: INFO: Got endpoints: latency-svc-6p54n [177.141908ms]
    Jun 22 11:42:04.118: INFO: Created: latency-svc-nd4vv
    Jun 22 11:42:04.123: INFO: Got endpoints: latency-svc-nd4vv [183.050153ms]
    Jun 22 11:42:04.135: INFO: Created: latency-svc-4cntf
    Jun 22 11:42:04.145: INFO: Got endpoints: latency-svc-4cntf [205.18056ms]
    Jun 22 11:42:04.151: INFO: Created: latency-svc-j5dsk
    Jun 22 11:42:04.168: INFO: Created: latency-svc-vtrmh
    Jun 22 11:42:04.177: INFO: Got endpoints: latency-svc-j5dsk [237.048782ms]
    Jun 22 11:42:04.179: INFO: Got endpoints: latency-svc-vtrmh [238.65701ms]
    Jun 22 11:42:04.187: INFO: Created: latency-svc-f8ts7
    Jun 22 11:42:04.195: INFO: Got endpoints: latency-svc-f8ts7 [254.818332ms]
    Jun 22 11:42:04.199: INFO: Created: latency-svc-rdlwp
    Jun 22 11:42:04.209: INFO: Got endpoints: latency-svc-rdlwp [243.404131ms]
    Jun 22 11:42:04.213: INFO: Created: latency-svc-xd6q7
    Jun 22 11:42:04.222: INFO: Got endpoints: latency-svc-xd6q7 [232.316355ms]
    Jun 22 11:42:04.227: INFO: Created: latency-svc-mdgbc
    Jun 22 11:42:04.236: INFO: Got endpoints: latency-svc-mdgbc [233.852678ms]
    Jun 22 11:42:04.241: INFO: Created: latency-svc-j786s
    Jun 22 11:42:04.248: INFO: Got endpoints: latency-svc-j786s [222.52898ms]
    Jun 22 11:42:04.255: INFO: Created: latency-svc-tx6sr
    Jun 22 11:42:04.263: INFO: Got endpoints: latency-svc-tx6sr [225.99081ms]
    Jun 22 11:42:04.266: INFO: Created: latency-svc-k6t6b
    Jun 22 11:42:04.275: INFO: Got endpoints: latency-svc-k6t6b [221.072229ms]
    Jun 22 11:42:04.285: INFO: Created: latency-svc-j9c45
    Jun 22 11:42:04.289: INFO: Got endpoints: latency-svc-j9c45 [219.761451ms]
    Jun 22 11:42:04.300: INFO: Created: latency-svc-xnshp
    Jun 22 11:42:04.314: INFO: Got endpoints: latency-svc-xnshp [230.687116ms]
    Jun 22 11:42:04.321: INFO: Created: latency-svc-sxf9j
    Jun 22 11:42:04.330: INFO: Got endpoints: latency-svc-sxf9j [232.963102ms]
    Jun 22 11:42:04.335: INFO: Created: latency-svc-8lq9g
    Jun 22 11:42:04.344: INFO: Got endpoints: latency-svc-8lq9g [227.441259ms]
    Jun 22 11:42:04.350: INFO: Created: latency-svc-nxbxp
    Jun 22 11:42:04.361: INFO: Got endpoints: latency-svc-nxbxp [238.291509ms]
    Jun 22 11:42:04.366: INFO: Created: latency-svc-2ht9k
    Jun 22 11:42:04.379: INFO: Got endpoints: latency-svc-2ht9k [233.78428ms]
    Jun 22 11:42:04.384: INFO: Created: latency-svc-v7bzs
    Jun 22 11:42:04.392: INFO: Got endpoints: latency-svc-v7bzs [215.144198ms]
    Jun 22 11:42:04.399: INFO: Created: latency-svc-rl4jz
    Jun 22 11:42:04.409: INFO: Got endpoints: latency-svc-rl4jz [230.857596ms]
    Jun 22 11:42:04.418: INFO: Created: latency-svc-7c6hm
    Jun 22 11:42:04.434: INFO: Got endpoints: latency-svc-7c6hm [238.683499ms]
    Jun 22 11:42:04.434: INFO: Created: latency-svc-rckpq
    Jun 22 11:42:04.445: INFO: Got endpoints: latency-svc-rckpq [236.355948ms]
    Jun 22 11:42:04.452: INFO: Created: latency-svc-dsrt5
    Jun 22 11:42:04.464: INFO: Got endpoints: latency-svc-dsrt5 [241.861966ms]
    Jun 22 11:42:04.469: INFO: Created: latency-svc-799nr
    Jun 22 11:42:04.475: INFO: Got endpoints: latency-svc-799nr [238.922946ms]
    Jun 22 11:42:04.486: INFO: Created: latency-svc-mdscm
    Jun 22 11:42:04.498: INFO: Got endpoints: latency-svc-mdscm [250.097157ms]
    Jun 22 11:42:04.506: INFO: Created: latency-svc-dvjzl
    Jun 22 11:42:04.513: INFO: Got endpoints: latency-svc-dvjzl [249.487071ms]
    Jun 22 11:42:04.522: INFO: Created: latency-svc-6vxqp
    Jun 22 11:42:04.530: INFO: Got endpoints: latency-svc-6vxqp [255.140507ms]
    Jun 22 11:42:04.538: INFO: Created: latency-svc-5kqp8
    Jun 22 11:42:04.542: INFO: Got endpoints: latency-svc-5kqp8 [252.551549ms]
    Jun 22 11:42:04.554: INFO: Created: latency-svc-gl6hz
    Jun 22 11:42:04.564: INFO: Got endpoints: latency-svc-gl6hz [249.623126ms]
    Jun 22 11:42:04.567: INFO: Created: latency-svc-f4rb8
    Jun 22 11:42:04.575: INFO: Got endpoints: latency-svc-f4rb8 [244.936124ms]
    Jun 22 11:42:04.581: INFO: Created: latency-svc-6w9g8
    Jun 22 11:42:04.591: INFO: Got endpoints: latency-svc-6w9g8 [247.073406ms]
    Jun 22 11:42:04.597: INFO: Created: latency-svc-fmm8c
    Jun 22 11:42:04.606: INFO: Got endpoints: latency-svc-fmm8c [244.797684ms]
    Jun 22 11:42:04.615: INFO: Created: latency-svc-srrrj
    Jun 22 11:42:04.618: INFO: Got endpoints: latency-svc-srrrj [239.349738ms]
    Jun 22 11:42:04.628: INFO: Created: latency-svc-vzbqm
    Jun 22 11:42:04.637: INFO: Got endpoints: latency-svc-vzbqm [245.195053ms]
    Jun 22 11:42:04.641: INFO: Created: latency-svc-gmxhj
    Jun 22 11:42:04.658: INFO: Created: latency-svc-srjn7
    Jun 22 11:42:04.678: INFO: Created: latency-svc-zftrw
    Jun 22 11:42:04.687: INFO: Got endpoints: latency-svc-gmxhj [277.432088ms]
    Jun 22 11:42:04.692: INFO: Created: latency-svc-sm4nx
    Jun 22 11:42:04.705: INFO: Created: latency-svc-cnph6
    Jun 22 11:42:04.717: INFO: Created: latency-svc-2q826
    Jun 22 11:42:04.735: INFO: Got endpoints: latency-svc-srjn7 [301.48944ms]
    Jun 22 11:42:04.737: INFO: Created: latency-svc-tmwcn
    Jun 22 11:42:04.752: INFO: Created: latency-svc-h5z5m
    Jun 22 11:42:04.765: INFO: Created: latency-svc-t9p4h
    Jun 22 11:42:04.780: INFO: Created: latency-svc-nn5lb
    Jun 22 11:42:04.783: INFO: Got endpoints: latency-svc-zftrw [336.859284ms]
    Jun 22 11:42:04.798: INFO: Created: latency-svc-mwbx6
    Jun 22 11:42:04.821: INFO: Created: latency-svc-mfd8t
    Jun 22 11:42:04.833: INFO: Created: latency-svc-h7kmg
    Jun 22 11:42:04.840: INFO: Got endpoints: latency-svc-sm4nx [376.069873ms]
    Jun 22 11:42:04.845: INFO: Created: latency-svc-pbrgr
    Jun 22 11:42:04.864: INFO: Created: latency-svc-c4j4h
    Jun 22 11:42:04.875: INFO: Created: latency-svc-pgh5s
    Jun 22 11:42:04.881: INFO: Got endpoints: latency-svc-cnph6 [406.321517ms]
    Jun 22 11:42:04.894: INFO: Created: latency-svc-8zh6c
    Jun 22 11:42:04.911: INFO: Created: latency-svc-brzlk
    Jun 22 11:42:04.932: INFO: Created: latency-svc-8nzwb
    Jun 22 11:42:04.938: INFO: Got endpoints: latency-svc-2q826 [440.533256ms]
    Jun 22 11:42:04.955: INFO: Created: latency-svc-9g4ch
    Jun 22 11:42:04.966: INFO: Created: latency-svc-s55dk
    Jun 22 11:42:04.986: INFO: Got endpoints: latency-svc-tmwcn [473.023244ms]
    Jun 22 11:42:05.013: INFO: Created: latency-svc-z69nc
    Jun 22 11:42:05.031: INFO: Got endpoints: latency-svc-h5z5m [501.219876ms]
    Jun 22 11:42:05.052: INFO: Created: latency-svc-kq4jd
    Jun 22 11:42:05.087: INFO: Got endpoints: latency-svc-t9p4h [545.105776ms]
    Jun 22 11:42:05.107: INFO: Created: latency-svc-nncnj
    Jun 22 11:42:05.135: INFO: Got endpoints: latency-svc-nn5lb [571.433757ms]
    Jun 22 11:42:05.157: INFO: Created: latency-svc-fdw2p
    Jun 22 11:42:05.186: INFO: Got endpoints: latency-svc-mwbx6 [610.464556ms]
    Jun 22 11:42:05.207: INFO: Created: latency-svc-zkgzd
    Jun 22 11:42:05.234: INFO: Got endpoints: latency-svc-mfd8t [642.932844ms]
    Jun 22 11:42:05.257: INFO: Created: latency-svc-gk9cw
    Jun 22 11:42:05.282: INFO: Got endpoints: latency-svc-h7kmg [675.772407ms]
    Jun 22 11:42:05.302: INFO: Created: latency-svc-h6xgj
    Jun 22 11:42:05.336: INFO: Got endpoints: latency-svc-pbrgr [717.758717ms]
    Jun 22 11:42:05.370: INFO: Created: latency-svc-rjgcw
    Jun 22 11:42:05.382: INFO: Got endpoints: latency-svc-c4j4h [744.480208ms]
    Jun 22 11:42:05.403: INFO: Created: latency-svc-5zsqz
    Jun 22 11:42:05.437: INFO: Got endpoints: latency-svc-pgh5s [749.508876ms]
    Jun 22 11:42:05.466: INFO: Created: latency-svc-jdkpw
    Jun 22 11:42:05.482: INFO: Got endpoints: latency-svc-8zh6c [746.934062ms]
    Jun 22 11:42:05.504: INFO: Created: latency-svc-b2pll
    Jun 22 11:42:05.532: INFO: Got endpoints: latency-svc-brzlk [748.682111ms]
    Jun 22 11:42:05.553: INFO: Created: latency-svc-ckpbr
    Jun 22 11:42:05.584: INFO: Got endpoints: latency-svc-8nzwb [743.655051ms]
    Jun 22 11:42:05.603: INFO: Created: latency-svc-7v6cx
    Jun 22 11:42:05.633: INFO: Got endpoints: latency-svc-9g4ch [751.103611ms]
    Jun 22 11:42:05.653: INFO: Created: latency-svc-dr5kb
    Jun 22 11:42:05.681: INFO: Got endpoints: latency-svc-s55dk [743.018448ms]
    Jun 22 11:42:05.700: INFO: Created: latency-svc-sv6r8
    Jun 22 11:42:05.733: INFO: Got endpoints: latency-svc-z69nc [747.395768ms]
    Jun 22 11:42:05.752: INFO: Created: latency-svc-dx2zm
    Jun 22 11:42:05.781: INFO: Got endpoints: latency-svc-kq4jd [750.026035ms]
    Jun 22 11:42:05.805: INFO: Created: latency-svc-j9n7f
    Jun 22 11:42:05.835: INFO: Got endpoints: latency-svc-nncnj [747.844267ms]
    Jun 22 11:42:05.859: INFO: Created: latency-svc-8q7sh
    Jun 22 11:42:05.883: INFO: Got endpoints: latency-svc-fdw2p [747.530679ms]
    Jun 22 11:42:05.906: INFO: Created: latency-svc-2r9x7
    Jun 22 11:42:05.934: INFO: Got endpoints: latency-svc-zkgzd [747.865277ms]
    Jun 22 11:42:05.958: INFO: Created: latency-svc-wsnkk
    Jun 22 11:42:05.984: INFO: Got endpoints: latency-svc-gk9cw [749.553553ms]
    Jun 22 11:42:06.006: INFO: Created: latency-svc-l6q95
    Jun 22 11:42:06.038: INFO: Got endpoints: latency-svc-h6xgj [756.531867ms]
    Jun 22 11:42:06.062: INFO: Created: latency-svc-cn67g
    Jun 22 11:42:06.084: INFO: Got endpoints: latency-svc-rjgcw [748.071056ms]
    Jun 22 11:42:06.110: INFO: Created: latency-svc-szvc7
    Jun 22 11:42:06.135: INFO: Got endpoints: latency-svc-5zsqz [752.548879ms]
    Jun 22 11:42:06.161: INFO: Created: latency-svc-wzxl5
    Jun 22 11:42:06.185: INFO: Got endpoints: latency-svc-jdkpw [748.267099ms]
    Jun 22 11:42:06.205: INFO: Created: latency-svc-kkz2d
    Jun 22 11:42:06.234: INFO: Got endpoints: latency-svc-b2pll [751.088765ms]
    Jun 22 11:42:06.260: INFO: Created: latency-svc-8cstz
    Jun 22 11:42:06.290: INFO: Got endpoints: latency-svc-ckpbr [757.915954ms]
    Jun 22 11:42:06.308: INFO: Created: latency-svc-n8nhr
    Jun 22 11:42:06.331: INFO: Got endpoints: latency-svc-7v6cx [747.24196ms]
    Jun 22 11:42:06.349: INFO: Created: latency-svc-gm8gw
    Jun 22 11:42:06.386: INFO: Got endpoints: latency-svc-dr5kb [753.475784ms]
    Jun 22 11:42:06.406: INFO: Created: latency-svc-85s2f
    Jun 22 11:42:06.433: INFO: Got endpoints: latency-svc-sv6r8 [751.615779ms]
    Jun 22 11:42:06.451: INFO: Created: latency-svc-x9jm2
    Jun 22 11:42:06.483: INFO: Got endpoints: latency-svc-dx2zm [750.139087ms]
    Jun 22 11:42:06.503: INFO: Created: latency-svc-4kgzs
    Jun 22 11:42:06.533: INFO: Got endpoints: latency-svc-j9n7f [751.527676ms]
    Jun 22 11:42:06.554: INFO: Created: latency-svc-klb79
    Jun 22 11:42:06.582: INFO: Got endpoints: latency-svc-8q7sh [747.267113ms]
    Jun 22 11:42:06.606: INFO: Created: latency-svc-zbsdl
    Jun 22 11:42:06.633: INFO: Got endpoints: latency-svc-2r9x7 [749.872689ms]
    Jun 22 11:42:06.653: INFO: Created: latency-svc-mt825
    Jun 22 11:42:06.685: INFO: Got endpoints: latency-svc-wsnkk [751.225869ms]
    Jun 22 11:42:06.705: INFO: Created: latency-svc-pzszz
    Jun 22 11:42:06.732: INFO: Got endpoints: latency-svc-l6q95 [748.097538ms]
    Jun 22 11:42:06.756: INFO: Created: latency-svc-hps97
    Jun 22 11:42:06.783: INFO: Got endpoints: latency-svc-cn67g [744.111596ms]
    Jun 22 11:42:06.800: INFO: Created: latency-svc-gbt4f
    Jun 22 11:42:06.833: INFO: Got endpoints: latency-svc-szvc7 [748.4292ms]
    Jun 22 11:42:06.853: INFO: Created: latency-svc-lh5z4
    Jun 22 11:42:06.881: INFO: Got endpoints: latency-svc-wzxl5 [746.262248ms]
    Jun 22 11:42:06.898: INFO: Created: latency-svc-xr7dl
    Jun 22 11:42:06.933: INFO: Got endpoints: latency-svc-kkz2d [748.432392ms]
    Jun 22 11:42:06.954: INFO: Created: latency-svc-tlhhb
    Jun 22 11:42:06.988: INFO: Got endpoints: latency-svc-8cstz [754.458526ms]
    Jun 22 11:42:07.011: INFO: Created: latency-svc-s6b8v
    Jun 22 11:42:07.039: INFO: Got endpoints: latency-svc-n8nhr [748.960661ms]
    Jun 22 11:42:07.061: INFO: Created: latency-svc-s8njh
    Jun 22 11:42:07.084: INFO: Got endpoints: latency-svc-gm8gw [752.390895ms]
    Jun 22 11:42:07.106: INFO: Created: latency-svc-t6rm8
    Jun 22 11:42:07.132: INFO: Got endpoints: latency-svc-85s2f [745.399071ms]
    Jun 22 11:42:07.156: INFO: Created: latency-svc-jj22g
    Jun 22 11:42:07.183: INFO: Got endpoints: latency-svc-x9jm2 [750.202223ms]
    Jun 22 11:42:07.206: INFO: Created: latency-svc-lcfm9
    Jun 22 11:42:07.233: INFO: Got endpoints: latency-svc-4kgzs [749.819707ms]
    Jun 22 11:42:07.254: INFO: Created: latency-svc-lrmnm
    Jun 22 11:42:07.282: INFO: Got endpoints: latency-svc-klb79 [749.594042ms]
    Jun 22 11:42:07.304: INFO: Created: latency-svc-cngsg
    Jun 22 11:42:07.335: INFO: Got endpoints: latency-svc-zbsdl [752.725468ms]
    Jun 22 11:42:07.358: INFO: Created: latency-svc-2svpd
    Jun 22 11:42:07.383: INFO: Got endpoints: latency-svc-mt825 [749.900234ms]
    Jun 22 11:42:07.402: INFO: Created: latency-svc-lqdkh
    Jun 22 11:42:07.434: INFO: Got endpoints: latency-svc-pzszz [748.576072ms]
    Jun 22 11:42:07.452: INFO: Created: latency-svc-km5gd
    Jun 22 11:42:07.485: INFO: Got endpoints: latency-svc-hps97 [752.507811ms]
    Jun 22 11:42:07.507: INFO: Created: latency-svc-5snft
    Jun 22 11:42:07.531: INFO: Got endpoints: latency-svc-gbt4f [748.016512ms]
    Jun 22 11:42:07.555: INFO: Created: latency-svc-x665w
    Jun 22 11:42:07.583: INFO: Got endpoints: latency-svc-lh5z4 [750.331496ms]
    Jun 22 11:42:07.618: INFO: Created: latency-svc-8kmxb
    Jun 22 11:42:07.633: INFO: Got endpoints: latency-svc-xr7dl [751.735285ms]
    Jun 22 11:42:07.651: INFO: Created: latency-svc-tzrhr
    Jun 22 11:42:07.683: INFO: Got endpoints: latency-svc-tlhhb [749.822745ms]
    Jun 22 11:42:07.701: INFO: Created: latency-svc-clqsl
    Jun 22 11:42:07.733: INFO: Got endpoints: latency-svc-s6b8v [744.830991ms]
    Jun 22 11:42:07.753: INFO: Created: latency-svc-4fm24
    Jun 22 11:42:07.783: INFO: Got endpoints: latency-svc-s8njh [744.595013ms]
    Jun 22 11:42:07.805: INFO: Created: latency-svc-wg7r8
    Jun 22 11:42:07.833: INFO: Got endpoints: latency-svc-t6rm8 [749.405427ms]
    Jun 22 11:42:07.856: INFO: Created: latency-svc-kcmd7
    Jun 22 11:42:07.884: INFO: Got endpoints: latency-svc-jj22g [751.66409ms]
    Jun 22 11:42:07.906: INFO: Created: latency-svc-lttxt
    Jun 22 11:42:07.930: INFO: Got endpoints: latency-svc-lcfm9 [746.847695ms]
    Jun 22 11:42:07.963: INFO: Created: latency-svc-2m88p
    Jun 22 11:42:07.984: INFO: Got endpoints: latency-svc-lrmnm [750.917045ms]
    Jun 22 11:42:08.009: INFO: Created: latency-svc-fbncw
    Jun 22 11:42:08.038: INFO: Got endpoints: latency-svc-cngsg [755.119364ms]
    Jun 22 11:42:08.062: INFO: Created: latency-svc-6vwwl
    Jun 22 11:42:08.083: INFO: Got endpoints: latency-svc-2svpd [747.797061ms]
    Jun 22 11:42:08.108: INFO: Created: latency-svc-grk7g
    Jun 22 11:42:08.135: INFO: Got endpoints: latency-svc-lqdkh [752.234729ms]
    Jun 22 11:42:08.163: INFO: Created: latency-svc-9kk9r
    Jun 22 11:42:08.181: INFO: Got endpoints: latency-svc-km5gd [746.774079ms]
    Jun 22 11:42:08.203: INFO: Created: latency-svc-bfqws
    Jun 22 11:42:08.233: INFO: Got endpoints: latency-svc-5snft [747.73005ms]
    Jun 22 11:42:08.254: INFO: Created: latency-svc-vkqsp
    Jun 22 11:42:08.286: INFO: Got endpoints: latency-svc-x665w [754.924288ms]
    Jun 22 11:42:08.308: INFO: Created: latency-svc-2zbsp
    Jun 22 11:42:08.334: INFO: Got endpoints: latency-svc-8kmxb [750.708493ms]
    Jun 22 11:42:08.355: INFO: Created: latency-svc-pvt77
    Jun 22 11:42:08.383: INFO: Got endpoints: latency-svc-tzrhr [750.197973ms]
    Jun 22 11:42:08.404: INFO: Created: latency-svc-nwf99
    Jun 22 11:42:08.434: INFO: Got endpoints: latency-svc-clqsl [750.426513ms]
    Jun 22 11:42:08.476: INFO: Created: latency-svc-4vd59
    Jun 22 11:42:08.481: INFO: Got endpoints: latency-svc-4fm24 [748.151477ms]
    Jun 22 11:42:08.499: INFO: Created: latency-svc-mfz4k
    Jun 22 11:42:08.534: INFO: Got endpoints: latency-svc-wg7r8 [750.341717ms]
    Jun 22 11:42:08.554: INFO: Created: latency-svc-d7xss
    Jun 22 11:42:08.582: INFO: Got endpoints: latency-svc-kcmd7 [749.021826ms]
    Jun 22 11:42:08.616: INFO: Created: latency-svc-fg2tj
    Jun 22 11:42:08.631: INFO: Got endpoints: latency-svc-lttxt [747.473968ms]
    Jun 22 11:42:08.650: INFO: Created: latency-svc-lnc4s
    Jun 22 11:42:08.685: INFO: Got endpoints: latency-svc-2m88p [754.36831ms]
    Jun 22 11:42:08.704: INFO: Created: latency-svc-wbj6j
    Jun 22 11:42:08.732: INFO: Got endpoints: latency-svc-fbncw [747.317347ms]
    Jun 22 11:42:08.755: INFO: Created: latency-svc-gtw7f
    Jun 22 11:42:09.034: INFO: Got endpoints: latency-svc-6vwwl [996.521158ms]
    Jun 22 11:42:09.043: INFO: Got endpoints: latency-svc-bfqws [862.078084ms]
    Jun 22 11:42:09.043: INFO: Got endpoints: latency-svc-grk7g [960.219737ms]
    Jun 22 11:42:09.043: INFO: Got endpoints: latency-svc-9kk9r [908.027406ms]
    Jun 22 11:42:09.044: INFO: Got endpoints: latency-svc-2zbsp [758.041608ms]
    Jun 22 11:42:09.044: INFO: Got endpoints: latency-svc-vkqsp [811.244923ms]
    Jun 22 11:42:09.066: INFO: Created: latency-svc-ckkh6
    Jun 22 11:42:09.084: INFO: Got endpoints: latency-svc-pvt77 [749.953614ms]
    Jun 22 11:42:09.097: INFO: Created: latency-svc-k7ttb
    Jun 22 11:42:09.116: INFO: Created: latency-svc-8zlsm
    Jun 22 11:42:09.135: INFO: Created: latency-svc-ng4g4
    Jun 22 11:42:09.155: INFO: Created: latency-svc-kx7sl
    Jun 22 11:42:09.168: INFO: Got endpoints: latency-svc-nwf99 [785.472671ms]
    Jun 22 11:42:09.175: INFO: Created: latency-svc-4mpsj
    Jun 22 11:42:09.200: INFO: Got endpoints: latency-svc-4vd59 [765.919389ms]
    Jun 22 11:42:09.213: INFO: Created: latency-svc-29knh
    Jun 22 11:42:09.229: INFO: Created: latency-svc-jdtb5
    Jun 22 11:42:09.233: INFO: Got endpoints: latency-svc-mfz4k [751.388375ms]
    Jun 22 11:42:09.256: INFO: Created: latency-svc-6jt97
    Jun 22 11:42:09.275: INFO: Created: latency-svc-hwcxk
    Jun 22 11:42:09.283: INFO: Got endpoints: latency-svc-d7xss [749.428717ms]
    Jun 22 11:42:09.299: INFO: Created: latency-svc-x2qk9
    Jun 22 11:42:09.333: INFO: Got endpoints: latency-svc-fg2tj [750.879451ms]
    Jun 22 11:42:09.360: INFO: Created: latency-svc-8nclf
    Jun 22 11:42:09.382: INFO: Got endpoints: latency-svc-lnc4s [750.897009ms]
    Jun 22 11:42:09.402: INFO: Created: latency-svc-qvls5
    Jun 22 11:42:09.437: INFO: Got endpoints: latency-svc-wbj6j [752.318849ms]
    Jun 22 11:42:09.471: INFO: Created: latency-svc-9wj82
    Jun 22 11:42:09.483: INFO: Got endpoints: latency-svc-gtw7f [751.193236ms]
    Jun 22 11:42:09.503: INFO: Created: latency-svc-xlw4m
    Jun 22 11:42:09.533: INFO: Got endpoints: latency-svc-ckkh6 [498.678332ms]
    Jun 22 11:42:09.553: INFO: Created: latency-svc-qxkgs
    Jun 22 11:42:09.586: INFO: Got endpoints: latency-svc-k7ttb [542.854336ms]
    Jun 22 11:42:09.608: INFO: Created: latency-svc-l4x9s
    Jun 22 11:42:09.633: INFO: Got endpoints: latency-svc-8zlsm [589.649906ms]
    Jun 22 11:42:09.652: INFO: Created: latency-svc-x7zzg
    Jun 22 11:42:09.684: INFO: Got endpoints: latency-svc-ng4g4 [640.361251ms]
    Jun 22 11:42:09.718: INFO: Created: latency-svc-w94wg
    Jun 22 11:42:09.736: INFO: Got endpoints: latency-svc-kx7sl [692.385147ms]
    Jun 22 11:42:09.755: INFO: Created: latency-svc-tfmwp
    Jun 22 11:42:09.783: INFO: Got endpoints: latency-svc-4mpsj [739.24873ms]
    Jun 22 11:42:09.802: INFO: Created: latency-svc-hrj7x
    Jun 22 11:42:09.834: INFO: Got endpoints: latency-svc-29knh [750.131539ms]
    Jun 22 11:42:09.867: INFO: Created: latency-svc-b2n5f
    Jun 22 11:42:09.883: INFO: Got endpoints: latency-svc-jdtb5 [714.171264ms]
    Jun 22 11:42:09.903: INFO: Created: latency-svc-96htd
    Jun 22 11:42:09.933: INFO: Got endpoints: latency-svc-6jt97 [733.034383ms]
    Jun 22 11:42:09.960: INFO: Created: latency-svc-q7cpp
    Jun 22 11:42:09.989: INFO: Got endpoints: latency-svc-hwcxk [755.685019ms]
    Jun 22 11:42:10.012: INFO: Created: latency-svc-fbdbc
    Jun 22 11:42:10.033: INFO: Got endpoints: latency-svc-x2qk9 [749.717746ms]
    Jun 22 11:42:10.059: INFO: Created: latency-svc-d2s7k
    Jun 22 11:42:10.083: INFO: Got endpoints: latency-svc-8nclf [749.736929ms]
    Jun 22 11:42:10.110: INFO: Created: latency-svc-7ppd8
    Jun 22 11:42:10.135: INFO: Got endpoints: latency-svc-qvls5 [752.580767ms]
    Jun 22 11:42:10.156: INFO: Created: latency-svc-zzqlb
    Jun 22 11:42:10.182: INFO: Got endpoints: latency-svc-9wj82 [745.057837ms]
    Jun 22 11:42:10.203: INFO: Created: latency-svc-q2688
    Jun 22 11:42:10.234: INFO: Got endpoints: latency-svc-xlw4m [751.200927ms]
    Jun 22 11:42:10.257: INFO: Created: latency-svc-fz6ft
    Jun 22 11:42:10.284: INFO: Got endpoints: latency-svc-qxkgs [750.738045ms]
    Jun 22 11:42:10.307: INFO: Created: latency-svc-fxwpf
    Jun 22 11:42:10.334: INFO: Got endpoints: latency-svc-l4x9s [748.178415ms]
    Jun 22 11:42:10.357: INFO: Created: latency-svc-2shq8
    Jun 22 11:42:10.383: INFO: Got endpoints: latency-svc-x7zzg [750.395148ms]
    Jun 22 11:42:10.407: INFO: Created: latency-svc-ltdxd
    Jun 22 11:42:10.434: INFO: Got endpoints: latency-svc-w94wg [749.952656ms]
    Jun 22 11:42:10.456: INFO: Created: latency-svc-bdj4r
    Jun 22 11:42:10.482: INFO: Got endpoints: latency-svc-tfmwp [745.802545ms]
    Jun 22 11:42:10.506: INFO: Created: latency-svc-9xq6k
    Jun 22 11:42:10.533: INFO: Got endpoints: latency-svc-hrj7x [749.979667ms]
    Jun 22 11:42:10.556: INFO: Created: latency-svc-n992x
    Jun 22 11:42:10.583: INFO: Got endpoints: latency-svc-b2n5f [748.889551ms]
    Jun 22 11:42:10.604: INFO: Created: latency-svc-456tt
    Jun 22 11:42:10.633: INFO: Got endpoints: latency-svc-96htd [749.79916ms]
    Jun 22 11:42:10.657: INFO: Created: latency-svc-vvrrl
    Jun 22 11:42:10.682: INFO: Got endpoints: latency-svc-q7cpp [749.532042ms]
    Jun 22 11:42:10.703: INFO: Created: latency-svc-hctrf
    Jun 22 11:42:10.731: INFO: Got endpoints: latency-svc-fbdbc [742.114111ms]
    Jun 22 11:42:10.753: INFO: Created: latency-svc-h2kmr
    Jun 22 11:42:10.782: INFO: Got endpoints: latency-svc-d2s7k [748.894078ms]
    Jun 22 11:42:10.806: INFO: Created: latency-svc-2bb4c
    Jun 22 11:42:10.835: INFO: Got endpoints: latency-svc-7ppd8 [752.207313ms]
    Jun 22 11:42:10.861: INFO: Created: latency-svc-bksxm
    Jun 22 11:42:10.882: INFO: Got endpoints: latency-svc-zzqlb [746.7908ms]
    Jun 22 11:42:10.907: INFO: Created: latency-svc-s56xw
    Jun 22 11:42:10.933: INFO: Got endpoints: latency-svc-q2688 [750.462318ms]
    Jun 22 11:42:10.960: INFO: Created: latency-svc-xvqzd
    Jun 22 11:42:10.987: INFO: Got endpoints: latency-svc-fz6ft [752.593542ms]
    Jun 22 11:42:11.014: INFO: Created: latency-svc-qvjmg
    Jun 22 11:42:11.037: INFO: Got endpoints: latency-svc-fxwpf [753.412401ms]
    Jun 22 11:42:11.084: INFO: Created: latency-svc-r72cc
    Jun 22 11:42:11.141: INFO: Got endpoints: latency-svc-2shq8 [806.892945ms]
    Jun 22 11:42:11.144: INFO: Got endpoints: latency-svc-ltdxd [761.150471ms]
    Jun 22 11:42:11.162: INFO: Created: latency-svc-l5ttl
    Jun 22 11:42:11.178: INFO: Created: latency-svc-2wr9r
    Jun 22 11:42:11.184: INFO: Got endpoints: latency-svc-bdj4r [750.500925ms]
    Jun 22 11:42:11.205: INFO: Created: latency-svc-qvq5s
    Jun 22 11:42:11.233: INFO: Got endpoints: latency-svc-9xq6k [750.645843ms]
    Jun 22 11:42:11.252: INFO: Created: latency-svc-lsgbl
    Jun 22 11:42:11.282: INFO: Got endpoints: latency-svc-n992x [748.503861ms]
    Jun 22 11:42:11.301: INFO: Created: latency-svc-kdjvv
    Jun 22 11:42:11.334: INFO: Got endpoints: latency-svc-456tt [750.569032ms]
    Jun 22 11:42:11.355: INFO: Created: latency-svc-gfk9k
    Jun 22 11:42:11.383: INFO: Got endpoints: latency-svc-vvrrl [750.844754ms]
    Jun 22 11:42:11.405: INFO: Created: latency-svc-x2jqh
    Jun 22 11:42:11.442: INFO: Got endpoints: latency-svc-hctrf [759.74576ms]
    Jun 22 11:42:11.465: INFO: Created: latency-svc-q2hlp
    Jun 22 11:42:11.484: INFO: Got endpoints: latency-svc-h2kmr [753.147767ms]
    Jun 22 11:42:11.503: INFO: Created: latency-svc-bprgt
    Jun 22 11:42:11.533: INFO: Got endpoints: latency-svc-2bb4c [750.736062ms]
    Jun 22 11:42:11.561: INFO: Created: latency-svc-2jrnv
    Jun 22 11:42:11.587: INFO: Got endpoints: latency-svc-bksxm [751.408636ms]
    Jun 22 11:42:11.609: INFO: Created: latency-svc-jlrtp
    Jun 22 11:42:11.633: INFO: Got endpoints: latency-svc-s56xw [751.385014ms]
    Jun 22 11:42:11.660: INFO: Created: latency-svc-46mv6
    Jun 22 11:42:11.683: INFO: Got endpoints: latency-svc-xvqzd [750.642008ms]
    Jun 22 11:42:11.704: INFO: Created: latency-svc-wxkph
    Jun 22 11:42:11.733: INFO: Got endpoints: latency-svc-qvjmg [746.434582ms]
    Jun 22 11:42:11.757: INFO: Created: latency-svc-csxn8
    Jun 22 11:42:11.782: INFO: Got endpoints: latency-svc-r72cc [744.783948ms]
    Jun 22 11:42:11.834: INFO: Got endpoints: latency-svc-l5ttl [693.214398ms]
    Jun 22 11:42:11.883: INFO: Got endpoints: latency-svc-2wr9r [738.847454ms]
    Jun 22 11:42:11.935: INFO: Got endpoints: latency-svc-qvq5s [750.781866ms]
    Jun 22 11:42:11.983: INFO: Got endpoints: latency-svc-lsgbl [750.549683ms]
    Jun 22 11:42:12.035: INFO: Got endpoints: latency-svc-kdjvv [753.098807ms]
    Jun 22 11:42:12.085: INFO: Got endpoints: latency-svc-gfk9k [751.077076ms]
    Jun 22 11:42:12.134: INFO: Got endpoints: latency-svc-x2jqh [750.27167ms]
    Jun 22 11:42:12.188: INFO: Got endpoints: latency-svc-q2hlp [745.442438ms]
    Jun 22 11:42:12.234: INFO: Got endpoints: latency-svc-bprgt [749.575718ms]
    Jun 22 11:42:12.284: INFO: Got endpoints: latency-svc-2jrnv [751.157846ms]
    Jun 22 11:42:12.333: INFO: Got endpoints: latency-svc-jlrtp [745.857392ms]
    Jun 22 11:42:12.383: INFO: Got endpoints: latency-svc-46mv6 [750.068199ms]
    Jun 22 11:42:12.434: INFO: Got endpoints: latency-svc-wxkph [750.35109ms]
    Jun 22 11:42:12.485: INFO: Got endpoints: latency-svc-csxn8 [751.714428ms]
    Jun 22 11:42:12.485: INFO: Latencies: [28.297329ms 51.96303ms 63.651829ms 86.630578ms 98.129412ms 114.708433ms 130.353939ms 143.828674ms 157.26967ms 177.141908ms 183.050153ms 205.18056ms 215.144198ms 219.761451ms 221.072229ms 222.52898ms 225.99081ms 227.441259ms 230.687116ms 230.857596ms 232.316355ms 232.963102ms 233.78428ms 233.852678ms 236.355948ms 237.048782ms 238.291509ms 238.65701ms 238.683499ms 238.922946ms 239.349738ms 241.861966ms 243.404131ms 244.797684ms 244.936124ms 245.195053ms 247.073406ms 249.487071ms 249.623126ms 250.097157ms 252.551549ms 254.818332ms 255.140507ms 277.432088ms 301.48944ms 336.859284ms 376.069873ms 406.321517ms 440.533256ms 473.023244ms 498.678332ms 501.219876ms 542.854336ms 545.105776ms 571.433757ms 589.649906ms 610.464556ms 640.361251ms 642.932844ms 675.772407ms 692.385147ms 693.214398ms 714.171264ms 717.758717ms 733.034383ms 738.847454ms 739.24873ms 742.114111ms 743.018448ms 743.655051ms 744.111596ms 744.480208ms 744.595013ms 744.783948ms 744.830991ms 745.057837ms 745.399071ms 745.442438ms 745.802545ms 745.857392ms 746.262248ms 746.434582ms 746.774079ms 746.7908ms 746.847695ms 746.934062ms 747.24196ms 747.267113ms 747.317347ms 747.395768ms 747.473968ms 747.530679ms 747.73005ms 747.797061ms 747.844267ms 747.865277ms 748.016512ms 748.071056ms 748.097538ms 748.151477ms 748.178415ms 748.267099ms 748.4292ms 748.432392ms 748.503861ms 748.576072ms 748.682111ms 748.889551ms 748.894078ms 748.960661ms 749.021826ms 749.405427ms 749.428717ms 749.508876ms 749.532042ms 749.553553ms 749.575718ms 749.594042ms 749.717746ms 749.736929ms 749.79916ms 749.819707ms 749.822745ms 749.872689ms 749.900234ms 749.952656ms 749.953614ms 749.979667ms 750.026035ms 750.068199ms 750.131539ms 750.139087ms 750.197973ms 750.202223ms 750.27167ms 750.331496ms 750.341717ms 750.35109ms 750.395148ms 750.426513ms 750.462318ms 750.500925ms 750.549683ms 750.569032ms 750.642008ms 750.645843ms 750.708493ms 750.736062ms 750.738045ms 750.781866ms 750.844754ms 750.879451ms 750.897009ms 750.917045ms 751.077076ms 751.088765ms 751.103611ms 751.157846ms 751.193236ms 751.200927ms 751.225869ms 751.385014ms 751.388375ms 751.408636ms 751.527676ms 751.615779ms 751.66409ms 751.714428ms 751.735285ms 752.207313ms 752.234729ms 752.318849ms 752.390895ms 752.507811ms 752.548879ms 752.580767ms 752.593542ms 752.725468ms 753.098807ms 753.147767ms 753.412401ms 753.475784ms 754.36831ms 754.458526ms 754.924288ms 755.119364ms 755.685019ms 756.531867ms 757.915954ms 758.041608ms 759.74576ms 761.150471ms 765.919389ms 785.472671ms 806.892945ms 811.244923ms 862.078084ms 908.027406ms 960.219737ms 996.521158ms]
    Jun 22 11:42:12.485: INFO: 50 %ile: 748.178415ms
    Jun 22 11:42:12.485: INFO: 90 %ile: 753.412401ms
    Jun 22 11:42:12.485: INFO: 99 %ile: 960.219737ms
    Jun 22 11:42:12.485: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:42:12.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-7159" for this suite. 06/22/23 11:42:12.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:42:12.506
Jun 22 11:42:12.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename resourcequota 06/22/23 11:42:12.507
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:42:12.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:42:12.531
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 06/22/23 11:42:12.536
STEP: Ensuring ResourceQuota status is calculated 06/22/23 11:42:12.543
STEP: Creating a ResourceQuota with not best effort scope 06/22/23 11:42:14.551
STEP: Ensuring ResourceQuota status is calculated 06/22/23 11:42:14.558
STEP: Creating a best-effort pod 06/22/23 11:42:16.565
W0622 11:42:16.587176      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring resource quota with best effort scope captures the pod usage 06/22/23 11:42:16.587
STEP: Ensuring resource quota with not best effort ignored the pod usage 06/22/23 11:42:18.593
STEP: Deleting the pod 06/22/23 11:42:20.599
STEP: Ensuring resource quota status released the pod usage 06/22/23 11:42:20.62
STEP: Creating a not best-effort pod 06/22/23 11:42:22.905
W0622 11:42:23.018202      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring resource quota with not best effort scope captures the pod usage 06/22/23 11:42:23.018
STEP: Ensuring resource quota with best effort scope ignored the pod usage 06/22/23 11:42:25.026
STEP: Deleting the pod 06/22/23 11:42:27.034
STEP: Ensuring resource quota status released the pod usage 06/22/23 11:42:27.056
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun 22 11:42:29.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5392" for this suite. 06/22/23 11:42:29.075
------------------------------
• [SLOW TEST] [16.581 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:42:12.506
    Jun 22 11:42:12.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename resourcequota 06/22/23 11:42:12.507
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:42:12.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:42:12.531
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 06/22/23 11:42:12.536
    STEP: Ensuring ResourceQuota status is calculated 06/22/23 11:42:12.543
    STEP: Creating a ResourceQuota with not best effort scope 06/22/23 11:42:14.551
    STEP: Ensuring ResourceQuota status is calculated 06/22/23 11:42:14.558
    STEP: Creating a best-effort pod 06/22/23 11:42:16.565
    W0622 11:42:16.587176      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring resource quota with best effort scope captures the pod usage 06/22/23 11:42:16.587
    STEP: Ensuring resource quota with not best effort ignored the pod usage 06/22/23 11:42:18.593
    STEP: Deleting the pod 06/22/23 11:42:20.599
    STEP: Ensuring resource quota status released the pod usage 06/22/23 11:42:20.62
    STEP: Creating a not best-effort pod 06/22/23 11:42:22.905
    W0622 11:42:23.018202      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 06/22/23 11:42:23.018
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 06/22/23 11:42:25.026
    STEP: Deleting the pod 06/22/23 11:42:27.034
    STEP: Ensuring resource quota status released the pod usage 06/22/23 11:42:27.056
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:42:29.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5392" for this suite. 06/22/23 11:42:29.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:42:29.087
Jun 22 11:42:29.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename crd-publish-openapi 06/22/23 11:42:29.088
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:42:29.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:42:29.121
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 06/22/23 11:42:29.126
Jun 22 11:42:29.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: rename a version 06/22/23 11:42:34.895
STEP: check the new version name is served 06/22/23 11:42:34.913
STEP: check the old version name is removed 06/22/23 11:42:37.014
STEP: check the other version is not changed 06/22/23 11:42:37.81
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:42:41.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2777" for this suite. 06/22/23 11:42:41.951
------------------------------
• [SLOW TEST] [12.874 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:42:29.087
    Jun 22 11:42:29.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename crd-publish-openapi 06/22/23 11:42:29.088
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:42:29.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:42:29.121
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 06/22/23 11:42:29.126
    Jun 22 11:42:29.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: rename a version 06/22/23 11:42:34.895
    STEP: check the new version name is served 06/22/23 11:42:34.913
    STEP: check the old version name is removed 06/22/23 11:42:37.014
    STEP: check the other version is not changed 06/22/23 11:42:37.81
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:42:41.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2777" for this suite. 06/22/23 11:42:41.951
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:42:41.961
Jun 22 11:42:41.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename configmap 06/22/23 11:42:41.962
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:42:41.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:42:41.986
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-2b67a0bd-20e1-42a5-b7c1-1ac018093af2 06/22/23 11:42:41.989
STEP: Creating a pod to test consume configMaps 06/22/23 11:42:41.995
W0622 11:42:42.011032      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:42:42.011: INFO: Waiting up to 5m0s for pod "pod-configmaps-d982f7a9-9fcf-4e5b-80a3-fda8f23a1a13" in namespace "configmap-7714" to be "Succeeded or Failed"
Jun 22 11:42:42.015: INFO: Pod "pod-configmaps-d982f7a9-9fcf-4e5b-80a3-fda8f23a1a13": Phase="Pending", Reason="", readiness=false. Elapsed: 3.929993ms
Jun 22 11:42:44.020: INFO: Pod "pod-configmaps-d982f7a9-9fcf-4e5b-80a3-fda8f23a1a13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009329733s
Jun 22 11:42:46.021: INFO: Pod "pod-configmaps-d982f7a9-9fcf-4e5b-80a3-fda8f23a1a13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010345801s
STEP: Saw pod success 06/22/23 11:42:46.021
Jun 22 11:42:46.021: INFO: Pod "pod-configmaps-d982f7a9-9fcf-4e5b-80a3-fda8f23a1a13" satisfied condition "Succeeded or Failed"
Jun 22 11:42:46.026: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-configmaps-d982f7a9-9fcf-4e5b-80a3-fda8f23a1a13 container agnhost-container: <nil>
STEP: delete the pod 06/22/23 11:42:46.05
Jun 22 11:42:46.062: INFO: Waiting for pod pod-configmaps-d982f7a9-9fcf-4e5b-80a3-fda8f23a1a13 to disappear
Jun 22 11:42:46.065: INFO: Pod pod-configmaps-d982f7a9-9fcf-4e5b-80a3-fda8f23a1a13 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun 22 11:42:46.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7714" for this suite. 06/22/23 11:42:46.071
------------------------------
• [4.127 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:42:41.961
    Jun 22 11:42:41.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename configmap 06/22/23 11:42:41.962
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:42:41.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:42:41.986
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-2b67a0bd-20e1-42a5-b7c1-1ac018093af2 06/22/23 11:42:41.989
    STEP: Creating a pod to test consume configMaps 06/22/23 11:42:41.995
    W0622 11:42:42.011032      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:42:42.011: INFO: Waiting up to 5m0s for pod "pod-configmaps-d982f7a9-9fcf-4e5b-80a3-fda8f23a1a13" in namespace "configmap-7714" to be "Succeeded or Failed"
    Jun 22 11:42:42.015: INFO: Pod "pod-configmaps-d982f7a9-9fcf-4e5b-80a3-fda8f23a1a13": Phase="Pending", Reason="", readiness=false. Elapsed: 3.929993ms
    Jun 22 11:42:44.020: INFO: Pod "pod-configmaps-d982f7a9-9fcf-4e5b-80a3-fda8f23a1a13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009329733s
    Jun 22 11:42:46.021: INFO: Pod "pod-configmaps-d982f7a9-9fcf-4e5b-80a3-fda8f23a1a13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010345801s
    STEP: Saw pod success 06/22/23 11:42:46.021
    Jun 22 11:42:46.021: INFO: Pod "pod-configmaps-d982f7a9-9fcf-4e5b-80a3-fda8f23a1a13" satisfied condition "Succeeded or Failed"
    Jun 22 11:42:46.026: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-configmaps-d982f7a9-9fcf-4e5b-80a3-fda8f23a1a13 container agnhost-container: <nil>
    STEP: delete the pod 06/22/23 11:42:46.05
    Jun 22 11:42:46.062: INFO: Waiting for pod pod-configmaps-d982f7a9-9fcf-4e5b-80a3-fda8f23a1a13 to disappear
    Jun 22 11:42:46.065: INFO: Pod pod-configmaps-d982f7a9-9fcf-4e5b-80a3-fda8f23a1a13 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:42:46.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7714" for this suite. 06/22/23 11:42:46.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:42:46.089
Jun 22 11:42:46.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename container-probe 06/22/23 11:42:46.09
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:42:46.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:42:46.118
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-b8f14ad0-c42d-4ed6-b788-975932fbb947 in namespace container-probe-7688 06/22/23 11:42:46.122
W0622 11:42:46.131313      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "busybox" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:42:46.131: INFO: Waiting up to 5m0s for pod "busybox-b8f14ad0-c42d-4ed6-b788-975932fbb947" in namespace "container-probe-7688" to be "not pending"
Jun 22 11:42:46.134: INFO: Pod "busybox-b8f14ad0-c42d-4ed6-b788-975932fbb947": Phase="Pending", Reason="", readiness=false. Elapsed: 3.414764ms
Jun 22 11:42:48.141: INFO: Pod "busybox-b8f14ad0-c42d-4ed6-b788-975932fbb947": Phase="Running", Reason="", readiness=true. Elapsed: 2.010099055s
Jun 22 11:42:48.141: INFO: Pod "busybox-b8f14ad0-c42d-4ed6-b788-975932fbb947" satisfied condition "not pending"
Jun 22 11:42:48.141: INFO: Started pod busybox-b8f14ad0-c42d-4ed6-b788-975932fbb947 in namespace container-probe-7688
STEP: checking the pod's current state and verifying that restartCount is present 06/22/23 11:42:48.141
Jun 22 11:42:48.146: INFO: Initial restart count of pod busybox-b8f14ad0-c42d-4ed6-b788-975932fbb947 is 0
Jun 22 11:43:38.298: INFO: Restart count of pod container-probe-7688/busybox-b8f14ad0-c42d-4ed6-b788-975932fbb947 is now 1 (50.152223696s elapsed)
STEP: deleting the pod 06/22/23 11:43:38.298
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun 22 11:43:38.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7688" for this suite. 06/22/23 11:43:38.329
------------------------------
• [SLOW TEST] [52.262 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:42:46.089
    Jun 22 11:42:46.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename container-probe 06/22/23 11:42:46.09
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:42:46.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:42:46.118
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-b8f14ad0-c42d-4ed6-b788-975932fbb947 in namespace container-probe-7688 06/22/23 11:42:46.122
    W0622 11:42:46.131313      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "busybox" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:42:46.131: INFO: Waiting up to 5m0s for pod "busybox-b8f14ad0-c42d-4ed6-b788-975932fbb947" in namespace "container-probe-7688" to be "not pending"
    Jun 22 11:42:46.134: INFO: Pod "busybox-b8f14ad0-c42d-4ed6-b788-975932fbb947": Phase="Pending", Reason="", readiness=false. Elapsed: 3.414764ms
    Jun 22 11:42:48.141: INFO: Pod "busybox-b8f14ad0-c42d-4ed6-b788-975932fbb947": Phase="Running", Reason="", readiness=true. Elapsed: 2.010099055s
    Jun 22 11:42:48.141: INFO: Pod "busybox-b8f14ad0-c42d-4ed6-b788-975932fbb947" satisfied condition "not pending"
    Jun 22 11:42:48.141: INFO: Started pod busybox-b8f14ad0-c42d-4ed6-b788-975932fbb947 in namespace container-probe-7688
    STEP: checking the pod's current state and verifying that restartCount is present 06/22/23 11:42:48.141
    Jun 22 11:42:48.146: INFO: Initial restart count of pod busybox-b8f14ad0-c42d-4ed6-b788-975932fbb947 is 0
    Jun 22 11:43:38.298: INFO: Restart count of pod container-probe-7688/busybox-b8f14ad0-c42d-4ed6-b788-975932fbb947 is now 1 (50.152223696s elapsed)
    STEP: deleting the pod 06/22/23 11:43:38.298
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:43:38.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7688" for this suite. 06/22/23 11:43:38.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:43:38.353
Jun 22 11:43:38.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename container-probe 06/22/23 11:43:38.354
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:43:38.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:43:38.394
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-f785a39f-6de5-4ac7-9b47-21bf0ac5db4c in namespace container-probe-7351 06/22/23 11:43:38.398
W0622 11:43:38.413813      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:43:38.413: INFO: Waiting up to 5m0s for pod "liveness-f785a39f-6de5-4ac7-9b47-21bf0ac5db4c" in namespace "container-probe-7351" to be "not pending"
Jun 22 11:43:38.418: INFO: Pod "liveness-f785a39f-6de5-4ac7-9b47-21bf0ac5db4c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.278355ms
Jun 22 11:43:40.424: INFO: Pod "liveness-f785a39f-6de5-4ac7-9b47-21bf0ac5db4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010056778s
Jun 22 11:43:40.424: INFO: Pod "liveness-f785a39f-6de5-4ac7-9b47-21bf0ac5db4c" satisfied condition "not pending"
Jun 22 11:43:40.424: INFO: Started pod liveness-f785a39f-6de5-4ac7-9b47-21bf0ac5db4c in namespace container-probe-7351
STEP: checking the pod's current state and verifying that restartCount is present 06/22/23 11:43:40.424
Jun 22 11:43:40.428: INFO: Initial restart count of pod liveness-f785a39f-6de5-4ac7-9b47-21bf0ac5db4c is 0
Jun 22 11:44:00.498: INFO: Restart count of pod container-probe-7351/liveness-f785a39f-6de5-4ac7-9b47-21bf0ac5db4c is now 1 (20.069643113s elapsed)
STEP: deleting the pod 06/22/23 11:44:00.498
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jun 22 11:44:00.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7351" for this suite. 06/22/23 11:44:00.52
------------------------------
• [SLOW TEST] [22.174 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:43:38.353
    Jun 22 11:43:38.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename container-probe 06/22/23 11:43:38.354
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:43:38.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:43:38.394
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-f785a39f-6de5-4ac7-9b47-21bf0ac5db4c in namespace container-probe-7351 06/22/23 11:43:38.398
    W0622 11:43:38.413813      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:43:38.413: INFO: Waiting up to 5m0s for pod "liveness-f785a39f-6de5-4ac7-9b47-21bf0ac5db4c" in namespace "container-probe-7351" to be "not pending"
    Jun 22 11:43:38.418: INFO: Pod "liveness-f785a39f-6de5-4ac7-9b47-21bf0ac5db4c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.278355ms
    Jun 22 11:43:40.424: INFO: Pod "liveness-f785a39f-6de5-4ac7-9b47-21bf0ac5db4c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010056778s
    Jun 22 11:43:40.424: INFO: Pod "liveness-f785a39f-6de5-4ac7-9b47-21bf0ac5db4c" satisfied condition "not pending"
    Jun 22 11:43:40.424: INFO: Started pod liveness-f785a39f-6de5-4ac7-9b47-21bf0ac5db4c in namespace container-probe-7351
    STEP: checking the pod's current state and verifying that restartCount is present 06/22/23 11:43:40.424
    Jun 22 11:43:40.428: INFO: Initial restart count of pod liveness-f785a39f-6de5-4ac7-9b47-21bf0ac5db4c is 0
    Jun 22 11:44:00.498: INFO: Restart count of pod container-probe-7351/liveness-f785a39f-6de5-4ac7-9b47-21bf0ac5db4c is now 1 (20.069643113s elapsed)
    STEP: deleting the pod 06/22/23 11:44:00.498
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:44:00.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7351" for this suite. 06/22/23 11:44:00.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:44:00.528
Jun 22 11:44:00.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename configmap 06/22/23 11:44:00.529
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:44:00.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:44:00.553
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-1ee21d92-52cd-4ec7-8d86-ca1fa195fdcb 06/22/23 11:44:00.557
STEP: Creating a pod to test consume configMaps 06/22/23 11:44:00.562
W0622 11:44:00.573492      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:44:00.573: INFO: Waiting up to 5m0s for pod "pod-configmaps-25f1679e-c8be-45ae-9cc3-69ae5fcc644e" in namespace "configmap-8654" to be "Succeeded or Failed"
Jun 22 11:44:00.578: INFO: Pod "pod-configmaps-25f1679e-c8be-45ae-9cc3-69ae5fcc644e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.148154ms
Jun 22 11:44:02.584: INFO: Pod "pod-configmaps-25f1679e-c8be-45ae-9cc3-69ae5fcc644e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010760127s
Jun 22 11:44:04.584: INFO: Pod "pod-configmaps-25f1679e-c8be-45ae-9cc3-69ae5fcc644e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010286787s
STEP: Saw pod success 06/22/23 11:44:04.584
Jun 22 11:44:04.584: INFO: Pod "pod-configmaps-25f1679e-c8be-45ae-9cc3-69ae5fcc644e" satisfied condition "Succeeded or Failed"
Jun 22 11:44:04.589: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-configmaps-25f1679e-c8be-45ae-9cc3-69ae5fcc644e container agnhost-container: <nil>
STEP: delete the pod 06/22/23 11:44:04.598
Jun 22 11:44:04.615: INFO: Waiting for pod pod-configmaps-25f1679e-c8be-45ae-9cc3-69ae5fcc644e to disappear
Jun 22 11:44:04.619: INFO: Pod pod-configmaps-25f1679e-c8be-45ae-9cc3-69ae5fcc644e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun 22 11:44:04.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8654" for this suite. 06/22/23 11:44:04.626
------------------------------
• [4.105 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:44:00.528
    Jun 22 11:44:00.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename configmap 06/22/23 11:44:00.529
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:44:00.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:44:00.553
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-1ee21d92-52cd-4ec7-8d86-ca1fa195fdcb 06/22/23 11:44:00.557
    STEP: Creating a pod to test consume configMaps 06/22/23 11:44:00.562
    W0622 11:44:00.573492      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:44:00.573: INFO: Waiting up to 5m0s for pod "pod-configmaps-25f1679e-c8be-45ae-9cc3-69ae5fcc644e" in namespace "configmap-8654" to be "Succeeded or Failed"
    Jun 22 11:44:00.578: INFO: Pod "pod-configmaps-25f1679e-c8be-45ae-9cc3-69ae5fcc644e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.148154ms
    Jun 22 11:44:02.584: INFO: Pod "pod-configmaps-25f1679e-c8be-45ae-9cc3-69ae5fcc644e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010760127s
    Jun 22 11:44:04.584: INFO: Pod "pod-configmaps-25f1679e-c8be-45ae-9cc3-69ae5fcc644e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010286787s
    STEP: Saw pod success 06/22/23 11:44:04.584
    Jun 22 11:44:04.584: INFO: Pod "pod-configmaps-25f1679e-c8be-45ae-9cc3-69ae5fcc644e" satisfied condition "Succeeded or Failed"
    Jun 22 11:44:04.589: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-configmaps-25f1679e-c8be-45ae-9cc3-69ae5fcc644e container agnhost-container: <nil>
    STEP: delete the pod 06/22/23 11:44:04.598
    Jun 22 11:44:04.615: INFO: Waiting for pod pod-configmaps-25f1679e-c8be-45ae-9cc3-69ae5fcc644e to disappear
    Jun 22 11:44:04.619: INFO: Pod pod-configmaps-25f1679e-c8be-45ae-9cc3-69ae5fcc644e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:44:04.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8654" for this suite. 06/22/23 11:44:04.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:44:04.634
Jun 22 11:44:04.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename webhook 06/22/23 11:44:04.635
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:44:04.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:44:04.661
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/22/23 11:44:04.683
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:44:05.014
STEP: Deploying the webhook pod 06/22/23 11:44:05.027
W0622 11:44:05.045297      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 11:44:05.045
Jun 22 11:44:05.054: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 06/22/23 11:44:07.069
STEP: Verifying the service has paired with the endpoint 06/22/23 11:44:07.085
Jun 22 11:44:08.086: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 06/22/23 11:44:08.091
STEP: create a pod that should be denied by the webhook 06/22/23 11:44:08.117
W0622 11:44:08.135824      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webhook-disallow" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webhook-disallow" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webhook-disallow" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webhook-disallow" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: create a pod that causes the webhook to hang 06/22/23 11:44:08.135
W0622 11:44:18.142239      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "wait-forever" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "wait-forever" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "wait-forever" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "wait-forever" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: create a configmap that should be denied by the webhook 06/22/23 11:44:18.147
STEP: create a configmap that should be admitted by the webhook 06/22/23 11:44:18.159
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 06/22/23 11:44:18.172
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 06/22/23 11:44:18.186
STEP: create a namespace that bypass the webhook 06/22/23 11:44:18.195
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 06/22/23 11:44:18.205
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:44:18.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3503" for this suite. 06/22/23 11:44:18.29
STEP: Destroying namespace "webhook-3503-markers" for this suite. 06/22/23 11:44:18.298
------------------------------
• [SLOW TEST] [13.674 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:44:04.634
    Jun 22 11:44:04.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename webhook 06/22/23 11:44:04.635
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:44:04.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:44:04.661
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/22/23 11:44:04.683
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:44:05.014
    STEP: Deploying the webhook pod 06/22/23 11:44:05.027
    W0622 11:44:05.045297      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 11:44:05.045
    Jun 22 11:44:05.054: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 06/22/23 11:44:07.069
    STEP: Verifying the service has paired with the endpoint 06/22/23 11:44:07.085
    Jun 22 11:44:08.086: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 06/22/23 11:44:08.091
    STEP: create a pod that should be denied by the webhook 06/22/23 11:44:08.117
    W0622 11:44:08.135824      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webhook-disallow" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webhook-disallow" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webhook-disallow" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webhook-disallow" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: create a pod that causes the webhook to hang 06/22/23 11:44:08.135
    W0622 11:44:18.142239      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "wait-forever" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "wait-forever" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "wait-forever" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "wait-forever" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: create a configmap that should be denied by the webhook 06/22/23 11:44:18.147
    STEP: create a configmap that should be admitted by the webhook 06/22/23 11:44:18.159
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 06/22/23 11:44:18.172
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 06/22/23 11:44:18.186
    STEP: create a namespace that bypass the webhook 06/22/23 11:44:18.195
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 06/22/23 11:44:18.205
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:44:18.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3503" for this suite. 06/22/23 11:44:18.29
    STEP: Destroying namespace "webhook-3503-markers" for this suite. 06/22/23 11:44:18.298
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:44:18.312
Jun 22 11:44:18.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename dns 06/22/23 11:44:18.313
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:44:18.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:44:18.342
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 06/22/23 11:44:18.346
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3274.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local; sleep 1; done
 06/22/23 11:44:18.352
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3274.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local; sleep 1; done
 06/22/23 11:44:18.353
STEP: creating a pod to probe DNS 06/22/23 11:44:18.353
STEP: submitting the pod to kubernetes 06/22/23 11:44:18.353
W0622 11:44:18.364740      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:44:18.365: INFO: Waiting up to 15m0s for pod "dns-test-797dd167-875c-4976-b10c-d77fdfee92f0" in namespace "dns-3274" to be "running"
Jun 22 11:44:18.372: INFO: Pod "dns-test-797dd167-875c-4976-b10c-d77fdfee92f0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.661489ms
Jun 22 11:44:20.379: INFO: Pod "dns-test-797dd167-875c-4976-b10c-d77fdfee92f0": Phase="Running", Reason="", readiness=true. Elapsed: 2.014052979s
Jun 22 11:44:20.379: INFO: Pod "dns-test-797dd167-875c-4976-b10c-d77fdfee92f0" satisfied condition "running"
STEP: retrieving the pod 06/22/23 11:44:20.379
STEP: looking for the results for each expected name from probers 06/22/23 11:44:20.383
Jun 22 11:44:20.399: INFO: File jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-797dd167-875c-4976-b10c-d77fdfee92f0 contains '' instead of 'foo.example.com.'
Jun 22 11:44:20.399: INFO: Lookups using dns-3274/dns-test-797dd167-875c-4976-b10c-d77fdfee92f0 failed for: [jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local]

Jun 22 11:44:25.405: INFO: File wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-797dd167-875c-4976-b10c-d77fdfee92f0 contains '' instead of 'foo.example.com.'
Jun 22 11:44:25.412: INFO: Lookups using dns-3274/dns-test-797dd167-875c-4976-b10c-d77fdfee92f0 failed for: [wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local]

Jun 22 11:44:30.411: INFO: DNS probes using dns-test-797dd167-875c-4976-b10c-d77fdfee92f0 succeeded

STEP: deleting the pod 06/22/23 11:44:30.411
STEP: changing the externalName to bar.example.com 06/22/23 11:44:30.427
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3274.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local; sleep 1; done
 06/22/23 11:44:30.438
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3274.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local; sleep 1; done
 06/22/23 11:44:30.438
STEP: creating a second pod to probe DNS 06/22/23 11:44:30.438
STEP: submitting the pod to kubernetes 06/22/23 11:44:30.438
W0622 11:44:30.447643      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:44:30.447: INFO: Waiting up to 15m0s for pod "dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5" in namespace "dns-3274" to be "running"
Jun 22 11:44:30.453: INFO: Pod "dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028854ms
Jun 22 11:44:32.459: INFO: Pod "dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5": Phase="Running", Reason="", readiness=true. Elapsed: 2.011716786s
Jun 22 11:44:32.459: INFO: Pod "dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5" satisfied condition "running"
STEP: retrieving the pod 06/22/23 11:44:32.459
STEP: looking for the results for each expected name from probers 06/22/23 11:44:32.463
Jun 22 11:44:32.470: INFO: File wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 11:44:32.475: INFO: File jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 11:44:32.475: INFO: Lookups using dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 failed for: [wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local]

Jun 22 11:44:37.486: INFO: File wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 11:44:37.491: INFO: File jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 11:44:37.491: INFO: Lookups using dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 failed for: [wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local]

Jun 22 11:44:42.485: INFO: File wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 11:44:42.491: INFO: File jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 11:44:42.491: INFO: Lookups using dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 failed for: [wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local]

Jun 22 11:44:47.485: INFO: File wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 11:44:47.492: INFO: File jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 11:44:47.492: INFO: Lookups using dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 failed for: [wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local]

Jun 22 11:44:52.490: INFO: DNS probes using dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 succeeded

STEP: deleting the pod 06/22/23 11:44:52.49
STEP: changing the service to type=ClusterIP 06/22/23 11:44:52.505
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3274.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local; sleep 1; done
 06/22/23 11:44:52.531
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3274.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local; sleep 1; done
 06/22/23 11:44:52.531
STEP: creating a third pod to probe DNS 06/22/23 11:44:52.531
STEP: submitting the pod to kubernetes 06/22/23 11:44:52.535
W0622 11:44:52.545169      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:44:52.545: INFO: Waiting up to 15m0s for pod "dns-test-aae90839-59ff-45f8-a679-52b8b58fe186" in namespace "dns-3274" to be "running"
Jun 22 11:44:52.548: INFO: Pod "dns-test-aae90839-59ff-45f8-a679-52b8b58fe186": Phase="Pending", Reason="", readiness=false. Elapsed: 3.614781ms
Jun 22 11:44:54.555: INFO: Pod "dns-test-aae90839-59ff-45f8-a679-52b8b58fe186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009826857s
Jun 22 11:44:56.555: INFO: Pod "dns-test-aae90839-59ff-45f8-a679-52b8b58fe186": Phase="Running", Reason="", readiness=true. Elapsed: 4.010336853s
Jun 22 11:44:56.555: INFO: Pod "dns-test-aae90839-59ff-45f8-a679-52b8b58fe186" satisfied condition "running"
STEP: retrieving the pod 06/22/23 11:44:56.555
STEP: looking for the results for each expected name from probers 06/22/23 11:44:56.56
Jun 22 11:44:56.573: INFO: DNS probes using dns-test-aae90839-59ff-45f8-a679-52b8b58fe186 succeeded

STEP: deleting the pod 06/22/23 11:44:56.573
STEP: deleting the test externalName service 06/22/23 11:44:56.587
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun 22 11:44:56.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3274" for this suite. 06/22/23 11:44:56.612
------------------------------
• [SLOW TEST] [38.308 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:44:18.312
    Jun 22 11:44:18.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename dns 06/22/23 11:44:18.313
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:44:18.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:44:18.342
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 06/22/23 11:44:18.346
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3274.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local; sleep 1; done
     06/22/23 11:44:18.352
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3274.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local; sleep 1; done
     06/22/23 11:44:18.353
    STEP: creating a pod to probe DNS 06/22/23 11:44:18.353
    STEP: submitting the pod to kubernetes 06/22/23 11:44:18.353
    W0622 11:44:18.364740      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:44:18.365: INFO: Waiting up to 15m0s for pod "dns-test-797dd167-875c-4976-b10c-d77fdfee92f0" in namespace "dns-3274" to be "running"
    Jun 22 11:44:18.372: INFO: Pod "dns-test-797dd167-875c-4976-b10c-d77fdfee92f0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.661489ms
    Jun 22 11:44:20.379: INFO: Pod "dns-test-797dd167-875c-4976-b10c-d77fdfee92f0": Phase="Running", Reason="", readiness=true. Elapsed: 2.014052979s
    Jun 22 11:44:20.379: INFO: Pod "dns-test-797dd167-875c-4976-b10c-d77fdfee92f0" satisfied condition "running"
    STEP: retrieving the pod 06/22/23 11:44:20.379
    STEP: looking for the results for each expected name from probers 06/22/23 11:44:20.383
    Jun 22 11:44:20.399: INFO: File jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-797dd167-875c-4976-b10c-d77fdfee92f0 contains '' instead of 'foo.example.com.'
    Jun 22 11:44:20.399: INFO: Lookups using dns-3274/dns-test-797dd167-875c-4976-b10c-d77fdfee92f0 failed for: [jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local]

    Jun 22 11:44:25.405: INFO: File wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-797dd167-875c-4976-b10c-d77fdfee92f0 contains '' instead of 'foo.example.com.'
    Jun 22 11:44:25.412: INFO: Lookups using dns-3274/dns-test-797dd167-875c-4976-b10c-d77fdfee92f0 failed for: [wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local]

    Jun 22 11:44:30.411: INFO: DNS probes using dns-test-797dd167-875c-4976-b10c-d77fdfee92f0 succeeded

    STEP: deleting the pod 06/22/23 11:44:30.411
    STEP: changing the externalName to bar.example.com 06/22/23 11:44:30.427
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3274.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local; sleep 1; done
     06/22/23 11:44:30.438
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3274.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local; sleep 1; done
     06/22/23 11:44:30.438
    STEP: creating a second pod to probe DNS 06/22/23 11:44:30.438
    STEP: submitting the pod to kubernetes 06/22/23 11:44:30.438
    W0622 11:44:30.447643      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:44:30.447: INFO: Waiting up to 15m0s for pod "dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5" in namespace "dns-3274" to be "running"
    Jun 22 11:44:30.453: INFO: Pod "dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028854ms
    Jun 22 11:44:32.459: INFO: Pod "dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5": Phase="Running", Reason="", readiness=true. Elapsed: 2.011716786s
    Jun 22 11:44:32.459: INFO: Pod "dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5" satisfied condition "running"
    STEP: retrieving the pod 06/22/23 11:44:32.459
    STEP: looking for the results for each expected name from probers 06/22/23 11:44:32.463
    Jun 22 11:44:32.470: INFO: File wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 22 11:44:32.475: INFO: File jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 22 11:44:32.475: INFO: Lookups using dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 failed for: [wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local]

    Jun 22 11:44:37.486: INFO: File wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 22 11:44:37.491: INFO: File jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 22 11:44:37.491: INFO: Lookups using dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 failed for: [wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local]

    Jun 22 11:44:42.485: INFO: File wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 22 11:44:42.491: INFO: File jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 22 11:44:42.491: INFO: Lookups using dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 failed for: [wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local]

    Jun 22 11:44:47.485: INFO: File wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 22 11:44:47.492: INFO: File jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local from pod  dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 22 11:44:47.492: INFO: Lookups using dns-3274/dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 failed for: [wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local]

    Jun 22 11:44:52.490: INFO: DNS probes using dns-test-85ffc633-55d6-44ff-b747-c745f74b5ab5 succeeded

    STEP: deleting the pod 06/22/23 11:44:52.49
    STEP: changing the service to type=ClusterIP 06/22/23 11:44:52.505
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3274.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3274.svc.cluster.local; sleep 1; done
     06/22/23 11:44:52.531
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3274.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3274.svc.cluster.local; sleep 1; done
     06/22/23 11:44:52.531
    STEP: creating a third pod to probe DNS 06/22/23 11:44:52.531
    STEP: submitting the pod to kubernetes 06/22/23 11:44:52.535
    W0622 11:44:52.545169      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:44:52.545: INFO: Waiting up to 15m0s for pod "dns-test-aae90839-59ff-45f8-a679-52b8b58fe186" in namespace "dns-3274" to be "running"
    Jun 22 11:44:52.548: INFO: Pod "dns-test-aae90839-59ff-45f8-a679-52b8b58fe186": Phase="Pending", Reason="", readiness=false. Elapsed: 3.614781ms
    Jun 22 11:44:54.555: INFO: Pod "dns-test-aae90839-59ff-45f8-a679-52b8b58fe186": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009826857s
    Jun 22 11:44:56.555: INFO: Pod "dns-test-aae90839-59ff-45f8-a679-52b8b58fe186": Phase="Running", Reason="", readiness=true. Elapsed: 4.010336853s
    Jun 22 11:44:56.555: INFO: Pod "dns-test-aae90839-59ff-45f8-a679-52b8b58fe186" satisfied condition "running"
    STEP: retrieving the pod 06/22/23 11:44:56.555
    STEP: looking for the results for each expected name from probers 06/22/23 11:44:56.56
    Jun 22 11:44:56.573: INFO: DNS probes using dns-test-aae90839-59ff-45f8-a679-52b8b58fe186 succeeded

    STEP: deleting the pod 06/22/23 11:44:56.573
    STEP: deleting the test externalName service 06/22/23 11:44:56.587
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:44:56.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3274" for this suite. 06/22/23 11:44:56.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:44:56.623
Jun 22 11:44:56.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename emptydir 06/22/23 11:44:56.624
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:44:56.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:44:56.648
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 06/22/23 11:44:56.651
W0622 11:44:56.663561      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:44:56.663: INFO: Waiting up to 5m0s for pod "pod-fe576f20-1e61-4a9a-a92c-9ae08fc96840" in namespace "emptydir-6929" to be "Succeeded or Failed"
Jun 22 11:44:56.667: INFO: Pod "pod-fe576f20-1e61-4a9a-a92c-9ae08fc96840": Phase="Pending", Reason="", readiness=false. Elapsed: 3.572111ms
Jun 22 11:44:58.673: INFO: Pod "pod-fe576f20-1e61-4a9a-a92c-9ae08fc96840": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009790125s
Jun 22 11:45:00.674: INFO: Pod "pod-fe576f20-1e61-4a9a-a92c-9ae08fc96840": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010290895s
STEP: Saw pod success 06/22/23 11:45:00.674
Jun 22 11:45:00.674: INFO: Pod "pod-fe576f20-1e61-4a9a-a92c-9ae08fc96840" satisfied condition "Succeeded or Failed"
Jun 22 11:45:00.678: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-fe576f20-1e61-4a9a-a92c-9ae08fc96840 container test-container: <nil>
STEP: delete the pod 06/22/23 11:45:00.687
Jun 22 11:45:00.700: INFO: Waiting for pod pod-fe576f20-1e61-4a9a-a92c-9ae08fc96840 to disappear
Jun 22 11:45:00.704: INFO: Pod pod-fe576f20-1e61-4a9a-a92c-9ae08fc96840 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jun 22 11:45:00.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6929" for this suite. 06/22/23 11:45:00.71
------------------------------
• [4.094 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:44:56.623
    Jun 22 11:44:56.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename emptydir 06/22/23 11:44:56.624
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:44:56.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:44:56.648
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 06/22/23 11:44:56.651
    W0622 11:44:56.663561      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:44:56.663: INFO: Waiting up to 5m0s for pod "pod-fe576f20-1e61-4a9a-a92c-9ae08fc96840" in namespace "emptydir-6929" to be "Succeeded or Failed"
    Jun 22 11:44:56.667: INFO: Pod "pod-fe576f20-1e61-4a9a-a92c-9ae08fc96840": Phase="Pending", Reason="", readiness=false. Elapsed: 3.572111ms
    Jun 22 11:44:58.673: INFO: Pod "pod-fe576f20-1e61-4a9a-a92c-9ae08fc96840": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009790125s
    Jun 22 11:45:00.674: INFO: Pod "pod-fe576f20-1e61-4a9a-a92c-9ae08fc96840": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010290895s
    STEP: Saw pod success 06/22/23 11:45:00.674
    Jun 22 11:45:00.674: INFO: Pod "pod-fe576f20-1e61-4a9a-a92c-9ae08fc96840" satisfied condition "Succeeded or Failed"
    Jun 22 11:45:00.678: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-fe576f20-1e61-4a9a-a92c-9ae08fc96840 container test-container: <nil>
    STEP: delete the pod 06/22/23 11:45:00.687
    Jun 22 11:45:00.700: INFO: Waiting for pod pod-fe576f20-1e61-4a9a-a92c-9ae08fc96840 to disappear
    Jun 22 11:45:00.704: INFO: Pod pod-fe576f20-1e61-4a9a-a92c-9ae08fc96840 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:45:00.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6929" for this suite. 06/22/23 11:45:00.71
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:45:00.717
Jun 22 11:45:00.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename ingressclass 06/22/23 11:45:00.718
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:45:00.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:45:00.74
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 06/22/23 11:45:00.744
STEP: getting /apis/networking.k8s.io 06/22/23 11:45:00.749
STEP: getting /apis/networking.k8s.iov1 06/22/23 11:45:00.753
STEP: creating 06/22/23 11:45:00.756
STEP: getting 06/22/23 11:45:00.776
STEP: listing 06/22/23 11:45:00.78
STEP: watching 06/22/23 11:45:00.784
Jun 22 11:45:00.784: INFO: starting watch
STEP: patching 06/22/23 11:45:00.786
STEP: updating 06/22/23 11:45:00.796
Jun 22 11:45:00.802: INFO: waiting for watch events with expected annotations
Jun 22 11:45:00.802: INFO: saw patched and updated annotations
STEP: deleting 06/22/23 11:45:00.802
STEP: deleting a collection 06/22/23 11:45:00.818
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Jun 22 11:45:00.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-877" for this suite. 06/22/23 11:45:00.845
------------------------------
• [0.138 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:45:00.717
    Jun 22 11:45:00.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename ingressclass 06/22/23 11:45:00.718
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:45:00.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:45:00.74
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 06/22/23 11:45:00.744
    STEP: getting /apis/networking.k8s.io 06/22/23 11:45:00.749
    STEP: getting /apis/networking.k8s.iov1 06/22/23 11:45:00.753
    STEP: creating 06/22/23 11:45:00.756
    STEP: getting 06/22/23 11:45:00.776
    STEP: listing 06/22/23 11:45:00.78
    STEP: watching 06/22/23 11:45:00.784
    Jun 22 11:45:00.784: INFO: starting watch
    STEP: patching 06/22/23 11:45:00.786
    STEP: updating 06/22/23 11:45:00.796
    Jun 22 11:45:00.802: INFO: waiting for watch events with expected annotations
    Jun 22 11:45:00.802: INFO: saw patched and updated annotations
    STEP: deleting 06/22/23 11:45:00.802
    STEP: deleting a collection 06/22/23 11:45:00.818
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:45:00.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-877" for this suite. 06/22/23 11:45:00.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:45:00.857
Jun 22 11:45:00.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename svcaccounts 06/22/23 11:45:00.858
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:45:00.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:45:00.879
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  06/22/23 11:45:00.885
W0622 11:45:00.897003      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:45:00.897: INFO: Waiting up to 5m0s for pod "test-pod-a4a155a1-505b-48ce-a257-927b14f58e40" in namespace "svcaccounts-9438" to be "Succeeded or Failed"
Jun 22 11:45:00.901: INFO: Pod "test-pod-a4a155a1-505b-48ce-a257-927b14f58e40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.48224ms
Jun 22 11:45:02.908: INFO: Pod "test-pod-a4a155a1-505b-48ce-a257-927b14f58e40": Phase="Running", Reason="", readiness=false. Elapsed: 2.010990936s
Jun 22 11:45:04.906: INFO: Pod "test-pod-a4a155a1-505b-48ce-a257-927b14f58e40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009528285s
STEP: Saw pod success 06/22/23 11:45:04.906
Jun 22 11:45:04.906: INFO: Pod "test-pod-a4a155a1-505b-48ce-a257-927b14f58e40" satisfied condition "Succeeded or Failed"
Jun 22 11:45:04.911: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod test-pod-a4a155a1-505b-48ce-a257-927b14f58e40 container agnhost-container: <nil>
STEP: delete the pod 06/22/23 11:45:04.92
Jun 22 11:45:04.931: INFO: Waiting for pod test-pod-a4a155a1-505b-48ce-a257-927b14f58e40 to disappear
Jun 22 11:45:04.935: INFO: Pod test-pod-a4a155a1-505b-48ce-a257-927b14f58e40 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun 22 11:45:04.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9438" for this suite. 06/22/23 11:45:04.94
------------------------------
• [4.089 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:45:00.857
    Jun 22 11:45:00.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename svcaccounts 06/22/23 11:45:00.858
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:45:00.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:45:00.879
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  06/22/23 11:45:00.885
    W0622 11:45:00.897003      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:45:00.897: INFO: Waiting up to 5m0s for pod "test-pod-a4a155a1-505b-48ce-a257-927b14f58e40" in namespace "svcaccounts-9438" to be "Succeeded or Failed"
    Jun 22 11:45:00.901: INFO: Pod "test-pod-a4a155a1-505b-48ce-a257-927b14f58e40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.48224ms
    Jun 22 11:45:02.908: INFO: Pod "test-pod-a4a155a1-505b-48ce-a257-927b14f58e40": Phase="Running", Reason="", readiness=false. Elapsed: 2.010990936s
    Jun 22 11:45:04.906: INFO: Pod "test-pod-a4a155a1-505b-48ce-a257-927b14f58e40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009528285s
    STEP: Saw pod success 06/22/23 11:45:04.906
    Jun 22 11:45:04.906: INFO: Pod "test-pod-a4a155a1-505b-48ce-a257-927b14f58e40" satisfied condition "Succeeded or Failed"
    Jun 22 11:45:04.911: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod test-pod-a4a155a1-505b-48ce-a257-927b14f58e40 container agnhost-container: <nil>
    STEP: delete the pod 06/22/23 11:45:04.92
    Jun 22 11:45:04.931: INFO: Waiting for pod test-pod-a4a155a1-505b-48ce-a257-927b14f58e40 to disappear
    Jun 22 11:45:04.935: INFO: Pod test-pod-a4a155a1-505b-48ce-a257-927b14f58e40 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:45:04.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9438" for this suite. 06/22/23 11:45:04.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:45:04.947
Jun 22 11:45:04.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename services 06/22/23 11:45:04.948
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:45:04.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:45:04.969
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun 22 11:45:04.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3436" for this suite. 06/22/23 11:45:04.982
------------------------------
• [0.043 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:45:04.947
    Jun 22 11:45:04.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename services 06/22/23 11:45:04.948
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:45:04.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:45:04.969
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:45:04.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3436" for this suite. 06/22/23 11:45:04.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:45:04.993
Jun 22 11:45:04.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename statefulset 06/22/23 11:45:04.995
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:45:05.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:45:05.014
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9049 06/22/23 11:45:05.018
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 06/22/23 11:45:05.025
W0622 11:45:05.035032      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:45:05.039: INFO: Found 0 stateful pods, waiting for 3
Jun 22 11:45:15.045: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 11:45:15.045: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 11:45:15.045: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 11:45:15.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-9049 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 11:45:15.273: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 11:45:15.273: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 11:45:15.273: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 06/22/23 11:45:25.303
W0622 11:45:25.332230      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:45:25.332: INFO: Updating stateful set ss2
STEP: Creating a new revision 06/22/23 11:45:25.332
STEP: Updating Pods in reverse ordinal order 06/22/23 11:45:35.358
Jun 22 11:45:35.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-9049 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 11:45:35.641: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 11:45:35.641: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 11:45:35.641: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 06/22/23 11:45:45.675
Jun 22 11:45:45.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-9049 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 22 11:45:45.860: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 22 11:45:45.860: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 22 11:45:45.860: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

W0622 11:45:55.907085      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:45:55.907: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 06/22/23 11:46:05.943
Jun 22 11:46:05.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-9049 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 22 11:46:06.165: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 22 11:46:06.165: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 22 11:46:06.165: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun 22 11:46:16.195: INFO: Deleting all statefulset in ns statefulset-9049
Jun 22 11:46:16.199: INFO: Scaling statefulset ss2 to 0
W0622 11:46:16.209460      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:46:26.220: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 11:46:26.224: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun 22 11:46:26.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9049" for this suite. 06/22/23 11:46:26.249
------------------------------
• [SLOW TEST] [81.264 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:45:04.993
    Jun 22 11:45:04.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename statefulset 06/22/23 11:45:04.995
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:45:05.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:45:05.014
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9049 06/22/23 11:45:05.018
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 06/22/23 11:45:05.025
    W0622 11:45:05.035032      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:45:05.039: INFO: Found 0 stateful pods, waiting for 3
    Jun 22 11:45:15.045: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 22 11:45:15.045: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 22 11:45:15.045: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jun 22 11:45:15.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-9049 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 22 11:45:15.273: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 22 11:45:15.273: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 22 11:45:15.273: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 06/22/23 11:45:25.303
    W0622 11:45:25.332230      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:45:25.332: INFO: Updating stateful set ss2
    STEP: Creating a new revision 06/22/23 11:45:25.332
    STEP: Updating Pods in reverse ordinal order 06/22/23 11:45:35.358
    Jun 22 11:45:35.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-9049 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 22 11:45:35.641: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 22 11:45:35.641: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 22 11:45:35.641: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 06/22/23 11:45:45.675
    Jun 22 11:45:45.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-9049 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 22 11:45:45.860: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 22 11:45:45.860: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 22 11:45:45.860: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    W0622 11:45:55.907085      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:45:55.907: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 06/22/23 11:46:05.943
    Jun 22 11:46:05.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=statefulset-9049 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 22 11:46:06.165: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 22 11:46:06.165: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 22 11:46:06.165: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun 22 11:46:16.195: INFO: Deleting all statefulset in ns statefulset-9049
    Jun 22 11:46:16.199: INFO: Scaling statefulset ss2 to 0
    W0622 11:46:16.209460      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:46:26.220: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 22 11:46:26.224: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:46:26.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9049" for this suite. 06/22/23 11:46:26.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:46:26.26
Jun 22 11:46:26.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename dns 06/22/23 11:46:26.267
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:46:26.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:46:26.291
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 06/22/23 11:46:26.295
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7913.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7913.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local;sleep 1; done
 06/22/23 11:46:26.301
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7913.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7913.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local;sleep 1; done
 06/22/23 11:46:26.301
STEP: creating a pod to probe DNS 06/22/23 11:46:26.301
STEP: submitting the pod to kubernetes 06/22/23 11:46:26.302
W0622 11:46:26.314261      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:46:26.314: INFO: Waiting up to 15m0s for pod "dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f" in namespace "dns-7913" to be "running"
Jun 22 11:46:26.318: INFO: Pod "dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.918323ms
Jun 22 11:46:28.324: INFO: Pod "dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009749332s
Jun 22 11:46:28.324: INFO: Pod "dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f" satisfied condition "running"
STEP: retrieving the pod 06/22/23 11:46:28.324
STEP: looking for the results for each expected name from probers 06/22/23 11:46:28.328
Jun 22 11:46:28.336: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:28.340: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:28.344: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:28.349: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:28.353: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:28.358: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:28.362: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:28.367: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:28.367: INFO: Lookups using dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local]

Jun 22 11:46:33.376: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:33.382: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:33.387: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:33.392: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:33.396: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:33.401: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:33.406: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:33.411: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:33.411: INFO: Lookups using dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local]

Jun 22 11:46:38.380: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:38.387: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:38.393: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:38.398: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:38.404: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:38.411: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:38.416: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:38.422: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:38.422: INFO: Lookups using dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local]

Jun 22 11:46:43.377: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:43.382: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:43.387: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:43.392: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:43.396: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:43.400: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:43.405: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:43.410: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:43.410: INFO: Lookups using dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local]

Jun 22 11:46:48.384: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:48.391: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:48.397: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:48.403: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:48.411: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:48.417: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:48.424: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:48.431: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:48.431: INFO: Lookups using dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local]

Jun 22 11:46:53.377: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:53.383: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:53.388: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:53.393: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:53.398: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:53.402: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:53.407: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:53.412: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:53.412: INFO: Lookups using dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local]

Jun 22 11:46:58.373: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:58.386: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:58.413: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
Jun 22 11:46:58.431: INFO: Lookups using dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local]

Jun 22 11:47:03.413: INFO: DNS probes using dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f succeeded

STEP: deleting the pod 06/22/23 11:47:03.413
STEP: deleting the test headless service 06/22/23 11:47:03.433
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun 22 11:47:03.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7913" for this suite. 06/22/23 11:47:03.458
------------------------------
• [SLOW TEST] [37.207 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:46:26.26
    Jun 22 11:46:26.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename dns 06/22/23 11:46:26.267
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:46:26.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:46:26.291
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 06/22/23 11:46:26.295
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7913.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7913.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local;sleep 1; done
     06/22/23 11:46:26.301
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7913.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7913.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local;sleep 1; done
     06/22/23 11:46:26.301
    STEP: creating a pod to probe DNS 06/22/23 11:46:26.301
    STEP: submitting the pod to kubernetes 06/22/23 11:46:26.302
    W0622 11:46:26.314261      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:46:26.314: INFO: Waiting up to 15m0s for pod "dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f" in namespace "dns-7913" to be "running"
    Jun 22 11:46:26.318: INFO: Pod "dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.918323ms
    Jun 22 11:46:28.324: INFO: Pod "dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009749332s
    Jun 22 11:46:28.324: INFO: Pod "dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f" satisfied condition "running"
    STEP: retrieving the pod 06/22/23 11:46:28.324
    STEP: looking for the results for each expected name from probers 06/22/23 11:46:28.328
    Jun 22 11:46:28.336: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:28.340: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:28.344: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:28.349: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:28.353: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:28.358: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:28.362: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:28.367: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:28.367: INFO: Lookups using dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local]

    Jun 22 11:46:33.376: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:33.382: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:33.387: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:33.392: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:33.396: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:33.401: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:33.406: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:33.411: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:33.411: INFO: Lookups using dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local]

    Jun 22 11:46:38.380: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:38.387: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:38.393: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:38.398: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:38.404: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:38.411: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:38.416: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:38.422: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:38.422: INFO: Lookups using dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local]

    Jun 22 11:46:43.377: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:43.382: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:43.387: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:43.392: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:43.396: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:43.400: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:43.405: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:43.410: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:43.410: INFO: Lookups using dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local]

    Jun 22 11:46:48.384: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:48.391: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:48.397: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:48.403: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:48.411: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:48.417: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:48.424: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:48.431: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:48.431: INFO: Lookups using dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local]

    Jun 22 11:46:53.377: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:53.383: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:53.388: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:53.393: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:53.398: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:53.402: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:53.407: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:53.412: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:53.412: INFO: Lookups using dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-test-service-2.dns-7913.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7913.svc.cluster.local]

    Jun 22 11:46:58.373: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:58.386: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:58.413: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local from pod dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f: the server could not find the requested resource (get pods dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f)
    Jun 22 11:46:58.431: INFO: Lookups using dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7913.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7913.svc.cluster.local]

    Jun 22 11:47:03.413: INFO: DNS probes using dns-7913/dns-test-4704fbff-5264-4afe-ad6a-11e53f1bbd8f succeeded

    STEP: deleting the pod 06/22/23 11:47:03.413
    STEP: deleting the test headless service 06/22/23 11:47:03.433
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:47:03.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7913" for this suite. 06/22/23 11:47:03.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:47:03.468
Jun 22 11:47:03.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename svcaccounts 06/22/23 11:47:03.47
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:47:03.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:47:03.493
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Jun 22 11:47:03.501: INFO: Got root ca configmap in namespace "svcaccounts-2392"
Jun 22 11:47:03.507: INFO: Deleted root ca configmap in namespace "svcaccounts-2392"
STEP: waiting for a new root ca configmap created 06/22/23 11:47:04.008
Jun 22 11:47:04.013: INFO: Recreated root ca configmap in namespace "svcaccounts-2392"
Jun 22 11:47:04.020: INFO: Updated root ca configmap in namespace "svcaccounts-2392"
STEP: waiting for the root ca configmap reconciled 06/22/23 11:47:04.52
Jun 22 11:47:04.525: INFO: Reconciled root ca configmap in namespace "svcaccounts-2392"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun 22 11:47:04.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2392" for this suite. 06/22/23 11:47:04.535
------------------------------
• [1.075 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:47:03.468
    Jun 22 11:47:03.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename svcaccounts 06/22/23 11:47:03.47
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:47:03.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:47:03.493
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Jun 22 11:47:03.501: INFO: Got root ca configmap in namespace "svcaccounts-2392"
    Jun 22 11:47:03.507: INFO: Deleted root ca configmap in namespace "svcaccounts-2392"
    STEP: waiting for a new root ca configmap created 06/22/23 11:47:04.008
    Jun 22 11:47:04.013: INFO: Recreated root ca configmap in namespace "svcaccounts-2392"
    Jun 22 11:47:04.020: INFO: Updated root ca configmap in namespace "svcaccounts-2392"
    STEP: waiting for the root ca configmap reconciled 06/22/23 11:47:04.52
    Jun 22 11:47:04.525: INFO: Reconciled root ca configmap in namespace "svcaccounts-2392"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:47:04.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2392" for this suite. 06/22/23 11:47:04.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:47:04.545
Jun 22 11:47:04.545: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename sysctl 06/22/23 11:47:04.546
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:47:04.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:47:04.567
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 06/22/23 11:47:04.571
W0622 11:47:04.582129      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Watching for error events or started pod 06/22/23 11:47:04.582
STEP: Waiting for pod completion 06/22/23 11:47:06.589
Jun 22 11:47:06.589: INFO: Waiting up to 3m0s for pod "sysctl-9c16100d-1a5a-4c6f-b8a1-bf6c78e9f7c3" in namespace "sysctl-3537" to be "completed"
Jun 22 11:47:06.595: INFO: Pod "sysctl-9c16100d-1a5a-4c6f-b8a1-bf6c78e9f7c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.28517ms
Jun 22 11:47:08.601: INFO: Pod "sysctl-9c16100d-1a5a-4c6f-b8a1-bf6c78e9f7c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01137181s
Jun 22 11:47:08.601: INFO: Pod "sysctl-9c16100d-1a5a-4c6f-b8a1-bf6c78e9f7c3" satisfied condition "completed"
STEP: Checking that the pod succeeded 06/22/23 11:47:08.605
STEP: Getting logs from the pod 06/22/23 11:47:08.605
STEP: Checking that the sysctl is actually updated 06/22/23 11:47:08.625
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:47:08.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-3537" for this suite. 06/22/23 11:47:08.634
------------------------------
• [4.097 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:47:04.545
    Jun 22 11:47:04.545: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename sysctl 06/22/23 11:47:04.546
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:47:04.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:47:04.567
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 06/22/23 11:47:04.571
    W0622 11:47:04.582129      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Watching for error events or started pod 06/22/23 11:47:04.582
    STEP: Waiting for pod completion 06/22/23 11:47:06.589
    Jun 22 11:47:06.589: INFO: Waiting up to 3m0s for pod "sysctl-9c16100d-1a5a-4c6f-b8a1-bf6c78e9f7c3" in namespace "sysctl-3537" to be "completed"
    Jun 22 11:47:06.595: INFO: Pod "sysctl-9c16100d-1a5a-4c6f-b8a1-bf6c78e9f7c3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.28517ms
    Jun 22 11:47:08.601: INFO: Pod "sysctl-9c16100d-1a5a-4c6f-b8a1-bf6c78e9f7c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01137181s
    Jun 22 11:47:08.601: INFO: Pod "sysctl-9c16100d-1a5a-4c6f-b8a1-bf6c78e9f7c3" satisfied condition "completed"
    STEP: Checking that the pod succeeded 06/22/23 11:47:08.605
    STEP: Getting logs from the pod 06/22/23 11:47:08.605
    STEP: Checking that the sysctl is actually updated 06/22/23 11:47:08.625
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:47:08.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-3537" for this suite. 06/22/23 11:47:08.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:47:08.649
Jun 22 11:47:08.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename var-expansion 06/22/23 11:47:08.65
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:47:08.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:47:08.67
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 06/22/23 11:47:08.674
W0622 11:47:08.686270      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:47:08.686: INFO: Waiting up to 5m0s for pod "var-expansion-549bb287-fd5e-4be2-8966-5e1f38234dd5" in namespace "var-expansion-2750" to be "Succeeded or Failed"
Jun 22 11:47:08.690: INFO: Pod "var-expansion-549bb287-fd5e-4be2-8966-5e1f38234dd5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.73504ms
Jun 22 11:47:10.696: INFO: Pod "var-expansion-549bb287-fd5e-4be2-8966-5e1f38234dd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009563623s
Jun 22 11:47:12.697: INFO: Pod "var-expansion-549bb287-fd5e-4be2-8966-5e1f38234dd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010538982s
STEP: Saw pod success 06/22/23 11:47:12.697
Jun 22 11:47:12.697: INFO: Pod "var-expansion-549bb287-fd5e-4be2-8966-5e1f38234dd5" satisfied condition "Succeeded or Failed"
Jun 22 11:47:12.701: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod var-expansion-549bb287-fd5e-4be2-8966-5e1f38234dd5 container dapi-container: <nil>
STEP: delete the pod 06/22/23 11:47:12.71
Jun 22 11:47:12.727: INFO: Waiting for pod var-expansion-549bb287-fd5e-4be2-8966-5e1f38234dd5 to disappear
Jun 22 11:47:12.731: INFO: Pod var-expansion-549bb287-fd5e-4be2-8966-5e1f38234dd5 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jun 22 11:47:12.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2750" for this suite. 06/22/23 11:47:12.739
------------------------------
• [4.098 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:47:08.649
    Jun 22 11:47:08.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename var-expansion 06/22/23 11:47:08.65
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:47:08.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:47:08.67
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 06/22/23 11:47:08.674
    W0622 11:47:08.686270      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:47:08.686: INFO: Waiting up to 5m0s for pod "var-expansion-549bb287-fd5e-4be2-8966-5e1f38234dd5" in namespace "var-expansion-2750" to be "Succeeded or Failed"
    Jun 22 11:47:08.690: INFO: Pod "var-expansion-549bb287-fd5e-4be2-8966-5e1f38234dd5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.73504ms
    Jun 22 11:47:10.696: INFO: Pod "var-expansion-549bb287-fd5e-4be2-8966-5e1f38234dd5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009563623s
    Jun 22 11:47:12.697: INFO: Pod "var-expansion-549bb287-fd5e-4be2-8966-5e1f38234dd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010538982s
    STEP: Saw pod success 06/22/23 11:47:12.697
    Jun 22 11:47:12.697: INFO: Pod "var-expansion-549bb287-fd5e-4be2-8966-5e1f38234dd5" satisfied condition "Succeeded or Failed"
    Jun 22 11:47:12.701: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod var-expansion-549bb287-fd5e-4be2-8966-5e1f38234dd5 container dapi-container: <nil>
    STEP: delete the pod 06/22/23 11:47:12.71
    Jun 22 11:47:12.727: INFO: Waiting for pod var-expansion-549bb287-fd5e-4be2-8966-5e1f38234dd5 to disappear
    Jun 22 11:47:12.731: INFO: Pod var-expansion-549bb287-fd5e-4be2-8966-5e1f38234dd5 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:47:12.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2750" for this suite. 06/22/23 11:47:12.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:47:12.748
Jun 22 11:47:12.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename services 06/22/23 11:47:12.75
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:47:12.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:47:12.775
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-2948 06/22/23 11:47:12.78
STEP: creating service affinity-clusterip in namespace services-2948 06/22/23 11:47:12.78
STEP: creating replication controller affinity-clusterip in namespace services-2948 06/22/23 11:47:12.796
W0622 11:47:12.806287      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "affinity-clusterip" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-clusterip" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-clusterip" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-clusterip" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0622 11:47:12.806967      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-2948, replica count: 3
I0622 11:47:15.859172      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 11:47:15.868: INFO: Creating new exec pod
W0622 11:47:15.879665      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:47:15.879: INFO: Waiting up to 5m0s for pod "execpod-affinityq6n49" in namespace "services-2948" to be "running"
Jun 22 11:47:15.884: INFO: Pod "execpod-affinityq6n49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.222781ms
Jun 22 11:47:17.890: INFO: Pod "execpod-affinityq6n49": Phase="Running", Reason="", readiness=true. Elapsed: 2.010867986s
Jun 22 11:47:17.890: INFO: Pod "execpod-affinityq6n49" satisfied condition "running"
Jun 22 11:47:18.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-2948 exec execpod-affinityq6n49 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jun 22 11:47:19.108: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jun 22 11:47:19.109: INFO: stdout: ""
Jun 22 11:47:19.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-2948 exec execpod-affinityq6n49 -- /bin/sh -x -c nc -v -z -w 2 100.71.92.2 80'
Jun 22 11:47:19.295: INFO: stderr: "+ nc -v -z -w 2 100.71.92.2 80\nConnection to 100.71.92.2 80 port [tcp/http] succeeded!\n"
Jun 22 11:47:19.295: INFO: stdout: ""
Jun 22 11:47:19.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-2948 exec execpod-affinityq6n49 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.71.92.2:80/ ; done'
Jun 22 11:47:19.583: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n"
Jun 22 11:47:19.583: INFO: stdout: "\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn"
Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
Jun 22 11:47:19.583: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-2948, will wait for the garbage collector to delete the pods 06/22/23 11:47:19.596
Jun 22 11:47:19.662: INFO: Deleting ReplicationController affinity-clusterip took: 9.515692ms
Jun 22 11:47:19.763: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.25918ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun 22 11:47:22.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2948" for this suite. 06/22/23 11:47:22.297
------------------------------
• [SLOW TEST] [9.557 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:47:12.748
    Jun 22 11:47:12.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename services 06/22/23 11:47:12.75
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:47:12.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:47:12.775
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-2948 06/22/23 11:47:12.78
    STEP: creating service affinity-clusterip in namespace services-2948 06/22/23 11:47:12.78
    STEP: creating replication controller affinity-clusterip in namespace services-2948 06/22/23 11:47:12.796
    W0622 11:47:12.806287      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "affinity-clusterip" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-clusterip" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-clusterip" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-clusterip" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0622 11:47:12.806967      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-2948, replica count: 3
    I0622 11:47:15.859172      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 22 11:47:15.868: INFO: Creating new exec pod
    W0622 11:47:15.879665      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:47:15.879: INFO: Waiting up to 5m0s for pod "execpod-affinityq6n49" in namespace "services-2948" to be "running"
    Jun 22 11:47:15.884: INFO: Pod "execpod-affinityq6n49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.222781ms
    Jun 22 11:47:17.890: INFO: Pod "execpod-affinityq6n49": Phase="Running", Reason="", readiness=true. Elapsed: 2.010867986s
    Jun 22 11:47:17.890: INFO: Pod "execpod-affinityq6n49" satisfied condition "running"
    Jun 22 11:47:18.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-2948 exec execpod-affinityq6n49 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jun 22 11:47:19.108: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jun 22 11:47:19.109: INFO: stdout: ""
    Jun 22 11:47:19.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-2948 exec execpod-affinityq6n49 -- /bin/sh -x -c nc -v -z -w 2 100.71.92.2 80'
    Jun 22 11:47:19.295: INFO: stderr: "+ nc -v -z -w 2 100.71.92.2 80\nConnection to 100.71.92.2 80 port [tcp/http] succeeded!\n"
    Jun 22 11:47:19.295: INFO: stdout: ""
    Jun 22 11:47:19.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-2948 exec execpod-affinityq6n49 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://100.71.92.2:80/ ; done'
    Jun 22 11:47:19.583: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://100.71.92.2:80/\n"
    Jun 22 11:47:19.583: INFO: stdout: "\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn\naffinity-clusterip-hk6fn"
    Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
    Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
    Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
    Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
    Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
    Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
    Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
    Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
    Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
    Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
    Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
    Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
    Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
    Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
    Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
    Jun 22 11:47:19.583: INFO: Received response from host: affinity-clusterip-hk6fn
    Jun 22 11:47:19.583: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-2948, will wait for the garbage collector to delete the pods 06/22/23 11:47:19.596
    Jun 22 11:47:19.662: INFO: Deleting ReplicationController affinity-clusterip took: 9.515692ms
    Jun 22 11:47:19.763: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.25918ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:47:22.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2948" for this suite. 06/22/23 11:47:22.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:47:22.308
Jun 22 11:47:22.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename secrets 06/22/23 11:47:22.31
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:47:22.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:47:22.336
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-8ff4a236-96e7-4efb-9017-c573b8268b43 06/22/23 11:47:22.34
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jun 22 11:47:22.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5568" for this suite. 06/22/23 11:47:22.351
------------------------------
• [0.050 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:47:22.308
    Jun 22 11:47:22.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename secrets 06/22/23 11:47:22.31
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:47:22.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:47:22.336
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-8ff4a236-96e7-4efb-9017-c573b8268b43 06/22/23 11:47:22.34
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:47:22.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5568" for this suite. 06/22/23 11:47:22.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:47:22.359
Jun 22 11:47:22.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename svcaccounts 06/22/23 11:47:22.36
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:47:22.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:47:22.384
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 06/22/23 11:47:22.388
STEP: watching for the ServiceAccount to be added 06/22/23 11:47:22.398
STEP: patching the ServiceAccount 06/22/23 11:47:22.4
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 06/22/23 11:47:22.409
STEP: deleting the ServiceAccount 06/22/23 11:47:22.415
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jun 22 11:47:22.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8176" for this suite. 06/22/23 11:47:22.439
------------------------------
• [0.088 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:47:22.359
    Jun 22 11:47:22.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename svcaccounts 06/22/23 11:47:22.36
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:47:22.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:47:22.384
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 06/22/23 11:47:22.388
    STEP: watching for the ServiceAccount to be added 06/22/23 11:47:22.398
    STEP: patching the ServiceAccount 06/22/23 11:47:22.4
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 06/22/23 11:47:22.409
    STEP: deleting the ServiceAccount 06/22/23 11:47:22.415
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:47:22.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8176" for this suite. 06/22/23 11:47:22.439
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:47:22.452
Jun 22 11:47:22.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename replication-controller 06/22/23 11:47:22.453
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:47:22.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:47:22.476
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 06/22/23 11:47:22.486
W0622 11:47:22.492651      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "rc-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "rc-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "rc-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "rc-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: waiting for RC to be added 06/22/23 11:47:22.492
STEP: waiting for available Replicas 06/22/23 11:47:22.493
STEP: patching ReplicationController 06/22/23 11:47:24.199
W0622 11:47:24.207102      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "rc-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "rc-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "rc-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "rc-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: waiting for RC to be modified 06/22/23 11:47:24.207
STEP: patching ReplicationController status 06/22/23 11:47:24.207
STEP: waiting for RC to be modified 06/22/23 11:47:24.214
STEP: waiting for available Replicas 06/22/23 11:47:24.214
STEP: fetching ReplicationController status 06/22/23 11:47:24.221
STEP: patching ReplicationController scale 06/22/23 11:47:24.226
STEP: waiting for RC to be modified 06/22/23 11:47:24.233
STEP: waiting for ReplicationController's scale to be the max amount 06/22/23 11:47:24.233
STEP: fetching ReplicationController; ensuring that it's patched 06/22/23 11:47:25.723
STEP: updating ReplicationController status 06/22/23 11:47:25.728
STEP: waiting for RC to be modified 06/22/23 11:47:25.736
STEP: listing all ReplicationControllers 06/22/23 11:47:25.736
STEP: checking that ReplicationController has expected values 06/22/23 11:47:25.742
STEP: deleting ReplicationControllers by collection 06/22/23 11:47:25.742
STEP: waiting for ReplicationController to have a DELETED watchEvent 06/22/23 11:47:25.753
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jun 22 11:47:25.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1456" for this suite. 06/22/23 11:47:25.794
------------------------------
• [3.349 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:47:22.452
    Jun 22 11:47:22.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename replication-controller 06/22/23 11:47:22.453
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:47:22.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:47:22.476
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 06/22/23 11:47:22.486
    W0622 11:47:22.492651      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "rc-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "rc-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "rc-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "rc-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: waiting for RC to be added 06/22/23 11:47:22.492
    STEP: waiting for available Replicas 06/22/23 11:47:22.493
    STEP: patching ReplicationController 06/22/23 11:47:24.199
    W0622 11:47:24.207102      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "rc-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "rc-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "rc-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "rc-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: waiting for RC to be modified 06/22/23 11:47:24.207
    STEP: patching ReplicationController status 06/22/23 11:47:24.207
    STEP: waiting for RC to be modified 06/22/23 11:47:24.214
    STEP: waiting for available Replicas 06/22/23 11:47:24.214
    STEP: fetching ReplicationController status 06/22/23 11:47:24.221
    STEP: patching ReplicationController scale 06/22/23 11:47:24.226
    STEP: waiting for RC to be modified 06/22/23 11:47:24.233
    STEP: waiting for ReplicationController's scale to be the max amount 06/22/23 11:47:24.233
    STEP: fetching ReplicationController; ensuring that it's patched 06/22/23 11:47:25.723
    STEP: updating ReplicationController status 06/22/23 11:47:25.728
    STEP: waiting for RC to be modified 06/22/23 11:47:25.736
    STEP: listing all ReplicationControllers 06/22/23 11:47:25.736
    STEP: checking that ReplicationController has expected values 06/22/23 11:47:25.742
    STEP: deleting ReplicationControllers by collection 06/22/23 11:47:25.742
    STEP: waiting for ReplicationController to have a DELETED watchEvent 06/22/23 11:47:25.753
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:47:25.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1456" for this suite. 06/22/23 11:47:25.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:47:25.811
Jun 22 11:47:25.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 11:47:25.812
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:47:25.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:47:25.833
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 06/22/23 11:47:25.837
W0622 11:47:25.850434      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:47:25.850: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b74aa24a-44cd-4df0-95cb-07d3a8bd4dda" in namespace "projected-6728" to be "Succeeded or Failed"
Jun 22 11:47:25.855: INFO: Pod "downwardapi-volume-b74aa24a-44cd-4df0-95cb-07d3a8bd4dda": Phase="Pending", Reason="", readiness=false. Elapsed: 4.343507ms
Jun 22 11:47:27.860: INFO: Pod "downwardapi-volume-b74aa24a-44cd-4df0-95cb-07d3a8bd4dda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009390892s
Jun 22 11:47:29.860: INFO: Pod "downwardapi-volume-b74aa24a-44cd-4df0-95cb-07d3a8bd4dda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010152904s
STEP: Saw pod success 06/22/23 11:47:29.86
Jun 22 11:47:29.861: INFO: Pod "downwardapi-volume-b74aa24a-44cd-4df0-95cb-07d3a8bd4dda" satisfied condition "Succeeded or Failed"
Jun 22 11:47:29.865: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-b74aa24a-44cd-4df0-95cb-07d3a8bd4dda container client-container: <nil>
STEP: delete the pod 06/22/23 11:47:29.873
Jun 22 11:47:29.884: INFO: Waiting for pod downwardapi-volume-b74aa24a-44cd-4df0-95cb-07d3a8bd4dda to disappear
Jun 22 11:47:29.888: INFO: Pod downwardapi-volume-b74aa24a-44cd-4df0-95cb-07d3a8bd4dda no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jun 22 11:47:29.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6728" for this suite. 06/22/23 11:47:29.895
------------------------------
• [4.090 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:47:25.811
    Jun 22 11:47:25.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 11:47:25.812
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:47:25.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:47:25.833
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 06/22/23 11:47:25.837
    W0622 11:47:25.850434      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:47:25.850: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b74aa24a-44cd-4df0-95cb-07d3a8bd4dda" in namespace "projected-6728" to be "Succeeded or Failed"
    Jun 22 11:47:25.855: INFO: Pod "downwardapi-volume-b74aa24a-44cd-4df0-95cb-07d3a8bd4dda": Phase="Pending", Reason="", readiness=false. Elapsed: 4.343507ms
    Jun 22 11:47:27.860: INFO: Pod "downwardapi-volume-b74aa24a-44cd-4df0-95cb-07d3a8bd4dda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009390892s
    Jun 22 11:47:29.860: INFO: Pod "downwardapi-volume-b74aa24a-44cd-4df0-95cb-07d3a8bd4dda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010152904s
    STEP: Saw pod success 06/22/23 11:47:29.86
    Jun 22 11:47:29.861: INFO: Pod "downwardapi-volume-b74aa24a-44cd-4df0-95cb-07d3a8bd4dda" satisfied condition "Succeeded or Failed"
    Jun 22 11:47:29.865: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod downwardapi-volume-b74aa24a-44cd-4df0-95cb-07d3a8bd4dda container client-container: <nil>
    STEP: delete the pod 06/22/23 11:47:29.873
    Jun 22 11:47:29.884: INFO: Waiting for pod downwardapi-volume-b74aa24a-44cd-4df0-95cb-07d3a8bd4dda to disappear
    Jun 22 11:47:29.888: INFO: Pod downwardapi-volume-b74aa24a-44cd-4df0-95cb-07d3a8bd4dda no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:47:29.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6728" for this suite. 06/22/23 11:47:29.895
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:47:29.903
Jun 22 11:47:29.903: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename sched-preemption 06/22/23 11:47:29.905
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:47:29.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:47:29.929
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jun 22 11:47:29.951: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 22 11:48:30.029: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 06/22/23 11:48:30.035
Jun 22 11:48:30.080: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun 22 11:48:30.090: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun 22 11:48:30.127: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun 22 11:48:30.135: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jun 22 11:48:30.165: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jun 22 11:48:30.173: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 06/22/23 11:48:30.173
Jun 22 11:48:30.173: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-852" to be "running"
Jun 22 11:48:30.179: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.416692ms
Jun 22 11:48:32.186: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.013086867s
Jun 22 11:48:32.187: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jun 22 11:48:32.187: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-852" to be "running"
Jun 22 11:48:32.191: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.384094ms
Jun 22 11:48:32.191: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jun 22 11:48:32.191: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-852" to be "running"
Jun 22 11:48:32.195: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.377426ms
Jun 22 11:48:32.195: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jun 22 11:48:32.195: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-852" to be "running"
Jun 22 11:48:32.200: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.185293ms
Jun 22 11:48:32.200: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jun 22 11:48:32.200: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-852" to be "running"
Jun 22 11:48:32.204: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.932967ms
Jun 22 11:48:32.204: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jun 22 11:48:32.204: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-852" to be "running"
Jun 22 11:48:32.208: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.837108ms
Jun 22 11:48:32.208: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 06/22/23 11:48:32.208
Jun 22 11:48:32.215: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-852" to be "running"
Jun 22 11:48:32.219: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006339ms
Jun 22 11:48:34.225: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010200656s
Jun 22 11:48:36.225: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010359532s
Jun 22 11:48:38.226: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.010837305s
Jun 22 11:48:38.226: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:48:38.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-852" for this suite. 06/22/23 11:48:38.35
------------------------------
• [SLOW TEST] [68.454 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:47:29.903
    Jun 22 11:47:29.903: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename sched-preemption 06/22/23 11:47:29.905
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:47:29.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:47:29.929
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jun 22 11:47:29.951: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 22 11:48:30.029: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 06/22/23 11:48:30.035
    Jun 22 11:48:30.080: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jun 22 11:48:30.090: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jun 22 11:48:30.127: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jun 22 11:48:30.135: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jun 22 11:48:30.165: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jun 22 11:48:30.173: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 06/22/23 11:48:30.173
    Jun 22 11:48:30.173: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-852" to be "running"
    Jun 22 11:48:30.179: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.416692ms
    Jun 22 11:48:32.186: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.013086867s
    Jun 22 11:48:32.187: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jun 22 11:48:32.187: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-852" to be "running"
    Jun 22 11:48:32.191: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.384094ms
    Jun 22 11:48:32.191: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun 22 11:48:32.191: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-852" to be "running"
    Jun 22 11:48:32.195: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.377426ms
    Jun 22 11:48:32.195: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun 22 11:48:32.195: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-852" to be "running"
    Jun 22 11:48:32.200: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.185293ms
    Jun 22 11:48:32.200: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun 22 11:48:32.200: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-852" to be "running"
    Jun 22 11:48:32.204: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.932967ms
    Jun 22 11:48:32.204: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun 22 11:48:32.204: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-852" to be "running"
    Jun 22 11:48:32.208: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.837108ms
    Jun 22 11:48:32.208: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 06/22/23 11:48:32.208
    Jun 22 11:48:32.215: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-852" to be "running"
    Jun 22 11:48:32.219: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006339ms
    Jun 22 11:48:34.225: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010200656s
    Jun 22 11:48:36.225: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010359532s
    Jun 22 11:48:38.226: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.010837305s
    Jun 22 11:48:38.226: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:48:38.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-852" for this suite. 06/22/23 11:48:38.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:48:38.359
Jun 22 11:48:38.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename gc 06/22/23 11:48:38.36
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:48:38.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:48:38.384
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 06/22/23 11:48:38.388
W0622 11:48:38.397392      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the rc 06/22/23 11:48:43.407
STEP: wait for all pods to be garbage collected 06/22/23 11:48:43.415
STEP: Gathering metrics 06/22/23 11:48:48.425
Jun 22 11:48:48.466: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" in namespace "kube-system" to be "running and ready"
Jun 22 11:48:48.471: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v": Phase="Running", Reason="", readiness=true. Elapsed: 4.360347ms
Jun 22 11:48:48.471: INFO: The phase of Pod kube-controller-manager-wl-antrea-prdzc-k4b4v is Running (Ready = true)
Jun 22 11:48:48.471: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" satisfied condition "running and ready"
Jun 22 11:48:48.555: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun 22 11:48:48.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5381" for this suite. 06/22/23 11:48:48.563
------------------------------
• [SLOW TEST] [10.212 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:48:38.359
    Jun 22 11:48:38.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename gc 06/22/23 11:48:38.36
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:48:38.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:48:38.384
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 06/22/23 11:48:38.388
    W0622 11:48:38.397392      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: delete the rc 06/22/23 11:48:43.407
    STEP: wait for all pods to be garbage collected 06/22/23 11:48:43.415
    STEP: Gathering metrics 06/22/23 11:48:48.425
    Jun 22 11:48:48.466: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" in namespace "kube-system" to be "running and ready"
    Jun 22 11:48:48.471: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v": Phase="Running", Reason="", readiness=true. Elapsed: 4.360347ms
    Jun 22 11:48:48.471: INFO: The phase of Pod kube-controller-manager-wl-antrea-prdzc-k4b4v is Running (Ready = true)
    Jun 22 11:48:48.471: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" satisfied condition "running and ready"
    Jun 22 11:48:48.555: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:48:48.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5381" for this suite. 06/22/23 11:48:48.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:48:48.573
Jun 22 11:48:48.573: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename dns 06/22/23 11:48:48.574
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:48:48.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:48:48.598
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 06/22/23 11:48:48.601
W0622 11:48:48.613003      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:48:48.613: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5324  3a783c15-2fb6-4c30-89ce-d73998f5422b 166817 0 2023-06-22 11:48:48 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-06-22 11:48:48 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jrg67,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jrg67,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 22 11:48:48.614: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5324" to be "running and ready"
Jun 22 11:48:48.618: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.875151ms
Jun 22 11:48:48.618: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun 22 11:48:50.624: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.009746825s
Jun 22 11:48:50.624: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jun 22 11:48:50.624: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 06/22/23 11:48:50.624
Jun 22 11:48:50.624: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5324 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 11:48:50.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 11:48:50.625: INFO: ExecWithOptions: Clientset creation
Jun 22 11:48:50.625: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/dns-5324/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 06/22/23 11:48:50.785
Jun 22 11:48:50.785: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5324 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 22 11:48:50.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
Jun 22 11:48:50.786: INFO: ExecWithOptions: Clientset creation
Jun 22 11:48:50.786: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/dns-5324/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 22 11:48:50.914: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jun 22 11:48:50.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5324" for this suite. 06/22/23 11:48:50.934
------------------------------
• [2.370 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:48:48.573
    Jun 22 11:48:48.573: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename dns 06/22/23 11:48:48.574
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:48:48.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:48:48.598
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 06/22/23 11:48:48.601
    W0622 11:48:48.613003      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:48:48.613: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5324  3a783c15-2fb6-4c30-89ce-d73998f5422b 166817 0 2023-06-22 11:48:48 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-06-22 11:48:48 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jrg67,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jrg67,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 22 11:48:48.614: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5324" to be "running and ready"
    Jun 22 11:48:48.618: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.875151ms
    Jun 22 11:48:48.618: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 11:48:50.624: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.009746825s
    Jun 22 11:48:50.624: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jun 22 11:48:50.624: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 06/22/23 11:48:50.624
    Jun 22 11:48:50.624: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5324 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 11:48:50.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 11:48:50.625: INFO: ExecWithOptions: Clientset creation
    Jun 22 11:48:50.625: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/dns-5324/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 06/22/23 11:48:50.785
    Jun 22 11:48:50.785: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5324 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 22 11:48:50.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    Jun 22 11:48:50.786: INFO: ExecWithOptions: Clientset creation
    Jun 22 11:48:50.786: INFO: ExecWithOptions: execute(POST https://100.64.0.1:443/api/v1/namespaces/dns-5324/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 22 11:48:50.914: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:48:50.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5324" for this suite. 06/22/23 11:48:50.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:48:50.944
Jun 22 11:48:50.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename statefulset 06/22/23 11:48:50.946
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:48:50.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:48:50.968
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-153 06/22/23 11:48:50.972
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-153 06/22/23 11:48:50.983
W0622 11:48:50.994268      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:48:51.000: INFO: Found 0 stateful pods, waiting for 1
Jun 22 11:49:01.006: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 06/22/23 11:49:01.015
STEP: updating a scale subresource 06/22/23 11:49:01.019
STEP: verifying the statefulset Spec.Replicas was modified 06/22/23 11:49:01.028
STEP: Patch a scale subresource 06/22/23 11:49:01.032
STEP: verifying the statefulset Spec.Replicas was modified 06/22/23 11:49:01.039
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jun 22 11:49:01.044: INFO: Deleting all statefulset in ns statefulset-153
Jun 22 11:49:01.050: INFO: Scaling statefulset ss to 0
W0622 11:49:01.062241      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:49:11.088: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 11:49:11.095: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jun 22 11:49:11.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-153" for this suite. 06/22/23 11:49:11.131
------------------------------
• [SLOW TEST] [20.196 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:48:50.944
    Jun 22 11:48:50.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename statefulset 06/22/23 11:48:50.946
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:48:50.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:48:50.968
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-153 06/22/23 11:48:50.972
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-153 06/22/23 11:48:50.983
    W0622 11:48:50.994268      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:48:51.000: INFO: Found 0 stateful pods, waiting for 1
    Jun 22 11:49:01.006: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 06/22/23 11:49:01.015
    STEP: updating a scale subresource 06/22/23 11:49:01.019
    STEP: verifying the statefulset Spec.Replicas was modified 06/22/23 11:49:01.028
    STEP: Patch a scale subresource 06/22/23 11:49:01.032
    STEP: verifying the statefulset Spec.Replicas was modified 06/22/23 11:49:01.039
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jun 22 11:49:01.044: INFO: Deleting all statefulset in ns statefulset-153
    Jun 22 11:49:01.050: INFO: Scaling statefulset ss to 0
    W0622 11:49:01.062241      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:49:11.088: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 22 11:49:11.095: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:49:11.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-153" for this suite. 06/22/23 11:49:11.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:49:11.149
Jun 22 11:49:11.149: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename proxy 06/22/23 11:49:11.157
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:49:11.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:49:11.185
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 06/22/23 11:49:11.212
STEP: creating replication controller proxy-service-4lmg9 in namespace proxy-9978 06/22/23 11:49:11.213
W0622 11:49:11.230048      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "proxy-service-4lmg9" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "proxy-service-4lmg9" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "proxy-service-4lmg9" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "proxy-service-4lmg9" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0622 11:49:11.230820      23 runners.go:193] Created replication controller with name: proxy-service-4lmg9, namespace: proxy-9978, replica count: 1
I0622 11:49:12.282415      23 runners.go:193] proxy-service-4lmg9 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 11:49:13.282829      23 runners.go:193] proxy-service-4lmg9 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 11:49:13.288: INFO: setup took 2.097472592s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 06/22/23 11:49:13.289
Jun 22 11:49:13.304: INFO: (0) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 14.527915ms)
Jun 22 11:49:13.304: INFO: (0) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 14.759372ms)
Jun 22 11:49:13.305: INFO: (0) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 14.676256ms)
Jun 22 11:49:13.305: INFO: (0) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 14.791941ms)
Jun 22 11:49:13.305: INFO: (0) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 14.920487ms)
Jun 22 11:49:13.305: INFO: (0) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 15.298187ms)
Jun 22 11:49:13.305: INFO: (0) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 15.742791ms)
Jun 22 11:49:13.305: INFO: (0) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 16.023776ms)
Jun 22 11:49:13.305: INFO: (0) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 16.112612ms)
Jun 22 11:49:13.305: INFO: (0) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 15.568978ms)
Jun 22 11:49:13.307: INFO: (0) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 16.569208ms)
Jun 22 11:49:13.313: INFO: (0) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 23.755861ms)
Jun 22 11:49:13.314: INFO: (0) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 24.218201ms)
Jun 22 11:49:13.314: INFO: (0) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 24.099954ms)
Jun 22 11:49:13.314: INFO: (0) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 23.999097ms)
Jun 22 11:49:13.314: INFO: (0) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 24.029309ms)
Jun 22 11:49:13.321: INFO: (1) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 6.703349ms)
Jun 22 11:49:13.322: INFO: (1) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 8.206393ms)
Jun 22 11:49:13.322: INFO: (1) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 8.02113ms)
Jun 22 11:49:13.325: INFO: (1) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 9.258909ms)
Jun 22 11:49:13.325: INFO: (1) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 9.161379ms)
Jun 22 11:49:13.325: INFO: (1) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 8.869864ms)
Jun 22 11:49:13.325: INFO: (1) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 9.453171ms)
Jun 22 11:49:13.325: INFO: (1) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.803617ms)
Jun 22 11:49:13.325: INFO: (1) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 9.120779ms)
Jun 22 11:49:13.326: INFO: (1) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 11.077877ms)
Jun 22 11:49:13.326: INFO: (1) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 11.578079ms)
Jun 22 11:49:13.327: INFO: (1) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 12.1942ms)
Jun 22 11:49:13.327: INFO: (1) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 12.189837ms)
Jun 22 11:49:13.327: INFO: (1) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 10.811732ms)
Jun 22 11:49:13.329: INFO: (1) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 12.398992ms)
Jun 22 11:49:13.329: INFO: (1) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 13.574282ms)
Jun 22 11:49:13.335: INFO: (2) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 5.935794ms)
Jun 22 11:49:13.336: INFO: (2) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 5.934537ms)
Jun 22 11:49:13.338: INFO: (2) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 7.878259ms)
Jun 22 11:49:13.338: INFO: (2) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 7.747298ms)
Jun 22 11:49:13.338: INFO: (2) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 8.302609ms)
Jun 22 11:49:13.338: INFO: (2) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 8.175072ms)
Jun 22 11:49:13.341: INFO: (2) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 11.414935ms)
Jun 22 11:49:13.342: INFO: (2) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 11.733445ms)
Jun 22 11:49:13.344: INFO: (2) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 13.808719ms)
Jun 22 11:49:13.344: INFO: (2) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 13.575502ms)
Jun 22 11:49:13.344: INFO: (2) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 14.481947ms)
Jun 22 11:49:13.345: INFO: (2) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 14.682445ms)
Jun 22 11:49:13.345: INFO: (2) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 14.582554ms)
Jun 22 11:49:13.345: INFO: (2) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 15.127405ms)
Jun 22 11:49:13.346: INFO: (2) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 15.642943ms)
Jun 22 11:49:13.346: INFO: (2) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 15.916602ms)
Jun 22 11:49:13.355: INFO: (3) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 8.152372ms)
Jun 22 11:49:13.355: INFO: (3) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 7.884725ms)
Jun 22 11:49:13.355: INFO: (3) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 7.46061ms)
Jun 22 11:49:13.355: INFO: (3) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 7.233533ms)
Jun 22 11:49:13.355: INFO: (3) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 7.457648ms)
Jun 22 11:49:13.355: INFO: (3) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 8.184404ms)
Jun 22 11:49:13.355: INFO: (3) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 9.368731ms)
Jun 22 11:49:13.356: INFO: (3) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 9.15048ms)
Jun 22 11:49:13.356: INFO: (3) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 9.586321ms)
Jun 22 11:49:13.357: INFO: (3) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 9.817375ms)
Jun 22 11:49:13.358: INFO: (3) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 10.54005ms)
Jun 22 11:49:13.360: INFO: (3) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 12.972751ms)
Jun 22 11:49:13.360: INFO: (3) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 12.918746ms)
Jun 22 11:49:13.360: INFO: (3) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 12.834202ms)
Jun 22 11:49:13.360: INFO: (3) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 13.601253ms)
Jun 22 11:49:13.360: INFO: (3) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 12.646204ms)
Jun 22 11:49:13.369: INFO: (4) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 8.777441ms)
Jun 22 11:49:13.370: INFO: (4) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 9.066883ms)
Jun 22 11:49:13.370: INFO: (4) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 8.819892ms)
Jun 22 11:49:13.370: INFO: (4) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 9.396123ms)
Jun 22 11:49:13.370: INFO: (4) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 9.783126ms)
Jun 22 11:49:13.371: INFO: (4) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 9.823442ms)
Jun 22 11:49:13.371: INFO: (4) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 10.143261ms)
Jun 22 11:49:13.371: INFO: (4) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 10.306646ms)
Jun 22 11:49:13.371: INFO: (4) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 10.102003ms)
Jun 22 11:49:13.371: INFO: (4) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 10.218193ms)
Jun 22 11:49:13.372: INFO: (4) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 11.143423ms)
Jun 22 11:49:13.374: INFO: (4) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 12.965652ms)
Jun 22 11:49:13.376: INFO: (4) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 14.815129ms)
Jun 22 11:49:13.376: INFO: (4) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 15.104077ms)
Jun 22 11:49:13.376: INFO: (4) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 14.900728ms)
Jun 22 11:49:13.376: INFO: (4) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 14.830348ms)
Jun 22 11:49:13.386: INFO: (5) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 9.417043ms)
Jun 22 11:49:13.386: INFO: (5) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 9.433389ms)
Jun 22 11:49:13.386: INFO: (5) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 10.141999ms)
Jun 22 11:49:13.387: INFO: (5) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.778818ms)
Jun 22 11:49:13.387: INFO: (5) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.975628ms)
Jun 22 11:49:13.387: INFO: (5) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 10.873508ms)
Jun 22 11:49:13.387: INFO: (5) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 11.421791ms)
Jun 22 11:49:13.387: INFO: (5) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 10.999441ms)
Jun 22 11:49:13.387: INFO: (5) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 11.040781ms)
Jun 22 11:49:13.388: INFO: (5) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 11.656967ms)
Jun 22 11:49:13.388: INFO: (5) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 11.89478ms)
Jun 22 11:49:13.390: INFO: (5) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 14.017109ms)
Jun 22 11:49:13.391: INFO: (5) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 15.392307ms)
Jun 22 11:49:13.391: INFO: (5) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 15.319912ms)
Jun 22 11:49:13.391: INFO: (5) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 15.288015ms)
Jun 22 11:49:13.392: INFO: (5) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 15.500394ms)
Jun 22 11:49:13.399: INFO: (6) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 6.605133ms)
Jun 22 11:49:13.399: INFO: (6) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 6.742336ms)
Jun 22 11:49:13.402: INFO: (6) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 9.686858ms)
Jun 22 11:49:13.402: INFO: (6) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 9.836318ms)
Jun 22 11:49:13.403: INFO: (6) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 10.545746ms)
Jun 22 11:49:13.403: INFO: (6) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 10.510935ms)
Jun 22 11:49:13.403: INFO: (6) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.856865ms)
Jun 22 11:49:13.403: INFO: (6) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 10.988002ms)
Jun 22 11:49:13.403: INFO: (6) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.69461ms)
Jun 22 11:49:13.403: INFO: (6) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 10.666652ms)
Jun 22 11:49:13.403: INFO: (6) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 11.140041ms)
Jun 22 11:49:13.404: INFO: (6) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 12.209327ms)
Jun 22 11:49:13.404: INFO: (6) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 12.695569ms)
Jun 22 11:49:13.405: INFO: (6) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 12.569185ms)
Jun 22 11:49:13.405: INFO: (6) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 12.62045ms)
Jun 22 11:49:13.405: INFO: (6) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 12.782071ms)
Jun 22 11:49:13.416: INFO: (7) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.928021ms)
Jun 22 11:49:13.417: INFO: (7) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 11.274319ms)
Jun 22 11:49:13.417: INFO: (7) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.788433ms)
Jun 22 11:49:13.417: INFO: (7) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 10.911409ms)
Jun 22 11:49:13.417: INFO: (7) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 11.026051ms)
Jun 22 11:49:13.417: INFO: (7) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 11.302351ms)
Jun 22 11:49:13.417: INFO: (7) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 11.595312ms)
Jun 22 11:49:13.417: INFO: (7) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 11.67868ms)
Jun 22 11:49:13.417: INFO: (7) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 11.833105ms)
Jun 22 11:49:13.417: INFO: (7) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 11.435631ms)
Jun 22 11:49:13.418: INFO: (7) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 12.647392ms)
Jun 22 11:49:13.419: INFO: (7) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 13.799677ms)
Jun 22 11:49:13.420: INFO: (7) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 14.379269ms)
Jun 22 11:49:13.420: INFO: (7) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 14.480126ms)
Jun 22 11:49:13.421: INFO: (7) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 15.407665ms)
Jun 22 11:49:13.422: INFO: (7) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 15.542188ms)
Jun 22 11:49:13.429: INFO: (8) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 6.885485ms)
Jun 22 11:49:13.430: INFO: (8) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 7.75792ms)
Jun 22 11:49:13.430: INFO: (8) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 7.983661ms)
Jun 22 11:49:13.430: INFO: (8) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 7.676524ms)
Jun 22 11:49:13.431: INFO: (8) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 8.660809ms)
Jun 22 11:49:13.433: INFO: (8) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.887833ms)
Jun 22 11:49:13.433: INFO: (8) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 10.054925ms)
Jun 22 11:49:13.434: INFO: (8) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 9.964455ms)
Jun 22 11:49:13.434: INFO: (8) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 11.425544ms)
Jun 22 11:49:13.434: INFO: (8) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 10.817963ms)
Jun 22 11:49:13.434: INFO: (8) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 11.747946ms)
Jun 22 11:49:13.435: INFO: (8) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 11.346797ms)
Jun 22 11:49:13.435: INFO: (8) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 11.681062ms)
Jun 22 11:49:13.436: INFO: (8) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 12.625043ms)
Jun 22 11:49:13.436: INFO: (8) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 13.433172ms)
Jun 22 11:49:13.436: INFO: (8) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 13.570714ms)
Jun 22 11:49:13.446: INFO: (9) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 8.460393ms)
Jun 22 11:49:13.446: INFO: (9) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 8.657418ms)
Jun 22 11:49:13.447: INFO: (9) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.072757ms)
Jun 22 11:49:13.448: INFO: (9) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 10.872542ms)
Jun 22 11:49:13.449: INFO: (9) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 11.530935ms)
Jun 22 11:49:13.449: INFO: (9) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 11.452047ms)
Jun 22 11:49:13.449: INFO: (9) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 11.810785ms)
Jun 22 11:49:13.449: INFO: (9) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 11.436504ms)
Jun 22 11:49:13.449: INFO: (9) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 11.664222ms)
Jun 22 11:49:13.449: INFO: (9) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 11.888844ms)
Jun 22 11:49:13.449: INFO: (9) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 12.013484ms)
Jun 22 11:49:13.450: INFO: (9) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 12.961244ms)
Jun 22 11:49:13.450: INFO: (9) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 13.606195ms)
Jun 22 11:49:13.450: INFO: (9) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 13.084744ms)
Jun 22 11:49:13.450: INFO: (9) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 13.450182ms)
Jun 22 11:49:13.451: INFO: (9) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 13.551787ms)
Jun 22 11:49:13.458: INFO: (10) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 7.462526ms)
Jun 22 11:49:13.459: INFO: (10) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 7.851059ms)
Jun 22 11:49:13.459: INFO: (10) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 7.984323ms)
Jun 22 11:49:13.460: INFO: (10) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 8.262687ms)
Jun 22 11:49:13.460: INFO: (10) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 8.656498ms)
Jun 22 11:49:13.460: INFO: (10) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 8.716645ms)
Jun 22 11:49:13.461: INFO: (10) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 8.947224ms)
Jun 22 11:49:13.462: INFO: (10) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 11.133129ms)
Jun 22 11:49:13.462: INFO: (10) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 11.220226ms)
Jun 22 11:49:13.462: INFO: (10) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 10.691324ms)
Jun 22 11:49:13.464: INFO: (10) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 12.219512ms)
Jun 22 11:49:13.464: INFO: (10) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 13.252841ms)
Jun 22 11:49:13.465: INFO: (10) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 13.772335ms)
Jun 22 11:49:13.466: INFO: (10) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 14.905307ms)
Jun 22 11:49:13.467: INFO: (10) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 15.131647ms)
Jun 22 11:49:13.467: INFO: (10) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 15.826125ms)
Jun 22 11:49:13.476: INFO: (11) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 8.164183ms)
Jun 22 11:49:13.476: INFO: (11) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 9.049689ms)
Jun 22 11:49:13.476: INFO: (11) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 8.391085ms)
Jun 22 11:49:13.477: INFO: (11) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 8.87242ms)
Jun 22 11:49:13.477: INFO: (11) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 9.627188ms)
Jun 22 11:49:13.477: INFO: (11) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 9.282481ms)
Jun 22 11:49:13.477: INFO: (11) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 9.452847ms)
Jun 22 11:49:13.478: INFO: (11) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.218614ms)
Jun 22 11:49:13.478: INFO: (11) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 9.632827ms)
Jun 22 11:49:13.478: INFO: (11) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.598664ms)
Jun 22 11:49:13.480: INFO: (11) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 12.391009ms)
Jun 22 11:49:13.481: INFO: (11) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 13.130381ms)
Jun 22 11:49:13.481: INFO: (11) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 13.065536ms)
Jun 22 11:49:13.482: INFO: (11) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 14.07777ms)
Jun 22 11:49:13.482: INFO: (11) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 14.739227ms)
Jun 22 11:49:13.482: INFO: (11) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 14.578782ms)
Jun 22 11:49:13.489: INFO: (12) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 6.743859ms)
Jun 22 11:49:13.492: INFO: (12) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 9.086854ms)
Jun 22 11:49:13.492: INFO: (12) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 9.262552ms)
Jun 22 11:49:13.492: INFO: (12) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 9.192182ms)
Jun 22 11:49:13.492: INFO: (12) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 9.739355ms)
Jun 22 11:49:13.493: INFO: (12) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 9.808156ms)
Jun 22 11:49:13.493: INFO: (12) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 10.375338ms)
Jun 22 11:49:13.493: INFO: (12) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 10.061303ms)
Jun 22 11:49:13.493: INFO: (12) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 10.700697ms)
Jun 22 11:49:13.494: INFO: (12) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 11.052721ms)
Jun 22 11:49:13.494: INFO: (12) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 10.662987ms)
Jun 22 11:49:13.495: INFO: (12) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 12.020395ms)
Jun 22 11:49:13.496: INFO: (12) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 13.224474ms)
Jun 22 11:49:13.498: INFO: (12) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 14.863346ms)
Jun 22 11:49:13.498: INFO: (12) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 14.961396ms)
Jun 22 11:49:13.498: INFO: (12) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 15.291528ms)
Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.309084ms)
Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 9.814724ms)
Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 9.715382ms)
Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 9.961847ms)
Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 9.787335ms)
Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 9.937086ms)
Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 10.295831ms)
Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.363117ms)
Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.496088ms)
Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 10.659018ms)
Jun 22 11:49:13.511: INFO: (13) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 12.543131ms)
Jun 22 11:49:13.512: INFO: (13) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 13.19458ms)
Jun 22 11:49:13.513: INFO: (13) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 14.355464ms)
Jun 22 11:49:13.514: INFO: (13) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 14.993385ms)
Jun 22 11:49:13.514: INFO: (13) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 14.907187ms)
Jun 22 11:49:13.514: INFO: (13) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 15.160958ms)
Jun 22 11:49:13.524: INFO: (14) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 10.012613ms)
Jun 22 11:49:13.524: INFO: (14) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 9.85984ms)
Jun 22 11:49:13.524: INFO: (14) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 9.937546ms)
Jun 22 11:49:13.524: INFO: (14) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 10.059609ms)
Jun 22 11:49:13.525: INFO: (14) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 10.722448ms)
Jun 22 11:49:13.525: INFO: (14) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.921987ms)
Jun 22 11:49:13.525: INFO: (14) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.636039ms)
Jun 22 11:49:13.525: INFO: (14) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 10.988444ms)
Jun 22 11:49:13.526: INFO: (14) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 11.398074ms)
Jun 22 11:49:13.526: INFO: (14) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 11.844697ms)
Jun 22 11:49:13.526: INFO: (14) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 12.089314ms)
Jun 22 11:49:13.529: INFO: (14) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 14.654876ms)
Jun 22 11:49:13.529: INFO: (14) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 15.091353ms)
Jun 22 11:49:13.529: INFO: (14) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 14.973181ms)
Jun 22 11:49:13.530: INFO: (14) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 15.427324ms)
Jun 22 11:49:13.531: INFO: (14) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 16.493719ms)
Jun 22 11:49:13.538: INFO: (15) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 6.760595ms)
Jun 22 11:49:13.539: INFO: (15) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 7.879667ms)
Jun 22 11:49:13.540: INFO: (15) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 8.243162ms)
Jun 22 11:49:13.541: INFO: (15) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 9.160002ms)
Jun 22 11:49:13.541: INFO: (15) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 10.140478ms)
Jun 22 11:49:13.543: INFO: (15) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 11.278191ms)
Jun 22 11:49:13.545: INFO: (15) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 13.556905ms)
Jun 22 11:49:13.545: INFO: (15) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 13.541676ms)
Jun 22 11:49:13.545: INFO: (15) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 13.600878ms)
Jun 22 11:49:13.545: INFO: (15) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 13.534779ms)
Jun 22 11:49:13.546: INFO: (15) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 14.102077ms)
Jun 22 11:49:13.546: INFO: (15) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 14.497213ms)
Jun 22 11:49:13.546: INFO: (15) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 14.641362ms)
Jun 22 11:49:13.546: INFO: (15) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 14.843451ms)
Jun 22 11:49:13.546: INFO: (15) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 14.550189ms)
Jun 22 11:49:13.546: INFO: (15) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 14.841648ms)
Jun 22 11:49:13.559: INFO: (16) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 12.245172ms)
Jun 22 11:49:13.559: INFO: (16) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 12.287914ms)
Jun 22 11:49:13.559: INFO: (16) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 12.755234ms)
Jun 22 11:49:13.559: INFO: (16) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 12.832734ms)
Jun 22 11:49:13.559: INFO: (16) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 12.737274ms)
Jun 22 11:49:13.559: INFO: (16) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 12.811074ms)
Jun 22 11:49:13.560: INFO: (16) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 13.076756ms)
Jun 22 11:49:13.560: INFO: (16) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 13.439024ms)
Jun 22 11:49:13.560: INFO: (16) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 12.969968ms)
Jun 22 11:49:13.560: INFO: (16) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 13.022908ms)
Jun 22 11:49:13.561: INFO: (16) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 15.081833ms)
Jun 22 11:49:13.562: INFO: (16) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 15.769035ms)
Jun 22 11:49:13.564: INFO: (16) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 17.106421ms)
Jun 22 11:49:13.564: INFO: (16) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 17.43848ms)
Jun 22 11:49:13.564: INFO: (16) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 17.773231ms)
Jun 22 11:49:13.565: INFO: (16) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 18.294635ms)
Jun 22 11:49:13.575: INFO: (17) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 9.343455ms)
Jun 22 11:49:13.576: INFO: (17) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 10.638248ms)
Jun 22 11:49:13.576: INFO: (17) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 11.587717ms)
Jun 22 11:49:13.577: INFO: (17) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 12.063256ms)
Jun 22 11:49:13.578: INFO: (17) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 12.23149ms)
Jun 22 11:49:13.578: INFO: (17) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 12.668793ms)
Jun 22 11:49:13.578: INFO: (17) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 12.930755ms)
Jun 22 11:49:13.579: INFO: (17) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 13.674813ms)
Jun 22 11:49:13.580: INFO: (17) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 15.074772ms)
Jun 22 11:49:13.580: INFO: (17) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 14.396065ms)
Jun 22 11:49:13.580: INFO: (17) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 15.102072ms)
Jun 22 11:49:13.581: INFO: (17) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 15.944886ms)
Jun 22 11:49:13.581: INFO: (17) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 16.042928ms)
Jun 22 11:49:13.581: INFO: (17) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 15.49962ms)
Jun 22 11:49:13.581: INFO: (17) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 16.091281ms)
Jun 22 11:49:13.582: INFO: (17) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 16.728289ms)
Jun 22 11:49:13.590: INFO: (18) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 8.034299ms)
Jun 22 11:49:13.592: INFO: (18) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 8.924806ms)
Jun 22 11:49:13.593: INFO: (18) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.159017ms)
Jun 22 11:49:13.593: INFO: (18) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 9.673099ms)
Jun 22 11:49:13.593: INFO: (18) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 9.787914ms)
Jun 22 11:49:13.593: INFO: (18) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 9.847972ms)
Jun 22 11:49:13.594: INFO: (18) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.832532ms)
Jun 22 11:49:13.594: INFO: (18) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 11.025496ms)
Jun 22 11:49:13.594: INFO: (18) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 10.528113ms)
Jun 22 11:49:13.594: INFO: (18) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.803075ms)
Jun 22 11:49:13.597: INFO: (18) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 14.327461ms)
Jun 22 11:49:13.597: INFO: (18) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 13.956074ms)
Jun 22 11:49:13.597: INFO: (18) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 14.05643ms)
Jun 22 11:49:13.597: INFO: (18) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 14.135746ms)
Jun 22 11:49:13.597: INFO: (18) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 13.864174ms)
Jun 22 11:49:13.597: INFO: (18) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 14.524415ms)
Jun 22 11:49:13.607: INFO: (19) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 9.195642ms)
Jun 22 11:49:13.608: INFO: (19) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 10.231293ms)
Jun 22 11:49:13.608: INFO: (19) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 10.597192ms)
Jun 22 11:49:13.608: INFO: (19) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 10.001655ms)
Jun 22 11:49:13.608: INFO: (19) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.213653ms)
Jun 22 11:49:13.608: INFO: (19) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 9.964362ms)
Jun 22 11:49:13.608: INFO: (19) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 10.182611ms)
Jun 22 11:49:13.608: INFO: (19) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.546897ms)
Jun 22 11:49:13.608: INFO: (19) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 11.20178ms)
Jun 22 11:49:13.609: INFO: (19) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 11.597948ms)
Jun 22 11:49:13.610: INFO: (19) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 12.445311ms)
Jun 22 11:49:13.610: INFO: (19) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 13.176323ms)
Jun 22 11:49:13.612: INFO: (19) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 14.927193ms)
Jun 22 11:49:13.612: INFO: (19) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 14.744788ms)
Jun 22 11:49:13.612: INFO: (19) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 14.800909ms)
Jun 22 11:49:13.613: INFO: (19) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 15.584505ms)
STEP: deleting ReplicationController proxy-service-4lmg9 in namespace proxy-9978, will wait for the garbage collector to delete the pods 06/22/23 11:49:13.613
Jun 22 11:49:13.678: INFO: Deleting ReplicationController proxy-service-4lmg9 took: 8.571726ms
Jun 22 11:49:13.779: INFO: Terminating ReplicationController proxy-service-4lmg9 pods took: 101.11728ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jun 22 11:49:16.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-9978" for this suite. 06/22/23 11:49:16.59
------------------------------
• [SLOW TEST] [5.449 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:49:11.149
    Jun 22 11:49:11.149: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename proxy 06/22/23 11:49:11.157
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:49:11.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:49:11.185
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 06/22/23 11:49:11.212
    STEP: creating replication controller proxy-service-4lmg9 in namespace proxy-9978 06/22/23 11:49:11.213
    W0622 11:49:11.230048      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "proxy-service-4lmg9" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "proxy-service-4lmg9" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "proxy-service-4lmg9" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "proxy-service-4lmg9" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0622 11:49:11.230820      23 runners.go:193] Created replication controller with name: proxy-service-4lmg9, namespace: proxy-9978, replica count: 1
    I0622 11:49:12.282415      23 runners.go:193] proxy-service-4lmg9 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0622 11:49:13.282829      23 runners.go:193] proxy-service-4lmg9 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 22 11:49:13.288: INFO: setup took 2.097472592s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 06/22/23 11:49:13.289
    Jun 22 11:49:13.304: INFO: (0) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 14.527915ms)
    Jun 22 11:49:13.304: INFO: (0) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 14.759372ms)
    Jun 22 11:49:13.305: INFO: (0) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 14.676256ms)
    Jun 22 11:49:13.305: INFO: (0) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 14.791941ms)
    Jun 22 11:49:13.305: INFO: (0) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 14.920487ms)
    Jun 22 11:49:13.305: INFO: (0) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 15.298187ms)
    Jun 22 11:49:13.305: INFO: (0) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 15.742791ms)
    Jun 22 11:49:13.305: INFO: (0) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 16.023776ms)
    Jun 22 11:49:13.305: INFO: (0) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 16.112612ms)
    Jun 22 11:49:13.305: INFO: (0) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 15.568978ms)
    Jun 22 11:49:13.307: INFO: (0) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 16.569208ms)
    Jun 22 11:49:13.313: INFO: (0) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 23.755861ms)
    Jun 22 11:49:13.314: INFO: (0) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 24.218201ms)
    Jun 22 11:49:13.314: INFO: (0) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 24.099954ms)
    Jun 22 11:49:13.314: INFO: (0) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 23.999097ms)
    Jun 22 11:49:13.314: INFO: (0) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 24.029309ms)
    Jun 22 11:49:13.321: INFO: (1) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 6.703349ms)
    Jun 22 11:49:13.322: INFO: (1) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 8.206393ms)
    Jun 22 11:49:13.322: INFO: (1) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 8.02113ms)
    Jun 22 11:49:13.325: INFO: (1) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 9.258909ms)
    Jun 22 11:49:13.325: INFO: (1) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 9.161379ms)
    Jun 22 11:49:13.325: INFO: (1) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 8.869864ms)
    Jun 22 11:49:13.325: INFO: (1) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 9.453171ms)
    Jun 22 11:49:13.325: INFO: (1) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.803617ms)
    Jun 22 11:49:13.325: INFO: (1) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 9.120779ms)
    Jun 22 11:49:13.326: INFO: (1) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 11.077877ms)
    Jun 22 11:49:13.326: INFO: (1) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 11.578079ms)
    Jun 22 11:49:13.327: INFO: (1) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 12.1942ms)
    Jun 22 11:49:13.327: INFO: (1) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 12.189837ms)
    Jun 22 11:49:13.327: INFO: (1) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 10.811732ms)
    Jun 22 11:49:13.329: INFO: (1) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 12.398992ms)
    Jun 22 11:49:13.329: INFO: (1) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 13.574282ms)
    Jun 22 11:49:13.335: INFO: (2) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 5.935794ms)
    Jun 22 11:49:13.336: INFO: (2) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 5.934537ms)
    Jun 22 11:49:13.338: INFO: (2) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 7.878259ms)
    Jun 22 11:49:13.338: INFO: (2) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 7.747298ms)
    Jun 22 11:49:13.338: INFO: (2) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 8.302609ms)
    Jun 22 11:49:13.338: INFO: (2) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 8.175072ms)
    Jun 22 11:49:13.341: INFO: (2) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 11.414935ms)
    Jun 22 11:49:13.342: INFO: (2) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 11.733445ms)
    Jun 22 11:49:13.344: INFO: (2) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 13.808719ms)
    Jun 22 11:49:13.344: INFO: (2) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 13.575502ms)
    Jun 22 11:49:13.344: INFO: (2) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 14.481947ms)
    Jun 22 11:49:13.345: INFO: (2) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 14.682445ms)
    Jun 22 11:49:13.345: INFO: (2) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 14.582554ms)
    Jun 22 11:49:13.345: INFO: (2) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 15.127405ms)
    Jun 22 11:49:13.346: INFO: (2) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 15.642943ms)
    Jun 22 11:49:13.346: INFO: (2) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 15.916602ms)
    Jun 22 11:49:13.355: INFO: (3) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 8.152372ms)
    Jun 22 11:49:13.355: INFO: (3) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 7.884725ms)
    Jun 22 11:49:13.355: INFO: (3) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 7.46061ms)
    Jun 22 11:49:13.355: INFO: (3) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 7.233533ms)
    Jun 22 11:49:13.355: INFO: (3) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 7.457648ms)
    Jun 22 11:49:13.355: INFO: (3) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 8.184404ms)
    Jun 22 11:49:13.355: INFO: (3) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 9.368731ms)
    Jun 22 11:49:13.356: INFO: (3) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 9.15048ms)
    Jun 22 11:49:13.356: INFO: (3) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 9.586321ms)
    Jun 22 11:49:13.357: INFO: (3) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 9.817375ms)
    Jun 22 11:49:13.358: INFO: (3) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 10.54005ms)
    Jun 22 11:49:13.360: INFO: (3) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 12.972751ms)
    Jun 22 11:49:13.360: INFO: (3) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 12.918746ms)
    Jun 22 11:49:13.360: INFO: (3) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 12.834202ms)
    Jun 22 11:49:13.360: INFO: (3) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 13.601253ms)
    Jun 22 11:49:13.360: INFO: (3) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 12.646204ms)
    Jun 22 11:49:13.369: INFO: (4) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 8.777441ms)
    Jun 22 11:49:13.370: INFO: (4) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 9.066883ms)
    Jun 22 11:49:13.370: INFO: (4) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 8.819892ms)
    Jun 22 11:49:13.370: INFO: (4) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 9.396123ms)
    Jun 22 11:49:13.370: INFO: (4) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 9.783126ms)
    Jun 22 11:49:13.371: INFO: (4) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 9.823442ms)
    Jun 22 11:49:13.371: INFO: (4) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 10.143261ms)
    Jun 22 11:49:13.371: INFO: (4) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 10.306646ms)
    Jun 22 11:49:13.371: INFO: (4) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 10.102003ms)
    Jun 22 11:49:13.371: INFO: (4) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 10.218193ms)
    Jun 22 11:49:13.372: INFO: (4) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 11.143423ms)
    Jun 22 11:49:13.374: INFO: (4) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 12.965652ms)
    Jun 22 11:49:13.376: INFO: (4) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 14.815129ms)
    Jun 22 11:49:13.376: INFO: (4) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 15.104077ms)
    Jun 22 11:49:13.376: INFO: (4) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 14.900728ms)
    Jun 22 11:49:13.376: INFO: (4) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 14.830348ms)
    Jun 22 11:49:13.386: INFO: (5) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 9.417043ms)
    Jun 22 11:49:13.386: INFO: (5) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 9.433389ms)
    Jun 22 11:49:13.386: INFO: (5) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 10.141999ms)
    Jun 22 11:49:13.387: INFO: (5) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.778818ms)
    Jun 22 11:49:13.387: INFO: (5) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.975628ms)
    Jun 22 11:49:13.387: INFO: (5) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 10.873508ms)
    Jun 22 11:49:13.387: INFO: (5) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 11.421791ms)
    Jun 22 11:49:13.387: INFO: (5) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 10.999441ms)
    Jun 22 11:49:13.387: INFO: (5) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 11.040781ms)
    Jun 22 11:49:13.388: INFO: (5) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 11.656967ms)
    Jun 22 11:49:13.388: INFO: (5) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 11.89478ms)
    Jun 22 11:49:13.390: INFO: (5) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 14.017109ms)
    Jun 22 11:49:13.391: INFO: (5) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 15.392307ms)
    Jun 22 11:49:13.391: INFO: (5) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 15.319912ms)
    Jun 22 11:49:13.391: INFO: (5) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 15.288015ms)
    Jun 22 11:49:13.392: INFO: (5) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 15.500394ms)
    Jun 22 11:49:13.399: INFO: (6) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 6.605133ms)
    Jun 22 11:49:13.399: INFO: (6) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 6.742336ms)
    Jun 22 11:49:13.402: INFO: (6) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 9.686858ms)
    Jun 22 11:49:13.402: INFO: (6) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 9.836318ms)
    Jun 22 11:49:13.403: INFO: (6) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 10.545746ms)
    Jun 22 11:49:13.403: INFO: (6) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 10.510935ms)
    Jun 22 11:49:13.403: INFO: (6) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.856865ms)
    Jun 22 11:49:13.403: INFO: (6) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 10.988002ms)
    Jun 22 11:49:13.403: INFO: (6) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.69461ms)
    Jun 22 11:49:13.403: INFO: (6) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 10.666652ms)
    Jun 22 11:49:13.403: INFO: (6) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 11.140041ms)
    Jun 22 11:49:13.404: INFO: (6) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 12.209327ms)
    Jun 22 11:49:13.404: INFO: (6) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 12.695569ms)
    Jun 22 11:49:13.405: INFO: (6) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 12.569185ms)
    Jun 22 11:49:13.405: INFO: (6) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 12.62045ms)
    Jun 22 11:49:13.405: INFO: (6) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 12.782071ms)
    Jun 22 11:49:13.416: INFO: (7) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.928021ms)
    Jun 22 11:49:13.417: INFO: (7) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 11.274319ms)
    Jun 22 11:49:13.417: INFO: (7) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.788433ms)
    Jun 22 11:49:13.417: INFO: (7) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 10.911409ms)
    Jun 22 11:49:13.417: INFO: (7) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 11.026051ms)
    Jun 22 11:49:13.417: INFO: (7) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 11.302351ms)
    Jun 22 11:49:13.417: INFO: (7) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 11.595312ms)
    Jun 22 11:49:13.417: INFO: (7) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 11.67868ms)
    Jun 22 11:49:13.417: INFO: (7) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 11.833105ms)
    Jun 22 11:49:13.417: INFO: (7) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 11.435631ms)
    Jun 22 11:49:13.418: INFO: (7) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 12.647392ms)
    Jun 22 11:49:13.419: INFO: (7) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 13.799677ms)
    Jun 22 11:49:13.420: INFO: (7) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 14.379269ms)
    Jun 22 11:49:13.420: INFO: (7) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 14.480126ms)
    Jun 22 11:49:13.421: INFO: (7) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 15.407665ms)
    Jun 22 11:49:13.422: INFO: (7) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 15.542188ms)
    Jun 22 11:49:13.429: INFO: (8) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 6.885485ms)
    Jun 22 11:49:13.430: INFO: (8) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 7.75792ms)
    Jun 22 11:49:13.430: INFO: (8) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 7.983661ms)
    Jun 22 11:49:13.430: INFO: (8) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 7.676524ms)
    Jun 22 11:49:13.431: INFO: (8) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 8.660809ms)
    Jun 22 11:49:13.433: INFO: (8) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.887833ms)
    Jun 22 11:49:13.433: INFO: (8) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 10.054925ms)
    Jun 22 11:49:13.434: INFO: (8) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 9.964455ms)
    Jun 22 11:49:13.434: INFO: (8) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 11.425544ms)
    Jun 22 11:49:13.434: INFO: (8) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 10.817963ms)
    Jun 22 11:49:13.434: INFO: (8) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 11.747946ms)
    Jun 22 11:49:13.435: INFO: (8) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 11.346797ms)
    Jun 22 11:49:13.435: INFO: (8) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 11.681062ms)
    Jun 22 11:49:13.436: INFO: (8) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 12.625043ms)
    Jun 22 11:49:13.436: INFO: (8) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 13.433172ms)
    Jun 22 11:49:13.436: INFO: (8) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 13.570714ms)
    Jun 22 11:49:13.446: INFO: (9) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 8.460393ms)
    Jun 22 11:49:13.446: INFO: (9) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 8.657418ms)
    Jun 22 11:49:13.447: INFO: (9) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.072757ms)
    Jun 22 11:49:13.448: INFO: (9) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 10.872542ms)
    Jun 22 11:49:13.449: INFO: (9) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 11.530935ms)
    Jun 22 11:49:13.449: INFO: (9) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 11.452047ms)
    Jun 22 11:49:13.449: INFO: (9) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 11.810785ms)
    Jun 22 11:49:13.449: INFO: (9) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 11.436504ms)
    Jun 22 11:49:13.449: INFO: (9) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 11.664222ms)
    Jun 22 11:49:13.449: INFO: (9) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 11.888844ms)
    Jun 22 11:49:13.449: INFO: (9) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 12.013484ms)
    Jun 22 11:49:13.450: INFO: (9) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 12.961244ms)
    Jun 22 11:49:13.450: INFO: (9) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 13.606195ms)
    Jun 22 11:49:13.450: INFO: (9) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 13.084744ms)
    Jun 22 11:49:13.450: INFO: (9) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 13.450182ms)
    Jun 22 11:49:13.451: INFO: (9) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 13.551787ms)
    Jun 22 11:49:13.458: INFO: (10) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 7.462526ms)
    Jun 22 11:49:13.459: INFO: (10) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 7.851059ms)
    Jun 22 11:49:13.459: INFO: (10) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 7.984323ms)
    Jun 22 11:49:13.460: INFO: (10) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 8.262687ms)
    Jun 22 11:49:13.460: INFO: (10) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 8.656498ms)
    Jun 22 11:49:13.460: INFO: (10) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 8.716645ms)
    Jun 22 11:49:13.461: INFO: (10) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 8.947224ms)
    Jun 22 11:49:13.462: INFO: (10) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 11.133129ms)
    Jun 22 11:49:13.462: INFO: (10) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 11.220226ms)
    Jun 22 11:49:13.462: INFO: (10) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 10.691324ms)
    Jun 22 11:49:13.464: INFO: (10) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 12.219512ms)
    Jun 22 11:49:13.464: INFO: (10) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 13.252841ms)
    Jun 22 11:49:13.465: INFO: (10) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 13.772335ms)
    Jun 22 11:49:13.466: INFO: (10) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 14.905307ms)
    Jun 22 11:49:13.467: INFO: (10) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 15.131647ms)
    Jun 22 11:49:13.467: INFO: (10) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 15.826125ms)
    Jun 22 11:49:13.476: INFO: (11) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 8.164183ms)
    Jun 22 11:49:13.476: INFO: (11) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 9.049689ms)
    Jun 22 11:49:13.476: INFO: (11) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 8.391085ms)
    Jun 22 11:49:13.477: INFO: (11) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 8.87242ms)
    Jun 22 11:49:13.477: INFO: (11) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 9.627188ms)
    Jun 22 11:49:13.477: INFO: (11) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 9.282481ms)
    Jun 22 11:49:13.477: INFO: (11) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 9.452847ms)
    Jun 22 11:49:13.478: INFO: (11) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.218614ms)
    Jun 22 11:49:13.478: INFO: (11) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 9.632827ms)
    Jun 22 11:49:13.478: INFO: (11) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.598664ms)
    Jun 22 11:49:13.480: INFO: (11) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 12.391009ms)
    Jun 22 11:49:13.481: INFO: (11) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 13.130381ms)
    Jun 22 11:49:13.481: INFO: (11) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 13.065536ms)
    Jun 22 11:49:13.482: INFO: (11) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 14.07777ms)
    Jun 22 11:49:13.482: INFO: (11) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 14.739227ms)
    Jun 22 11:49:13.482: INFO: (11) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 14.578782ms)
    Jun 22 11:49:13.489: INFO: (12) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 6.743859ms)
    Jun 22 11:49:13.492: INFO: (12) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 9.086854ms)
    Jun 22 11:49:13.492: INFO: (12) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 9.262552ms)
    Jun 22 11:49:13.492: INFO: (12) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 9.192182ms)
    Jun 22 11:49:13.492: INFO: (12) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 9.739355ms)
    Jun 22 11:49:13.493: INFO: (12) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 9.808156ms)
    Jun 22 11:49:13.493: INFO: (12) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 10.375338ms)
    Jun 22 11:49:13.493: INFO: (12) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 10.061303ms)
    Jun 22 11:49:13.493: INFO: (12) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 10.700697ms)
    Jun 22 11:49:13.494: INFO: (12) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 11.052721ms)
    Jun 22 11:49:13.494: INFO: (12) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 10.662987ms)
    Jun 22 11:49:13.495: INFO: (12) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 12.020395ms)
    Jun 22 11:49:13.496: INFO: (12) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 13.224474ms)
    Jun 22 11:49:13.498: INFO: (12) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 14.863346ms)
    Jun 22 11:49:13.498: INFO: (12) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 14.961396ms)
    Jun 22 11:49:13.498: INFO: (12) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 15.291528ms)
    Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.309084ms)
    Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 9.814724ms)
    Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 9.715382ms)
    Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 9.961847ms)
    Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 9.787335ms)
    Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 9.937086ms)
    Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 10.295831ms)
    Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.363117ms)
    Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.496088ms)
    Jun 22 11:49:13.509: INFO: (13) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 10.659018ms)
    Jun 22 11:49:13.511: INFO: (13) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 12.543131ms)
    Jun 22 11:49:13.512: INFO: (13) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 13.19458ms)
    Jun 22 11:49:13.513: INFO: (13) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 14.355464ms)
    Jun 22 11:49:13.514: INFO: (13) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 14.993385ms)
    Jun 22 11:49:13.514: INFO: (13) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 14.907187ms)
    Jun 22 11:49:13.514: INFO: (13) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 15.160958ms)
    Jun 22 11:49:13.524: INFO: (14) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 10.012613ms)
    Jun 22 11:49:13.524: INFO: (14) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 9.85984ms)
    Jun 22 11:49:13.524: INFO: (14) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 9.937546ms)
    Jun 22 11:49:13.524: INFO: (14) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 10.059609ms)
    Jun 22 11:49:13.525: INFO: (14) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 10.722448ms)
    Jun 22 11:49:13.525: INFO: (14) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.921987ms)
    Jun 22 11:49:13.525: INFO: (14) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.636039ms)
    Jun 22 11:49:13.525: INFO: (14) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 10.988444ms)
    Jun 22 11:49:13.526: INFO: (14) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 11.398074ms)
    Jun 22 11:49:13.526: INFO: (14) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 11.844697ms)
    Jun 22 11:49:13.526: INFO: (14) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 12.089314ms)
    Jun 22 11:49:13.529: INFO: (14) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 14.654876ms)
    Jun 22 11:49:13.529: INFO: (14) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 15.091353ms)
    Jun 22 11:49:13.529: INFO: (14) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 14.973181ms)
    Jun 22 11:49:13.530: INFO: (14) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 15.427324ms)
    Jun 22 11:49:13.531: INFO: (14) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 16.493719ms)
    Jun 22 11:49:13.538: INFO: (15) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 6.760595ms)
    Jun 22 11:49:13.539: INFO: (15) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 7.879667ms)
    Jun 22 11:49:13.540: INFO: (15) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 8.243162ms)
    Jun 22 11:49:13.541: INFO: (15) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 9.160002ms)
    Jun 22 11:49:13.541: INFO: (15) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 10.140478ms)
    Jun 22 11:49:13.543: INFO: (15) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 11.278191ms)
    Jun 22 11:49:13.545: INFO: (15) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 13.556905ms)
    Jun 22 11:49:13.545: INFO: (15) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 13.541676ms)
    Jun 22 11:49:13.545: INFO: (15) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 13.600878ms)
    Jun 22 11:49:13.545: INFO: (15) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 13.534779ms)
    Jun 22 11:49:13.546: INFO: (15) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 14.102077ms)
    Jun 22 11:49:13.546: INFO: (15) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 14.497213ms)
    Jun 22 11:49:13.546: INFO: (15) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 14.641362ms)
    Jun 22 11:49:13.546: INFO: (15) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 14.843451ms)
    Jun 22 11:49:13.546: INFO: (15) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 14.550189ms)
    Jun 22 11:49:13.546: INFO: (15) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 14.841648ms)
    Jun 22 11:49:13.559: INFO: (16) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 12.245172ms)
    Jun 22 11:49:13.559: INFO: (16) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 12.287914ms)
    Jun 22 11:49:13.559: INFO: (16) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 12.755234ms)
    Jun 22 11:49:13.559: INFO: (16) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 12.832734ms)
    Jun 22 11:49:13.559: INFO: (16) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 12.737274ms)
    Jun 22 11:49:13.559: INFO: (16) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 12.811074ms)
    Jun 22 11:49:13.560: INFO: (16) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 13.076756ms)
    Jun 22 11:49:13.560: INFO: (16) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 13.439024ms)
    Jun 22 11:49:13.560: INFO: (16) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 12.969968ms)
    Jun 22 11:49:13.560: INFO: (16) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 13.022908ms)
    Jun 22 11:49:13.561: INFO: (16) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 15.081833ms)
    Jun 22 11:49:13.562: INFO: (16) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 15.769035ms)
    Jun 22 11:49:13.564: INFO: (16) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 17.106421ms)
    Jun 22 11:49:13.564: INFO: (16) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 17.43848ms)
    Jun 22 11:49:13.564: INFO: (16) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 17.773231ms)
    Jun 22 11:49:13.565: INFO: (16) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 18.294635ms)
    Jun 22 11:49:13.575: INFO: (17) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 9.343455ms)
    Jun 22 11:49:13.576: INFO: (17) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 10.638248ms)
    Jun 22 11:49:13.576: INFO: (17) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 11.587717ms)
    Jun 22 11:49:13.577: INFO: (17) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 12.063256ms)
    Jun 22 11:49:13.578: INFO: (17) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 12.23149ms)
    Jun 22 11:49:13.578: INFO: (17) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 12.668793ms)
    Jun 22 11:49:13.578: INFO: (17) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 12.930755ms)
    Jun 22 11:49:13.579: INFO: (17) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 13.674813ms)
    Jun 22 11:49:13.580: INFO: (17) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 15.074772ms)
    Jun 22 11:49:13.580: INFO: (17) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 14.396065ms)
    Jun 22 11:49:13.580: INFO: (17) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 15.102072ms)
    Jun 22 11:49:13.581: INFO: (17) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 15.944886ms)
    Jun 22 11:49:13.581: INFO: (17) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 16.042928ms)
    Jun 22 11:49:13.581: INFO: (17) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 15.49962ms)
    Jun 22 11:49:13.581: INFO: (17) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 16.091281ms)
    Jun 22 11:49:13.582: INFO: (17) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 16.728289ms)
    Jun 22 11:49:13.590: INFO: (18) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 8.034299ms)
    Jun 22 11:49:13.592: INFO: (18) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 8.924806ms)
    Jun 22 11:49:13.593: INFO: (18) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.159017ms)
    Jun 22 11:49:13.593: INFO: (18) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 9.673099ms)
    Jun 22 11:49:13.593: INFO: (18) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 9.787914ms)
    Jun 22 11:49:13.593: INFO: (18) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 9.847972ms)
    Jun 22 11:49:13.594: INFO: (18) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.832532ms)
    Jun 22 11:49:13.594: INFO: (18) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 11.025496ms)
    Jun 22 11:49:13.594: INFO: (18) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 10.528113ms)
    Jun 22 11:49:13.594: INFO: (18) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.803075ms)
    Jun 22 11:49:13.597: INFO: (18) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 14.327461ms)
    Jun 22 11:49:13.597: INFO: (18) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 13.956074ms)
    Jun 22 11:49:13.597: INFO: (18) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 14.05643ms)
    Jun 22 11:49:13.597: INFO: (18) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 14.135746ms)
    Jun 22 11:49:13.597: INFO: (18) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 13.864174ms)
    Jun 22 11:49:13.597: INFO: (18) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 14.524415ms)
    Jun 22 11:49:13.607: INFO: (19) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:462/proxy/: tls qux (200; 9.195642ms)
    Jun 22 11:49:13.608: INFO: (19) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname2/proxy/: bar (200; 10.231293ms)
    Jun 22 11:49:13.608: INFO: (19) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">... (200; 10.597192ms)
    Jun 22 11:49:13.608: INFO: (19) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl/proxy/rewriteme">test</a> (200; 10.001655ms)
    Jun 22 11:49:13.608: INFO: (19) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 10.213653ms)
    Jun 22 11:49:13.608: INFO: (19) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:1080/proxy/rewriteme">test<... (200; 9.964362ms)
    Jun 22 11:49:13.608: INFO: (19) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:460/proxy/: tls baz (200; 10.182611ms)
    Jun 22 11:49:13.608: INFO: (19) /api/v1/namespaces/proxy-9978/pods/proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 10.546897ms)
    Jun 22 11:49:13.608: INFO: (19) /api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/: <a href="/api/v1/namespaces/proxy-9978/pods/https:proxy-service-4lmg9-smqwl:443/proxy/tlsrewritem... (200; 11.20178ms)
    Jun 22 11:49:13.609: INFO: (19) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:162/proxy/: bar (200; 11.597948ms)
    Jun 22 11:49:13.610: INFO: (19) /api/v1/namespaces/proxy-9978/services/proxy-service-4lmg9:portname1/proxy/: foo (200; 12.445311ms)
    Jun 22 11:49:13.610: INFO: (19) /api/v1/namespaces/proxy-9978/pods/http:proxy-service-4lmg9-smqwl:160/proxy/: foo (200; 13.176323ms)
    Jun 22 11:49:13.612: INFO: (19) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname2/proxy/: tls qux (200; 14.927193ms)
    Jun 22 11:49:13.612: INFO: (19) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname1/proxy/: foo (200; 14.744788ms)
    Jun 22 11:49:13.612: INFO: (19) /api/v1/namespaces/proxy-9978/services/https:proxy-service-4lmg9:tlsportname1/proxy/: tls baz (200; 14.800909ms)
    Jun 22 11:49:13.613: INFO: (19) /api/v1/namespaces/proxy-9978/services/http:proxy-service-4lmg9:portname2/proxy/: bar (200; 15.584505ms)
    STEP: deleting ReplicationController proxy-service-4lmg9 in namespace proxy-9978, will wait for the garbage collector to delete the pods 06/22/23 11:49:13.613
    Jun 22 11:49:13.678: INFO: Deleting ReplicationController proxy-service-4lmg9 took: 8.571726ms
    Jun 22 11:49:13.779: INFO: Terminating ReplicationController proxy-service-4lmg9 pods took: 101.11728ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:49:16.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-9978" for this suite. 06/22/23 11:49:16.59
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:49:16.602
Jun 22 11:49:16.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename security-context-test 06/22/23 11:49:16.604
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:49:16.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:49:16.634
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
W0622 11:49:16.651835      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:49:16.652: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8" in namespace "security-context-test-2550" to be "Succeeded or Failed"
Jun 22 11:49:16.656: INFO: Pod "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064462ms
Jun 22 11:49:18.662: INFO: Pod "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010349758s
Jun 22 11:49:20.663: INFO: Pod "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011495411s
Jun 22 11:49:20.663: INFO: Pod "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8" satisfied condition "Succeeded or Failed"
Jun 22 11:49:20.686: INFO: Got logs for pod "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jun 22 11:49:20.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-2550" for this suite. 06/22/23 11:49:20.697
------------------------------
• [4.103 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:49:16.602
    Jun 22 11:49:16.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename security-context-test 06/22/23 11:49:16.604
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:49:16.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:49:16.634
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    W0622 11:49:16.651835      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:49:16.652: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8" in namespace "security-context-test-2550" to be "Succeeded or Failed"
    Jun 22 11:49:16.656: INFO: Pod "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064462ms
    Jun 22 11:49:18.662: INFO: Pod "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010349758s
    Jun 22 11:49:20.663: INFO: Pod "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011495411s
    Jun 22 11:49:20.663: INFO: Pod "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8" satisfied condition "Succeeded or Failed"
    Jun 22 11:49:20.686: INFO: Got logs for pod "busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:49:20.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-2550" for this suite. 06/22/23 11:49:20.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:49:20.708
Jun 22 11:49:20.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename init-container 06/22/23 11:49:20.71
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:49:20.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:49:20.735
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 06/22/23 11:49:20.74
Jun 22 11:49:20.741: INFO: PodSpec: initContainers in spec.initContainers
W0622 11:49:20.754365      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "init1", "init2", "run1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "init1", "init2", "run1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "init1", "init2", "run1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "init1", "init2", "run1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:49:24.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7279" for this suite. 06/22/23 11:49:24.572
------------------------------
• [3.871 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:49:20.708
    Jun 22 11:49:20.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename init-container 06/22/23 11:49:20.71
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:49:20.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:49:20.735
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 06/22/23 11:49:20.74
    Jun 22 11:49:20.741: INFO: PodSpec: initContainers in spec.initContainers
    W0622 11:49:20.754365      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "init1", "init2", "run1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "init1", "init2", "run1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "init1", "init2", "run1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "init1", "init2", "run1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:49:24.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7279" for this suite. 06/22/23 11:49:24.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:49:24.583
Jun 22 11:49:24.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename sched-pred 06/22/23 11:49:24.585
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:49:24.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:49:24.609
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jun 22 11:49:24.613: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 11:49:24.631: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 11:49:24.636: INFO: 
Logging pods the apiserver thinks is on node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k before test
Jun 22 11:49:24.653: INFO: corgi-test-8c78878b-6r5tv from corgi-test-privileged started at 2023-06-22 06:12:10 +0000 UTC (1 container statuses recorded)
Jun 22 11:49:24.653: INFO: 	Container nginx ready: true, restart count 0
Jun 22 11:49:24.653: INFO: antrea-agent-dxs2v from kube-system started at 2023-06-22 05:45:55 +0000 UTC (2 container statuses recorded)
Jun 22 11:49:24.653: INFO: 	Container antrea-agent ready: true, restart count 0
Jun 22 11:49:24.653: INFO: 	Container antrea-ovs ready: true, restart count 0
Jun 22 11:49:24.653: INFO: kube-proxy-czx75 from kube-system started at 2023-06-22 05:45:55 +0000 UTC (1 container statuses recorded)
Jun 22 11:49:24.653: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 11:49:24.653: INFO: sonobuoy-e2e-job-38d4831e07f74ae5 from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
Jun 22 11:49:24.653: INFO: 	Container e2e ready: true, restart count 0
Jun 22 11:49:24.653: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 11:49:24.653: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-5d74n from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
Jun 22 11:49:24.654: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 11:49:24.654: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 11:49:24.654: INFO: register-placeholder-q89nh from vmware-system-antrea started at 2023-06-22 11:40:07 +0000 UTC (1 container statuses recorded)
Jun 22 11:49:24.654: INFO: 	Container register ready: false, restart count 0
Jun 22 11:49:24.654: INFO: vsphere-csi-node-mp5v2 from vmware-system-csi started at 2023-06-22 05:45:55 +0000 UTC (3 container statuses recorded)
Jun 22 11:49:24.654: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 22 11:49:24.654: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 22 11:49:24.654: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jun 22 11:49:24.654: INFO: 
Logging pods the apiserver thinks is on node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw before test
Jun 22 11:49:24.669: INFO: pod-init-ac15a9d2-5180-4deb-a7b5-df079e6d1e33 from init-container-7279 started at 2023-06-22 11:49:20 +0000 UTC (1 container statuses recorded)
Jun 22 11:49:24.669: INFO: 	Container run1 ready: false, restart count 0
Jun 22 11:49:24.669: INFO: antrea-agent-n9smp from kube-system started at 2023-06-22 05:45:26 +0000 UTC (2 container statuses recorded)
Jun 22 11:49:24.669: INFO: 	Container antrea-agent ready: true, restart count 0
Jun 22 11:49:24.669: INFO: 	Container antrea-ovs ready: true, restart count 0
Jun 22 11:49:24.669: INFO: kube-proxy-rlz8k from kube-system started at 2023-06-22 05:45:26 +0000 UTC (1 container statuses recorded)
Jun 22 11:49:24.669: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 11:49:24.669: INFO: busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8 from security-context-test-2550 started at 2023-06-22 11:49:16 +0000 UTC (1 container statuses recorded)
Jun 22 11:49:24.669: INFO: 	Container busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8 ready: false, restart count 0
Jun 22 11:49:24.669: INFO: sonobuoy from sonobuoy started at 2023-06-22 10:19:41 +0000 UTC (1 container statuses recorded)
Jun 22 11:49:24.669: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 11:49:24.669: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-f4lqb from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
Jun 22 11:49:24.669: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 11:49:24.669: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 11:49:24.669: INFO: vsphere-csi-node-v6pm5 from vmware-system-csi started at 2023-06-22 05:45:26 +0000 UTC (3 container statuses recorded)
Jun 22 11:49:24.669: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 22 11:49:24.669: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 22 11:49:24.669: INFO: 	Container vsphere-csi-node ready: true, restart count 0
Jun 22 11:49:24.669: INFO: 
Logging pods the apiserver thinks is on node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 before test
Jun 22 11:49:24.690: INFO: antrea-agent-94vtc from kube-system started at 2023-06-22 05:45:16 +0000 UTC (2 container statuses recorded)
Jun 22 11:49:24.690: INFO: 	Container antrea-agent ready: true, restart count 1
Jun 22 11:49:24.690: INFO: 	Container antrea-ovs ready: true, restart count 0
Jun 22 11:49:24.690: INFO: kube-proxy-8rvhg from kube-system started at 2023-06-22 05:45:16 +0000 UTC (1 container statuses recorded)
Jun 22 11:49:24.690: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 11:49:24.690: INFO: metrics-server-64f66d5b9-8j569 from kube-system started at 2023-06-22 05:45:49 +0000 UTC (1 container statuses recorded)
Jun 22 11:49:24.690: INFO: 	Container metrics-server ready: true, restart count 0
Jun 22 11:49:24.690: INFO: secretgen-controller-7bdd5787fd-h9nfx from secretgen-controller started at 2023-06-22 05:45:49 +0000 UTC (1 container statuses recorded)
Jun 22 11:49:24.690: INFO: 	Container secretgen-controller ready: true, restart count 0
Jun 22 11:49:24.690: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-7x88w from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
Jun 22 11:49:24.690: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 11:49:24.690: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 11:49:24.690: INFO: vsphere-csi-node-wsgg4 from vmware-system-csi started at 2023-06-22 05:45:17 +0000 UTC (3 container statuses recorded)
Jun 22 11:49:24.690: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 22 11:49:24.690: INFO: 	Container node-driver-registrar ready: true, restart count 0
Jun 22 11:49:24.690: INFO: 	Container vsphere-csi-node ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 06/22/23 11:49:24.69
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.176af8cbd8a14d4d], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling..] 06/22/23 11:49:24.812
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:49:25.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-1009" for this suite. 06/22/23 11:49:25.816
------------------------------
• [1.240 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:49:24.583
    Jun 22 11:49:24.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename sched-pred 06/22/23 11:49:24.585
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:49:24.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:49:24.609
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jun 22 11:49:24.613: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun 22 11:49:24.631: INFO: Waiting for terminating namespaces to be deleted...
    Jun 22 11:49:24.636: INFO: 
    Logging pods the apiserver thinks is on node wl-antrea-md-0-7m2jg-7bbf47946fxzkjbz-gtf2k before test
    Jun 22 11:49:24.653: INFO: corgi-test-8c78878b-6r5tv from corgi-test-privileged started at 2023-06-22 06:12:10 +0000 UTC (1 container statuses recorded)
    Jun 22 11:49:24.653: INFO: 	Container nginx ready: true, restart count 0
    Jun 22 11:49:24.653: INFO: antrea-agent-dxs2v from kube-system started at 2023-06-22 05:45:55 +0000 UTC (2 container statuses recorded)
    Jun 22 11:49:24.653: INFO: 	Container antrea-agent ready: true, restart count 0
    Jun 22 11:49:24.653: INFO: 	Container antrea-ovs ready: true, restart count 0
    Jun 22 11:49:24.653: INFO: kube-proxy-czx75 from kube-system started at 2023-06-22 05:45:55 +0000 UTC (1 container statuses recorded)
    Jun 22 11:49:24.653: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 22 11:49:24.653: INFO: sonobuoy-e2e-job-38d4831e07f74ae5 from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
    Jun 22 11:49:24.653: INFO: 	Container e2e ready: true, restart count 0
    Jun 22 11:49:24.653: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 22 11:49:24.653: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-5d74n from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
    Jun 22 11:49:24.654: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 22 11:49:24.654: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 22 11:49:24.654: INFO: register-placeholder-q89nh from vmware-system-antrea started at 2023-06-22 11:40:07 +0000 UTC (1 container statuses recorded)
    Jun 22 11:49:24.654: INFO: 	Container register ready: false, restart count 0
    Jun 22 11:49:24.654: INFO: vsphere-csi-node-mp5v2 from vmware-system-csi started at 2023-06-22 05:45:55 +0000 UTC (3 container statuses recorded)
    Jun 22 11:49:24.654: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 22 11:49:24.654: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 22 11:49:24.654: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Jun 22 11:49:24.654: INFO: 
    Logging pods the apiserver thinks is on node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw before test
    Jun 22 11:49:24.669: INFO: pod-init-ac15a9d2-5180-4deb-a7b5-df079e6d1e33 from init-container-7279 started at 2023-06-22 11:49:20 +0000 UTC (1 container statuses recorded)
    Jun 22 11:49:24.669: INFO: 	Container run1 ready: false, restart count 0
    Jun 22 11:49:24.669: INFO: antrea-agent-n9smp from kube-system started at 2023-06-22 05:45:26 +0000 UTC (2 container statuses recorded)
    Jun 22 11:49:24.669: INFO: 	Container antrea-agent ready: true, restart count 0
    Jun 22 11:49:24.669: INFO: 	Container antrea-ovs ready: true, restart count 0
    Jun 22 11:49:24.669: INFO: kube-proxy-rlz8k from kube-system started at 2023-06-22 05:45:26 +0000 UTC (1 container statuses recorded)
    Jun 22 11:49:24.669: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 22 11:49:24.669: INFO: busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8 from security-context-test-2550 started at 2023-06-22 11:49:16 +0000 UTC (1 container statuses recorded)
    Jun 22 11:49:24.669: INFO: 	Container busybox-privileged-false-de1a88db-d56e-41cf-8fb4-d963603444b8 ready: false, restart count 0
    Jun 22 11:49:24.669: INFO: sonobuoy from sonobuoy started at 2023-06-22 10:19:41 +0000 UTC (1 container statuses recorded)
    Jun 22 11:49:24.669: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun 22 11:49:24.669: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-f4lqb from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
    Jun 22 11:49:24.669: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 22 11:49:24.669: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 22 11:49:24.669: INFO: vsphere-csi-node-v6pm5 from vmware-system-csi started at 2023-06-22 05:45:26 +0000 UTC (3 container statuses recorded)
    Jun 22 11:49:24.669: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 22 11:49:24.669: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 22 11:49:24.669: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    Jun 22 11:49:24.669: INFO: 
    Logging pods the apiserver thinks is on node wl-antrea-md-2-klkg5-655c788c79xgkjlg-9j8v9 before test
    Jun 22 11:49:24.690: INFO: antrea-agent-94vtc from kube-system started at 2023-06-22 05:45:16 +0000 UTC (2 container statuses recorded)
    Jun 22 11:49:24.690: INFO: 	Container antrea-agent ready: true, restart count 1
    Jun 22 11:49:24.690: INFO: 	Container antrea-ovs ready: true, restart count 0
    Jun 22 11:49:24.690: INFO: kube-proxy-8rvhg from kube-system started at 2023-06-22 05:45:16 +0000 UTC (1 container statuses recorded)
    Jun 22 11:49:24.690: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 22 11:49:24.690: INFO: metrics-server-64f66d5b9-8j569 from kube-system started at 2023-06-22 05:45:49 +0000 UTC (1 container statuses recorded)
    Jun 22 11:49:24.690: INFO: 	Container metrics-server ready: true, restart count 0
    Jun 22 11:49:24.690: INFO: secretgen-controller-7bdd5787fd-h9nfx from secretgen-controller started at 2023-06-22 05:45:49 +0000 UTC (1 container statuses recorded)
    Jun 22 11:49:24.690: INFO: 	Container secretgen-controller ready: true, restart count 0
    Jun 22 11:49:24.690: INFO: sonobuoy-systemd-logs-daemon-set-2da7e803efcf48a1-7x88w from sonobuoy started at 2023-06-22 10:19:42 +0000 UTC (2 container statuses recorded)
    Jun 22 11:49:24.690: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 22 11:49:24.690: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 22 11:49:24.690: INFO: vsphere-csi-node-wsgg4 from vmware-system-csi started at 2023-06-22 05:45:17 +0000 UTC (3 container statuses recorded)
    Jun 22 11:49:24.690: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 22 11:49:24.690: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Jun 22 11:49:24.690: INFO: 	Container vsphere-csi-node ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 06/22/23 11:49:24.69
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.176af8cbd8a14d4d], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling..] 06/22/23 11:49:24.812
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:49:25.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-1009" for this suite. 06/22/23 11:49:25.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:49:25.828
Jun 22 11:49:25.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename events 06/22/23 11:49:25.83
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:49:25.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:49:25.854
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 06/22/23 11:49:25.858
STEP: listing all events in all namespaces 06/22/23 11:49:25.868
STEP: patching the test event 06/22/23 11:49:25.875
STEP: fetching the test event 06/22/23 11:49:25.883
STEP: updating the test event 06/22/23 11:49:25.887
STEP: getting the test event 06/22/23 11:49:25.898
STEP: deleting the test event 06/22/23 11:49:25.902
STEP: listing all events in all namespaces 06/22/23 11:49:25.913
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jun 22 11:49:25.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-3445" for this suite. 06/22/23 11:49:25.932
------------------------------
• [0.111 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:49:25.828
    Jun 22 11:49:25.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename events 06/22/23 11:49:25.83
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:49:25.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:49:25.854
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 06/22/23 11:49:25.858
    STEP: listing all events in all namespaces 06/22/23 11:49:25.868
    STEP: patching the test event 06/22/23 11:49:25.875
    STEP: fetching the test event 06/22/23 11:49:25.883
    STEP: updating the test event 06/22/23 11:49:25.887
    STEP: getting the test event 06/22/23 11:49:25.898
    STEP: deleting the test event 06/22/23 11:49:25.902
    STEP: listing all events in all namespaces 06/22/23 11:49:25.913
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:49:25.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-3445" for this suite. 06/22/23 11:49:25.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:49:25.944
Jun 22 11:49:25.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename webhook 06/22/23 11:49:25.945
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:49:25.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:49:25.975
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 06/22/23 11:49:25.993
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:49:26.521
STEP: Deploying the webhook pod 06/22/23 11:49:26.532
W0622 11:49:26.549539      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 11:49:26.549
Jun 22 11:49:26.560: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/22/23 11:49:28.575
STEP: Verifying the service has paired with the endpoint 06/22/23 11:49:28.591
Jun 22 11:49:29.591: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 06/22/23 11:49:29.597
STEP: create a namespace for the webhook 06/22/23 11:49:29.626
STEP: create a configmap should be unconditionally rejected by the webhook 06/22/23 11:49:29.635
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:49:29.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8445" for this suite. 06/22/23 11:49:29.715
STEP: Destroying namespace "webhook-8445-markers" for this suite. 06/22/23 11:49:29.723
------------------------------
• [3.789 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:49:25.944
    Jun 22 11:49:25.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename webhook 06/22/23 11:49:25.945
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:49:25.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:49:25.975
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 06/22/23 11:49:25.993
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/22/23 11:49:26.521
    STEP: Deploying the webhook pod 06/22/23 11:49:26.532
    W0622 11:49:26.549539      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 11:49:26.549
    Jun 22 11:49:26.560: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/22/23 11:49:28.575
    STEP: Verifying the service has paired with the endpoint 06/22/23 11:49:28.591
    Jun 22 11:49:29.591: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 06/22/23 11:49:29.597
    STEP: create a namespace for the webhook 06/22/23 11:49:29.626
    STEP: create a configmap should be unconditionally rejected by the webhook 06/22/23 11:49:29.635
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:49:29.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8445" for this suite. 06/22/23 11:49:29.715
    STEP: Destroying namespace "webhook-8445-markers" for this suite. 06/22/23 11:49:29.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:49:29.735
Jun 22 11:49:29.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename resourcequota 06/22/23 11:49:29.736
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:49:29.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:49:29.771
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 06/22/23 11:49:29.775
STEP: Creating a ResourceQuota 06/22/23 11:49:34.781
STEP: Ensuring resource quota status is calculated 06/22/23 11:49:34.79
STEP: Creating a Pod that fits quota 06/22/23 11:49:36.796
W0622 11:49:36.817673      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring ResourceQuota status captures the pod usage 06/22/23 11:49:36.817
STEP: Not allowing a pod to be created that exceeds remaining quota 06/22/23 11:49:38.823
W0622 11:49:38.827755      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 06/22/23 11:49:38.827
W0622 11:49:38.831322      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a pod cannot update its resource requirements 06/22/23 11:49:38.831
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 06/22/23 11:49:38.838
STEP: Deleting the pod 06/22/23 11:49:40.844
STEP: Ensuring resource quota status released the pod usage 06/22/23 11:49:40.855
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun 22 11:49:42.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6531" for this suite. 06/22/23 11:49:42.87
------------------------------
• [SLOW TEST] [13.142 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:49:29.735
    Jun 22 11:49:29.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename resourcequota 06/22/23 11:49:29.736
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:49:29.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:49:29.771
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 06/22/23 11:49:29.775
    STEP: Creating a ResourceQuota 06/22/23 11:49:34.781
    STEP: Ensuring resource quota status is calculated 06/22/23 11:49:34.79
    STEP: Creating a Pod that fits quota 06/22/23 11:49:36.796
    W0622 11:49:36.817673      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring ResourceQuota status captures the pod usage 06/22/23 11:49:36.817
    STEP: Not allowing a pod to be created that exceeds remaining quota 06/22/23 11:49:38.823
    W0622 11:49:38.827755      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 06/22/23 11:49:38.827
    W0622 11:49:38.831322      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring a pod cannot update its resource requirements 06/22/23 11:49:38.831
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 06/22/23 11:49:38.838
    STEP: Deleting the pod 06/22/23 11:49:40.844
    STEP: Ensuring resource quota status released the pod usage 06/22/23 11:49:40.855
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:49:42.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6531" for this suite. 06/22/23 11:49:42.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:49:42.88
Jun 22 11:49:42.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename crd-webhook 06/22/23 11:49:42.881
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:49:42.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:49:42.903
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 06/22/23 11:49:42.906
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/22/23 11:49:43.283
STEP: Deploying the custom resource conversion webhook pod 06/22/23 11:49:43.289
W0622 11:49:43.304779      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-crd-conversion-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-crd-conversion-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-crd-conversion-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-crd-conversion-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the deployment to be ready 06/22/23 11:49:43.304
Jun 22 11:49:43.312: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/22/23 11:49:45.326
STEP: Verifying the service has paired with the endpoint 06/22/23 11:49:45.351
Jun 22 11:49:46.351: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jun 22 11:49:46.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Creating a v1 custom resource 06/22/23 11:49:48.959
STEP: Create a v2 custom resource 06/22/23 11:49:48.979
STEP: List CRs in v1 06/22/23 11:49:49.032
STEP: List CRs in v2 06/22/23 11:49:49.038
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:49:49.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-9629" for this suite. 06/22/23 11:49:49.615
------------------------------
• [SLOW TEST] [6.741 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:49:42.88
    Jun 22 11:49:42.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename crd-webhook 06/22/23 11:49:42.881
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:49:42.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:49:42.903
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 06/22/23 11:49:42.906
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/22/23 11:49:43.283
    STEP: Deploying the custom resource conversion webhook pod 06/22/23 11:49:43.289
    W0622 11:49:43.304779      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "sample-crd-conversion-webhook" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "sample-crd-conversion-webhook" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "sample-crd-conversion-webhook" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "sample-crd-conversion-webhook" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the deployment to be ready 06/22/23 11:49:43.304
    Jun 22 11:49:43.312: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/22/23 11:49:45.326
    STEP: Verifying the service has paired with the endpoint 06/22/23 11:49:45.351
    Jun 22 11:49:46.351: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jun 22 11:49:46.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Creating a v1 custom resource 06/22/23 11:49:48.959
    STEP: Create a v2 custom resource 06/22/23 11:49:48.979
    STEP: List CRs in v1 06/22/23 11:49:49.032
    STEP: List CRs in v2 06/22/23 11:49:49.038
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:49:49.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-9629" for this suite. 06/22/23 11:49:49.615
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:49:49.622
Jun 22 11:49:49.622: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename gc 06/22/23 11:49:49.623
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:49:49.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:49:49.648
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 06/22/23 11:49:49.659
W0622 11:49:49.665929      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the rc 06/22/23 11:49:54.672
STEP: wait for the rc to be deleted 06/22/23 11:49:54.682
Jun 22 11:49:55.725: INFO: 80 pods remaining
Jun 22 11:49:55.725: INFO: 80 pods has nil DeletionTimestamp
Jun 22 11:49:55.725: INFO: 
Jun 22 11:49:56.702: INFO: 71 pods remaining
Jun 22 11:49:56.702: INFO: 71 pods has nil DeletionTimestamp
Jun 22 11:49:56.702: INFO: 
Jun 22 11:49:57.704: INFO: 60 pods remaining
Jun 22 11:49:57.704: INFO: 59 pods has nil DeletionTimestamp
Jun 22 11:49:57.704: INFO: 
Jun 22 11:49:58.694: INFO: 40 pods remaining
Jun 22 11:49:58.694: INFO: 40 pods has nil DeletionTimestamp
Jun 22 11:49:58.694: INFO: 
Jun 22 11:49:59.696: INFO: 32 pods remaining
Jun 22 11:49:59.696: INFO: 31 pods has nil DeletionTimestamp
Jun 22 11:49:59.696: INFO: 
Jun 22 11:50:00.696: INFO: 19 pods remaining
Jun 22 11:50:00.696: INFO: 19 pods has nil DeletionTimestamp
Jun 22 11:50:00.696: INFO: 
STEP: Gathering metrics 06/22/23 11:50:01.7
Jun 22 11:50:01.729: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" in namespace "kube-system" to be "running and ready"
Jun 22 11:50:01.733: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v": Phase="Running", Reason="", readiness=true. Elapsed: 4.013494ms
Jun 22 11:50:01.733: INFO: The phase of Pod kube-controller-manager-wl-antrea-prdzc-k4b4v is Running (Ready = true)
Jun 22 11:50:01.733: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" satisfied condition "running and ready"
Jun 22 11:50:01.797: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun 22 11:50:01.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4665" for this suite. 06/22/23 11:50:01.808
------------------------------
• [SLOW TEST] [12.197 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:49:49.622
    Jun 22 11:49:49.622: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename gc 06/22/23 11:49:49.623
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:49:49.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:49:49.648
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 06/22/23 11:49:49.659
    W0622 11:49:49.665929      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: delete the rc 06/22/23 11:49:54.672
    STEP: wait for the rc to be deleted 06/22/23 11:49:54.682
    Jun 22 11:49:55.725: INFO: 80 pods remaining
    Jun 22 11:49:55.725: INFO: 80 pods has nil DeletionTimestamp
    Jun 22 11:49:55.725: INFO: 
    Jun 22 11:49:56.702: INFO: 71 pods remaining
    Jun 22 11:49:56.702: INFO: 71 pods has nil DeletionTimestamp
    Jun 22 11:49:56.702: INFO: 
    Jun 22 11:49:57.704: INFO: 60 pods remaining
    Jun 22 11:49:57.704: INFO: 59 pods has nil DeletionTimestamp
    Jun 22 11:49:57.704: INFO: 
    Jun 22 11:49:58.694: INFO: 40 pods remaining
    Jun 22 11:49:58.694: INFO: 40 pods has nil DeletionTimestamp
    Jun 22 11:49:58.694: INFO: 
    Jun 22 11:49:59.696: INFO: 32 pods remaining
    Jun 22 11:49:59.696: INFO: 31 pods has nil DeletionTimestamp
    Jun 22 11:49:59.696: INFO: 
    Jun 22 11:50:00.696: INFO: 19 pods remaining
    Jun 22 11:50:00.696: INFO: 19 pods has nil DeletionTimestamp
    Jun 22 11:50:00.696: INFO: 
    STEP: Gathering metrics 06/22/23 11:50:01.7
    Jun 22 11:50:01.729: INFO: Waiting up to 5m0s for pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" in namespace "kube-system" to be "running and ready"
    Jun 22 11:50:01.733: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v": Phase="Running", Reason="", readiness=true. Elapsed: 4.013494ms
    Jun 22 11:50:01.733: INFO: The phase of Pod kube-controller-manager-wl-antrea-prdzc-k4b4v is Running (Ready = true)
    Jun 22 11:50:01.733: INFO: Pod "kube-controller-manager-wl-antrea-prdzc-k4b4v" satisfied condition "running and ready"
    Jun 22 11:50:01.797: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:50:01.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4665" for this suite. 06/22/23 11:50:01.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:50:01.824
Jun 22 11:50:01.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 11:50:01.826
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:50:01.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:50:01.856
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-791c5745-974d-4f6e-965b-7232b23f1f15 06/22/23 11:50:01.86
STEP: Creating a pod to test consume configMaps 06/22/23 11:50:01.867
W0622 11:50:01.876762      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:50:01.876: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203" in namespace "projected-5578" to be "Succeeded or Failed"
Jun 22 11:50:01.881: INFO: Pod "pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.387375ms
Jun 22 11:50:03.887: INFO: Pod "pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010088023s
Jun 22 11:50:05.888: INFO: Pod "pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011402899s
Jun 22 11:50:07.886: INFO: Pod "pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009466012s
Jun 22 11:50:09.886: INFO: Pod "pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009906164s
STEP: Saw pod success 06/22/23 11:50:09.886
Jun 22 11:50:09.887: INFO: Pod "pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203" satisfied condition "Succeeded or Failed"
Jun 22 11:50:09.890: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203 container agnhost-container: <nil>
STEP: delete the pod 06/22/23 11:50:09.899
Jun 22 11:50:09.910: INFO: Waiting for pod pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203 to disappear
Jun 22 11:50:09.914: INFO: Pod pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun 22 11:50:09.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5578" for this suite. 06/22/23 11:50:09.921
------------------------------
• [SLOW TEST] [8.103 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:50:01.824
    Jun 22 11:50:01.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 11:50:01.826
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:50:01.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:50:01.856
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-791c5745-974d-4f6e-965b-7232b23f1f15 06/22/23 11:50:01.86
    STEP: Creating a pod to test consume configMaps 06/22/23 11:50:01.867
    W0622 11:50:01.876762      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:50:01.876: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203" in namespace "projected-5578" to be "Succeeded or Failed"
    Jun 22 11:50:01.881: INFO: Pod "pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.387375ms
    Jun 22 11:50:03.887: INFO: Pod "pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010088023s
    Jun 22 11:50:05.888: INFO: Pod "pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011402899s
    Jun 22 11:50:07.886: INFO: Pod "pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009466012s
    Jun 22 11:50:09.886: INFO: Pod "pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009906164s
    STEP: Saw pod success 06/22/23 11:50:09.886
    Jun 22 11:50:09.887: INFO: Pod "pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203" satisfied condition "Succeeded or Failed"
    Jun 22 11:50:09.890: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203 container agnhost-container: <nil>
    STEP: delete the pod 06/22/23 11:50:09.899
    Jun 22 11:50:09.910: INFO: Waiting for pod pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203 to disappear
    Jun 22 11:50:09.914: INFO: Pod pod-projected-configmaps-1b349a68-6198-436d-97e7-d960468a0203 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:50:09.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5578" for this suite. 06/22/23 11:50:09.921
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:50:09.928
Jun 22 11:50:09.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename sched-preemption 06/22/23 11:50:09.93
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:50:09.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:50:09.952
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jun 22 11:50:09.970: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 22 11:51:10.039: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:51:10.044
Jun 22 11:51:10.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename sched-preemption-path 06/22/23 11:51:10.045
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:51:10.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:51:10.067
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 06/22/23 11:51:10.071
STEP: Trying to launch a pod without a label to get a node which can launch it. 06/22/23 11:51:10.071
Jun 22 11:51:10.081: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-8164" to be "running"
Jun 22 11:51:10.085: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.865116ms
Jun 22 11:51:12.091: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009897328s
Jun 22 11:51:12.091: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 06/22/23 11:51:12.095
Jun 22 11:51:12.110: INFO: found a healthy node: wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Jun 22 11:51:18.195: INFO: pods created so far: [1 1 1]
Jun 22 11:51:18.196: INFO: length of pods created so far: 3
Jun 22 11:51:20.210: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Jun 22 11:51:27.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:51:27.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-8164" for this suite. 06/22/23 11:51:27.323
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-314" for this suite. 06/22/23 11:51:27.331
------------------------------
• [SLOW TEST] [77.410 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:50:09.928
    Jun 22 11:50:09.928: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename sched-preemption 06/22/23 11:50:09.93
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:50:09.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:50:09.952
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jun 22 11:50:09.970: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 22 11:51:10.039: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:51:10.044
    Jun 22 11:51:10.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename sched-preemption-path 06/22/23 11:51:10.045
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:51:10.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:51:10.067
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 06/22/23 11:51:10.071
    STEP: Trying to launch a pod without a label to get a node which can launch it. 06/22/23 11:51:10.071
    Jun 22 11:51:10.081: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-8164" to be "running"
    Jun 22 11:51:10.085: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.865116ms
    Jun 22 11:51:12.091: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009897328s
    Jun 22 11:51:12.091: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 06/22/23 11:51:12.095
    Jun 22 11:51:12.110: INFO: found a healthy node: wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Jun 22 11:51:18.195: INFO: pods created so far: [1 1 1]
    Jun 22 11:51:18.196: INFO: length of pods created so far: 3
    Jun 22 11:51:20.210: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:51:27.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:51:27.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-8164" for this suite. 06/22/23 11:51:27.323
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-314" for this suite. 06/22/23 11:51:27.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:51:27.339
Jun 22 11:51:27.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename secrets 06/22/23 11:51:27.34
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:51:27.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:51:27.364
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-4ae3082d-b677-4926-afac-8fb59ae65d2e 06/22/23 11:51:27.367
STEP: Creating a pod to test consume secrets 06/22/23 11:51:27.372
W0622 11:51:27.383146      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:51:27.383: INFO: Waiting up to 5m0s for pod "pod-secrets-1b0b050f-0272-4f06-be55-5c339d45de3a" in namespace "secrets-2006" to be "Succeeded or Failed"
Jun 22 11:51:27.387: INFO: Pod "pod-secrets-1b0b050f-0272-4f06-be55-5c339d45de3a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.751355ms
Jun 22 11:51:29.392: INFO: Pod "pod-secrets-1b0b050f-0272-4f06-be55-5c339d45de3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009247359s
Jun 22 11:51:31.394: INFO: Pod "pod-secrets-1b0b050f-0272-4f06-be55-5c339d45de3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011009789s
STEP: Saw pod success 06/22/23 11:51:31.394
Jun 22 11:51:31.394: INFO: Pod "pod-secrets-1b0b050f-0272-4f06-be55-5c339d45de3a" satisfied condition "Succeeded or Failed"
Jun 22 11:51:31.398: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-secrets-1b0b050f-0272-4f06-be55-5c339d45de3a container secret-volume-test: <nil>
STEP: delete the pod 06/22/23 11:51:31.406
Jun 22 11:51:31.417: INFO: Waiting for pod pod-secrets-1b0b050f-0272-4f06-be55-5c339d45de3a to disappear
Jun 22 11:51:31.421: INFO: Pod pod-secrets-1b0b050f-0272-4f06-be55-5c339d45de3a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jun 22 11:51:31.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2006" for this suite. 06/22/23 11:51:31.427
------------------------------
• [4.097 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:51:27.339
    Jun 22 11:51:27.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename secrets 06/22/23 11:51:27.34
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:51:27.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:51:27.364
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-4ae3082d-b677-4926-afac-8fb59ae65d2e 06/22/23 11:51:27.367
    STEP: Creating a pod to test consume secrets 06/22/23 11:51:27.372
    W0622 11:51:27.383146      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:51:27.383: INFO: Waiting up to 5m0s for pod "pod-secrets-1b0b050f-0272-4f06-be55-5c339d45de3a" in namespace "secrets-2006" to be "Succeeded or Failed"
    Jun 22 11:51:27.387: INFO: Pod "pod-secrets-1b0b050f-0272-4f06-be55-5c339d45de3a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.751355ms
    Jun 22 11:51:29.392: INFO: Pod "pod-secrets-1b0b050f-0272-4f06-be55-5c339d45de3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009247359s
    Jun 22 11:51:31.394: INFO: Pod "pod-secrets-1b0b050f-0272-4f06-be55-5c339d45de3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011009789s
    STEP: Saw pod success 06/22/23 11:51:31.394
    Jun 22 11:51:31.394: INFO: Pod "pod-secrets-1b0b050f-0272-4f06-be55-5c339d45de3a" satisfied condition "Succeeded or Failed"
    Jun 22 11:51:31.398: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-secrets-1b0b050f-0272-4f06-be55-5c339d45de3a container secret-volume-test: <nil>
    STEP: delete the pod 06/22/23 11:51:31.406
    Jun 22 11:51:31.417: INFO: Waiting for pod pod-secrets-1b0b050f-0272-4f06-be55-5c339d45de3a to disappear
    Jun 22 11:51:31.421: INFO: Pod pod-secrets-1b0b050f-0272-4f06-be55-5c339d45de3a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:51:31.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2006" for this suite. 06/22/23 11:51:31.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:51:31.437
Jun 22 11:51:31.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename pods 06/22/23 11:51:31.438
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:51:31.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:51:31.461
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 06/22/23 11:51:31.464
STEP: submitting the pod to kubernetes 06/22/23 11:51:31.465
W0622 11:51:31.474269      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: verifying QOS class is set on the pod 06/22/23 11:51:31.474
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Jun 22 11:51:31.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9101" for this suite. 06/22/23 11:51:31.484
------------------------------
• [0.058 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:51:31.437
    Jun 22 11:51:31.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename pods 06/22/23 11:51:31.438
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:51:31.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:51:31.461
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 06/22/23 11:51:31.464
    STEP: submitting the pod to kubernetes 06/22/23 11:51:31.465
    W0622 11:51:31.474269      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: verifying QOS class is set on the pod 06/22/23 11:51:31.474
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:51:31.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9101" for this suite. 06/22/23 11:51:31.484
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:51:31.495
Jun 22 11:51:31.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename replicaset 06/22/23 11:51:31.496
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:51:31.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:51:31.524
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 06/22/23 11:51:31.528
W0622 11:51:31.534474      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Verify that the required pods have come up 06/22/23 11:51:31.534
Jun 22 11:51:31.538: INFO: Pod name sample-pod: Found 0 pods out of 3
Jun 22 11:51:36.545: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 06/22/23 11:51:36.545
Jun 22 11:51:36.550: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 06/22/23 11:51:36.55
STEP: DeleteCollection of the ReplicaSets 06/22/23 11:51:36.557
STEP: After DeleteCollection verify that ReplicaSets have been deleted 06/22/23 11:51:36.568
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jun 22 11:51:36.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3395" for this suite. 06/22/23 11:51:36.583
------------------------------
• [SLOW TEST] [5.099 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:51:31.495
    Jun 22 11:51:31.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename replicaset 06/22/23 11:51:31.496
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:51:31.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:51:31.524
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 06/22/23 11:51:31.528
    W0622 11:51:31.534474      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Verify that the required pods have come up 06/22/23 11:51:31.534
    Jun 22 11:51:31.538: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jun 22 11:51:36.545: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 06/22/23 11:51:36.545
    Jun 22 11:51:36.550: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 06/22/23 11:51:36.55
    STEP: DeleteCollection of the ReplicaSets 06/22/23 11:51:36.557
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 06/22/23 11:51:36.568
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:51:36.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3395" for this suite. 06/22/23 11:51:36.583
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:51:36.595
Jun 22 11:51:36.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename kubectl 06/22/23 11:51:36.596
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:51:36.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:51:36.633
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 06/22/23 11:51:36.637
Jun 22 11:51:36.638: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jun 22 11:51:36.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 create -f -'
Jun 22 11:51:38.198: INFO: stderr: ""
Jun 22 11:51:38.198: INFO: stdout: "service/agnhost-replica created\n"
Jun 22 11:51:38.198: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jun 22 11:51:38.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 create -f -'
Jun 22 11:51:38.760: INFO: stderr: ""
Jun 22 11:51:38.760: INFO: stdout: "service/agnhost-primary created\n"
Jun 22 11:51:38.760: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 22 11:51:38.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 create -f -'
Jun 22 11:51:40.379: INFO: stderr: ""
Jun 22 11:51:40.379: INFO: stdout: "service/frontend created\n"
Jun 22 11:51:40.382: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun 22 11:51:40.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 create -f -'
Jun 22 11:51:41.761: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"guestbook-frontend\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"guestbook-frontend\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"guestbook-frontend\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"guestbook-frontend\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Jun 22 11:51:41.761: INFO: stdout: "deployment.apps/frontend created\n"
Jun 22 11:51:41.761: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 22 11:51:41.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 create -f -'
Jun 22 11:51:42.120: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"primary\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"primary\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"primary\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"primary\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Jun 22 11:51:42.120: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jun 22 11:51:42.120: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 22 11:51:42.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 create -f -'
Jun 22 11:51:42.581: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"replica\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"replica\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"replica\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"replica\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
Jun 22 11:51:42.582: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 06/22/23 11:51:42.582
Jun 22 11:51:42.582: INFO: Waiting for all frontend pods to be Running.
Jun 22 11:51:47.633: INFO: Waiting for frontend to serve content.
Jun 22 11:51:47.654: INFO: Trying to add a new entry to the guestbook.
Jun 22 11:51:47.678: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 06/22/23 11:51:47.69
Jun 22 11:51:47.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 delete --grace-period=0 --force -f -'
Jun 22 11:51:47.799: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 11:51:47.799: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 06/22/23 11:51:47.799
Jun 22 11:51:47.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 delete --grace-period=0 --force -f -'
Jun 22 11:51:47.902: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 11:51:47.902: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 06/22/23 11:51:47.903
Jun 22 11:51:47.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 delete --grace-period=0 --force -f -'
Jun 22 11:51:48.009: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 11:51:48.009: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 06/22/23 11:51:48.009
Jun 22 11:51:48.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 delete --grace-period=0 --force -f -'
Jun 22 11:51:48.097: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 11:51:48.097: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 06/22/23 11:51:48.097
Jun 22 11:51:48.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 delete --grace-period=0 --force -f -'
Jun 22 11:51:48.213: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 11:51:48.213: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 06/22/23 11:51:48.213
Jun 22 11:51:48.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 delete --grace-period=0 --force -f -'
Jun 22 11:51:48.305: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 11:51:48.305: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jun 22 11:51:48.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9670" for this suite. 06/22/23 11:51:48.312
------------------------------
• [SLOW TEST] [11.726 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:51:36.595
    Jun 22 11:51:36.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename kubectl 06/22/23 11:51:36.596
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:51:36.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:51:36.633
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 06/22/23 11:51:36.637
    Jun 22 11:51:36.638: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jun 22 11:51:36.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 create -f -'
    Jun 22 11:51:38.198: INFO: stderr: ""
    Jun 22 11:51:38.198: INFO: stdout: "service/agnhost-replica created\n"
    Jun 22 11:51:38.198: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jun 22 11:51:38.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 create -f -'
    Jun 22 11:51:38.760: INFO: stderr: ""
    Jun 22 11:51:38.760: INFO: stdout: "service/agnhost-primary created\n"
    Jun 22 11:51:38.760: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jun 22 11:51:38.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 create -f -'
    Jun 22 11:51:40.379: INFO: stderr: ""
    Jun 22 11:51:40.379: INFO: stdout: "service/frontend created\n"
    Jun 22 11:51:40.382: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jun 22 11:51:40.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 create -f -'
    Jun 22 11:51:41.761: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"guestbook-frontend\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"guestbook-frontend\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"guestbook-frontend\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"guestbook-frontend\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
    Jun 22 11:51:41.761: INFO: stdout: "deployment.apps/frontend created\n"
    Jun 22 11:51:41.761: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jun 22 11:51:41.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 create -f -'
    Jun 22 11:51:42.120: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"primary\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"primary\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"primary\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"primary\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
    Jun 22 11:51:42.120: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jun 22 11:51:42.120: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jun 22 11:51:42.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 create -f -'
    Jun 22 11:51:42.581: INFO: stderr: "Warning: would violate PodSecurity \"restricted:v1.24\": allowPrivilegeEscalation != false (container \"replica\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \"replica\" must set securityContext.capabilities.drop=[\"ALL\"]), runAsNonRoot != true (pod or container \"replica\" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \"replica\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n"
    Jun 22 11:51:42.582: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 06/22/23 11:51:42.582
    Jun 22 11:51:42.582: INFO: Waiting for all frontend pods to be Running.
    Jun 22 11:51:47.633: INFO: Waiting for frontend to serve content.
    Jun 22 11:51:47.654: INFO: Trying to add a new entry to the guestbook.
    Jun 22 11:51:47.678: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 06/22/23 11:51:47.69
    Jun 22 11:51:47.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 delete --grace-period=0 --force -f -'
    Jun 22 11:51:47.799: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 22 11:51:47.799: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 06/22/23 11:51:47.799
    Jun 22 11:51:47.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 delete --grace-period=0 --force -f -'
    Jun 22 11:51:47.902: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 22 11:51:47.902: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 06/22/23 11:51:47.903
    Jun 22 11:51:47.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 delete --grace-period=0 --force -f -'
    Jun 22 11:51:48.009: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 22 11:51:48.009: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 06/22/23 11:51:48.009
    Jun 22 11:51:48.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 delete --grace-period=0 --force -f -'
    Jun 22 11:51:48.097: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 22 11:51:48.097: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 06/22/23 11:51:48.097
    Jun 22 11:51:48.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 delete --grace-period=0 --force -f -'
    Jun 22 11:51:48.213: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 22 11:51:48.213: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 06/22/23 11:51:48.213
    Jun 22 11:51:48.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=kubectl-9670 delete --grace-period=0 --force -f -'
    Jun 22 11:51:48.305: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 22 11:51:48.305: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:51:48.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9670" for this suite. 06/22/23 11:51:48.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:51:48.323
Jun 22 11:51:48.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename endpointslice 06/22/23 11:51:48.324
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:51:48.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:51:48.351
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
W0622 11:51:48.366843      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "container1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "container1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "container1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "container1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0622 11:51:48.374240      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "container1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "container1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "container1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "container1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: referencing a single matching pod 06/22/23 11:51:53.446
STEP: referencing matching pods with named port 06/22/23 11:51:58.457
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 06/22/23 11:52:03.471
STEP: recreating EndpointSlices after they've been deleted 06/22/23 11:52:08.482
Jun 22 11:52:08.507: INFO: EndpointSlice for Service endpointslice-408/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jun 22 11:52:18.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-408" for this suite. 06/22/23 11:52:18.534
------------------------------
• [SLOW TEST] [30.219 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:51:48.323
    Jun 22 11:51:48.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename endpointslice 06/22/23 11:51:48.324
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:51:48.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:51:48.351
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    W0622 11:51:48.366843      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "container1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "container1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "container1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "container1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0622 11:51:48.374240      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "container1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "container1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "container1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "container1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: referencing a single matching pod 06/22/23 11:51:53.446
    STEP: referencing matching pods with named port 06/22/23 11:51:58.457
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 06/22/23 11:52:03.471
    STEP: recreating EndpointSlices after they've been deleted 06/22/23 11:52:08.482
    Jun 22 11:52:08.507: INFO: EndpointSlice for Service endpointslice-408/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:52:18.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-408" for this suite. 06/22/23 11:52:18.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:52:18.543
Jun 22 11:52:18.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename projected 06/22/23 11:52:18.544
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:52:18.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:52:18.567
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-da035612-65e8-4147-ac9d-9fd3ebbe554e 06/22/23 11:52:18.578
STEP: Creating configMap with name cm-test-opt-upd-38d430d6-15be-496d-bc9e-7f87eed9f475 06/22/23 11:52:18.582
STEP: Creating the pod 06/22/23 11:52:18.587
W0622 11:52:18.599748      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:52:18.599: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5875442c-60ba-45ea-a1cf-e42d2a6aa9ba" in namespace "projected-4345" to be "running and ready"
Jun 22 11:52:18.603: INFO: Pod "pod-projected-configmaps-5875442c-60ba-45ea-a1cf-e42d2a6aa9ba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.582693ms
Jun 22 11:52:18.603: INFO: The phase of Pod pod-projected-configmaps-5875442c-60ba-45ea-a1cf-e42d2a6aa9ba is Pending, waiting for it to be Running (with Ready = true)
Jun 22 11:52:20.609: INFO: Pod "pod-projected-configmaps-5875442c-60ba-45ea-a1cf-e42d2a6aa9ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.009663818s
Jun 22 11:52:20.609: INFO: The phase of Pod pod-projected-configmaps-5875442c-60ba-45ea-a1cf-e42d2a6aa9ba is Running (Ready = true)
Jun 22 11:52:20.609: INFO: Pod "pod-projected-configmaps-5875442c-60ba-45ea-a1cf-e42d2a6aa9ba" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-da035612-65e8-4147-ac9d-9fd3ebbe554e 06/22/23 11:52:20.651
STEP: Updating configmap cm-test-opt-upd-38d430d6-15be-496d-bc9e-7f87eed9f475 06/22/23 11:52:20.661
STEP: Creating configMap with name cm-test-opt-create-0d9555bc-ecb6-4673-b5ee-de516d6bf99b 06/22/23 11:52:20.667
STEP: waiting to observe update in volume 06/22/23 11:52:20.672
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jun 22 11:52:24.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4345" for this suite. 06/22/23 11:52:24.724
------------------------------
• [SLOW TEST] [6.191 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:52:18.543
    Jun 22 11:52:18.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename projected 06/22/23 11:52:18.544
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:52:18.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:52:18.567
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-da035612-65e8-4147-ac9d-9fd3ebbe554e 06/22/23 11:52:18.578
    STEP: Creating configMap with name cm-test-opt-upd-38d430d6-15be-496d-bc9e-7f87eed9f475 06/22/23 11:52:18.582
    STEP: Creating the pod 06/22/23 11:52:18.587
    W0622 11:52:18.599748      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "delcm-volume-test", "updcm-volume-test", "createcm-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:52:18.599: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5875442c-60ba-45ea-a1cf-e42d2a6aa9ba" in namespace "projected-4345" to be "running and ready"
    Jun 22 11:52:18.603: INFO: Pod "pod-projected-configmaps-5875442c-60ba-45ea-a1cf-e42d2a6aa9ba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.582693ms
    Jun 22 11:52:18.603: INFO: The phase of Pod pod-projected-configmaps-5875442c-60ba-45ea-a1cf-e42d2a6aa9ba is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 11:52:20.609: INFO: Pod "pod-projected-configmaps-5875442c-60ba-45ea-a1cf-e42d2a6aa9ba": Phase="Running", Reason="", readiness=true. Elapsed: 2.009663818s
    Jun 22 11:52:20.609: INFO: The phase of Pod pod-projected-configmaps-5875442c-60ba-45ea-a1cf-e42d2a6aa9ba is Running (Ready = true)
    Jun 22 11:52:20.609: INFO: Pod "pod-projected-configmaps-5875442c-60ba-45ea-a1cf-e42d2a6aa9ba" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-da035612-65e8-4147-ac9d-9fd3ebbe554e 06/22/23 11:52:20.651
    STEP: Updating configmap cm-test-opt-upd-38d430d6-15be-496d-bc9e-7f87eed9f475 06/22/23 11:52:20.661
    STEP: Creating configMap with name cm-test-opt-create-0d9555bc-ecb6-4673-b5ee-de516d6bf99b 06/22/23 11:52:20.667
    STEP: waiting to observe update in volume 06/22/23 11:52:20.672
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:52:24.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4345" for this suite. 06/22/23 11:52:24.724
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:52:24.735
Jun 22 11:52:24.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename resourcequota 06/22/23 11:52:24.736
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:52:24.755
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:52:24.759
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 06/22/23 11:52:24.764
STEP: Ensuring ResourceQuota status is calculated 06/22/23 11:52:24.769
STEP: Creating a ResourceQuota with not terminating scope 06/22/23 11:52:26.776
STEP: Ensuring ResourceQuota status is calculated 06/22/23 11:52:26.782
STEP: Creating a long running pod 06/22/23 11:52:28.788
W0622 11:52:28.803468      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring resource quota with not terminating scope captures the pod usage 06/22/23 11:52:28.803
STEP: Ensuring resource quota with terminating scope ignored the pod usage 06/22/23 11:52:30.809
STEP: Deleting the pod 06/22/23 11:52:32.815
STEP: Ensuring resource quota status released the pod usage 06/22/23 11:52:32.825
STEP: Creating a terminating pod 06/22/23 11:52:34.831
W0622 11:52:34.846041      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring resource quota with terminating scope captures the pod usage 06/22/23 11:52:34.846
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 06/22/23 11:52:36.852
STEP: Deleting the pod 06/22/23 11:52:38.858
STEP: Ensuring resource quota status released the pod usage 06/22/23 11:52:38.87
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jun 22 11:52:40.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8571" for this suite. 06/22/23 11:52:40.881
------------------------------
• [SLOW TEST] [16.154 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:52:24.735
    Jun 22 11:52:24.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename resourcequota 06/22/23 11:52:24.736
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:52:24.755
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:52:24.759
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 06/22/23 11:52:24.764
    STEP: Ensuring ResourceQuota status is calculated 06/22/23 11:52:24.769
    STEP: Creating a ResourceQuota with not terminating scope 06/22/23 11:52:26.776
    STEP: Ensuring ResourceQuota status is calculated 06/22/23 11:52:26.782
    STEP: Creating a long running pod 06/22/23 11:52:28.788
    W0622 11:52:28.803468      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 06/22/23 11:52:28.803
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 06/22/23 11:52:30.809
    STEP: Deleting the pod 06/22/23 11:52:32.815
    STEP: Ensuring resource quota status released the pod usage 06/22/23 11:52:32.825
    STEP: Creating a terminating pod 06/22/23 11:52:34.831
    W0622 11:52:34.846041      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "pause" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pause" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pause" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pause" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring resource quota with terminating scope captures the pod usage 06/22/23 11:52:34.846
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 06/22/23 11:52:36.852
    STEP: Deleting the pod 06/22/23 11:52:38.858
    STEP: Ensuring resource quota status released the pod usage 06/22/23 11:52:38.87
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:52:40.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8571" for this suite. 06/22/23 11:52:40.881
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:52:40.89
Jun 22 11:52:40.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename sched-preemption 06/22/23 11:52:40.891
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:52:40.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:52:40.913
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Jun 22 11:52:40.932: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 22 11:53:40.995: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:53:41
Jun 22 11:53:41.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename sched-preemption-path 06/22/23 11:53:41
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:53:41.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:53:41.025
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Jun 22 11:53:41.046: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jun 22 11:53:41.051: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Jun 22 11:53:41.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jun 22 11:53:41.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-3907" for this suite. 06/22/23 11:53:41.152
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-4861" for this suite. 06/22/23 11:53:41.16
------------------------------
• [SLOW TEST] [60.278 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:52:40.89
    Jun 22 11:52:40.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename sched-preemption 06/22/23 11:52:40.891
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:52:40.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:52:40.913
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Jun 22 11:52:40.932: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 22 11:53:40.995: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:53:41
    Jun 22 11:53:41.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename sched-preemption-path 06/22/23 11:53:41
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:53:41.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:53:41.025
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Jun 22 11:53:41.046: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jun 22 11:53:41.051: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:53:41.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:53:41.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-3907" for this suite. 06/22/23 11:53:41.152
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-4861" for this suite. 06/22/23 11:53:41.16
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:53:41.168
Jun 22 11:53:41.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename gc 06/22/23 11:53:41.169
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:53:41.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:53:41.193
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
W0622 11:53:41.208761      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0622 11:53:41.217984      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0622 11:53:41.227191      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:53:41.248: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"51caff23-4064-4534-b04b-77f7aa74d1eb", Controller:(*bool)(0xc00540c1ca), BlockOwnerDeletion:(*bool)(0xc00540c1cb)}}
Jun 22 11:53:41.266: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"ac05cc7e-e664-44b1-a0ef-7abcb7e9a54b", Controller:(*bool)(0xc00540c5ea), BlockOwnerDeletion:(*bool)(0xc00540c5eb)}}
Jun 22 11:53:41.278: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"74bfddf8-9998-4cca-89b0-943e654cabc9", Controller:(*bool)(0xc0007ce59a), BlockOwnerDeletion:(*bool)(0xc0007ce59b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jun 22 11:53:46.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5926" for this suite. 06/22/23 11:53:46.3
------------------------------
• [SLOW TEST] [5.138 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:53:41.168
    Jun 22 11:53:41.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename gc 06/22/23 11:53:41.169
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:53:41.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:53:41.193
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    W0622 11:53:41.208761      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0622 11:53:41.217984      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0622 11:53:41.227191      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:53:41.248: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"51caff23-4064-4534-b04b-77f7aa74d1eb", Controller:(*bool)(0xc00540c1ca), BlockOwnerDeletion:(*bool)(0xc00540c1cb)}}
    Jun 22 11:53:41.266: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"ac05cc7e-e664-44b1-a0ef-7abcb7e9a54b", Controller:(*bool)(0xc00540c5ea), BlockOwnerDeletion:(*bool)(0xc00540c5eb)}}
    Jun 22 11:53:41.278: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"74bfddf8-9998-4cca-89b0-943e654cabc9", Controller:(*bool)(0xc0007ce59a), BlockOwnerDeletion:(*bool)(0xc0007ce59b)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:53:46.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5926" for this suite. 06/22/23 11:53:46.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:53:46.307
Jun 22 11:53:46.307: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename configmap 06/22/23 11:53:46.308
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:53:46.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:53:46.331
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-1369/configmap-test-a38aacac-c063-4366-8f55-e19478aebb3b 06/22/23 11:53:46.335
STEP: Creating a pod to test consume configMaps 06/22/23 11:53:46.341
W0622 11:53:46.352762      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "env-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "env-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "env-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "env-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:53:46.352: INFO: Waiting up to 5m0s for pod "pod-configmaps-f66dfbee-cb20-436e-add7-e97fc542f5a5" in namespace "configmap-1369" to be "Succeeded or Failed"
Jun 22 11:53:46.356: INFO: Pod "pod-configmaps-f66dfbee-cb20-436e-add7-e97fc542f5a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.460941ms
Jun 22 11:53:48.361: INFO: Pod "pod-configmaps-f66dfbee-cb20-436e-add7-e97fc542f5a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009080469s
Jun 22 11:53:50.361: INFO: Pod "pod-configmaps-f66dfbee-cb20-436e-add7-e97fc542f5a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008858758s
STEP: Saw pod success 06/22/23 11:53:50.361
Jun 22 11:53:50.362: INFO: Pod "pod-configmaps-f66dfbee-cb20-436e-add7-e97fc542f5a5" satisfied condition "Succeeded or Failed"
Jun 22 11:53:50.365: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-configmaps-f66dfbee-cb20-436e-add7-e97fc542f5a5 container env-test: <nil>
STEP: delete the pod 06/22/23 11:53:50.384
Jun 22 11:53:50.398: INFO: Waiting for pod pod-configmaps-f66dfbee-cb20-436e-add7-e97fc542f5a5 to disappear
Jun 22 11:53:50.402: INFO: Pod pod-configmaps-f66dfbee-cb20-436e-add7-e97fc542f5a5 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun 22 11:53:50.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1369" for this suite. 06/22/23 11:53:50.408
------------------------------
• [4.108 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:53:46.307
    Jun 22 11:53:46.307: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename configmap 06/22/23 11:53:46.308
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:53:46.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:53:46.331
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-1369/configmap-test-a38aacac-c063-4366-8f55-e19478aebb3b 06/22/23 11:53:46.335
    STEP: Creating a pod to test consume configMaps 06/22/23 11:53:46.341
    W0622 11:53:46.352762      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "env-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "env-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "env-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "env-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:53:46.352: INFO: Waiting up to 5m0s for pod "pod-configmaps-f66dfbee-cb20-436e-add7-e97fc542f5a5" in namespace "configmap-1369" to be "Succeeded or Failed"
    Jun 22 11:53:46.356: INFO: Pod "pod-configmaps-f66dfbee-cb20-436e-add7-e97fc542f5a5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.460941ms
    Jun 22 11:53:48.361: INFO: Pod "pod-configmaps-f66dfbee-cb20-436e-add7-e97fc542f5a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009080469s
    Jun 22 11:53:50.361: INFO: Pod "pod-configmaps-f66dfbee-cb20-436e-add7-e97fc542f5a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008858758s
    STEP: Saw pod success 06/22/23 11:53:50.361
    Jun 22 11:53:50.362: INFO: Pod "pod-configmaps-f66dfbee-cb20-436e-add7-e97fc542f5a5" satisfied condition "Succeeded or Failed"
    Jun 22 11:53:50.365: INFO: Trying to get logs from node wl-antrea-md-1-pw9cx-6fb544549dxbhg8p-stnrw pod pod-configmaps-f66dfbee-cb20-436e-add7-e97fc542f5a5 container env-test: <nil>
    STEP: delete the pod 06/22/23 11:53:50.384
    Jun 22 11:53:50.398: INFO: Waiting for pod pod-configmaps-f66dfbee-cb20-436e-add7-e97fc542f5a5 to disappear
    Jun 22 11:53:50.402: INFO: Pod pod-configmaps-f66dfbee-cb20-436e-add7-e97fc542f5a5 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:53:50.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1369" for this suite. 06/22/23 11:53:50.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:53:50.416
Jun 22 11:53:50.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename disruption 06/22/23 11:53:50.417
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:53:50.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:53:50.439
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:53:50.443
Jun 22 11:53:50.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename disruption-2 06/22/23 11:53:50.444
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:53:50.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:53:50.467
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 06/22/23 11:53:50.479
STEP: Waiting for the pdb to be processed 06/22/23 11:53:52.495
STEP: Waiting for the pdb to be processed 06/22/23 11:53:54.512
STEP: listing a collection of PDBs across all namespaces 06/22/23 11:53:56.522
STEP: listing a collection of PDBs in namespace disruption-5725 06/22/23 11:53:56.527
STEP: deleting a collection of PDBs 06/22/23 11:53:56.532
STEP: Waiting for the PDB collection to be deleted 06/22/23 11:53:56.545
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Jun 22 11:53:56.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jun 22 11:53:56.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-7964" for this suite. 06/22/23 11:53:56.561
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5725" for this suite. 06/22/23 11:53:56.568
------------------------------
• [SLOW TEST] [6.158 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:53:50.416
    Jun 22 11:53:50.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename disruption 06/22/23 11:53:50.417
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:53:50.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:53:50.439
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:53:50.443
    Jun 22 11:53:50.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename disruption-2 06/22/23 11:53:50.444
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:53:50.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:53:50.467
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 06/22/23 11:53:50.479
    STEP: Waiting for the pdb to be processed 06/22/23 11:53:52.495
    STEP: Waiting for the pdb to be processed 06/22/23 11:53:54.512
    STEP: listing a collection of PDBs across all namespaces 06/22/23 11:53:56.522
    STEP: listing a collection of PDBs in namespace disruption-5725 06/22/23 11:53:56.527
    STEP: deleting a collection of PDBs 06/22/23 11:53:56.532
    STEP: Waiting for the PDB collection to be deleted 06/22/23 11:53:56.545
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:53:56.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:53:56.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-7964" for this suite. 06/22/23 11:53:56.561
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5725" for this suite. 06/22/23 11:53:56.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:53:56.588
Jun 22 11:53:56.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename configmap 06/22/23 11:53:56.59
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:53:56.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:53:56.613
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jun 22 11:53:56.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8376" for this suite. 06/22/23 11:53:56.664
------------------------------
• [0.085 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:53:56.588
    Jun 22 11:53:56.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename configmap 06/22/23 11:53:56.59
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:53:56.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:53:56.613
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:53:56.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8376" for this suite. 06/22/23 11:53:56.664
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:53:56.675
Jun 22 11:53:56.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename services 06/22/23 11:53:56.676
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:53:56.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:53:56.698
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-9462 06/22/23 11:53:56.702
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9462 to expose endpoints map[] 06/22/23 11:53:56.718
Jun 22 11:53:56.723: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jun 22 11:53:57.735: INFO: successfully validated that service endpoint-test2 in namespace services-9462 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9462 06/22/23 11:53:57.735
W0622 11:53:57.744552      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:53:57.744: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9462" to be "running and ready"
Jun 22 11:53:57.748: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.908896ms
Jun 22 11:53:57.748: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 11:53:59.756: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012289835s
Jun 22 11:53:59.757: INFO: The phase of Pod pod1 is Running (Ready = true)
Jun 22 11:53:59.757: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9462 to expose endpoints map[pod1:[80]] 06/22/23 11:53:59.761
Jun 22 11:53:59.775: INFO: successfully validated that service endpoint-test2 in namespace services-9462 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 06/22/23 11:53:59.775
Jun 22 11:53:59.775: INFO: Creating new exec pod
W0622 11:53:59.782723      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:53:59.782: INFO: Waiting up to 5m0s for pod "execpod9v7cq" in namespace "services-9462" to be "running"
Jun 22 11:53:59.787: INFO: Pod "execpod9v7cq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.917572ms
Jun 22 11:54:01.794: INFO: Pod "execpod9v7cq": Phase="Running", Reason="", readiness=true. Elapsed: 2.011460484s
Jun 22 11:54:01.794: INFO: Pod "execpod9v7cq" satisfied condition "running"
Jun 22 11:54:02.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-9462 exec execpod9v7cq -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jun 22 11:54:03.009: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun 22 11:54:03.009: INFO: stdout: ""
Jun 22 11:54:03.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-9462 exec execpod9v7cq -- /bin/sh -x -c nc -v -z -w 2 100.70.138.243 80'
Jun 22 11:54:03.185: INFO: stderr: "+ nc -v -z -w 2 100.70.138.243 80\nConnection to 100.70.138.243 80 port [tcp/http] succeeded!\n"
Jun 22 11:54:03.185: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-9462 06/22/23 11:54:03.185
W0622 11:54:03.194505      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jun 22 11:54:03.194: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9462" to be "running and ready"
Jun 22 11:54:03.198: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.268828ms
Jun 22 11:54:03.198: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 22 11:54:05.205: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010420367s
Jun 22 11:54:05.205: INFO: The phase of Pod pod2 is Running (Ready = true)
Jun 22 11:54:05.205: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9462 to expose endpoints map[pod1:[80] pod2:[80]] 06/22/23 11:54:05.209
Jun 22 11:54:05.225: INFO: successfully validated that service endpoint-test2 in namespace services-9462 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 06/22/23 11:54:05.225
Jun 22 11:54:06.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-9462 exec execpod9v7cq -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jun 22 11:54:06.429: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun 22 11:54:06.429: INFO: stdout: ""
Jun 22 11:54:06.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-9462 exec execpod9v7cq -- /bin/sh -x -c nc -v -z -w 2 100.70.138.243 80'
Jun 22 11:54:06.627: INFO: stderr: "+ nc -v -z -w 2 100.70.138.243 80\nConnection to 100.70.138.243 80 port [tcp/http] succeeded!\n"
Jun 22 11:54:06.627: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-9462 06/22/23 11:54:06.627
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9462 to expose endpoints map[pod2:[80]] 06/22/23 11:54:06.644
Jun 22 11:54:06.656: INFO: successfully validated that service endpoint-test2 in namespace services-9462 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 06/22/23 11:54:06.656
Jun 22 11:54:07.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-9462 exec execpod9v7cq -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jun 22 11:54:07.955: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun 22 11:54:07.955: INFO: stdout: ""
Jun 22 11:54:07.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-9462 exec execpod9v7cq -- /bin/sh -x -c nc -v -z -w 2 100.70.138.243 80'
Jun 22 11:54:08.131: INFO: stderr: "+ nc -v -z -w 2 100.70.138.243 80\nConnection to 100.70.138.243 80 port [tcp/http] succeeded!\n"
Jun 22 11:54:08.131: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-9462 06/22/23 11:54:08.131
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9462 to expose endpoints map[] 06/22/23 11:54:08.148
Jun 22 11:54:09.171: INFO: successfully validated that service endpoint-test2 in namespace services-9462 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jun 22 11:54:09.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9462" for this suite. 06/22/23 11:54:09.2
------------------------------
• [SLOW TEST] [12.532 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:53:56.675
    Jun 22 11:53:56.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename services 06/22/23 11:53:56.676
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:53:56.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:53:56.698
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-9462 06/22/23 11:53:56.702
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9462 to expose endpoints map[] 06/22/23 11:53:56.718
    Jun 22 11:53:56.723: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jun 22 11:53:57.735: INFO: successfully validated that service endpoint-test2 in namespace services-9462 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9462 06/22/23 11:53:57.735
    W0622 11:53:57.744552      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:53:57.744: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9462" to be "running and ready"
    Jun 22 11:53:57.748: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.908896ms
    Jun 22 11:53:57.748: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 11:53:59.756: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.012289835s
    Jun 22 11:53:59.757: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jun 22 11:53:59.757: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9462 to expose endpoints map[pod1:[80]] 06/22/23 11:53:59.761
    Jun 22 11:53:59.775: INFO: successfully validated that service endpoint-test2 in namespace services-9462 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 06/22/23 11:53:59.775
    Jun 22 11:53:59.775: INFO: Creating new exec pod
    W0622 11:53:59.782723      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:53:59.782: INFO: Waiting up to 5m0s for pod "execpod9v7cq" in namespace "services-9462" to be "running"
    Jun 22 11:53:59.787: INFO: Pod "execpod9v7cq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.917572ms
    Jun 22 11:54:01.794: INFO: Pod "execpod9v7cq": Phase="Running", Reason="", readiness=true. Elapsed: 2.011460484s
    Jun 22 11:54:01.794: INFO: Pod "execpod9v7cq" satisfied condition "running"
    Jun 22 11:54:02.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-9462 exec execpod9v7cq -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jun 22 11:54:03.009: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jun 22 11:54:03.009: INFO: stdout: ""
    Jun 22 11:54:03.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-9462 exec execpod9v7cq -- /bin/sh -x -c nc -v -z -w 2 100.70.138.243 80'
    Jun 22 11:54:03.185: INFO: stderr: "+ nc -v -z -w 2 100.70.138.243 80\nConnection to 100.70.138.243 80 port [tcp/http] succeeded!\n"
    Jun 22 11:54:03.185: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-9462 06/22/23 11:54:03.185
    W0622 11:54:03.194505      23 warnings.go:70] would violate PodSecurity "restricted:v1.24": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jun 22 11:54:03.194: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9462" to be "running and ready"
    Jun 22 11:54:03.198: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.268828ms
    Jun 22 11:54:03.198: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun 22 11:54:05.205: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010420367s
    Jun 22 11:54:05.205: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jun 22 11:54:05.205: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9462 to expose endpoints map[pod1:[80] pod2:[80]] 06/22/23 11:54:05.209
    Jun 22 11:54:05.225: INFO: successfully validated that service endpoint-test2 in namespace services-9462 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 06/22/23 11:54:05.225
    Jun 22 11:54:06.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-9462 exec execpod9v7cq -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jun 22 11:54:06.429: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jun 22 11:54:06.429: INFO: stdout: ""
    Jun 22 11:54:06.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-9462 exec execpod9v7cq -- /bin/sh -x -c nc -v -z -w 2 100.70.138.243 80'
    Jun 22 11:54:06.627: INFO: stderr: "+ nc -v -z -w 2 100.70.138.243 80\nConnection to 100.70.138.243 80 port [tcp/http] succeeded!\n"
    Jun 22 11:54:06.627: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-9462 06/22/23 11:54:06.627
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9462 to expose endpoints map[pod2:[80]] 06/22/23 11:54:06.644
    Jun 22 11:54:06.656: INFO: successfully validated that service endpoint-test2 in namespace services-9462 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 06/22/23 11:54:06.656
    Jun 22 11:54:07.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-9462 exec execpod9v7cq -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jun 22 11:54:07.955: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jun 22 11:54:07.955: INFO: stdout: ""
    Jun 22 11:54:07.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2746018265 --namespace=services-9462 exec execpod9v7cq -- /bin/sh -x -c nc -v -z -w 2 100.70.138.243 80'
    Jun 22 11:54:08.131: INFO: stderr: "+ nc -v -z -w 2 100.70.138.243 80\nConnection to 100.70.138.243 80 port [tcp/http] succeeded!\n"
    Jun 22 11:54:08.131: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-9462 06/22/23 11:54:08.131
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9462 to expose endpoints map[] 06/22/23 11:54:08.148
    Jun 22 11:54:09.171: INFO: successfully validated that service endpoint-test2 in namespace services-9462 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:54:09.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9462" for this suite. 06/22/23 11:54:09.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 06/22/23 11:54:09.208
Jun 22 11:54:09.208: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
STEP: Building a namespace api object, basename endpointslice 06/22/23 11:54:09.209
STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:54:09.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:54:09.248
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jun 22 11:54:11.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3710" for this suite. 06/22/23 11:54:11.317
------------------------------
• [2.115 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 06/22/23 11:54:09.208
    Jun 22 11:54:09.208: INFO: >>> kubeConfig: /tmp/kubeconfig-2746018265
    STEP: Building a namespace api object, basename endpointslice 06/22/23 11:54:09.209
    STEP: Waiting for a default service account to be provisioned in namespace 06/22/23 11:54:09.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/22/23 11:54:09.248
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jun 22 11:54:11.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3710" for this suite. 06/22/23 11:54:11.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Jun 22 11:54:11.330: INFO: Running AfterSuite actions on node 1
Jun 22 11:54:11.330: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Jun 22 11:54:11.330: INFO: Running AfterSuite actions on node 1
    Jun 22 11:54:11.330: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.115 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5666.970 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h34m27.822713846s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

